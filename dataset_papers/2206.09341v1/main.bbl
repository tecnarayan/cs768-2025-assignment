\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal \& Goyal(2012)Agrawal and Goyal]{COLT12_agrawal2012analysis}
Agrawal, S. and Goyal, N.
\newblock Analysis of {Thompson} sampling for the multi-armed bandit problem.
\newblock In \emph{Proc. {COLT}}, pp.\  39.1--39.26, 2012.

\bibitem[Agrawal \& Goyal(2013{\natexlab{a}})Agrawal and
  Goyal]{AISTATS13_agrawal2013further}
Agrawal, S. and Goyal, N.
\newblock Further optimal regret bounds for {Thompson} sampling.
\newblock In \emph{Proc. {AISTATS}}, pp.\  99--107, 2013{\natexlab{a}}.

\bibitem[Agrawal \& Goyal(2013{\natexlab{b}})Agrawal and
  Goyal]{ICML13_agrawal2013thompson}
Agrawal, S. and Goyal, N.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{Proc. {ICML}}, pp.\  127--135, 2013{\natexlab{b}}.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{ML02_auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine Learning}, pp.\  235--256, 2002.

\bibitem[Balakrishnan et~al.(2020)Balakrishnan, Nguyen, Low, and Soh]{bala20}
Balakrishnan, S., Nguyen, Q.~P., Low, B. K.~H., and Soh, H.
\newblock Efficient exploration of reward functions in inverse reinforcement
  learning via {Bayesian} optimization.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  4187--4198, 2020.

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson,
  and Bakshy]{NeurIPS20_balandat2020botorch}
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.~G.,
  and Bakshy, E.
\newblock {BoTorch: a framework for efficient Monte-Carlo Bayesian
  optimization}.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  21524--21538, 2020.

\bibitem[Besson \& Kaufmann(2018)Besson and
  Kaufmann]{Arxiv18_besson2018doubling}
Besson, L. and Kaufmann, E.
\newblock What doubling tricks can and can't do for multi-armed bandits.
\newblock {arXiv preprint arXiv}:1803.06971, 2018.

\bibitem[Chapelle(2014)]{KDD14_chapelle2014modeling}
Chapelle, O.
\newblock Modeling delayed feedback in display advertising.
\newblock In \emph{Proc. {KDD}}, pp.\  1097--1105, 2014.

\bibitem[Chapelle \& Li(2011)Chapelle and Li]{NIPS11_chapelle2011empirical}
Chapelle, O. and Li, L.
\newblock An empirical evaluation of {Thompson} sampling.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  2249--2257, 2011.

\bibitem[Chen et~al.(2012)Chen, Low, Tan, Oran, Jaillet, Dolan, and
  Sukhatme]{LowUAI12}
Chen, J., Low, K.~H., Tan, C. K.-Y., Oran, A., Jaillet, P., Dolan, J.~M., and
  Sukhatme, G.~S.
\newblock Decentralized data fusion and active sensing with mobile sensors for
  modeling and predicting spatiotemporal traffic phenomena.
\newblock In \emph{Proc. UAI}, pp.\  163--173, 2012.

\bibitem[Chen et~al.(2013{\natexlab{a}})Chen, Cao, Low, Ouyang, Tan, and
  Jaillet]{Chen13}
Chen, J., Cao, N., Low, K.~H., Ouyang, R., Tan, C. K.-Y., and Jaillet, P.
\newblock Parallel {Gaussian} process regression with low-rank covariance
  matrix approximations.
\newblock In \emph{Proc. UAI}, pp.\  152--161, 2013{\natexlab{a}}.

\bibitem[Chen et~al.(2013{\natexlab{b}})Chen, Low, and Tan]{LowRSS13}
Chen, J., Low, K.~H., and Tan, C. K.-Y.
\newblock {Gaussian} process-based decentralized data fusion and active sensing
  for mobility-on-demand system.
\newblock In \emph{Proc. RSS}, 2013{\natexlab{b}}.

\bibitem[Chen et~al.(2015)Chen, Low, Jaillet, and Yao]{LowTASE15}
Chen, J., Low, K.~H., Jaillet, P., and Yao, Y.
\newblock Gaussian process decentralized data fusion and active sensing for
  spatiotemporal traffic modeling and prediction in mobility-on-demand systems.
\newblock \emph{{IEEE} Trans. Autom. Sci. Eng.}, 12:\penalty0 901--921, 2015.

\bibitem[Chow \& Chang(2006)Chow and Chang]{Book_chow2006adaptive}
Chow, S.-C. and Chang, M.
\newblock \emph{Adaptive Design Methods in Clinical Trials}.
\newblock CRC Press, 2006.

\bibitem[Chowdhury \& Gopalan(2017)Chowdhury and
  Gopalan]{chowdhury2017kernelized}
Chowdhury, S.~R. and Gopalan, A.
\newblock On kernelized multi-armed bandits.
\newblock In \emph{Proc. {ICML}}, pp.\  844--853, 2017.

\bibitem[Chowdhury \& Gopalan(2019)Chowdhury and Gopalan]{chowdhury2019batch}
Chowdhury, S.~R. and Gopalan, A.
\newblock On batch {Bayesian} optimization.
\newblock {arXiv preprint arXiv}:1911.01032, 2019.

\bibitem[Chu et~al.(2011)Chu, Li, Reyzin, and
  Schapire]{AISTATS11_chu2011contextual}
Chu, W., Li, L., Reyzin, L., and Schapire, R.~E.
\newblock Contextual bandits with linear payoff functions.
\newblock In \emph{Proc. {AISTATS}}, pp.\  208--214, 2011.

\bibitem[Contal et~al.(2013)Contal, Buffoni, Robicquet, and
  Vayatis]{contal2013parallel}
Contal, E., Buffoni, D., Robicquet, A., and Vayatis, N.
\newblock Parallel {Gaussian} process optimization with upper confidence bound
  and pure exploration.
\newblock In \emph{Proc. {ECML/PKDD}}, pp.\  225--240, 2013.

\bibitem[Dai et~al.(2019)Dai, Yu, Low, and Jaillet]{dai2019}
Dai, Z., Yu, H., Low, B. K.~H., and Jaillet, P.
\newblock Bayesian optimization meets {Bayesian} optimal stopping.
\newblock In \emph{Proc. {ICML}}, pp.\  1496--1506, 2019.

\bibitem[Dai et~al.(2020{\natexlab{a}})Dai, Chen, Low, Jaillet, and
  Ho]{dai2020}
Dai, Z., Chen, Y., Low, B. K.~H., Jaillet, P., and Ho, T.-H.
\newblock {R2-B2}: Recursive reasoning-based {Bayesian} optimization for
  no-regret learning in games.
\newblock In \emph{Proc. {ICML}}, pp.\  2291--2301, 2020{\natexlab{a}}.

\bibitem[Dai et~al.(2020{\natexlab{b}})Dai, Low, and Jaillet]{dai2020federated}
Dai, Z., Low, B. K.~H., and Jaillet, P.
\newblock Federated {Bayesian} optimization via {Thompson} sampling.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  9687--9699, 2020{\natexlab{b}}.

\bibitem[Dai et~al.(2021)Dai, Low, and Jaillet]{dai2021differentially}
Dai, Z., Low, B. K.~H., and Jaillet, P.
\newblock Differentially private federated {Bayesian} optimization with
  distributed exploration.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  9125--9139, 2021.

\bibitem[Dai et~al.(2022)Dai, Chen, Yu, Low, and Jaillet]{metaBO}
Dai, Z., Chen, Y., Yu, H., Low, B. K.~H., and Jaillet, P.
\newblock On provably robust meta-{Bayesian} optimization.
\newblock In \emph{Proc. {UAI}}, 2022.

\bibitem[Daxberger \& Low(2017)Daxberger and Low]{daxberger2017distributed}
Daxberger, E.~A. and Low, B. K.~H.
\newblock Distributed batch {Gaussian} process optimization.
\newblock In \emph{Proc. {ICML}}, pp.\  951--960, 2017.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{desautels2014parallelizing}
Desautels, T., Krause, A., and Burdick, J.~W.
\newblock Parallelizing exploration-exploitation tradeoffs in {Gaussian}
  process bandit optimization.
\newblock \emph{JMLR}, 15:\penalty0 3873--3923, 2014.

\bibitem[Diemert et~al.(2017)Diemert, Meynet, Galland, and
  Lefortier]{ADKDD17_diemert2017attribution}
Diemert, E., Meynet, J., Galland, P., and Lefortier, D.
\newblock Attribution modeling increases efficiency of bidding in display
  advertising.
\newblock In \emph{Proc. {KDD} Workshop on AdKDD and TargetAd}, 2017.

\bibitem[Frazier(2018)]{Arxiv18_frazier2018tutorial}
Frazier, P.~I.
\newblock A tutorial on {Bayesian} optimization.
\newblock {arXiv preprint arXiv}:1807.02811, 2018.

\bibitem[Garivier \& Capp{\'e}(2011)Garivier and
  Capp{\'e}]{COLT11_garivier2011kl}
Garivier, A. and Capp{\'e}, O.
\newblock The {KL-UCB} algorithm for bounded stochastic bandits and beyond.
\newblock In \emph{Proc. {COLT}}, pp.\  359--376, 2011.

\bibitem[Garnett(2022)]{Book_garnett2021bayesian}
Garnett, R.
\newblock \emph{Bayesian Optimization}.
\newblock Cambridge Univ. Press, 2022.

\bibitem[Gong et~al.(2019)Gong, Peng, and Liu]{ICML19_gong2019quantile}
Gong, C., Peng, J., and Liu, Q.
\newblock Quantile {Stein} variational gradient descent for batch {Bayesian}
  optimization.
\newblock In \emph{Proc. {ICML}}, pp.\  2347--2356, 2019.

\bibitem[Gonz{\'a}lez et~al.(2016)Gonz{\'a}lez, Dai, Hennig, and
  Lawrence]{AIStats16_gonzalez2016batch}
Gonz{\'a}lez, J., Dai, Z., Hennig, P., and Lawrence, N.
\newblock Batch {Bayesian} optimization via local penalization.
\newblock In \emph{Proc. {AISTATS}}, pp.\  648--657, 2016.

\bibitem[Grover et~al.(2018)Grover, Markov, Attia, Jin, Perkins, Cheong, Chen,
  Yang, Harris, Chueh, and Ermon]{AISTATS18_grover2018best}
Grover, A., Markov, T., Attia, P., Jin, N., Perkins, N., Cheong, B., Chen, M.,
  Yang, Z., Harris, S., Chueh, W., and Ermon, S.
\newblock Best arm identification in multi-armed bandits with delayed feedback.
\newblock In \emph{Proc. {AISTATS}}, pp.\  833--842, 2018.

\bibitem[Hoang et~al.(2017)Hoang, Hoang, and Low]{MinhAAAI17}
Hoang, Q.~M., Hoang, T.~N., and Low, K.~H.
\newblock A generalized stochastic variational {Bayesian} hyperparameter
  learning framework for sparse spectrum {Gaussian} process regression.
\newblock In \emph{Proc. {AAAI}}, pp.\  2007--2014, 2017.

\bibitem[Hoang et~al.(2015)Hoang, Hoang, and Low]{NghiaICML16}
Hoang, T.~N., Hoang, Q.~M., and Low, K.~H.
\newblock A unifying framework of anytime sparse {Gaussian} process regression
  models with stochastic variational inference for big data.
\newblock In \emph{Proc. {ICML}}, pp.\  569--578, 2015.

\bibitem[Hoang et~al.(2016)Hoang, Hoang, and Low]{HoangICML16}
Hoang, T.~N., Hoang, Q.~M., and Low, K.~H.
\newblock A distributed variational inference framework for unifying parallel
  sparse {Gaussian} process regression models.
\newblock In \emph{Proc. ICML}, pp.\  382--391, 2016.

\bibitem[Hoang et~al.(2018)Hoang, Hoang, and Low]{NghiaAAAI18}
Hoang, T.~N., Hoang, Q.~M., and Low, B. K.~H.
\newblock Decentralized high-dimensional {Bayesian} optimization with factor
  graphs.
\newblock In \emph{Proc. {AAAI}}, pp.\  3231--3238, 2018.

\bibitem[Hoang et~al.(2019)Hoang, Hoang, Low, and How]{NghiaAAAI19}
Hoang, T.~N., Hoang, Q.~M., Low, K.~H., and How, J.~P.
\newblock Collective online learning of {Gaussian} processes in massive
  multi-agent systems.
\newblock In \emph{Proc. {AAAI}}, 2019.

\bibitem[Joulani et~al.(2013)Joulani, Gyorgy, and
  Szepesv{\'a}ri]{ICML13_joulani2013online}
Joulani, P., Gyorgy, A., and Szepesv{\'a}ri, C.
\newblock Online learning under delayed feedback.
\newblock In \emph{Proc. {ICML}}, pp.\  1453--1461, 2013.

\bibitem[Kandasamy et~al.(2018)Kandasamy, Krishnamurthy, Schneider, and
  P{\'o}czos]{kandasamy2018parallelised}
Kandasamy, K., Krishnamurthy, A., Schneider, J., and P{\'o}czos, B.
\newblock Parallelised {Bayesian} optimisation via {Thompson sampling}.
\newblock In \emph{Proc. {AISTATS}}, pp.\  133--142, 2018.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and
  Munos]{ALT12_kaufmann2012thompson}
Kaufmann, E., Korda, N., and Munos, R.
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \emph{Proc. {ALT}}, pp.\  199--213, 2012.

\bibitem[Kharkovskii et~al.(2020{\natexlab{a}})Kharkovskii, Dai, and
  Low]{dmitrii20b}
Kharkovskii, D., Dai, Z., and Low, B. K.~H.
\newblock Private outsourced {Bayesian} optimization.
\newblock In \emph{Proc. {ICML}}, pp.\  5231--5242, 2020{\natexlab{a}}.

\bibitem[Kharkovskii et~al.(2020{\natexlab{b}})Kharkovskii, Ling, and
  Low]{dmitrii20a}
Kharkovskii, D., Ling, C.~K., and Low, B. K.~H.
\newblock Nonmyopic {Gaussian} process optimization with macro-actions.
\newblock In \emph{Proc. AISTATS}, pp.\  4593--4604, 2020{\natexlab{b}}.

\bibitem[Krause \& Ong(2011)Krause and Ong]{NIPS11_krause2011contextual}
Krause, A. and Ong, C.~S.
\newblock Contextual {Gaussian} process bandit optimization.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  2447--2455, 2011.

\bibitem[Lattimore \& Szepesv\'ari(2020)Lattimore and
  Szepesv\'ari]{Book_lattimore2020bandit}
Lattimore, T. and Szepesv\'ari, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge Univ. Press, 2020.

\bibitem[Li et~al.(2019)Li, Chen, and Giannakis]{AIStats19_li2019bandit}
Li, B., Chen, T., and Giannakis, G.~B.
\newblock Bandit online learning with unknown delays.
\newblock In \emph{Proc. {AISTATS}}, pp.\  993--1002, 2019.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and
  Schapire]{WWW10_li2010contextual}
Li, L., Chu, W., Langford, J., and Schapire, R.~E.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \emph{Proc. {WWW}}, pp.\  661--670, 2010.

\bibitem[Li et~al.(2017)Li, Lu, and Zhou]{ICML17_li2017provably}
Li, L., Lu, Y., and Zhou, D.
\newblock Provably optimal algorithms for generalized linear contextual
  bandits.
\newblock In \emph{Proc. {ICML}}, pp.\  2071--2080, 2017.

\bibitem[Li \& Scarlett(2022)Li and Scarlett]{Arxiv21_li2021gaussian}
Li, Z. and Scarlett, J.
\newblock Gaussian process bandit optimization with few batches.
\newblock In \emph{Proc. {AISTATS}}, pp.\  92--107, 2022.

\bibitem[Ling et~al.(2016)Ling, Low, and Jaillet]{ling16}
Ling, C.~K., Low, B. K.~H., and Jaillet, P.
\newblock {Gaussian} process planning with {Lipschitz} continuous reward
  functions: Towards unifying {Bayesian} optimization, active learning, and
  beyond.
\newblock In \emph{Proc. {AAAI}}, pp.\  1860--1866, 2016.

\bibitem[Low et~al.(2014)Low, Xu, Chen, Lim, and {\"{O}zg\"{u}l}]{LowECML14a}
Low, K.~H., Xu, N., Chen, J., Lim, K.~K., and {\"{O}zg\"{u}l}, E.~B.
\newblock Generalized online sparse {Gaussian} processes with application to
  persistent mobile robot localization.
\newblock In \emph{Proc. {ECML/PKDD Nectar Track}}, pp.\  499--503, 2014.

\bibitem[Low et~al.(2015)Low, Yu, Chen, and Jaillet]{low15}
Low, K.~H., Yu, J., Chen, J., and Jaillet, P.
\newblock Parallel {Gaussian} process regression for big data: Low-rank
  representation meets {M}arkov approximation.
\newblock In \emph{Proc. {AAAI}}, pp.\  2821--2827, 2015.

\bibitem[Nguyen et~al.(2021{\natexlab{a}})Nguyen, Dai, Low, and
  Jaillet]{nguyen2021conditional}
Nguyen, Q.~P., Dai, Z., Low, B. K.~H., and Jaillet, P.
\newblock Optimizing conditional value-at-risk of black-box functions.
\newblock In \emph{Proc. NeurIPS}, pp.\  4170--4180, 2021{\natexlab{a}}.

\bibitem[Nguyen et~al.(2021{\natexlab{b}})Nguyen, Dai, Low, and
  Jaillet]{nguyen2021value}
Nguyen, Q.~P., Dai, Z., Low, B. K.~H., and Jaillet, P.
\newblock Value-at-risk optimization with {Gaussian} processes.
\newblock In \emph{Proc. ICML}, pp.\  8063--8072, 2021{\natexlab{b}}.

\bibitem[Nguyen et~al.(2021{\natexlab{c}})Nguyen, Low, and Jaillet]{LSE}
Nguyen, Q.~P., Low, B. K.~H., and Jaillet, P.
\newblock An information-theoretic framework for unifying active learning
  problems.
\newblock In \emph{Proc. {AAAI}}, pp.\  9126--9134, 2021{\natexlab{c}}.

\bibitem[Nguyen et~al.(2021{\natexlab{d}})Nguyen, Tay, Low, and Jaillet]{topk}
Nguyen, Q.~P., Tay, S., Low, B. K.~H., and Jaillet, P.
\newblock Top-$k$ ranking {Bayesian} optimization.
\newblock In \emph{Proc. {AAAI}}, pp.\  9135--9143, 2021{\natexlab{d}}.

\bibitem[Nguyen et~al.(2021{\natexlab{e}})Nguyen, Wu, Low, and Jaillet]{TMES}
Nguyen, Q.~P., Wu, Z., Low, B. K.~H., and Jaillet, P.
\newblock Trusted-maximizers entropy search for efﬁcient {Bayesian}
  optimization.
\newblock In \emph{Proc. {UAI}}, pp.\  1486--1495, 2021{\natexlab{e}}.

\bibitem[Ouyang \& Low(2018)Ouyang and Low]{Ruofei18}
Ouyang, R. and Low, K.~H.
\newblock Gaussian process decentralized data fusion meets transfer learning in
  large-scale distributed cooperative perception.
\newblock In \emph{Proc. AAAI}, pp.\  3876--3883, 2018.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and
  Williams]{Book_rasmussen2003gaussian}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{{Gaussian} Processes for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem[Shahriari et~al.(2015)Shahriari, Swersky, Wang, Adams, and
  De~Freitas]{IEEE15_shahriari2015taking}
Shahriari, B., Swersky, K., Wang, Z., Adams, R.~P., and De~Freitas, N.
\newblock Taking the human out of the loop: {A} review of {Bayesian}
  optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2015.

\bibitem[Shu et~al.(2022{\natexlab{a}})Shu, Cai, Dai, Ooi, and Low]{nasi}
Shu, Y., Cai, S., Dai, Z., Ooi, B.~C., and Low, B. K.~H.
\newblock {NASI}: Label- and data-agnostic neural architecture search at
  initialization.
\newblock In \emph{Proc. {ICLR}}, 2022{\natexlab{a}}.

\bibitem[Shu et~al.(2022{\natexlab{b}})Shu, Chen, Dai, and Low]{nes}
Shu, Y., Chen, Y., Dai, Z., and Low, B. K.~H.
\newblock Neural ensemble search via {Bayesian} sampling.
\newblock In \emph{Proc. {UAI}}, 2022{\natexlab{b}}.

\bibitem[Sim et~al.(2021)Sim, Zhang, Low, and Jaillet]{sim2021collaborative}
Sim, R. H.~L., Zhang, Y., Low, B. K.~H., and Jaillet, P.
\newblock Collaborative {Bayesian} optimization with fair regret.
\newblock In \emph{Proc. {ICML}}, pp.\  9691--9701, 2021.

\bibitem[Slivkins(2019)]{Book_slivkins2019introduction}
Slivkins, A.
\newblock Introduction to multi-armed bandits.
\newblock \emph{Foundations and Trends\textregistered\ in Machine Learning},
  12\penalty0 (1--2):\penalty0 1--286, 2019.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and
  Adams]{NIPS12_snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical {Bayesian} optimization of machine learning algorithms.
\newblock In \emph{Proc. {NeurIPS}}, 2012.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{ICML10_srinival2010gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{Proc. {ICML}}, pp.\  1015–1022, 2010.

\bibitem[Takahashi \& Suzuki(2021)Takahashi and
  Suzuki]{PS21_takahashi2021bayesian}
Takahashi, A. and Suzuki, T.
\newblock Bayesian optimization design for dose-finding based on toxicity and
  efficacy outcomes in phase {I/II} clinical trials.
\newblock \emph{Pharmaceutical Statistics}, 20\penalty0 (3):\penalty0 422--439,
  2021.

\bibitem[Tay et~al.(2022)Tay, Foo, Urano, Leong, and Low]{SebICML22}
Tay, S.~S., Foo, C.~S., Urano, D., Leong, R. C.~X., and Low, B. K.~H.
\newblock Efficient distributionally robust {Bayesian} optimization with
  worst-case sensitivity.
\newblock In \emph{Proc. ICML}, 2022.

\bibitem[Thompson(1933)]{JSTOR33_thompson1933likelihood}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3--4):\penalty0 285--294, 1933.

\bibitem[Ueno et~al.(2016)Ueno, Rhone, Hou, Mizoguchi, and
  Tsuda]{MD16_ueno2016combo}
Ueno, T., Rhone, T.~D., Hou, Z., Mizoguchi, T., and Tsuda, K.
\newblock Combo: {An} efficient {Bayesian} optimization library for materials
  science.
\newblock \emph{Materials Discovery}, 4:\penalty0 18--21, 2016.

\bibitem[Valko et~al.(2013)Valko, Korda, Munos, Flaounas, and
  Cristianini]{UAI13_valko2013finite}
Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock In \emph{Proc. {UAI}}, pp.\  654--663, 2013.

\bibitem[Verma \& Hanawal(2020)Verma and
  Hanawal]{INFOCOM20_verma2020stochastic}
Verma, A. and Hanawal, M.~K.
\newblock Stochastic network utility maximization with unknown utilities:
  Multi-armed bandits approach.
\newblock In \emph{Proc. {IEEE INFOCOM}}, pp.\  189--198, 2020.

\bibitem[Verma et~al.(2019)Verma, Hanawal, Rajkumar, and
  Sankaran]{NeurIPS19_verma2019censored}
Verma, A., Hanawal, M., Rajkumar, A., and Sankaran, R.
\newblock Censored semi-bandits: A framework for resource allocation with
  censored feedback.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  14499--14509, 2019.

\bibitem[Verma et~al.(2021)Verma, Hanawal, Rajkumar, and
  Sankaran]{Arxiv21_verma2021censored}
Verma, A., Hanawal, M.~K., Rajkumar, A., and Sankaran, R.
\newblock Censored semi-bandits for resource allocation.
\newblock {arXiv preprint arXiv:2104.05781}, 2021.

\bibitem[Vernade et~al.(2017)Vernade, Capp{\'e}, and
  Perchet]{UAI17_vernade2017stochastic}
Vernade, C., Capp{\'e}, O., and Perchet, V.
\newblock Stochastic bandit models for delayed conversions.
\newblock In \emph{Proc. {UAI}}, 2017.

\bibitem[Vernade et~al.(2020)Vernade, Carpentier, Lattimore, Zappella, Ermis,
  and Brueckner]{ICML20_vernade2020linear}
Vernade, C., Carpentier, A., Lattimore, T., Zappella, G., Ermis, B., and
  Brueckner, M.
\newblock Linear bandits with stochastic delayed feedback.
\newblock In \emph{Proc. {ICML}}, pp.\  9712--9721, 2020.

\bibitem[Wistuba et~al.(2015)Wistuba, Schilling, and
  Schmidt-Thieme]{wistuba2015learning}
Wistuba, M., Schilling, N., and Schmidt-Thieme, L.
\newblock Learning hyperparameter optimization initializations.
\newblock In \emph{Proc. {IEEE} International Conference on Data Science and
  Advanced Analytics}, 2015.

\bibitem[Xu et~al.(2014)Xu, Low, Chen, Lim, and {\"{O}zg\"{u}l}]{LowAAAI14}
Xu, N., Low, K.~H., Chen, J., Lim, K.~K., and {\"{O}zg\"{u}l}, E.~B.
\newblock {GP-Localize}: Persistent mobile robot localization using online
  sparse {Gaussian} process observation model.
\newblock In \emph{Proc. {AAAI}}, pp.\  2585--2592, 2014.

\bibitem[Yu et~al.(2019{\natexlab{a}})Yu, Chen, Dai, Low, and
  Jaillet]{haibinneurips19}
Yu, H., Chen, Y., Dai, Z., Low, B. K.~H., and Jaillet, P.
\newblock Implicit posterior variational inference for deep {Gaussian}
  processes.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  14475--14486, 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2019{\natexlab{b}})Yu, Hoang, Low, and Jaillet]{HaibinAPP}
Yu, H., Hoang, T.~N., Low, K.~H., and Jaillet, P.
\newblock Stochastic variational inference for {Bayesian} sparse {Gaussian}
  process regression.
\newblock In \emph{Proc. {IJCNN}}, 2019{\natexlab{b}}.

\bibitem[Yu et~al.(2021)Yu, Liu, Low, and Jaillet]{HaibinAPP2}
Yu, H., Liu, D., Low, K.~H., and Jaillet, P.
\newblock Convolutional normalizing flows for deep {Gaussian} processes.
\newblock In \emph{Proc. {IJCNN}}, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Hoang, Low, and Kankanhalli]{yehong17}
Zhang, Y., Hoang, T.~N., Low, B. K.~H., and Kankanhalli, M.
\newblock Information-based multi-fidelity {Bayesian} optimization.
\newblock In \emph{Proc. {NIPS} Workshop on {Bayesian} Optimization}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Dai, and Low]{ZhangUAI19}
Zhang, Y., Dai, Z., and Low, B. K.~H.
\newblock Bayesian optimization with binary auxiliary information.
\newblock In \emph{Proc. UAI}, pp.\  1222--1232, 2019.

\bibitem[Zhong et~al.(2017)Zhong, Huang, and
  Liu]{Arxiv17_zhong2017asynchronous}
Zhong, J., Huang, Y., and Liu, J.
\newblock Asynchronous parallel empirical variance guided algorithms for the
  thresholding bandit problem.
\newblock {arXiv preprint arXiv:1704.04567}, 2017.

\bibitem[Zhou et~al.(2019)Zhou, Xu, and Blanchet]{NeurIPS19_zhou2019learning}
Zhou, Z., Xu, R., and Blanchet, J.
\newblock Learning in generalized linear contextual bandits with stochastic
  delays.
\newblock In \emph{Proc. {NeurIPS}}, pp.\  5197--5208, 2019.

\end{thebibliography}
