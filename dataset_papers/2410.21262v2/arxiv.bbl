\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson, Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4015--4026, 2023.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Huh et~al.(2022)Huh, Mobahi, Zhang, Cheung, Agrawal, and Isola]{huh2022low}
Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola.
\newblock The low-rank simplicity bias in deep networks.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Yaras et~al.(2023)Yaras, Wang, Hu, Zhu, Balzano, and Qu]{yaras2023law}
Can Yaras, Peng Wang, Wei Hu, Zhihui Zhu, Laura Balzano, and Qing Qu.
\newblock The law of parsimony in gradient descent for learning deep linear networks.
\newblock \emph{arXiv preprint arXiv:2306.01154}, 2023.

\bibitem[Kwon et~al.(2024)Kwon, Zhang, Song, Balzano, and Qu]{kwon2024efficient}
Soo~Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, and Qing Qu.
\newblock Efficient low-dimensional compression of overparameterized models.
\newblock In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, \emph{Proceedings of The 27th International Conference on Artificial Intelligence and Statistics}, volume 238 of \emph{Proceedings of Machine Learning Research}, pages 1009--1017. PMLR, 02--04 May 2024.
\newblock URL \url{https://proceedings.mlr.press/v238/min-kwon24a.html}.

\bibitem[Wang et~al.(2023)Wang, Li, Yaras, Zhu, Balzano, Hu, and Qu]{wang2023understanding}
Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu.
\newblock Understanding deep representation learning via layerwise feature compression and discrimination.
\newblock \emph{arXiv preprint arXiv:2311.02960}, 2023.

\bibitem[Frankle and Carbin(2019)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frantar and Alistarh(2023)]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot.
\newblock In \emph{International Conference on Machine Learning}, pages 10323--10337. PMLR, 2023.

\bibitem[Lee and Kim(2024)]{lee2024differentiable}
Changwoo Lee and Hun-Seok Kim.
\newblock Differentiable learning of generalized structured matrices for efficient deep neural networks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=pAVJKp3Dvn}.

\bibitem[Peebles and Xie(2023)]{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem[Dao et~al.(2022)Dao, Chen, Sohoni, Desai, Poli, Grogan, Liu, Rao, Rudra, and R{\'e}]{dao2022monarch}
Tri Dao, Beidi Chen, Nimit~S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate training.
\newblock In \emph{International Conference on Machine Learning}, pages 4690--4721. PMLR, 2022.

\bibitem[Amestoy et~al.(2015)Amestoy, Ashcraft, Boiteau, Buttari, L'Excellent, and Weisbecker]{amestoy2015improving}
Patrick Amestoy, Cleve Ashcraft, Olivier Boiteau, Alfredo Buttari, Jean-Yves L'Excellent, and Cl{\'e}ment Weisbecker.
\newblock Improving multifrontal methods by means of block low-rank representations.
\newblock \emph{SIAM Journal on Scientific Computing}, 37\penalty0 (3):\penalty0 A1451--A1474, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chen et~al.(2022)Chen, Dao, Liang, Yang, Song, Rudra, and Re]{chen2022pixelated}
Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re.
\newblock Pixelated butterfly: Simple and efficient sparse training for neural network models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Nfl-iXa-y7R}.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Tong et~al.(2021)Tong, Ma, and Chi]{tong2021accelerating}
Tian Tong, Cong Ma, and Yuejie Chi.
\newblock Accelerating ill-conditioned low-rank matrix estimation via scaled gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (150):\penalty0 1--63, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Fattahi, and Zhang]{zhang2021preconditioned}
Jialun Zhang, Salar Fattahi, and Richard~Y Zhang.
\newblock Preconditioned gradient descent for over-parameterized nonconvex matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5985--5996, 2021.

\bibitem[Xu et~al.(2023)Xu, Shen, Chi, and Ma]{xu2023power}
Xingyu Xu, Yandi Shen, Yuejie Chi, and Cong Ma.
\newblock The power of preconditioning in overparameterized low-rank matrix sensing.
\newblock In \emph{International Conference on Machine Learning}, pages 38611--38654. PMLR, 2023.

\bibitem[Ye and Du(2021)]{ye2021global}
Tian Ye and Simon~S Du.
\newblock Global convergence of gradient descent for asymmetric low-rank matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1429--1439, 2021.

\bibitem[St{\"o}ger and Soltanolkotabi(2021)]{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23831--23843, 2021.

\bibitem[Krizhevsky and Hinton(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet15russakovsky}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0 (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{heusel2017fid}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Nash et~al.(2021)Nash, Menick, Dieleman, and Battaglia]{nash2021generating}
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia.
\newblock Generating images with sparse representations.
\newblock In \emph{International Conference on Machine Learning}, pages 7958--7968. PMLR, 2021.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford, and Chen]{salimans2016inceptionscore}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Ashcraft et~al.(2021)Ashcraft, Buttari, and Mary]{ashcraft2021block}
Cleve Ashcraft, Alfredo Buttari, and Theo Mary.
\newblock Block low-rank matrices with shared bases: Potential and limitations of the blr \^{}2 format.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 42\penalty0 (2):\penalty0 990--1010, 2021.

\bibitem[Yen et~al.(2023)Yen, Duvvuri, Dhillon, and Hsieh]{yen2023block}
Jui-Nan Yen, Sai~Surya Duvvuri, Inderjit~S Dhillon, and Cho-Jui Hsieh.
\newblock Block low-rank preconditioner with shared basis for stochastic optimization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=JzQlGqBm8d}.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock \emph{Advances in neural information processing systems}, 2, 1989.

\bibitem[Hassibi and Stork(1992)]{hassibi1992second}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock \emph{Advances in neural information processing systems}, 5, 1992.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 1389--1397, 2017.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{ma2023llm}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 21702--21720, 2023.

\bibitem[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh, Yu, and Micikevicius]{mishra2021accelerating}
Asit Mishra, Jorge~Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius.
\newblock Accelerating sparse deep neural networks.
\newblock \emph{arXiv preprint arXiv:2104.08378}, 2021.

\bibitem[Tai et~al.(2016)Tai, Xiao, Wang, and E]{tai2016convolutional}
Cheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E.
\newblock Convolutional neural networks with low-rank regularization.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.06067}.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and Zisserman]{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock \emph{arXiv preprint arXiv:1405.3866}, 2014.

\bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Hajimolahoseini et~al.(2022)Hajimolahoseini, Ahmed, Rezagholizadeh, Partovinia, and Liu]{hajimolahoseini2022strategies}
Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, and Yang Liu.
\newblock Strategies for applying low rank decomposition to transformer-based models.
\newblock In \emph{36th Conference on Neural Information Processing Systems (NeurIPS2022)}, 2022.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R{\'e}.
\newblock Learning fast algorithms for linear transforms using butterfly factorizations.
\newblock In \emph{International conference on machine learning}, pages 1517--1527. PMLR, 2019.

\bibitem[Yaras et~al.(2024)Yaras, Wang, Balzano, and Qu]{yarascompressible}
Can Yaras, Peng Wang, Laura Balzano, and Qing Qu.
\newblock Compressible dynamics in deep overparameterized low-rank learning \& adaptation.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Hinton(2015)]{hinton2015distilling}
Geoffrey Hinton.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Sreenivas et~al.(2024)Sreenivas, Muralidharan, Joshi, Chochowski, Patwary, Shoeybi, Catanzaro, Kautz, and Molchanov]{sreenivas2024llm}
Sharath~Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov.
\newblock Llm pruning and distillation in practice: The minitron approach.
\newblock \emph{arXiv preprint arXiv:2408.11796}, 2024.

\bibitem[Wang et~al.(2024)Wang, Zhang, Zhang, Chen, Ma, and Qu]{wang2024diffusion}
Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi~Ma, and Qing Qu.
\newblock Diffusion models learn low-dimensional distributions via subspace clustering.
\newblock \emph{arXiv preprint arXiv:2409.02426}, 2024.

\bibitem[Li and Yan(2024)]{li2024adapting}
Gen Li and Yuling Yan.
\newblock Adapting to unknown low-dimensional structures in score-based diffusion models.
\newblock \emph{arXiv preprint arXiv:2405.14861}, 2024.

\bibitem[Chen et~al.(2024)Chen, Zhang, Guo, Lu, Wang, and Qu]{chen2024exploring}
Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, and Qing Qu.
\newblock Exploring low-dimensional subspace in diffusion models for controllable image editing.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.02374}.

\bibitem[Horn and Johnson(2012)]{horn2012matrix}
Roger~A Horn and Charles~R Johnson.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Horn and Johnson(1994)]{horn1994topics}
Roger~A Horn and Charles~R Johnson.
\newblock \emph{Topics in matrix analysis}.
\newblock Cambridge university press, 1994.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Peng et~al.(2024)Peng, Zhou, Guo, Gao, Yan, Qiu, and Lin]{peng2024data}
Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, and Dahua Lin.
\newblock Data-freeweight compress and denoise for large language models.
\newblock \emph{arXiv preprint arXiv:2402.16319}, 2024.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\end{thebibliography}
