\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and Saligrama]{acar2021federated}
Acar, D. A.~E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., and Saligrama, V.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=B7v4QMR6Z9w}.

\bibitem[Avdiukhin \& Kasiviswanathan(2021)Avdiukhin and Kasiviswanathan]{pmlr-v139-avdiukhin21a}
Avdiukhin, D. and Kasiviswanathan, S.
\newblock Federated learning under arbitrary communication patterns.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, pp.\  425--435, 2021.

\bibitem[Bornstein et~al.(2023)Bornstein, Rabbani, Wang, Bedi, and Huang]{bornstein2023swift}
Bornstein, M., Rabbani, T., Wang, E.~Z., Bedi, A., and Huang, F.
\newblock {SWIFT}: Rapid decentralized federated learning via wait-free model communication.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jh1nCir1R3d}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Zhou, Tang, Yang, Cao, and Gu]{chen2020closing}
Chen, J., Zhou, D., Tang, Y., Yang, Z., Cao, Y., and Gu, Q.
\newblock Closing the generalization gap of adaptive gradient methods in training deep neural networks.
\newblock In \emph{Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Jin, Sun, and Yin]{chen2020vafl}
Chen, T., Jin, X., Sun, Y., and Yin, W.
\newblock Vafl: a method of vertical asynchronous federated learning.
\newblock \emph{arXiv preprint arXiv:2007.06081}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
Chen, X., Liu, S., Sun, R., and Hong, M.
\newblock On the convergence of a class of adam-type algorithms for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1808.02941}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Glasgow \& Wootters(2022)Glasgow and Wootters]{glasgow2022asynchronous}
Glasgow, M.~R. and Wootters, M.
\newblock Asynchronous distributed optimization with stochastic delays.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  9247--9279. PMLR, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jhunjhunwala et~al.(2021)Jhunjhunwala, Gadhikar, Joshi, and Eldar]{jhunjhunwala2021adaptive}
Jhunjhunwala, D., Gadhikar, A., Joshi, G., and Eldar, Y.~C.
\newblock Adaptive quantization of model updates for communication-efficient federated learning.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  3110--3114. IEEE, 2021.

\bibitem[Jhunjhunwala et~al.(2023)Jhunjhunwala, Wang, and Joshi]{jhunjhunwala2023fedexp}
Jhunjhunwala, D., Wang, S., and Joshi, G.
\newblock {FedExP}: Speeding up federated averaging via extrapolation.
\newblock \emph{arXiv preprint arXiv:2301.09604}, 2023.

\bibitem[Jin et~al.(2022)Jin, Ren, Zhou, Lyu, Liu, and Dou]{jin2022accelerated}
Jin, J., Ren, J., Zhou, Y., Lyu, L., Liu, J., and Dou, D.
\newblock Accelerated federated learning with decoupled adaptive optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10298--10322. PMLR, 2022.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5132--5143. PMLR, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Koloskova et~al.(2022)Koloskova, Stich, and Jaggi]{koloskova2022sharper}
Koloskova, A., Stich, S.~U., and Jaggi, M.
\newblock Sharper convergence guarantees for asynchronous {SGD} for distributed and federated learning.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=4_oCZgBIVI}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Leblond et~al.(2018)Leblond, Pedregosa, and Lacoste-Julien]{JMLRasync}
Leblond, R., Pedregosa, F., and Lacoste-Julien, S.
\newblock Improved asynchronous parallel optimization analysis for stochastic incremental methods.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (81):\penalty0 1--68, 2018.

\bibitem[Li et~al.(2023)Li, Huang, and Huang]{li2023fedda}
Li, J., Huang, F., and Huang, H.
\newblock Fedda: Faster framework of local adaptive gradient methods via restarted dual averaging.
\newblock \emph{arXiv preprint arXiv:2302.06103}, 2023.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of {FedAvg} on non-iid data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Yang, Wang, and Zhang]{li2019communication}
Li, X., Yang, W., Wang, S., and Zhang, Z.
\newblock Communication-efficient local decentralized {SGD} methods.
\newblock \emph{arXiv preprint arXiv:1910.09126}, 2019{\natexlab{b}}.

\bibitem[Lin et~al.(2018)Lin, Stich, Patel, and Jaggi]{lin2018don}
Lin, T., Stich, S.~U., Patel, K.~K., and Jaggi, M.
\newblock Don't use large mini-batches, use local sgd.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and Jordan]{mania2017perturbed}
Mania, H., Pan, X., Papailiopoulos, D., Recht, B., Ramchandran, K., and Jordan, M.~I.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0 2202--2229, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282. PMLR, 2017.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Bach, Even, and Woodworth]{mishchenko2022asynchronous}
Mishchenko, K., Bach, F., Even, M., and Woodworth, B.~E.
\newblock Asynchronous {SGD} beats minibatch {SGD} under arbitrary delays.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 420--433, 2022.

\bibitem[Nguyen et~al.(2022)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and Huba]{nguyen2022federated}
Nguyen, J., Malik, K., Zhan, H., Yousefpour, A., Rabbat, M., Malek, M., and Huba, D.
\newblock Federated learning with buffered asynchronous aggregation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3581--3607. PMLR, 2022.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg, and Takac]{pmlr-v80-nguyen18c}
Nguyen, L., Nguyen, P.~H., van Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M.
\newblock {SGD} and hogwild! {C}onvergence without the bounded gradients assumption.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  3750--3758. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/nguyen18c.html}.

\bibitem[Niu et~al.(2011)Niu, Recht, Re, and Wright]{10.5555/2986459.2986537}
Niu, F., Recht, B., Re, C., and Wright, S.~J.
\newblock Hogwild! a lock-free approach to parallelizing stochastic gradient descent.
\newblock In \emph{Proceedings of the 24th International Conference on Neural Information Processing Systems}, NIPS'11, pp.\  693â€“701, Red Hook, NY, USA, 2011. Curran Associates Inc.
\newblock ISBN 9781618395993.

\bibitem[Ortega \& Jafarkhani(2023)Ortega and Jafarkhani]{ortega2023asynchronous}
Ortega, T. and Jafarkhani, H.
\newblock Asynchronous federated learning with bidirectional quantized communications and buffered aggregation.
\newblock \emph{arXiv preprint arXiv:2308.00263}, 2023.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{j.2018on}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush, Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Kone{\v{c}}n{\'y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and Pedarsani]{reisizadeh2020fedpaq}
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
\newblock Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  2021--2031. PMLR, 2020.

\bibitem[Stich et~al.(2021)Stich, Mohtashami, and Jaggi]{stich2021critical}
Stich, S., Mohtashami, A., and Jaggi, M.
\newblock Critical parameters for scalable distributed learning with large batches and asynchronous updates.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  4042--4050. PMLR, 2021.

\bibitem[Stich(2018)]{stich2018local}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Shen, Chen, Sun, Li, Sun, and Tao]{sun2023fedlalr}
Sun, H., Shen, L., Chen, S., Sun, J., Li, J., Sun, G., and Tao, D.
\newblock Fedlalr: Client-specific adaptive learning rates achieve linear speedup for non-iid data.
\newblock \emph{arXiv preprint arXiv:2309.09719}, 2023{\natexlab{a}}.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Sun, Ding, and Tao]{sun2023efficient}
Sun, Y., Shen, L., Sun, H., Ding, L., and Tao, D.
\newblock Efficient federated learning via local adaptive amended optimizer with linear speedup.
\newblock \emph{arXiv preprint arXiv:2308.00522}, 2023{\natexlab{b}}.

\bibitem[Tan et~al.(2022)Tan, Zhang, Zhou, Che, Hu, Chen, and Wu]{tan2022adafed}
Tan, L., Zhang, X., Zhou, Y., Che, X., Hu, M., Chen, X., and Wu, D.
\newblock Adafed: Optimizing participation-aware federated learning with adaptive aggregation weights.
\newblock \emph{IEEE Transactions on Network Science and Engineering}, 9\penalty0 (4):\penalty0 2708--2720, 2022.

\bibitem[Tieleman et~al.(2012)Tieleman, Hinton, et~al.]{tieleman2012lecture}
Tieleman, T., Hinton, G., et~al.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0 (2):\penalty0 26--31, 2012.

\bibitem[Toghani \& Uribe(2022)Toghani and Uribe]{toghani2022unbounded}
Toghani, M.~T. and Uribe, C.~A.
\newblock Unbounded gradients in federated learning with buffered asynchronous aggregation.
\newblock In \emph{2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, pp.\  1--8. IEEE, 2022.

\bibitem[Tong et~al.(2020)Tong, Liang, and Bi]{tong2020effective}
Tong, Q., Liang, G., and Bi, J.
\newblock Effective federated adaptive gradient methods with non-iid decentralized data.
\newblock \emph{arXiv preprint arXiv:2009.06557}, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Yurochkin, Sun, Papailiopoulos, and Khazaeni]{Wang2020Federated}
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=BkluqlSFDS}.

\bibitem[Wang \& Joshi(2021)Wang and Joshi]{wang2021cooperative}
Wang, J. and Joshi, G.
\newblock Cooperative sgd: A unified framework for the design and analysis of local-update {SGD} algorithms.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 9709--9758, 2021.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock \emph{arXiv preprint arXiv:2007.07481}, 2020{\natexlab{b}}.

\bibitem[Wang \& Ji(2023)Wang and Ji]{wang2023lightweight}
Wang, S. and Ji, M.
\newblock A lightweight method for tackling unknown participation probabilities in federated averaging.
\newblock \emph{arXiv preprint arXiv:2306.03401}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Lin, and Chen]{wang2022cdadam}
Wang, Y., Lin, L., and Chen, J.
\newblock Communication-compressed adaptive gradient method for distributed nonconvex optimization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  6292--6320. PMLR, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Lin, and Chen]{wang2022communication}
Wang, Y., Lin, L., and Chen, J.
\newblock Communication-efficient adaptive federated learning.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, pp.\  22802--22838. PMLR, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Cao, Wu, Chen, and Chen]{wang2023tackling}
Wang, Y., Cao, Y., Wu, J., Chen, R., and Chen, J.
\newblock Tackling the data heterogeneity in asynchronous federated learning with cached update calibration.
\newblock In \emph{Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities}, 2023.

\bibitem[Wu et~al.(2022)Wu, Magnusson, Feyzmahdavian, and Johansson]{wu2022delay}
Wu, X., Magnusson, S., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Delay-adaptive step-sizes for asynchronous learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  24093--24113. PMLR, 2022.

\bibitem[Wu et~al.(2023)Wu, Huang, Hu, and Huang]{wu2023faster}
Wu, X., Huang, F., Hu, Z., and Huang, H.
\newblock Faster adaptive federated learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  10379--10387, 2023.

\bibitem[Xie et~al.(2019)Xie, Koyejo, and Gupta]{xie2019asynchronous}
Xie, C., Koyejo, S., and Gupta, I.
\newblock Asynchronous federated optimization.
\newblock \emph{arXiv preprint arXiv:1903.03934}, 2019.

\bibitem[Yang et~al.(2021)Yang, Fang, and Liu]{yang2021achieving}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in non-{IID} federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yang et~al.(2022)Yang, Zhang, Khanduri, and Liu]{yang2022anarchic}
Yang, H., Zhang, X., Khanduri, P., and Liu, J.
\newblock Anarchic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  25331--25363. PMLR, 2022.

\end{thebibliography}
