@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in applied probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}

@inproceedings{xu2010distributionally,
  title={Distributionally robust {M}arkov decision processes},
  author={Xu, Huan and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2505--2513},
  year={2010}
}


@article{wiesemann2013robust,
  title={Robust {M}arkov decision processes},
  author={Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c{c}}},
  journal={Mathematics of Operations Research},
  volume={38},
  number={1},
  pages={153--183},
  year={2013},
  publisher={INFORMS}
}

@article{mannor2016robust,
  title={Robust MDPs with k-rectangular uncertainty},
  author={Mannor, Shie and Mebel, Ofir and Xu, Huan},
  journal={Mathematics of Operations Research},
  volume={41},
  number={4},
  pages={1484--1509},
  year={2016},
  publisher={INFORMS}
}


@article{russel2019beyond,
  title={Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs},
  author={Russel, Reazul Hasan and Petrik, Marek},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{yang2022toward,
  title={Toward theoretical understandings of robust Markov decision processes: Sample complexity and asymptotics},
  author={Yang, Wenhao and Zhang, Liangyu and Zhang, Zhihua},
  journal={The Annals of Statistics},
  volume={50},
  number={6},
  pages={3223--3248},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}


@inproceedings{zhou2021finite,
  title={Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning},
  author={Zhou, Zhengqing and Bai, Qinxun and Zhou, Zhengyuan and Qiu, Linhai and Blanchet, Jose and Glynn, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3331--3339},
  year={2021},
}



@InProceedings{panaganti22a,
  title = 	 {Sample Complexity of Robust Reinforcement Learning with a Generative Model},
  author =       {Panaganti, Kishan and Kalathil, Dileep},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {9582--9602},
  year = 	 {2022}
}


@inproceedings{xu2023improved,
  title={Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning},
  author={Xu, Zaiyan and Panaganti, Kishan and Kalathil, Dileep},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={9728--9754},
  year={2023}
}


@article{shi2022distributionally,
  title={Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity},
  author={Shi, Laixi and Chi, Yuejie},
  journal={arXiv preprint arXiv:2208.05767},
  year={2022}
}


@article{goyal2023robust,
  title={Robust Markov decision processes: Beyond rectangularity},
  author={Goyal, Vineet and Grand-Clement, Julien},
  journal={Mathematics of Operations Research},
  volume={48},
  number={1},
  pages={203--226},
  year={2023},
  publisher={INFORMS}
}


@article{mannor2007bias,
  title={Bias and variance approximation in value function estimates},
  author={Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
  journal={Management Science},
  volume={53},
  number={2},
  pages={308--322},
  year={2007},
  publisher={INFORMS}
}




@article{liang2023single,
  title={Single-Trajectory Distributionally Robust Reinforcement Learning},
  author={Liang, Zhipeng and Ma, Xiaoteng and Blanchet, Jose and Zhang, Jiheng and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2301.11721},
  year={2023}
}

@article{ma2022distributionally,
  title={Distributionally robust offline reinforcement learning with linear function approximation},
  author={Ma, Xiaoteng and Liang, Zhipeng and Xia, Li and Zhang, Jiheng and Blanchet, Jose and Liu, Mingwen and Zhao, Qianchuan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.06620},
  year={2022}
}



@inproceedings{liu2022distributionally,
  title={Distributionally Robust $ Q $-Learning},
  author={Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan},
  booktitle={International Conference on Machine Learning},
  pages={13623--13643},
  year={2022},
  organization={PMLR}
}

@article{neufeld2022robust,
  title={Robust $ Q $-learning Algorithm for Markov Decision Processes under Wasserstein Uncertainty},
  author={Neufeld, Ariel and Sester, Julian},
  journal={arXiv preprint arXiv:2210.00898},
  year={2022}
}

@inproceedings{wang2023finite,
  title={A Finite Sample Complexity Bound for Distributionally Robust Q-learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3370--3398},
  year={2023},
  organization={PMLR}
}

























@inproceedings{melo2008analysis,
  title={An analysis of reinforcement learning with function approximation},
  author={Melo, Francisco S and Meyn, Sean P and Ribeiro, M Isabel},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={664--671},
  year={2008}
}

@article{kruger2003frechet,
  title={On fr{\'e}chet subdifferentials},
  author={Kruger, A Ya},
  journal={Journal of Mathematical Sciences},
  volume={116},
  number={3},
  pages={3325--3358},
  year={2003},
  publisher={Springer}
}

@article{jiang2018bernstein,
  title={Bernstein's inequality for general Markov chains},
  author={Jiang, Bai and Sun, Qiang and Fan, Jianqing},
  journal={arXiv preprint arXiv:1805.10721},
  year={2018}
}

@inproceedings{tamar2014scaling,
  title={Scaling up robust MDPs using function approximation},
  author={Tamar, Aviv and Mannor, Shie and Xu, Huan},
  booktitle={International conference on machine learning},
  pages={181--189},
  year={2014},
  organization={PMLR}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{cayci2021linear,
  title={Linear convergence of entropy-regularized natural policy gradient with linear function approximation},
  author={Cayci, Semih and He, Niao and Srikant, Rayadurgam},
  journal={arXiv preprint arXiv:2106.04096},
  year={2021}
}

@inproceedings{khodadadian2021linear,
  title={On the linear convergence of natural policy gradient algorithm},
  author={Khodadadian, Sajad and Jhunjhunwala, Prakirt Raj and Varma, Sushil Mahavir and Maguluri, Siva Theja},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  pages={3794--3799},
  year={2021},
  organization={IEEE}
}

@article{alfano2022linear,
  title={Linear convergence for natural policy gradient with log-linear policy parametrization},
  author={Alfano, Carlo and Rebeschini, Patrick},
  journal={arXiv preprint arXiv:2209.15382},
  year={2022}
}

@article{yuan2022linear,
  title={Linear convergence of natural policy gradient methods with log-linear policies},
  author={Yuan, Rui and Du, Simon S and Gower, Robert M and Lazaric, Alessandro and Xiao, Lin},
  journal={arXiv preprint arXiv:2210.01400},
  year={2022}
}

@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  pages={10--4},
  year={2019}
}

@article{zhou2022anchor,
  title={Anchor-Changing Regularized Natural Policy Gradient for Multi-Objective Reinforcement Learning},
  author={Zhou, Ruida and Liu, Tao and Kalathil, Dileep and Kumar, PR and Tian, Chao},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@article{nilim2005robust,
  title={Robust control of Markov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}




@article{li2022first,
  title={First-order policy optimization for robust Markov decision process},
  author={Li, Yan and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2209.10579},
  year={2022}
}

@article{chen2022finite,
  title={Finite-sample analysis of nonlinear stochastic approximation with applications in reinforcement learning},
  author={Chen, Zaiwei and Zhang, Sheng and Doan, Thinh T and Clarke, John-Paul and Maguluri, Siva Theja},
  journal={Automatica},
  volume={146},
  pages={110623},
  year={2022},
  publisher={Elsevier}
}

@article{chen2022finite-nac,
  title={Finite-sample analysis of off-policy natural actor--critic with linear function approximation},
  author={Chen, Zaiwei and Khodadadian, Sajad and Maguluri, Siva Theja},
  journal={IEEE Control Systems Letters},
  volume={6},
  pages={2611--2616},
  year={2022},
  publisher={IEEE}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={Proceedings of the Nineteenth International Conference on Machine Learning},
  pages={267--274},
  year={2002}
}

@book{federer2014geometric,
  title={Geometric measure theory},
  author={Federer, Herbert},
  year={2014},
  publisher={Springer}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4431--4506},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{wang2021online,
  title={Online robust reinforcement learning with model uncertainty},
  author={Wang, Yue and Zou, Shaofeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7193--7206},
  year={2021}
}

@inproceedings{wang2022policy,
  title={Policy gradient method for robust reinforcement learning},
  author={Wang, Yue and Zou, Shaofeng},
  booktitle={International Conference on Machine Learning},
  pages={23484--23526},
  year={2022},
  organization={PMLR}
}



@article{kumar2023policy,
  title={Policy Gradient for s-Rectangular Robust Markov Decision Processes},
  author={Kumar, Navdeep and Derman, Esther and Geist, Matthieu and Levy, Kfir and Mannor, Shie},
  journal={arXiv preprint arXiv:2301.13589},
  year={2023}
}

@article{tsitsiklis1996analysis,
  title={Analysis of temporal-diffference learning with function approximation},
  author={Tsitsiklis, John and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{xu2020improving,
  title={Improving sample complexity bounds for (natural) actor-critic algorithms},
  author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4358--4369},
  year={2020}
}

@article{qiu2021finite,
  title={On finite-time convergence of actor-critic algorithm},
  author={Qiu, Shuang and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={2},
  number={2},
  pages={652--664},
  year={2021},
  publisher={IEEE}
}

@article{bhatnagar2009natural,
  title={Natural actor--critic algorithms},
  author={Bhatnagar, Shalabh and Sutton, Richard S and Ghavamzadeh, Mohammad and Lee, Mark},
  journal={Automatica},
  volume={45},
  number={11},
  pages={2471--2482},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{abdullah2019wasserstein,
  title={Wasserstein robust reinforcement learning},
  author={Abdullah, Mohammed Amin and Ren, Hang and Ammar, Haitham Bou and Milenkovic, Vladimir and Luo, Rui and Zhang, Mingtian and Wang, Jun},
  journal={arXiv preprint arXiv:1907.13196},
  year={2019}
}

@inproceedings{kuang2022learning,
  title={Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization},
  author={Kuang, Yufei and Lu, Miao and Wang, Jie and Zhou, Qi and Li, Bin and Li, Houqiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7247--7254},
  year={2022}
}

@inproceedings{eysenbachmaximum,
  title={Maximum Entropy RL (Provably) Solves Some Robust RL Problems},
  author={Eysenbach, Benjamin and Levine, Sergey},
  booktitle={International Conference on Learning Representations}
}

@article{duchi2021learning,
  title={Learning models with uniform performance via distributionally robust optimization},
  author={Duchi, John C and Namkoong, Hongseok},
  journal={The Annals of Statistics},
  volume={49},
  number={3},
  pages={1378--1406},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{panaganti2022robust,
  title={Robust reinforcement learning using offline data},
  author={Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
booktitle = {Advances in Neural Information Processing Systems},
 pages = {32211--32224},
 year = {2022}
}

@inproceedings{panaganti2021robust,
  title={Robust reinforcement learning using least squares policy iteration with provable performance guarantees},
  author={Panaganti, Kishan and Kalathil, Dileep},
  booktitle={International Conference on Machine Learning},
  pages={511--520},
  year={2021},
  organization={PMLR}
}

@article{roy2017reinforcement,
  title={Reinforcement learning under model mismatch},
  author={Roy, Aurko and Xu, Huan and Pokutta, Sebastian},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{ho2021partial,
  title={Partial policy iteration for L1-robust Markov decision processes},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={12612--12657},
  year={2021},
  publisher={JMLRORG}
}

@article{kumar2022efficient,
  title={Efficient policy iteration for robust markov decision processes via regularization},
  author={Kumar, Navdeep and Levy, Kfir and Wang, Kaixin and Mannor, Shie},
  journal={arXiv preprint arXiv:2205.14327},
  year={2022}
}

@article{derman2021twice,
  title={Twice regularized MDPs and the equivalence between robustness and regularization},
  author={Derman, Esther and Geist, Matthieu and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22274--22287},
  year={2021}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inbook{turtlebot,
    author = {Amsters, Robin and Slaets, Peter},
    year = {2020},
    month = {01},
    pages = {170-181},
    title = {Turtlebot 3 as a Robotics Education Platform},
    isbn = {978-3-030-26944-9},
    doi = {10.1007/978-3-030-26945-6_16}
}

@article{sunderhauf2018limits,
  title={The limits and potentials of deep learning for robotics},
  author={S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  journal={The International journal of robotics research},
  volume={37},
  number={4-5},
  pages={405--420},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{peng2018sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={3803--3810},
  year={2018},
  organization={IEEE}
}

@article{wang2022convergence,
  title={On the Convergence of Policy Gradient in Robust MDPs},
  author={Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek},
  journal={arXiv preprint arXiv:2212.10439},
  year={2022}
}

@inproceedings{grand2021scalable,
  title={Scalable first-order methods for robust MDPs},
  author={Grand-Cl{\'e}ment, Julien and Kroer, Christian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={12086--12094},
  year={2021}
}

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages={23--30},
  year={2017},
  organization={IEEE}
}

@article{yang2022dichotomy,
  title={Dichotomy of control: Separating what you can control from what you cannot},
  author={Yang, Mengjiao and Schuurmans, Dale and Abbeel, Pieter and Nachum, Ofir},
  journal={arXiv preprint arXiv:2210.13435},
  year={2022}
}

@article{ho2022robust,
  title={Robust Phi-Divergence MDPs},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  journal={arXiv preprint arXiv:2205.14202},
  year={2022}
}

@inproceedings{tessler2019action,
  title={Action robust reinforcement learning and applications in continuous control},
  author={Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6215--6224},
  year={2019},
  organization={PMLR}
}


@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}

@article{cen2022fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  volume={70},
  number={4},
  pages={2563--2578},
  year={2022},
  publisher={INFORMS}
}

@article{derman2018soft,
  title={Soft-robust actor-critic policy-gradient},
  author={Derman, Esther and Mankowitz, Daniel J and Mann, Timothy A and Mannor, Shie},
  journal={arXiv preprint arXiv:1803.04848},
  year={2018}
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
  organization={PMLR}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{blanchet2023double,
  title={Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage},
  author={Blanchet, Jose and Lu, Miao and Zhang, Tong and Zhong, Han},
  journal={arXiv preprint arXiv:2305.09659},
  year={2023}
}

@inproceedings{jakobi1995noise,
  title={Noise and the reality gap: The use of simulation in evolutionary robotics},
  author={Jakobi, Nick and Husbands, Phil and Harvey, Inman},
  booktitle={Advances in Artificial Life: Third European Conference on Artificial Life Granada, Spain, June 4--6, 1995 Proceedings 3},
  pages={704--720},
  year={1995},
  organization={Springer}
}

@article{liu2021policy,
  title={Policy Optimization for Constrained MDPs with Provable Fast Global Convergence},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, PR and Tian, Chao},
  journal={arXiv preprint arXiv:2111.00552},
  year={2021}
}

@article{zhang2022global,
  title={On the global convergence rates of decentralized softmax gradient play in markov potential games},
  author={Zhang, Runyu and Mei, Jincheng and Dai, Bo and Schuurmans, Dale and Li, Na},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1923--1935},
  year={2022}
}

@article{sun2023provably,
  title={Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games},
  author={Sun, Youbang and Liu, Tao and Zhou, Ruida and Kumar, PR and Shahrampour, Shahin},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}