\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agustsson \& Timofte(2017)Agustsson and Timofte]{agustsson2017ntire}
Agustsson, E. and Timofte, R.
\newblock Ntire 2017 challenge on single image super-resolution: Dataset and
  study.
\newblock In \emph{\cvpr~Workshops}, pp.\  126--135, 2017.

\bibitem[Ball{\'e} et~al.(2018)Ball{\'e}, Johnston, and
  Minnen]{balle2018integer}
Ball{\'e}, J., Johnston, N., and Minnen, D.
\newblock Integer networks for data compression with latent-variable models.
\newblock In \emph{\iclr}, 2018.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Bengio, Y., L{\'e}onard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Bird et~al.(2021)Bird, Kingma, and Barber]{bird2020reducing}
Bird, T., Kingma, F.~H., and Barber, D.
\newblock Reducing the computational cost of deep generative models with binary
  neural networks.
\newblock In \emph{\iclr}, 2021.

\bibitem[Boutell \& Lane(1997)Boutell and Lane]{boutell1997png}
Boutell, T. and Lane, T.
\newblock Png (portable network graphics) specification version 1.0.
\newblock \emph{Network Working Group}, pp.\  1--102, 1997.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Gai, Yao, Mahoney, and
  Gonzalez]{chen2020statistical}
Chen, J., Gai, Y., Yao, Z., Mahoney, M.~W., and Gonzalez, J.~E.
\newblock A statistical framework for low-bitwidth training of deep neural
  networks.
\newblock In \emph{\nips}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Lu, Chenli, Zhu, and
  Tian]{chen2020vflow}
Chen, J., Lu, C., Chenli, B., Zhu, J., and Tian, T.
\newblock Vflow: More expressive generative flows with variational data
  augmentation.
\newblock In \emph{\icml}, pp.\  1660--1669. PMLR, 2020{\natexlab{b}}.

\bibitem[Choi et~al.(2018)Choi, Wang, Venkataramani, Chuang, Srinivasan, and
  Gopalakrishnan]{choi2018pact}
Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., and
  Gopalakrishnan, K.
\newblock Pact: Parameterized clipping activation for quantized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1805.06085}, 2018.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{\nips}, pp.\  3123--3131, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{\cvpr}, pp.\  248--255. IEEE, 2009.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using real nvp.
\newblock In \emph{\iclr}, 2017.

\bibitem[Domke et~al.(2008)Domke, Karapurkar, and Aloimonos]{4587817}
Domke, J., Karapurkar, A., and Aloimonos, Y.
\newblock Who killed the directed model?
\newblock In \emph{\cvpr}, pp.\  1--8, 2008.

\bibitem[Dong et~al.(2019)Dong, Yao, Gholami, Mahoney, and
  Keutzer]{dong2019hawq}
Dong, Z., Yao, Z., Gholami, A., Mahoney, M.~W., and Keutzer, K.
\newblock Hawq: Hessian aware quantization of neural networks with
  mixed-precision.
\newblock In \emph{\cvpr}, pp.\  293--302, 2019.

\bibitem[Duda(2009)]{duda2009asymmetric}
Duda, J.
\newblock Asymmetric numeral systems.
\newblock \emph{arXiv preprint arXiv:0902.0271}, 2009.

\bibitem[Duda(2013)]{duda2013asymmetric}
Duda, J.
\newblock Asymmetric numeral systems: entropy coding combining speed of
  {H}uffman coding with compression rate of arithmetic coding.
\newblock \emph{arXiv preprint arXiv:1311.2540}, 2013.

\bibitem[Esser et~al.(2020)Esser, McKinstry, Bablani, Appuswamy, and
  Modha]{esser2019learned}
Esser, S.~K., McKinstry, J.~L., Bablani, D., Appuswamy, R., and Modha, D.~S.
\newblock Learned step size quantization.
\newblock In \emph{\iclr}, 2020.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{\iclr}, 2019.

\bibitem[Grci{\'c} et~al.(2021)Grci{\'c}, Grubi{\v{s}}i{\'c}, and
  {\v{S}}egvi{\'c}]{grcic2021densely}
Grci{\'c}, M., Grubi{\v{s}}i{\'c}, I., and {\v{S}}egvi{\'c}, S.
\newblock Densely connected normalizing flows.
\newblock In \emph{\nips}, volume~34, pp.\  23968--23982, 2021.

\bibitem[Group et~al.(2000)]{joint2000jpeg2000}
Group, J. P.~E. et~al.
\newblock Jpeg2000 image coding system.
\newblock \emph{ISO/IEC FCD 15444-1}, 2000.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{\nips}, 2015.

\bibitem[Hazami et~al.(2022)Hazami, Mama, and
  Thurairatnam]{hazami2022efficient}
Hazami, L., Mama, R., and Thurairatnam, R.
\newblock Efficient-vdvae: Less is more.
\newblock \emph{arXiv preprint arXiv:2203.13751}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{\cvpr}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{\cvpr}, pp.\  1389--1397, 2017.

\bibitem[Ho et~al.(2019{\natexlab{a}})Ho, Chen, Srinivas, Duan, and
  Abbeel]{ho2019flow++}
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In \emph{\icml}, pp.\  2722--2730. PMLR, 2019{\natexlab{a}}.

\bibitem[Ho et~al.(2019{\natexlab{b}})Ho, Lohn, and Abbeel]{ho2019compression}
Ho, J., Lohn, E., and Abbeel, P.
\newblock Compression with flows via local bits-back coding.
\newblock In \emph{\nips}, 2019{\natexlab{b}}.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{\nips}, 2020.

\bibitem[Hoogeboom et~al.(2019)Hoogeboom, Peters, van~den Berg, and
  Welling]{hoogeboom2019integer}
Hoogeboom, E., Peters, J., van~den Berg, R., and Welling, M.
\newblock Integer discrete flows and lossless compression.
\newblock In \emph{\nips}, volume~32, 2019.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{\cvpr}, pp.\  4700--4708, 2017.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock In \emph{\nips}, volume~29, 2016.

\bibitem[Huffman(1952)]{huffman1952method}
Huffman, D.~A.
\newblock A method for the construction of minimum-redundancy codes.
\newblock \emph{Proceedings of the IRE}, 40\penalty0 (9):\penalty0 1098--1101,
  1952.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2018quantization}
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and
  Kalenichenko, D.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{\cvpr}, pp.\  2704--2713, 2018.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{\nips}, 2018.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{Kingma2014AutoEncodingVB}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{\iclr}, 2013.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and
  Ho]{kingma2021variational}
Kingma, D.~P., Salimans, T., Poole, B., and Ho, J.
\newblock Variational diffusion models.
\newblock In \emph{\nips}, 2021.

\bibitem[Larochelle \& Murray(2011)Larochelle and Murray]{larochelle2011neural}
Larochelle, H. and Murray, I.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  29--37. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Lebedev \& Lempitsky(2016)Lebedev and Lempitsky]{lebedev2016fast}
Lebedev, V. and Lempitsky, V.
\newblock Fast convnets using group-wise brain damage.
\newblock In \emph{\cvpr}, pp.\  2554--2564, 2016.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{\nips}, pp.\  598--605, 1990.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{\iclr}, 2017.

\bibitem[Lu et~al.(2021)Lu, Chen, Li, Wang, and Zhu]{lu2021implicit}
Lu, C., Chen, J., Li, C., Wang, Q., and Zhu, J.
\newblock Implicit normalizing flows.
\newblock In \emph{\iclr}, 2021.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Maal{\o}e, L., Fraccaro, M., Li{\'e}vin, V., and Winther, O.
\newblock Biva: A very deep hierarchy of latent variables for generative
  modeling.
\newblock In \emph{\nips}, volume~32, 2019.

\bibitem[NVIDIA(2018)]{tensorrt}
NVIDIA.
\newblock Tensorrt.
\newblock \url{https://developer.nvidia.com/tensorrt}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{\nips}, volume~32, pp.\  8026--8037, 2019.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{\icml}, pp.\  1278--1286. PMLR, 2014.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock In \emph{\iclr}, 2017.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Shannon, C.~E.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3):\penalty0
  379--423, 1948.

\bibitem[Sneyers \& Wuille(2016)Sneyers and Wuille]{sneyers2016flif}
Sneyers, J. and Wuille, P.
\newblock Flif: Free lossless image format based on maniac compression.
\newblock In \emph{2016 IEEE international conference on image processing
  (ICIP)}, pp.\  66--70. IEEE, 2016.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{\icml}, pp.\  2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{song2020denoising}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock In \emph{\iclr}, 2021.

\bibitem[Townsend et~al.(2019{\natexlab{a}})Townsend, Bird, and
  Barber]{townsend2019practical}
Townsend, J., Bird, T., and Barber, D.
\newblock Practical lossless compression with latent variables using bits back
  coding.
\newblock In \emph{\iclr}, 2019{\natexlab{a}}.

\bibitem[Townsend et~al.(2019{\natexlab{b}})Townsend, Bird, Kunze, and
  Barber]{townsend2019hilloc}
Townsend, J., Bird, T., Kunze, J., and Barber, D.
\newblock Hilloc: Lossless image compression with hierarchical latent variable
  models.
\newblock In \emph{\iclr}, 2019{\natexlab{b}}.

\bibitem[Van~Baalen et~al.(2020)Van~Baalen, Louizos, Nagel, Amjad, Wang,
  Blankevoort, and Welling]{van2020bayesian}
Van~Baalen, M., Louizos, C., Nagel, M., Amjad, R.~A., Wang, Y., Blankevoort,
  T., and Welling, M.
\newblock Bayesian bits: Unifying quantization and pruning.
\newblock In \emph{\nips}, volume~33, pp.\  5741--5752, 2020.

\bibitem[van~den Berg et~al.(2020)van~den Berg, Gritsenko, Dehghani,
  S{\o}nderby, and Salimans]{van2020idf++}
van~den Berg, R., Gritsenko, A.~A., Dehghani, M., S{\o}nderby, C.~K., and
  Salimans, T.
\newblock Idf++: Analyzing and improving integer discrete flows for lossless
  compression.
\newblock In \emph{\iclr}, 2020.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock \emph{\nips}, 29:\penalty0 2074--2082, 2016.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Benz, Argaw, Lee, Kim, Rameau,
  Bazin, and Kweon]{zhang2021resnet}
Zhang, C., Benz, P., Argaw, D.~M., Lee, S., Kim, J., Rameau, F., Bazin, J.-C.,
  and Kweon, I.~S.
\newblock Resnet or densenet? introducing dense shortcuts to resnet.
\newblock In \emph{\cvpr}, pp.\  3550--3559, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Kang, Ryder, and
  Li]{zhang2021iflow}
Zhang, S., Kang, N., Ryder, T., and Li, Z.
\newblock iflow: Numerically invertible flows for efficient lossless
  compression via a uniform coder.
\newblock In \emph{\nips}, volume~34, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Zhang, Kang, and
  Li]{zhang2021ivpf}
Zhang, S., Zhang, C., Kang, N., and Li, Z.
\newblock ivpf: Numerical invertible volume preserving flow for efficient
  lossless compression.
\newblock In \emph{\cvpr}, pp.\  620--629, 2021{\natexlab{c}}.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{zhou2016dorefa}
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
