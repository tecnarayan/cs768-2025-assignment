\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem[Arakawa et~al.(2018)Arakawa, Kobayashi, Unno, Tsuboi, and
  Maeda]{arakawa2018dqn}
Riku Arakawa, Sosuke Kobayashi, Yuya Unno, Yuta Tsuboi, and Shin-ichi Maeda.
\newblock Dqn-tamer: Human-in-the-loop reinforcement learning with intractable
  feedback.
\newblock \emph{arXiv preprint arXiv:1810.11748}, 2018.

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{arulkumaran2017deep}
Kai Arulkumaran, Marc~Peter Deisenroth, Miles Brundage, and Anil~Anthony
  Bharath.
\newblock Deep reinforcement learning: A brief survey.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (6):\penalty0
  26--38, 2017.

\bibitem[Arumugam et~al.(2019)Arumugam, Lee, Saskin, and
  Littman]{arumugam2019deep}
Dilip Arumugam, Jun~Ki Lee, Sophie Saskin, and Michael~L Littman.
\newblock Deep reinforcement learning from policy-dependent human feedback.
\newblock \emph{arXiv preprint arXiv:1902.04257}, 2019.

\bibitem[Bobu et~al.(2021)Bobu, Wiggert, Tomlin, and Dragan]{bobu2021feature}
Andreea Bobu, Marius Wiggert, Claire Tomlin, and Anca~D Dragan.
\newblock Feature expansive reward learning: Rethinking human input.
\newblock In \emph{Proceedings of the 2021 ACM/IEEE International Conference on
  Human-Robot Interaction}, pages 216--224, 2021.

\bibitem[Cederborg et~al.(2015)Cederborg, Grover, Isbell, and
  Thomaz]{cederborg2015policy}
Thomas Cederborg, Ishaan Grover, Charles~L Isbell, and Andrea~L Thomaz.
\newblock Policy shaping with human teachers.
\newblock In \emph{Twenty-Fourth International Joint Conference on Artificial
  Intelligence}, 2015.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4299--4307, 2017.

\bibitem[Cui et~al.(2020)Cui, Zhang, Allievi, Stone, Niekum, and
  Knox]{cui2020empathic}
Yuchen Cui, Qiping Zhang, Alessandro Allievi, Peter Stone, Scott Niekum, and
  W~Bradley Knox.
\newblock The empathic framework for task learning from implicit human
  feedback.
\newblock In \emph{Conference on Robot Learning}. PMLR, 2020.

\bibitem[Dietterich(2000)]{dietterich2000hierarchical}
Thomas~G Dietterich.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition.
\newblock \emph{Journal of artificial intelligence research}, 13:\penalty0
  227--303, 2000.

\bibitem[Greydanus et~al.(2017)Greydanus, Koul, Dodge, and
  Fern]{greydanus2017visualizing}
Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern.
\newblock Visualizing and understanding atari agents.
\newblock \emph{arXiv preprint arXiv:1711.00138}, 2017.

\bibitem[Griffith et~al.(2013)Griffith, Subramanian, Scholz, Isbell, and
  Thomaz]{griffith2013policy}
Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles~L Isbell, and
  Andrea~L Thomaz.
\newblock Policy shaping: Integrating human feedback with reinforcement
  learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  2625--2633, 2013.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Kambhampati et~al.(2022)Kambhampati, Sreedharan, Verma, Zha, and
  Guan]{kambhampati2022symbols}
Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, and Lin
  Guan.
\newblock Symbols as a lingua franca for bridging human-ai chasm for
  explainable and advisable ai systems.
\newblock In \emph{AAAI}, 2022.

\bibitem[Kim et~al.(2020)Kim, Ohmura, and Kuniyoshi]{kim2020using}
Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi.
\newblock Using human gaze to improve robustness against irrelevant objects in
  robot manipulation tasks.
\newblock \emph{IEEE Robotics and Automation Letters}, 2020.

\bibitem[Knox and Stone(2009)]{knox2009interactively}
W~Bradley Knox and Peter Stone.
\newblock Interactively shaping agents via human reinforcement: The tamer
  framework.
\newblock In \emph{Proceedings of the fifth international conference on
  Knowledge capture}, pages 9--16, 2009.

\bibitem[Knox and Stone(2010)]{knox2010combining}
W~Bradley Knox and Peter Stone.
\newblock Combining manual feedback with subsequent mdp reward signals for
  reinforcement learning.
\newblock In \emph{Proceedings of the 9th International Conference on
  Autonomous Agents and Multiagent Systems: volume 1-Volume 1}, pages 5--12.
  Citeseer, 2010.

\bibitem[Knox and Stone(2012)]{knox2012reinforcement}
W~Bradley Knox and Peter Stone.
\newblock Reinforcement learning from simultaneous human and mdp reward.
\newblock In \emph{AAMAS}, pages 475--482, 2012.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
Ilya Kostrikov, Denis Yarats, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Krening et~al.(2016)Krening, Harrison, Feigh, Isbell, Riedl, and
  Thomaz]{krening2016learning}
Samantha Krening, Brent Harrison, Karen~M Feigh, Charles~Lee Isbell, Mark
  Riedl, and Andrea Thomaz.
\newblock Learning from explanations using sentiment and advice in rl.
\newblock \emph{IEEE Transactions on Cognitive and Developmental Systems},
  9\penalty0 (1):\penalty0 44--55, 2016.

\bibitem[{Laskin} et~al.(2020){Laskin}, {Lee}, {Stooke}, {Pinto}, {Abbeel}, and
  {Srinivas}]{laskin2020reinforcement}
Michael {Laskin}, Kimin {Lee}, Adam {Stooke}, Lerrel {Pinto}, Pieter {Abbeel},
  and Aravind {Srinivas}.
\newblock Reinforcement learning with augmented data.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[{Lee} et~al.(2021){Lee}, {Smith}, and {Abbeel}]{lee2021pebble}
Kimin {Lee}, Laura {Smith}, and Pieter {Abbeel}.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock In \emph{ICML 2021: 38th International Conference on Machine
  Learning}, pages 6152--6163, 2021.

\bibitem[Loftin et~al.(2014)Loftin, Peng, MacGlashan, Littman, Taylor, Huang,
  and Roberts]{loftin2014learning}
Robert Loftin, Bei Peng, James MacGlashan, Michael~L Littman, Matthew~E Taylor,
  Jeff Huang, and David~L Roberts.
\newblock Learning something from nothing: Leveraging implicit human feedback
  strategies.
\newblock In \emph{The 23rd IEEE International Symposium on Robot and Human
  Interactive Communication}, pages 607--612. IEEE, 2014.

\bibitem[Loftin et~al.(2016)Loftin, Peng, MacGlashan, Littman, Taylor, Huang,
  and Roberts]{loftin2016learning}
Robert Loftin, Bei Peng, James MacGlashan, Michael~L Littman, Matthew~E Taylor,
  Jeff Huang, and David~L Roberts.
\newblock Learning behaviors via human-delivered discrete feedback: modeling
  implicit feedback strategies to speed up learning.
\newblock \emph{Autonomous agents and multi-agent systems}, 30\penalty0
  (1):\penalty0 30--59, 2016.

\bibitem[MacGlashan et~al.(2017)MacGlashan, Ho, Loftin, Peng, Wang, Roberts,
  Taylor, and Littman]{macglashan2017interactive}
James MacGlashan, Mark~K Ho, Robert Loftin, Bei Peng, Guan Wang, David~L
  Roberts, Matthew~E Taylor, and Michael~L Littman.
\newblock Interactive learning from policy-dependent human feedback.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2285--2294. JMLR. org, 2017.

\bibitem[Mitrovic et~al.(2020)Mitrovic, McWilliams, Walker, Buesing, and
  Blundell]{mitrovic2020representation}
Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles
  Blundell.
\newblock Representation learning via invariant causal mechanisms.
\newblock \emph{arXiv preprint arXiv:2010.07922}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Andrew~Y Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{ICML}, volume~99, pages 278--287, 1999.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Andrew~Y Ng, Stuart~J Russell, et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, page~2, 2000.

\bibitem[Nikulin et~al.(2019)Nikulin, Ianina, Aliev, and
  Nikolenko]{nikulin2019free}
Dmitry Nikulin, Anastasia Ianina, Vladimir Aliev, and Sergey Nikolenko.
\newblock Free-lunch saliency via attention in atari agents.
\newblock \emph{arXiv preprint arXiv:1908.02511}, 2019.

\bibitem[Piot et~al.(2014)Piot, Geist, and Pietquin]{piot2014boosted}
Bilal Piot, Matthieu Geist, and Olivier Pietquin.
\newblock Boosted bellman residual minimization handling expert demonstrations.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 549--564. Springer, 2014.

\bibitem[Purushwalkam and Gupta(2020)]{purushwalkam2020demystifying}
Senthil Purushwalkam and Abhinav Gupta.
\newblock Demystifying contrastive self-supervised learning: Invariances,
  augmentations and dataset biases.
\newblock \emph{arXiv preprint arXiv:2007.13916}, 2020.

\bibitem[Raileanu et~al.(2020)Raileanu, Goldstein, Yarats, Kostrikov, and
  Fergus]{raileanu2020automatic}
Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus.
\newblock Automatic data augmentation for generalization in deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.12862}, 2020.

\bibitem[Rieger et~al.(2020)Rieger, Singh, Murdoch, and
  Yu]{rieger2020interpretations}
Laura Rieger, Chandan Singh, William Murdoch, and Bin Yu.
\newblock Interpretations are useful: penalizing explanations to align neural
  networks with prior knowledge.
\newblock In \emph{International Conference on Machine Learning}, pages
  8116--8126. PMLR, 2020.

\bibitem[Ross et~al.(2017)Ross, Hughes, and Doshi-Velez]{ross2017right}
Andrew~Slavin Ross, Michael~C Hughes, and Finale Doshi-Velez.
\newblock Right for the right reasons: Training differentiable models by
  constraining their explanations.
\newblock \emph{arXiv preprint arXiv:1703.03717}, 2017.

\bibitem[Saran et~al.(2020)Saran, Zhang, Short, and
  Niekum]{saran2020efficiently}
Akanksha Saran, Ruohan Zhang, Elaine~Schaertl Short, and Scott Niekum.
\newblock Efficiently guiding imitation learning algorithms with human gaze.
\newblock \emph{arXiv preprint arXiv:2002.12500}, 2020.

\bibitem[Schaal(1997)]{schaal1997learning}
Stefan Schaal.
\newblock Learning from demonstration.
\newblock In \emph{Advances in neural information processing systems}, pages
  1040--1046, 1997.

\bibitem[{Schaul} et~al.(2016){Schaul}, {Quan}, {Antonoglou}, and
  {Silver}]{schaul2016prioritized}
Tom {Schaul}, John {Quan}, Ioannis {Antonoglou}, and David {Silver}.
\newblock Prioritized experience replay.
\newblock In \emph{ICLR 2016 : International Conference on Learning
  Representations 2016}, 2016.

\bibitem[Schramowski et~al.(2020)Schramowski, Stammer, Teso, Brugger, Herbert,
  Shao, Luigs, Mahlein, and Kersting]{schramowski2020making}
Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska
  Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian
  Kersting.
\newblock Making deep neural networks right for the right scientific reasons by
  interacting with their explanations.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (8):\penalty0
  476--486, 2020.

\bibitem[Shorten and Khoshgoftaar(2019)]{shorten2019survey}
Connor Shorten and Taghi~M Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 6\penalty0 (1):\penalty0 60, 2019.

\bibitem[{Such} et~al.(2019){Such}, {Madhavan}, {Liu}, {Wang}, {Castro}, {Li},
  {Zhi}, {Schubert}, {Bellemare}, {Clune}, and {Lehman}]{such2019an}
Felipe~Petroski {Such}, Vashisht {Madhavan}, Rosanne {Liu}, Rui {Wang},
  Pablo~Samuel {Castro}, Yulun {Li}, Jiale {Zhi}, Ludwig {Schubert}, Marc~G.
  {Bellemare}, Jeff {Clune}, and Joel {Lehman}.
\newblock An atari model zoo for analyzing, visualizing, and comparing deep
  reinforcement learning agents.
\newblock In \emph{Proceedings of the Twenty-Eighth International Joint
  Conference on Artificial Intelligence}, pages 3260--3267, 2019.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Teso and Kersting(2019)]{teso2019explanatory}
Stefano Teso and Kristian Kersting.
\newblock Explanatory interactive machine learning.
\newblock In \emph{Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 239--245, 2019.

\bibitem[Thomaz et~al.(2006)Thomaz, Breazeal, et~al.]{thomaz2006reinforcement}
Andrea~Lockerd Thomaz, Cynthia Breazeal, et~al.
\newblock Reinforcement learning with human teachers: Evidence of feedback and
  guidance with implications for learning performance.
\newblock In \emph{Aaai}, volume~6, pages 1000--1005. Boston, MA, 2006.

\bibitem[Warnell et~al.(2018)Warnell, Waytowich, Lawhern, and
  Stone]{warnell2018deep}
Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone.
\newblock Deep tamer: Interactive agent shaping in high-dimensional state
  spaces.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{\"u}rnkranz]{wirth2017survey}
Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F{\"u}rnkranz.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4945--4990, 2017.

\bibitem[Xiao et~al.(2020)Xiao, Lu, Ramasubramanian, Clark, Bushnell, and
  Poovendran]{xiao2020fresh}
Baicen Xiao, Qifan Lu, Bhaskar Ramasubramanian, Andrew Clark, Linda Bushnell,
  and Radha Poovendran.
\newblock Fresh: Interactive reward shaping in high-dimensional state spaces
  using human feedback.
\newblock \emph{arXiv preprint arXiv:2001.06781}, 2020.

\bibitem[Yeh et~al.(2018)Yeh, Gervasio, Sanchez, Crossley, and
  Myers]{yeh2018bridging}
Eric Yeh, Melinda Gervasio, Daniel Sanchez, Matthew Crossley, and Karen Myers.
\newblock Bridging the gap: Converting human advice into imagined examples.
\newblock \emph{Advances in Cognitive Systems}, 6:\penalty0 1168--1176, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Saran, Liu, Zhu, Guo, Niekum, Ballard, and
  Hayhoe]{zhang2020human}
R~Zhang, A~Saran, B~Liu, Y~Zhu, S~Guo, S~Niekum, D~Ballard, and M~Hayhoe.
\newblock Human gaze assisted artificial intelligence: A review.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2020.

\bibitem[Zhang et~al.(2018)Zhang, Liu, Zhang, Whritner, Muller, Hayhoe, and
  Ballard]{zhang2018agil}
Ruohan Zhang, Zhuode Liu, Luxin Zhang, Jake~A Whritner, Karl~S Muller, Mary~M
  Hayhoe, and Dana~H Ballard.
\newblock Agil: Learning attention from human for visuomotor tasks.
\newblock In \emph{Proceedings of the european conference on computer vision
  (eccv)}, pages 663--679, 2018.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Torabi, Guan, Ballard, and
  Stone]{ijcai2019-884}
Ruohan Zhang, Faraz Torabi, Lin Guan, Dana~H. Ballard, and Peter Stone.
\newblock Leveraging human guidance for deep reinforcement learning tasks.
\newblock In \emph{Proceedings of the Twenty-Eighth International Joint
  Conference on Artificial Intelligence, {IJCAI-19}}, pages 6339--6346.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2019{\natexlab{a}}.
\newblock \doi{10.24963/ijcaai.2019/884}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Walshe, Liu, Guan, Muller,
  Whritner, Zhang, Hayhoe, and Ballard]{zhang2019atari}
Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl~S Muller, Jake~A
  Whritner, Luxin Zhang, Mary~M Hayhoe, and Dana~H Ballard.
\newblock Atari-head: Atari human eye-tracking and demonstration dataset.
\newblock \emph{arXiv preprint arXiv:1903.06754}, 2019{\natexlab{b}}.

\end{thebibliography}
