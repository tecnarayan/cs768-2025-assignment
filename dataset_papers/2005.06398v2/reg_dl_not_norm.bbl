\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2011)Acar, Dunlavy, Kolda, and M{\o}rup]{acar2011scalable}
Evrim Acar, Daniel~M Dunlavy, Tamara~G Kolda, and Morten M{\o}rup.
\newblock Scalable tensor factorizations for incomplete data.
\newblock \emph{Chemometrics and Intelligent Laboratory Systems}, 106\penalty0
  (1):\penalty0 41--56, 2011.

\bibitem[Advani and Saxe(2017)]{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Alnur Ali, Edgar Dobriban, and Ryan~J Tibshirani.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Anandkumar et~al.(2014)Anandkumar, Ge, Hsu, Kakade, and
  Telgarsky]{anandkumar2014tensor}
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham~M Kakade, and Matus Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 2773--2832,
  2014.

\bibitem[Arora et~al.(2020)Arora, Bartlett, Mianjy, and
  Srebro]{arora2020dropout}
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro.
\newblock Dropout: Explicit forms and capacity control.
\newblock \emph{arXiv preprint arXiv:2003.03397}, 2020.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  244--253, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora2019convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Cohen, Hu, and
  Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 7413--7424, 2019{\natexlab{b}}.

\bibitem[Balda et~al.(2018)Balda, Behboodi, and Mathar]{balda2018tensor}
Emilio~Rafael Balda, Arash Behboodi, and Rudolf Mathar.
\newblock A tensor analysis on dense connectivity via convolutional arithmetic
  circuits.
\newblock 2018.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and
  Long]{bartlett2018gradient}
Peter Bartlett, Dave Helmbold, and Phil Long.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  520--529, 2018.

\bibitem[Belabbas(2020)]{belabbas2020implicit}
Mohamed~Ali Belabbas.
\newblock On implicit regularization: Morse functions and applications to
  matrix factorization.
\newblock \emph{arXiv preprint arXiv:2001.04264}, 2020.

\bibitem[Brutzkus and Globerson(2020)]{brutzkus2020inductive}
Alon Brutzkus and Amir Globerson.
\newblock On the inductive bias of a cnn for orthogonal patterns distributions.
\newblock \emph{arXiv preprint arXiv:2002.09781}, 2020.

\bibitem[Burer and Monteiro(2003)]{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock \emph{Mathematical Programming}, 95\penalty0 (2):\penalty0 329--357,
  2003.

\bibitem[Cai et~al.(2019)Cai, Li, Poor, and Chen]{cai2019nonconvex}
Changxiao Cai, Gen Li, H~Vincent Poor, and Yuxin Chen.
\newblock Nonconvex low-rank tensor completion from noisy data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1863--1874, 2019.

\bibitem[Cand{\`e}s and Recht(2009)]{candes2009exact}
Emmanuel~J Cand{\`e}s and Benjamin Recht.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717, 2009.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi2019nonconvex}
Yuejie Chi, Yue~M Lu, and Yuxin Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (20):\penalty0 5239--5269, 2019.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 1305--1338,
  2020.

\bibitem[Cohen and Shashua(2014)]{cohen2014simnets}
Nadav Cohen and Amnon Shashua.
\newblock Simnets: A generalization of convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS),
  Deep Learning Workshop}, 2014.

\bibitem[Cohen and Shashua(2016)]{cohen2016convolutional}
Nadav Cohen and Amnon Shashua.
\newblock Convolutional rectifier networks as generalized tensor
  decompositions.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Cohen and Shashua(2017)]{cohen2017inductive}
Nadav Cohen and Amnon Shashua.
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Cohen et~al.(2016{\natexlab{a}})Cohen, Sharir, and
  Shashua]{cohen2016deep}
Nadav Cohen, Or~Sharir, and Amnon Shashua.
\newblock Deep simnets.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016{\natexlab{a}}.

\bibitem[Cohen et~al.(2016{\natexlab{b}})Cohen, Sharir, and
  Shashua]{cohen2016expressive}
Nadav Cohen, Or~Sharir, and Amnon Shashua.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock \emph{Conference On Learning Theory (COLT)}, 2016{\natexlab{b}}.

\bibitem[Cohen et~al.(2017)Cohen, Sharir, Levine, Tamari, Yakira, and
  Shashua]{cohen2017analysis}
Nadav Cohen, Or~Sharir, Yoav Levine, Ronen Tamari, David Yakira, and Amnon
  Shashua.
\newblock Analysis and design of convolutional networks via hierarchical tensor
  decompositions.
\newblock \emph{Intel Collaborative Research Institute for Computational
  Intelligence (ICRI-CI) Special Issue on Deep Learning Theory}, 2017.

\bibitem[Cohen et~al.(2018)Cohen, Tamari, and Shashua]{cohen2018boosting}
Nadav Cohen, Ronen Tamari, and Amnon Shashua.
\newblock Boosting dilated convolutional networks with mixed tensor
  decompositions.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Dauber et~al.(2020)Dauber, Feder, Koren, and Livni]{dauber2020can}
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni.
\newblock Can implicit bias explain generalization? stochastic convex
  optimization as a case study.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Davenport and Romberg(2016)]{davenport2016overview}
Mark~A Davenport and Justin Romberg.
\newblock An overview of low-rank matrix recovery from incomplete observations.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (4):\penalty0 608--622, 2016.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 384--395, 2018.

\bibitem[Geyer et~al.(2020)Geyer, Kyrillidis, and Kalev]{geyer2020uniqueness}
Kelly Geyer, Anastasios Kyrillidis, and Amir Kalev.
\newblock Low-rank regularization and solution uniqueness in over-parameterized
  matrix sensing.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, pages 930--940, 2020.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3196--3206, 2019.

\bibitem[Gissin et~al.(2020)Gissin, Shalev-Shwartz, and
  Daniely]{gissin2020implicit}
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov{\'a}]{goldt2019dynamics}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6979--6989, 2019.

\bibitem[Golub and Van~Loan(2012)]{golub2012matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock \emph{Matrix computations}, volume~3.
\newblock JHU press, 2012.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, volume~80, pages 1832--1841, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9461--9471, 2018{\natexlab{b}}.

\bibitem[Hackbusch(2012)]{hackbusch2012tensor}
Wolfgang Hackbusch.
\newblock \emph{Tensor spaces and numerical tensor calculus}, volume~42.
\newblock Springer, 2012.

\bibitem[Hardt and Ma(2016)]{hardt2016identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[H{\aa}stad(1990)]{haastad1990tensor}
Johan H{\aa}stad.
\newblock Tensor rank is np-complete.
\newblock \emph{Journal of algorithms (Print)}, 11\penalty0 (4):\penalty0
  644--654, 1990.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Ilyashenko and Yakovenko(2008)]{ilyashenko2008lectures}
Yulij Ilyashenko and Sergei Yakovenko.
\newblock \emph{Lectures on analytic differential equations}, volume~86.
\newblock American Mathematical Soc., 2008.

\bibitem[Ipsen and Rehman(2008)]{ipsen2008perturbation}
Ilse~CF Ipsen and Rizwana Rehman.
\newblock Perturbation bounds for determinants and characteristic polynomials.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30\penalty0
  (2):\penalty0 762--776, 2008.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 8571--8580, 2018.

\bibitem[Jain and Oh(2014)]{jain2014provable}
Prateek Jain and Sewoong Oh.
\newblock Provable tensor factorization with missing data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1431--1439, 2014.

\bibitem[Ji and Telgarsky(2019{\natexlab{a}})]{ji2019gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019{\natexlab{a}}.

\bibitem[Ji and Telgarsky(2019{\natexlab{b}})]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 1772--1798,
  2019{\natexlab{b}}.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang,
  Barak, and Zhang]{kalimeris2019sgd}
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan
  Yang, Boaz Barak, and Haofeng Zhang.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3491--3501, 2019.

\bibitem[Karlsson et~al.(2016)Karlsson, Kressner, and
  Uschmajew]{karlsson2016parallel}
Lars Karlsson, Daniel Kressner, and Andr{\'e} Uschmajew.
\newblock Parallel algorithms for tensor completion in the cp format.
\newblock \emph{Parallel Computing}, 57:\penalty0 222--234, 2016.

\bibitem[Kato(2013)]{kato2013perturbation}
Tosio Kato.
\newblock \emph{Perturbation theory for linear operators}, volume 132.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Kolda and Bader(2009)]{kolda2009tensor}
Tamara~G Kolda and Brett~W Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Krantz and Parks(2002)]{krantz2002primer}
Steven~G Krantz and Harold~R Parks.
\newblock \emph{A primer of real analytic functions}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Lampinen and Ganguli(2019)]{lampinen2019analytic}
Andrew~K Lampinen and Surya Ganguli.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Levine et~al.(2018)Levine, Yakira, Cohen, and Shashua]{levine2018deep}
Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua.
\newblock Deep learning and quantum entanglement: Fundamental connections with
  implications to network design.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Levine et~al.(2019)Levine, Sharir, Cohen, and
  Shashua]{levine2019quantum}
Yoav Levine, Or~Sharir, Nadav Cohen, and Amnon Shashua.
\newblock Quantum entanglement in deep learning architectures.
\newblock \emph{To appear in Physical Review Letters}, 2019.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory
  (COLT)}, pages 2--47, 2018.

\bibitem[Lyu and Li(2020)]{lyu2020gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Ma et~al.(2018)Ma, Wang, Chi, and Chen]{ma2018implicit}
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  3351--3360, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 2388--2464,
  2019.

\bibitem[Mulayoff and Michaeli(2020)]{mulayoff2020unique}
Rotem Mulayoff and Tomer Michaeli.
\newblock Unique properties of wide minima in deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Nacson et~al.(2019{\natexlab{a}})Nacson, Gunasekar, Lee, Srebro, and
  Soudry]{nacson2019lexicographic}
Mor~Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel
  Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and
  non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  4683--4692, 2019{\natexlab{a}}.

\bibitem[Nacson et~al.(2019{\natexlab{b}})Nacson, Lee, Gunasekar, Savarese,
  Srebro, and Soudry]{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{Proceedings of Machine Learning Research}, volume~89, pages
  3420--3428, 2019{\natexlab{b}}.

\bibitem[Narita et~al.(2012)Narita, Hayashi, Tomioka, and
  Kashima]{narita2012tensor}
Atsuhiro Narita, Kohei Hayashi, Ryota Tomioka, and Hisashi Kashima.
\newblock Tensor factorization using auxiliary information.
\newblock \emph{Data Mining and Knowledge Discovery}, 25\penalty0 (2):\penalty0
  298--324, 2012.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Behnam Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock \emph{PhD thesis}, 2017.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5947--5956, 2017.

\bibitem[Oymak and Soltanolkotabi(2019)]{oymak2019overparameterized}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  4951--4960, 2019.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Powers and St{\o}rmer(1970)]{powers1970free}
Robert~T Powers and Erling St{\o}rmer.
\newblock Free states of the canonical anticommutation relations.
\newblock \emph{Communications in Mathematical Physics}, 16\penalty0
  (1):\penalty0 1--33, 1970.

\bibitem[Radhakrishnan et~al.(2020)Radhakrishnan, Nichani, Bernstein, and
  Uhler]{radhakrishnan2020balancedness}
Adityanarayanan Radhakrishnan, Eshaan Nichani, Daniel Bernstein, and Caroline
  Uhler.
\newblock Balancedness and alignment are unlikely in linear neural networks.
\newblock \emph{arXiv preprint arXiv:2003.06340}, 2020.

\bibitem[Rahaman et~al.(2019)Rahaman, Arpit, Baratin, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2018spectral}
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred~A
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  5301--5310, 2019.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Recht et~al.(2011)Recht, Xu, and Hassibi]{recht2011null}
Benjamin Recht, Weiyu Xu, and Babak Hassibi.
\newblock Null space conditions and thresholds for rank minimization.
\newblock \emph{Mathematical programming}, 127\penalty0 (1):\penalty0 175--202,
  2011.

\bibitem[Roy and Vetterli(2007)]{roy2007effective}
Olivier Roy and Martin Vetterli.
\newblock The effective rank: A measure of effective dimensionality.
\newblock In \emph{2007 15th European Signal Processing Conference}, pages
  606--610. IEEE, 2007.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2014.

\bibitem[Shah et~al.(2018)Shah, Kyrillidis, and Sanghavi]{shah2018minimum}
Vatsal Shah, Anastasios Kyrillidis, and Sujay Sanghavi.
\newblock Minimum weight norm models do not always generalize well for
  over-parameterized problems.
\newblock \emph{arXiv preprint arXiv:1811.07055}, 2018.

\bibitem[Sharir and Shashua(2018)]{sharir2018expressive}
Or~Sharir and Amnon Shashua.
\newblock On the expressive power of overlapping architectures of deep
  learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Sharir et~al.(2016)Sharir, Tamari, Cohen, and
  Shashua]{sharir2016tensorial}
Or~Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua.
\newblock Tensorial mixture models.
\newblock \emph{arXiv preprint}, 2016.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 10608--10619, 2018.

\bibitem[Teschl(2012)]{teschl2012ordinary}
Gerald Teschl.
\newblock \emph{Ordinary differential equations and dynamical systems}, volume
  140.
\newblock American Mathematical Soc., 2012.

\bibitem[Tu et~al.(2016)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2016low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  964--973, 2016.

\bibitem[Wei et~al.(2020)Wei, Kakade, and Ma]{wei2020implicit}
Colin Wei, Sham Kakade, and Tengyu Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 3635--3673,
  2020.

\bibitem[Wu et~al.(2019)Wu, Dobriban, Ren, Wu, Li, Gunasekar, Ward, and
  Liu]{wu2019implicit}
Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya
  Gunasekar, Rachel Ward, and Qiang Liu.
\newblock Implicit regularization of normalization methods.
\newblock \emph{arXiv preprint arXiv:1911.07956}, 2019.

\bibitem[Xia and Yuan(2017)]{xia2017polynomial}
Dong Xia and Ming Yuan.
\newblock On polynomial time methods for exact low rank tensor completion.
\newblock \emph{arXiv preprint arXiv:1702.06980}, 2017.

\bibitem[Yokota et~al.(2016)Yokota, Zhao, and Cichocki]{yokota2016smooth}
Tatsuya Yokota, Qibin Zhao, and Andrzej Cichocki.
\newblock Smooth parafac decomposition for tensor completion.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (20):\penalty0 5423--5436, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Zhou et~al.(2017)Zhou, Lu, Lin, and Zhang]{zhou2017tensor}
Pan Zhou, Canyi Lu, Zhouchen Lin, and Chao Zhang.
\newblock Tensor factorization for low-rank tensor completion.
\newblock \emph{IEEE Transactions on Image Processing}, 27\penalty0
  (3):\penalty0 1152--1163, 2017.

\end{thebibliography}
