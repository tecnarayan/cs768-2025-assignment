\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{http://tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Adigun \& Kosko(2019)Adigun and Kosko]{adigun2019bidirectional}
Adigun, O. and Kosko, B.
\newblock Bidirectional backpropagation.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  50\penalty0 (5):\penalty0 1982--1994, 2019.

\bibitem[Ahmad et~al.(2020)Ahmad, van Gerven, and Ambrogioni]{ahmad2020gait}
Ahmad, N., van Gerven, M. A.~J., and Ambrogioni, L.
\newblock {GAIT}-prop: {A} biologically plausible learning rule derived from
  backpropagation of error.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
Akrout, M., Wilson, C., Humphreys, P.~C., Lillicrap, T.~P., and Tweed, D.~B.
\newblock Deep learning without weight transport.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  974--982, 2019.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Colmenarejo, Hoffman,
  Pfau, Schaul, and de~Freitas]{andrychowicz2016learning}
Andrychowicz, M., Denil, M., Colmenarejo, S.~G., Hoffman, M.~W., Pfau, D.,
  Schaul, T., and de~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In Lee, D.~D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pp.\  3981--3989, 2016.

\bibitem[Belilovsky et~al.(2019)Belilovsky, Eickenberg, and
  Oyallon]{belilovsky2019greedy}
Belilovsky, E., Eickenberg, M., and Oyallon, E.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pp.\  583--593. {PMLR}, 2019.

\bibitem[Bengio et~al.(1995)Bengio, Bengio, and Cloutier]{bengio1995search}
Bengio, S., Bengio, Y., and Cloutier, J.
\newblock On the search for new learning rules for {ANNs}.
\newblock \emph{Neural Process. Lett.}, 2\penalty0 (4):\penalty0 26--30, 1995.
\newblock \doi{10.1007/BF02279935}.

\bibitem[Bengio(2014)]{bengio2014autoencoders}
Bengio, Y.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock \emph{CoRR}, abs/1407.7906, 2014.

\bibitem[Bengio et~al.(2006)Bengio, Lamblin, Popovici, and
  Larochelle]{bengio2006greedy}
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
\newblock Greedy layer-wise training of deep networks.
\newblock In Sch{\"{o}}lkopf, B., Platt, J.~C., and Hofmann, T. (eds.),
  \emph{Advances in Neural Information Processing Systems 19, Proceedings of
  the Twentieth Annual Conference on Neural Information Processing Systems,
  Vancouver, British Columbia, Canada, December 4-7, 2006}, pp.\  153--160.
  {MIT} Press, 2006.

\bibitem[Bengio et~al.(2015)Bengio, Lee, Bornschein, and
  Lin]{bengio2015towards}
Bengio, Y., Lee, D., Bornschein, J., and Lin, Z.
\newblock Towards biologically plausible deep learning.
\newblock \emph{CoRR}, abs/1502.04156, 2015.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Camp et~al.(2020)Camp, Mandivarapu, and Estrada]{camp2020continual}
Camp, B., Mandivarapu, J.~K., and Estrada, R.
\newblock Continual learning with deep artificial neurons.
\newblock \emph{CoRR}, abs/2011.07035, 2020.

\bibitem[Carandini \& Heeger(2012)Carandini and
  Heeger]{carandini2012normalization}
Carandini, M. and Heeger, D.~J.
\newblock Normalization as a canonical neural computation.
\newblock \emph{Nature Reviews Neuroscience}, 13\penalty0 (1):\penalty0 51--62,
  2012.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and van Schaik]{emnist}
Cohen, G., Afshar, S., Tapson, J., and van Schaik, A.
\newblock {EMNIST:} an extension of {MNIST} to handwritten letters.
\newblock \emph{CoRR}, abs/1702.05373, 2017.
\newblock URL \url{http://arxiv.org/abs/1702.05373}.

\bibitem[Confavreux et~al.(2020)Confavreux, Zenke, Agnes, Lillicrap, and
  Vogels]{confavreux2020approach}
Confavreux, B., Zenke, F., Agnes, E.~J., Lillicrap, T.~P., and Vogels, T.~P.
\newblock A meta-learning approach to (re)discover plasticity rules that carve
  a desired function into a neural network.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{NeurIPS}, 2020.

\bibitem[Conley(1988)]{conley1988gradient}
Conley, C.
\newblock The gradient structure of a flow: {I}.
\newblock \emph{Ergodic Theory and Dynamical Systems}, 8\penalty0
  (8*):\penalty0 11--26, 1988.

\bibitem[Conley(1978)]{conley1978isolated}
Conley, C.~C.
\newblock \emph{Isolated invariant sets and the Morse index}.
\newblock American Mathematical Soc., 1978.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{adagrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12\penalty0 (null):\penalty0
  2121â€“2159, July 2011.
\newblock ISSN 1532-4435.

\bibitem[Farber et~al.(2003)Farber, Kappeler, Latschev, and
  Zehnder]{farber2003smooth}
Farber, M., Kappeler, T., Latschev, J., and Zehnder, E.
\newblock Smooth lyapunov 1-forms.
\newblock \emph{arXiv preprint math/0304137}, 2003.

\bibitem[Franks(2017)]{franks2017notes}
Franks, J.
\newblock Notes on chain recurrence and lyapunonv functions.
\newblock \emph{arXiv preprint arXiv:1704.07264}, 2017.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}
Ha, D., Dai, A., and Le, Q.~V.
\newblock Hypernetworks.
\newblock \emph{arXiv preprint arXiv:1609.09106}, 2016.

\bibitem[{Hansen} \& {Ostermeier}(1996){Hansen} and {Ostermeier}]{cma-es}
{Hansen}, N. and {Ostermeier}, A.
\newblock Adapting arbitrary normal mutation distributions in evolution
  strategies: the covariance matrix adaptation.
\newblock In \emph{Proceedings of IEEE International Conference on Evolutionary
  Computation}, pp.\  312--317, 1996.
\newblock \doi{10.1109/ICEC.1996.542381}.

\bibitem[Hansen et~al.(2019)Hansen, Akimoto, and Baudis]{hansen2019pycma}
Hansen, N., Akimoto, Y., and Baudis, P.
\newblock {CMA-ES/pycma} on {G}ithub.
\newblock Zenodo, DOI:10.5281/zenodo.2559634, February 2019.
\newblock URL \url{https://doi.org/10.5281/zenodo.2559634}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{ResNet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015.

\bibitem[Hebb(1949)]{hebb1949organization}
Hebb, D.~O.
\newblock \emph{The organization of behavior: a neuropsychological theory}.
\newblock Science editions, 1949.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[Kaplanis et~al.(2018)Kaplanis, Shanahan, and
  Clopath]{kaplanis2018continual}
Kaplanis, C., Shanahan, M., and Clopath, C.
\newblock Continual reinforcement learning with complex synapses.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2502--2511. {PMLR},
  2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014AdamAM}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.

\bibitem[Kirsch \& Schmidhuber(2020)Kirsch and Schmidhuber]{kirsch2020meta}
Kirsch, L. and Schmidhuber, J.
\newblock Meta learning backpropagation and improving it.
\newblock \emph{arXiv preprint arXiv:2012.14905}, 2020.

\bibitem[{Lecun} et~al.(1998){Lecun}, {Bottou}, {Bengio}, and {Haffner}]{mnist}
{Lecun}, Y., {Bottou}, L., {Bengio}, Y., and {Haffner}, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee2015difference}
Lee, D., Zhang, S., Fischer, A., and Bengio, Y.
\newblock Difference target propagation.
\newblock In Appice, A., Rodrigues, P.~P., Costa, V.~S., Soares, C., Gama, J.,
  and Jorge, A. (eds.), \emph{Machine Learning and Knowledge Discovery in
  Databases - European Conference, {ECML} {PKDD} 2015, Porto, Portugal,
  September 7-11, 2015, Proceedings, Part {I}}, volume 9284 of \emph{Lecture
  Notes in Computer Science}, pp.\  498--515. Springer, 2015.
\newblock \doi{10.1007/978-3-319-23528-8\_31}.

\bibitem[Lee(2013)]{lee2013smooth}
Lee, J.~M.
\newblock \emph{Introduction to Smooth Manifolds}.
\newblock Springer, 2013.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Lillicrap, T.~P., Cownden, D., Tweed, D.~B., and Akerman, C.~J.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature communications}, 7\penalty0 (1):\penalty0 1--10, 2016.

\bibitem[Lindsey \& Litwin{-}Kumar(2020)Lindsey and
  Litwin{-}Kumar]{lindsey2020learning}
Lindsey, J. and Litwin{-}Kumar, A.
\newblock Learning to learn with feedback and local plasticity.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[L{\"{o}}we et~al.(2019)L{\"{o}}we, O'Connor, and
  Veeling]{lowe2019putting}
L{\"{o}}we, S., O'Connor, P., and Veeling, B.~S.
\newblock Putting an end to end-to-end: Gradient-isolated learning of
  representations.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  3033--3045, 2019.

\bibitem[Maheswaranathan et~al.(2020)Maheswaranathan, Sussillo, Metz, Sun, and
  Sohl{-}Dickstein]{maheswaranathan2020reverse}
Maheswaranathan, N., Sussillo, D., Metz, L., Sun, R., and Sohl{-}Dickstein, J.
\newblock Reverse engineering learned optimizers reveals known and novel
  mechanisms.
\newblock \emph{CoRR}, abs/2011.02159, 2020.

\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Nixon, Freeman, and
  Sohl-Dickstein]{metz2019understanding}
Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., and Sohl-Dickstein, J.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4556--4565. PMLR, 2019.

\bibitem[Metz et~al.(2020)Metz, Maheswaranathan, Freeman, Poole, and
  Sohl-Dickstein]{metz2020tasks}
Metz, L., Maheswaranathan, N., Freeman, C.~D., Poole, B., and Sohl-Dickstein,
  J.
\newblock Tasks, stability, architecture, and compute: Training more effective
  learned optimizers, and using them to train themselves.
\newblock \emph{arXiv preprint arXiv:2009.11243}, 2020.

\bibitem[Miconi et~al.(2018)Miconi, Stanley, and
  Clune]{miconi2018differentiable}
Miconi, T., Stanley, K.~O., and Clune, J.
\newblock Differentiable plasticity: training plastic neural networks with
  backpropagation.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3556--3565. {PMLR},
  2018.

\bibitem[Miconi et~al.(2019)Miconi, Rawal, Clune, and
  Stanley]{miconi2019backpropamine}
Miconi, T., Rawal, A., Clune, J., and Stanley, K.~O.
\newblock Backpropamine: training self-modifying neural networks with
  differentiable neuromodulated plasticity.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Minsky \& Papert(1969)Minsky and Papert]{minsky69perceptrons}
Minsky, M. and Papert, S.
\newblock \emph{Perceptrons: An Introduction to Computational Geometry}.
\newblock MIT Press, Cambridge, MA, USA, 1969.

\bibitem[Munkhdalai et~al.(2019)Munkhdalai, Sordoni, Wang, and
  Trischler]{munkhdalai2019metalearned}
Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A.
\newblock Metalearned neural memory.
\newblock \emph{arXiv preprint arXiv:1907.09720}, 2019.

\bibitem[Najarro \& Risi(2020)Najarro and Risi]{najarro2020meta}
Najarro, E. and Risi, S.
\newblock Meta-learning through hebbian plasticity in random networks.
\newblock \emph{arXiv preprint arXiv:2007.02686}, 2020.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
N{\o}kland, A.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In Lee, D.~D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pp.\  1037--1045, 2016.

\bibitem[Norouzzadeh \& Clune(2016)Norouzzadeh and
  Clune]{norouzzadeh2016neuromodulation}
Norouzzadeh, M.~S. and Clune, J.
\newblock Neuromodulation improves the evolution of forward models.
\newblock In \emph{Proceedings of the Genetic and Evolutionary Computation
  Conference 2016}, pp.\  157--164, 2016.

\bibitem[Oja(1982)]{oja1982simplified}
Oja, E.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of mathematical biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Pontes-Filho \& Liwicki(2019)Pontes-Filho and
  Liwicki]{pontes2019bidirectional}
Pontes-Filho, S. and Liwicki, M.
\newblock Bidirectional learning for robust neural networks.
\newblock In \emph{2019 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--8. IEEE, 2019.

\bibitem[Randazzo et~al.(2020)Randazzo, Niklasson, and
  Mordvintsev]{randazzo2020mplp}
Randazzo, E., Niklasson, E., and Mordvintsev, A.
\newblock Mplp: Learning a message passing learning protocol.
\newblock \emph{arXiv preprint arXiv:2007.00970}, 2020.

\bibitem[Ranganathan \& Lewandowski(2020)Ranganathan and
  Lewandowski]{ranganathan2020zorb}
Ranganathan, V. and Lewandowski, A.
\newblock {ZORB:} {A} derivative-free backpropagation algorithm for neural
  networks.
\newblock \emph{CoRR}, abs/2011.08895, 2020.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{ravi2017optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[Real et~al.(2020)Real, Liang, So, and Le]{real2020automl}
Real, E., Liang, C., So, D., and Le, Q.
\newblock Automl-zero: evolving machine learning algorithms from scratch.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8007--8019. PMLR, 2020.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{SGD-robbins1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{Ann. Math. Statist.}, 22\penalty0 (3):\penalty0 400--407, 09
  1951.
\newblock \doi{10.1214/aoms/1177729586}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177729586}.

\bibitem[Rosenblatt(1957)]{rosenblatt1957perceptron}
Rosenblatt, F.
\newblock The perceptron: {A}~perceiving and recognizing automaton.
\newblock Report 85-460-1, Project PARA, Cornell Aeronautical Laboratory,
  Ithaca, New York, January 1957.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
Schmidhuber, J.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Schmidt et~al.(2020)Schmidt, Schneider, and
  Hennig]{schmidt2020descending}
Schmidt, R.~M., Schneider, F., and Hennig, P.
\newblock Descending through a crowded valley - benchmarking deep learning
  optimizers.
\newblock \emph{CoRR}, abs/2007.01547, 2020.

\bibitem[Soltoggio et~al.(2018)Soltoggio, Stanley, and Risi]{soltoggio2018born}
Soltoggio, A., Stanley, K.~O., and Risi, S.
\newblock Born to learn: the inspiration, progress, and future of evolved
  plastic artificial neural networks.
\newblock \emph{Neural Networks}, 108:\penalty0 48--67, 2018.

\bibitem[Taylor et~al.(2016)Taylor, Burmeister, Xu, Singh, Patel, and
  Goldstein]{taylor2016training}
Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A.~B., and Goldstein, T.
\newblock Training neural networks without gradients: {A} scalable {ADMM}
  approach.
\newblock In Balcan, M. and Weinberger, K.~Q. (eds.), \emph{Proceedings of the
  33nd International Conference on Machine Learning, {ICML} 2016, New York
  City, NY, USA, June 19-24, 2016}, volume~48 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pp.\  2722--2731. JMLR.org, 2016.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and Lempitsky]{instancenorm}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.~S.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{CoRR}, abs/1607.08022, 2016.
\newblock URL \url{http://arxiv.org/abs/1607.08022}.

\bibitem[Velez \& Clune(2017)Velez and Clune]{velez2017diffusion}
Velez, R. and Clune, J.
\newblock Diffusion-based neuromodulation can eliminate catastrophic forgetting
  in simple neural networks.
\newblock \emph{PloS one}, 12\penalty0 (11):\penalty0 e0187736, 2017.

\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,
  Colmenarejo, Denil, de~Freitas, and Sohl{-}Dickstein]{wichrowska2017learned}
Wichrowska, O., Maheswaranathan, N., Hoffman, M.~W., Colmenarejo, S.~G., Denil,
  M., de~Freitas, N., and Sohl{-}Dickstein, J.
\newblock Learned optimizers that scale and generalize.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Wilson et~al.(2018)Wilson, Cussat{-}Blanc, Luga, and
  Harrington]{wilson2018neuromodulated}
Wilson, D.~G., Cussat{-}Blanc, S., Luga, H., and Harrington, K.~I.
\newblock Neuromodulated learning in deep neural networks.
\newblock \emph{CoRR}, abs/1812.03365, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{CoRR}, abs/1708.07747, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.07747}.

\bibitem[Xiao et~al.(2018)Xiao, Chen, Liao, and Poggio]{xiao2018biologically}
Xiao, W., Chen, H., Liao, Q., and Poggio, T.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock \emph{arXiv preprint arXiv:1811.03567}, 2018.

\bibitem[Xiong et~al.(2020)Xiong, Ren, and Urtasun]{xiong2020loco}
Xiong, Y., Ren, M., and Urtasun, R.
\newblock {LoCo}: Local contrastive representation learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\end{thebibliography}
