\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balsubramani et~al.(2013)Balsubramani, Dasgupta, and
  Freund]{BalsubramaniDaFr13}
A.~Balsubramani, S.~Dasgupta, and Y.~Freund.
\newblock The fast convergence of incremental pca.
\newblock In \emph{NIPS}, 2013.

\bibitem[Byrd et~al.(2016)Byrd, Hansen, Nocedal, and Singer]{ByrdHaNoSi14}
R.~H. Byrd, S.~Hansen, J.~Nocedal, and Y.~Singer.
\newblock A stochastic quasi-newton method for large-scale optimization.
\newblock \emph{SIAM Journal on Optimization}, 26:\penalty0 1008--1031, 2016.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{CesabianchiLu06}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Cesa-Bianchi et~al.(2005)Cesa-Bianchi, Conconi, and
  Gentile]{CesabianchiCoGe05}
N.~Cesa-Bianchi, A.~Conconi, and C.~Gentile.
\newblock A second-order perceptron algorithm.
\newblock \emph{SIAM Journal on Computing}, 34\penalty0 (3):\penalty0 640--668,
  2005.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DuchiHaSi2011}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{JMLR}, 12:\penalty0 2121--2159, 2011.

\bibitem[Erdogdu and Montanari(2015)]{ErdogduMo15}
M.~A. Erdogdu and A.~Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In \emph{NIPS}, 2015.

\bibitem[Frank and Wolfe(1956)]{FrankWo56}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Gao et~al.(2013)Gao, Jin, Zhu, and Zhou]{GaoJiZhZh13}
W.~Gao, R.~Jin, S.~Zhu, and Z.-H. Zhou.
\newblock One-pass auc optimization.
\newblock In \emph{ICML}, 2013.

\bibitem[Garber and Hazan(2016)]{GarberHa13}
D.~Garber and E.~Hazan.
\newblock A linearly convergent conditional gradient algorithm with
  applications to online and stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 26:\penalty0 1493--1528, 2016.

\bibitem[Garber et~al.(2015)Garber, Hazan, and Ma]{garber2015online}
D.~Garber, E.~Hazan, and T.~Ma.
\newblock Online learning of eigenvectors.
\newblock In \emph{ICML}, 2015.

\bibitem[Ghashami et~al.(2015)Ghashami, Liberty, Phillips, and
  Woodruff]{GhashamiLiPhWo15}
M.~Ghashami, E.~Liberty, J.~M. Phillips, and D.~P. Woodruff.
\newblock Frequent directions: Simple and deterministic matrix sketching.
\newblock \emph{SIAM Journal on Computing}, 45:\penalty0 1762--1792, 2015.

\bibitem[Ghashami et~al.(2016)Ghashami, Liberty, and Phillips]{GhashamiLiPh16}
M.~Ghashami, E.~Liberty, and J.~M. Phillips.
\newblock Efficient frequent directions algorithm for sparse matrices.
\newblock In \emph{KDD}, 2016.

\bibitem[Gonen and Shalev-Shwartz(2015)]{GonenSh15}
A.~Gonen and S.~Shalev-Shwartz.
\newblock Faster sgd using sketched conditioning.
\newblock \emph{arXiv:1506.02649}, 2015.

\bibitem[Gonen et~al.(2016)Gonen, Orabona, and Shalev-Shwartz]{GonenOrSh16}
A.~Gonen, F.~Orabona, and S.~Shalev-Shwartz.
\newblock Solving ridge regression using sketched preconditioned svrg.
\newblock In \emph{ICML}, 2016.

\bibitem[Hardt and Price(2014)]{HardtPr14}
M.~Hardt and E.~Price.
\newblock The noisy power method: A meta algorithm with applications.
\newblock In \emph{NIPS}, 2014.

\bibitem[Hazan and Kale(2012)]{HazanKa12}
E.~Hazan and S.~Kale.
\newblock Projection-free online learning.
\newblock In \emph{ICML}, 2012.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{HazanAgKa07}
E.~Hazan, A.~Agarwal, and S.~Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Jaggi(2013)]{Jaggi13}
M.~Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{ICML}, 2013.

\bibitem[Li et~al.(2015)Li, Lin, and Lu]{LiLiLu15}
C.-L. Li, H.-T. Lin, and C.-J. Lu.
\newblock Rivalry of two families of algorithms for memory-restricted streaming
  pca.
\newblock \emph{arXiv:1506.01490}, 2015.

\bibitem[Liberty(2013)]{Liberty13}
E.~Liberty.
\newblock Simple and deterministic matrix sketching.
\newblock In \emph{KDD}, 2013.

\bibitem[Liu and Nocedal(1989)]{LiuNo89}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1-3):\penalty0
  503--528, 1989.

\bibitem[McMahan and Streeter(2010)]{McMahanSt2010}
H.~B. McMahan and M.~Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{COLT}, 2010.

\bibitem[Mokhtari and Ribeiro(2015)]{MokhtariRi14}
A.~Mokhtari and A.~Ribeiro.
\newblock Global convergence of online limited memory bfgs.
\newblock \emph{JMLR}, 16:\penalty0 3151--3181, 2015.

\bibitem[Moritz et~al.(2016)Moritz, Nishihara, and Jordan]{MoritzNiJo15}
P.~Moritz, R.~Nishihara, and M.~I. Jordan.
\newblock A linearly-convergent stochastic l-bfgs algorithm.
\newblock In \emph{AISTATS}, 2016.

\bibitem[Oja(1982)]{Oja82}
E.~Oja.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of mathematical biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Oja and Karhunen(1985)]{OjaKa85}
E.~Oja and J.~Karhunen.
\newblock On stochastic approximation of the eigenvectors and eigenvalues of
  the expectation of a random matrix.
\newblock \emph{Journal of mathematical analysis and applications},
  106\penalty0 (1):\penalty0 69--84, 1985.

\bibitem[Orabona and P{\'a}l(2015)]{OrabonaPa15}
F.~Orabona and D.~P{\'a}l.
\newblock Scale-free algorithms for online linear optimization.
\newblock In \emph{ALT}, 2015.

\bibitem[Orabona et~al.(2015)Orabona, Crammer, and Cesa-Bianchi]{orabona2015}
F.~Orabona, K.~Crammer, and N.~Cesa-Bianchi.
\newblock A generalized online mirror descent with applications to
  classification and regression.
\newblock \emph{Machine Learning}, 99\penalty0 (3):\penalty0 411--435, 2015.

\bibitem[Pilanci and Wainwright(2015)]{PilanciWa15}
M.~Pilanci and M.~J. Wainwright.
\newblock Newton sketch: A linear-time optimization algorithm with
  linear-quadratic convergence.
\newblock \emph{arXiv:1505.02250}, 2015.

\bibitem[Ross et~al.(2013)Ross, Mineiro, and Langford]{RossMiLa13}
S.~Ross, P.~Mineiro, and J.~Langford.
\newblock Normalized online learning.
\newblock In \emph{UAI}, 2013.

\bibitem[Schraudolph et~al.(2007)Schraudolph, Yu, and
  G{\"u}nter]{SchraudolphYuGu07}
N.~N. Schraudolph, J.~Yu, and S.~G{\"u}nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In \emph{AISTATS}, 2007.

\bibitem[Sohl-Dickstein et~al.(2014)Sohl-Dickstein, Poole, and
  Ganguli]{SohldicksteinPoGa14}
J.~Sohl-Dickstein, B.~Poole, and S.~Ganguli.
\newblock Fast large-scale optimization by unifying stochastic gradient and
  quasi-newton methods.
\newblock In \emph{ICML}, 2014.

\bibitem[Woodruff(2014)]{Woodruff14}
D.~P. Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends in Machine Learning}, 10\penalty0
  (1-2):\penalty0 1--157, 2014.

\end{thebibliography}
