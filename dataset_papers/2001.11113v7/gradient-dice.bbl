\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock \emph{Machine Learning}, 1995.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{mid:beck2003}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Boyan(1999)]{boyan1999least}
Boyan, J.~A.
\newblock Least-squares temporal difference learning.
\newblock In \emph{Proceedings of the 16th International Conference on Machine
  Learning}, 1999.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Dai et~al.(2016)Dai, He, Pan, Boots, and Song]{dai2016learning}
Dai, B., He, N., Pan, Y., Boots, B., and Song, L.
\newblock Learning from conditional distributions via dual embeddings.
\newblock \emph{arXiv preprint arXiv:1607.04579}, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Gelada \& Bellemare(2019)Gelada and Bellemare]{gelada2019off}
Gelada, C. and Bellemare, M.~G.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In \emph{Proceedings of the 33rd AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Hallak \& Mannor(2017)Hallak and Mannor]{hallak2017consistent}
Hallak, A. and Mannor, S.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{horn2012matrix}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix analysis (2nd Edition)}.
\newblock Cambridge university press, 2012.

\bibitem[Jiang \& Li(2015)Jiang and Li]{jiang2015doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1511.03722}, 2015.

\bibitem[Lakshminarayanan \& Szepesvari(2018)Lakshminarayanan and
  Szepesvari]{lakshminarayanan2018linear}
Lakshminarayanan, C. and Szepesvari, C.
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In \emph{The 21st International Conference on Artificial Intelligence
  and Statistics}, 2018.

\bibitem[Lin(1992)]{lin1992self}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine Learning}, 1992.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{liu2015finite}
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{UAI}, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Maei(2011)]{maei2011gradient}
Maei, H.~R.
\newblock \emph{Gradient temporal-difference learning algorithms}.
\newblock PhD thesis, University of Alberta, 2011.

\bibitem[Mousavi et~al.(2020)Mousavi, Li, Liu, and Zhou]{mousavi2020blackbox}
Mousavi, A., Li, L., Liu, Q., and Zhou, D.
\newblock Black-box off-policy estimation for infinite-horizon reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock \emph{arXiv preprint arXiv:1906.04733}, 2019.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{nair2010rectified}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning}, 2010.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 2009.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
Nowozin, S., Cseke, B., and Tomioka, R.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  271--279, 2016.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{Proceedings of the 18th International Conference on Machine
  Learning}, 2001.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 1951.

\bibitem[Shapiro et~al.(2014)Shapiro, Dentcheva, and
  Ruszczy{\'n}ski]{shapiro2014lectures}
Shapiro, A., Dentcheva, D., and Ruszczy{\'n}ski, A.
\newblock \emph{Lectures on stochastic programming: modeling and theory}.
\newblock SIAM, 2014.

\bibitem[Sherman \& Morrison(1950)Sherman and Morrison]{sherman1950adjustment}
Sherman, J. and Morrison, W.~J.
\newblock Adjustment of an inverse matrix corresponding to a change in one
  element of a given matrix.
\newblock \emph{The Annals of Mathematical Statistics}, 1950.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 1988.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction (2nd Edition)}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th International Conference on Machine
  Learning}, 2009{\natexlab{a}}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, and
  Szepesv{\'a}ri]{sutton2009convergent}
Sutton, R.~S., Maei, H.~R., and Szepesv{\'a}ri, C.
\newblock A convergent $ o (n) $ temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2009{\natexlab{b}}.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{Proceedings of the 10th International Conference on
  Autonomous Agents and Multiagent Systems}, 2011.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{sutton2016emphatic}
Sutton, R.~S., Mahmood, A.~R., and White, M.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 2016.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and
  Van~Roy]{tsitsiklis1997analysis}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1997.

\bibitem[Uehara \& Jiang(2019)Uehara and Jiang]{uehara2019minimax}
Uehara, M. and Jiang, N.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1910.12809}, 2019.

\bibitem[Wang et~al.(2007)Wang, Bowling, and Schuurmans]{wang2007dual}
Wang, T., Bowling, M., and Schuurmans, D.
\newblock Dual representations for dynamic programming and reinforcement
  learning.
\newblock In \emph{2007 IEEE International Symposium on Approximate Dynamic
  Programming and Reinforcement Learning}, 2007.

\bibitem[Wang et~al.(2008)Wang, Bowling, Schuurmans, and
  Lizotte]{wang2008stable}
Wang, T., Bowling, M., Schuurmans, D., and Lizotte, D.~J.
\newblock Stable dual dynamic programming.
\newblock In \emph{Advances in neural information processing systems}, 2008.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Zhang, R., Dai, B., Li, L., and Schuurmans, D.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, Yao, and
  Whiteson]{zhang2019provably}
Zhang, S., Liu, B., Yao, H., and Whiteson, S.
\newblock Provably convergent two-timescale off-policy actor-critic with
  function approximation.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020{\natexlab{b}}.

\end{thebibliography}
