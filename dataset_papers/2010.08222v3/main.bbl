\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alimisis et~al.(2021)Alimisis, Davies, and
  Alistarh]{alimisis2021communication}
Foivos Alimisis, Peter Davies, and Dan Alistarh.
\newblock Communication-efficient distributed optimization with quantized
  preconditioners.
\newblock \emph{arXiv preprint arXiv:2102.07214}, 2021.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS
  2017)}, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and Cedric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2018)}, volume~31, 2018.

\bibitem[Arjevani and Shamir(2015)]{NIPS2015_5731}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 28 (NIPS
  2015)}, pages 1756--1764, 2015.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi.
\newblock {Qsparse}-local-{SGD}: Distributed {SGD} with quantization,
  sparsification, and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2019)}, volume~32, 2019.

\bibitem[Braverman et~al.(2013)Braverman, Ellen, Oshman, Pitassi, and
  Vaikuntanathan]{braverman2013tight}
Mark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, and Vinod
  Vaikuntanathan.
\newblock A tight bound for set disjointness in the message-passing model.
\newblock In \emph{Proc.\ 54th Annual Symposium on Foundations of Computer
  Science (FOCS 2013)}, pages 668--677. IEEE, 2013.

\bibitem[Braverman et~al.(2016)Braverman, Garg, Ma, Nguyen, and
  Woodruff]{braverman2016communication}
Mark Braverman, Ankit Garg, Tengyu Ma, Huy~L Nguyen, and David~P Woodruff.
\newblock Communication lower bounds for statistical estimation problems via a
  distributed data processing inequality.
\newblock In \emph{Proceedings of the 48th Annual ACM symposium on Theory of
  Computing (STOC 2016)}, pages 1011--1020, 2016.

\bibitem[Bubeck(2015)]{bubeck}
Sébastien Bubeck.
\newblock \emph{Convex Optimization: Algorithms and Complexity}.
\newblock Now Publishers, 2015.

\bibitem[Chakrabarti and Regev(2011)]{gap-hamming}
Amit Chakrabarti and Oded Regev.
\newblock An optimal lower bound on the communication complexity of
  {G}ap-{H}amming-{D}istance.
\newblock In \emph{Proc.\ 43rd ACM Symposium on Theory of Computing (STOC
  2011)}, 2011.
\newblock \doi{10.1145/1993636.1993644}.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project {Adam}: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{11th {USENIX} Symposium on Operating Systems Design and
  Implementation (OSDI 2014)}, pages 571--582, 2014.

\bibitem[Davies et~al.(2021)Davies, Gurunanthan, Moshrefi, Ashkboos, and
  Alistarh]{alistarh2020distributed}
Peter Davies, Vijaykrishna Gurunanthan, Niusha Moshrefi, Saleh Ashkboos, and
  Dan Alistarh.
\newblock New bounds for distributed mean estimation and variance reduction.
\newblock In \emph{Proc.\ 9th International Conference on Learning
  Representations (ICLR 2021)}, 2021.

\bibitem[Dolev and Feder(1992)]{dolev1992determinism}
Danny Dolev and Tom{\'a}s Feder.
\newblock Determinism vs. nondeterminism in multiparty communication
  complexity.
\newblock \emph{SIAM Journal on Computing}, 21\penalty0 (5):\penalty0 889--895,
  1992.

\bibitem[Garg et~al.(2014)Garg, Ma, and Nguyen]{NIPS2014_5442}
Ankit Garg, Tengyu Ma, and Huy Nguyen.
\newblock On communication cost of distributed statistical estimation and
  dimensionality.
\newblock In \emph{Advances in Neural Information Processing Systems 27 (NIPS
  2014)}. 2014.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt{\'a}rik]{gorbunov2020linearly}
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt{\'a}rik.
\newblock Linearly converging error compensated sgd.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2020)}, 2020.

\bibitem[Islamov et~al.(2021)Islamov, Qian, and
  Richt{\'a}rik]{islamov2021distributed}
Rustem Islamov, Xun Qian, and Peter Richt{\'a}rik.
\newblock Distributed second order methods with fast rates and compressed
  communication.
\newblock \emph{arXiv preprint (arXiv:2102.07158)}, 2021.
\newblock Accepted to ICML 2021.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems (NIPS
  2013)}, 2013.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock Error feedback fixes sign{SGD} and other gradient compression
  schemes.
\newblock In \emph{International Conference on Machine Learning}, pages
  3252--3261. PMLR, 2019.

\bibitem[Khirirat et~al.(2018)Khirirat, Johansson, and Alistarh]{khirirat18}
Sarit Khirirat, Mikael Johansson, and Dan Alistarh.
\newblock Gradient compression for communication-limited convex optimization.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)}, pages
  166--171, 2018.
\newblock \doi{10.1109/CDC.2018.8619625}.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  3478--3487. PMLR, 2019.

\bibitem[Konečný et~al.(2016)Konečný, McMahan, Yu, Richtarik, Suresh, and
  Bacon]{konevcny2016federated}
Jakub Konečný, H.~Brendan McMahan, Felix~X. Yu, Peter Richtarik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Workshop on Private Multi-Party Machine Learning},
  2016.

\bibitem[Kushilevitz and Nisan(1996)]{kushilevitz_nisan_1996}
Eyal Kushilevitz and Noam Nisan.
\newblock \emph{Communication Complexity}.
\newblock Cambridge University Press, 1996.
\newblock \doi{10.1017/CBO9780511574948}.

\bibitem[Künstner(2017)]{kunstner}
Frederik Künstner.
\newblock Fully quantized distributed gradient descent.
\newblock Master's thesis, École polytechnique fédérale de Lausanne, 2017.
\newblock URL \url{http://infoscience.epfl.ch/record/234548}.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{PS}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{Proc.\ 11th {USENIX} Symposium on Operating Systems Design
  and Implementation ({OSDI} 2014)}, pages 583--598, 2014.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS
  2017)}, 2017.

\bibitem[{Magnússon} et~al.(2019){Magnússon}, {Shokri-Ghadikolaei}, and
  {Li}]{magnusson2019maintaining}
S.~{Magnússon}, H.~{Shokri-Ghadikolaei}, and N.~{Li}.
\newblock On maintaining linear convergence of distributed learning and
  optimization under limited communication.
\newblock In \emph{Proc.\ 53rd Asilomar Conference on Signals, Systems, and
  Computers (ACSSC 2019)}, 2019.
\newblock \doi{10.1109/IEEECONF44664.2019.9049052}.

\bibitem[Phillips et~al.(2012)Phillips, Verbin, and Zhang]{phillips2012lower}
Jeff~M Phillips, Elad Verbin, and Qin Zhang.
\newblock Lower bounds for number-in-hand multiparty communication complexity,
  made easy.
\newblock In \emph{Proc.\ 23rd Annual ACM-SIAM symposium on Discrete Algorithms
  (SODA 2012)}, pages 486--501, 2012.

\bibitem[Ramezani-Kebrya et~al.(2021)Ramezani-Kebrya, Faghri, Markov, Aksenov,
  Alistarh, and Roy]{ramezani2021nuqsgd}
Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan
  Alistarh, and Daniel~M Roy.
\newblock Nuqsgd: Provably communication-efficient data-parallel sgd via
  nonuniform quantization.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (114):\penalty0 1--43, 2021.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML 2017)}, pages 3027--3036, 2017.

\bibitem[Shamir(2014)]{NIPS2014_5386}
Ohad Shamir.
\newblock Fundamental limits of online and distributed algorithms for
  statistical learning and estimation.
\newblock In \emph{Advances in Neural Information Processing Systems 27 (NIPS
  2014)}, pages 163--171, 2014.

\bibitem[Stich(2019)]{stich2018local}
Sebastian~Urban Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2019)}, 2019.

\bibitem[Suresh et~al.(2017)Suresh, Yu, Kumar, and
  McMahan]{suresh2017distributed}
Ananda~Theertha Suresh, Felix~X Yu, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML 2017)}, pages 3329--3337. JMLR. org, 2017.

\bibitem[Tsitsiklis and Luo(1987)]{tsitsiklis1987communication}
John~N. Tsitsiklis and Zhi-Quan Luo.
\newblock Communication complexity of convex optimization.
\newblock \emph{Journal of Complexity}, 3\penalty0 (3):\penalty0 231--243,
  1987.

\bibitem[Vempala et~al.(2020)Vempala, Wang, and
  Woodruff]{vempala2020communication}
Santosh~S. Vempala, Ruosong Wang, and David~P. Woodruff.
\newblock The communication complexity of optimization.
\newblock In \emph{Proceedings of the 2020 ACM-SIAM Symposium on Discrete
  Algorithms (SODA 2020)}, pages 1733--1752, 2020.
\newblock \doi{10.1137/1.9781611975994.106}.

\bibitem[Vogels et~al.(2019)Vogels, Karimireddy, and Jaggi]{vogels2019powersgd}
Thijs Vogels, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock {PowerSGD}: Practical low-rank gradient compression for distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2019)}, 2019.

\bibitem[Wang et~al.(2018)Wang, Sievert, Charles, Liu, Wright, and
  Papailiopoulos]{wang2018atomo}
Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and
  Dimitris Papailiopoulos.
\newblock {ATOMO}: communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS 2018)}, 2018.

\bibitem[Wang et~al.(2020)Wang, Liang, and Joshi]{wang2020overlap}
Jianyu Wang, Hao Liang, and Gauri Joshi.
\newblock Overlap local-{SGD}: An algorithmic approach to hide communication
  delays in distributed {SGD}.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 8871--8875. IEEE, 2020.

\bibitem[Woodruff and Zhang(2017)]{10.1007/s00446-014-0218-3}
David~P. Woodruff and Qin Zhang.
\newblock When distributed computation is communication expensive.
\newblock \emph{Distributed Computing}, 30\penalty0 (5):\penalty0 309–323,
  2017.
\newblock \doi{10.1007/s00446-014-0218-3}.

\bibitem[Woodworth and Srebro(2016)]{NIPS2016_6058}
Blake~E Woodworth and Nati Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems 29 (NIPS
  2016)}, pages 3639--3647, 2016.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, Smith, McMahan, and
  Srebro]{NIPS2018_8069}
Blake~E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 31 (NIPS
  2018)}, pages 8496--8506, 2018.

\bibitem[Yao(1977)]{10.1109/SFCS.1977.24}
Andrew Chi-Chin Yao.
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \emph{Proc.\ 18th Annual Symposium on Foundations of Computer
  Science (FOCS 1977)}, page 222–227, 1977.
\newblock \doi{10.1109/SFCS.1977.24}.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, Jordan, and Wainwright]{NIPS2013_4902}
Yuchen Zhang, John Duchi, Michael~I Jordan, and Martin~J Wainwright.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In \emph{Advances in Neural Information Processing Systems 26 (NIPS
  2013)}, pages 2328--2336, 2013.

\end{thebibliography}
