\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[A{\"\i}t-Sahalia et~al.(2010)A{\"\i}t-Sahalia, Fan, and
  Xiu]{ait2010high}
A{\"\i}t-Sahalia, Y., Fan, J., and Xiu, D.
\newblock High-frequency covariance estimates with noisy and asynchronous
  financial data.
\newblock \emph{Journal of the American Statistical Association}, 105\penalty0
  (492):\penalty0 1504--1517, 2010.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, and Bengio]{arpit2017closer}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.,
  Maharaj, T., Fischer, A., Courville, A., and Bengio, Y.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Bengio(2014)]{bengio2014evolving}
Bengio, Y.
\newblock Evolving culture versus local minima.
\newblock In \emph{Growing Adaptive Machines}, pp.\  109--138. 2014.

\bibitem[Blum \& Mitchell(1998)Blum and Mitchell]{blum1998combining}
Blum, A. and Mitchell, T.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In \emph{COLT}, 1998.

\bibitem[Chapelle et~al.(2009)Chapelle, Scholkopf, and Zien]{chapelle2009semi}
Chapelle, O., Scholkopf, B., and Zien, A.
\newblock Semi-supervised learning.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (3):\penalty0 542--542, 2009.

\bibitem[Dgani et~al.(2018)Dgani, Greenspan, and
  Goldberger]{dganitraining18training}
Dgani, Y., Greenspan, H., and Goldberger, J.
\newblock Training a neural network based on unreliable human annotation of
  medical images.
\newblock In \emph{ISBI}, 2018.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2016training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., and Sugiyama, M.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018coteaching}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, pp.\  630--645. Springer, 2016.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Kiryo et~al.(2017)Kiryo, Niu, Du~Plessis, and
  Sugiyama]{kiryo2017positive}
Kiryo, R., Niu, G., Du~Plessis, M., and Sugiyama, M.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{lee2019robust}
Lee, K., Yun, S., Lee, K., Lee, H., Li, B., and Shin, J.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock In \emph{ICML}, 2019.

\bibitem[Li et~al.(2017)Li, Yang, Song, Cao, Luo, and Li]{li2017learning}
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, J.
\newblock Learning from noisy labels with distillation.
\newblock In \emph{ICCV}, 2017.

\bibitem[Liu \& Tao(2016)Liu and Tao]{liu2016classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 38\penalty0 (3):\penalty0 447--461, 2016.

\bibitem[Liu et~al.(2011)Liu, Jiang, Luo, and Chang]{liu2011noise}
Liu, W., Jiang, Y., Luo, J., and Chang, S.
\newblock Noise resistant graph ranking for improved web image search.
\newblock In \emph{CVPR}, 2011.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Ma, X., Wang, Y., Houle, M., Zhou, S., Erfani, S., Xia, S., Wijewickrema, S.,
  and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Malach \& Shalev-Shwartz(2017)Malach and
  Shalev-Shwartz]{malach2017decoupling}
Malach, E. and Shalev-Shwartz, S.
\newblock Decoupling" when to update" from" how to update".
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Masnadi-Shirazi \& Vasconcelos(2009)Masnadi-Shirazi and
  Vasconcelos]{masnadi2009design}
Masnadi-Shirazi, H. and Vasconcelos, N.
\newblock On the design of loss functions for classification: theory,
  robustness to outliers, and savageboost.
\newblock In \emph{NeurIPS}, 2009.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{ICML}, 2015.

\bibitem[Miyato et~al.(2016)Miyato, Dai, and Goodfellow]{miyato2016virtual}
Miyato, T., Dai, A., and Goodfellow, I.
\newblock Virtual adversarial training for semi-supervised text classification.
\newblock In \emph{ICLR}, 2016.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I., Ravikumar, P., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{NeurIPS}, 2013.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, 2017.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{EMNLP}, 2014.

\bibitem[Raykar et~al.(2010)Raykar, Yu, Zhao, Valadez, Florin, Bogoni, and
  Moy]{raykar2010learning}
Raykar, V., Yu, S., Zhao, L., Valadez, G., Florin, C., Bogoni, L., and Moy, L.
\newblock Learning from crowds.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1297--1322, 2010.

\bibitem[Reed et~al.(2015)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock In \emph{ICLR}, 2015.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Rodrigues \& Pereira(2018)Rodrigues and Pereira]{rodrigues2017deep}
Rodrigues, F. and Pereira, F.
\newblock Deep learning from crowds.
\newblock In \emph{AAAI}, 2018.

\bibitem[Sanderson \& Scott(2014)Sanderson and Scott]{sanderson2014class}
Sanderson, T. and Scott, C.
\newblock Class proportion estimation with application to multiclass anomaly
  rejection.
\newblock In \emph{AISTATS}, 2014.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, 2018.

\bibitem[van Rooyen et~al.(2015)van Rooyen, Menon, and
  Williamson]{van2015learning}
van Rooyen, B., Menon, A., and Williamson, B.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Veit et~al.(2017)Veit, Alldrin, Chechik, Krasin, Gupta, and
  Belongie]{veit2017learning}
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{CVPR}, 2017.

\bibitem[Wang \& Zhou(2017)Wang and Zhou]{wang2017theoretical}
Wang, W. and Zhou, Z.-H.
\newblock Theoretical foundation of co-training and disagreement-based
  algorithms.
\newblock \emph{arXiv preprint arXiv:1708.04403}, 2017.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{wang2018iterative}
Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{CVPR}, 2018.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Perona, and
  Belongie]{welinder2010multidimensional}
Welinder, P., Branson, S., Perona, P., and Belongie, S.
\newblock The multidimensional wisdom of crowds.
\newblock In \emph{NeurIPS}, 2010.

\bibitem[Yan et~al.(2014)Yan, Rosales, Fung, Subramanian, and
  Dy]{yan2014learning}
Yan, Y., Rosales, R., Fung, G., Subramanian, R., and Dy, J.
\newblock Learning from multiple annotators with varying expertise.
\newblock \emph{Machine Learning}, 95\penalty0 (3):\penalty0 291--327, 2014.

\bibitem[Yu et~al.(2018)Yu, Liu, Gong, Batmanghelich, and Tao]{yu2018efficient}
Yu, X., Liu, T., Gong, M., Batmanghelich, K., and Tao, D.
\newblock An efficient and provable approach for mixture proportion estimation
  using linear independence assumption.
\newblock In \emph{CVPR}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{NeurIPS}, 2018.

\end{thebibliography}
