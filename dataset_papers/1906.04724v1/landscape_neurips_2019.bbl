\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Farkhoor, Liu, and
  Yosinski]{uberAIintrinsic}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes,
  2018{\natexlab{a}}.

\bibitem[Fort and Scherlis(2018)]{fort2018goldilocks}
Stanislav Fort and Adam Scherlis.
\newblock The goldilocks zone: Towards better understanding of neural network
  loss landscapes, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Ian~J. Goodfellow, Oriol Vinyals, and Andrew~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems,
  2014.

\bibitem[Frankle and Carbin(2019)]{frankle2018}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzbski2017factors}
Stanis≈Çaw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd, 2017.

\bibitem[Smith et~al.(2018)Smith, Kindermans, and Le]{sam2018dont}
Samuel~L. Smith, Pieter-Jan Kindermans, and Quoc~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1Yy1BxCZ}.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization,
  2018.

\bibitem[{Jastrz{\k{e}}bski} et~al.(2018){Jastrz{\k{e}}bski}, {Kenton},
  {Ballas}, {Fischer}, {Bengio}, and {Storkey}]{jastrzebski2018}
Stanis{\l}aw {Jastrz{\k{e}}bski}, Zachary {Kenton}, Nicolas {Ballas}, Asja
  {Fischer}, Yoshua {Bengio}, and Amos {Storkey}.
\newblock {On the Relation Between the Sharpest Directions of DNN Loss and the
  SGD Step Length}.
\newblock \emph{arXiv e-prints}, art. arXiv:1807.05031, Jul 2018.

\bibitem[{Xing} et~al.(2018){Xing}, {Arpit}, {Tsirigotis}, and
  {Bengio}]{xing2018}
Chen {Xing}, Devansh {Arpit}, Christos {Tsirigotis}, and Yoshua {Bengio}.
\newblock {A Walk with SGD}.
\newblock \emph{arXiv e-prints}, art. arXiv:1802.08770, Feb 2018.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{NIPS2018_8095}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 8789--8798. Curran Associates, Inc., 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf}.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, {Ben Arous}, and
  LeCun]{choromanska2015}
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard {Ben Arous}, and Yann
  LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock \emph{Journal of Machine Learning Research}, 38:\penalty0 192--204,
  2015.
\newblock ISSN 1532-4435.

\bibitem[Nguyen and Hein(2017)]{Nguyen2017}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pages 2603--2612. JMLR.org, 2017.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3305890.3305950}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{{ICLR}}. OpenReview.net, 2017.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Xu, Taylor, Studer, and
  Goldstein]{li2018}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Inform`ation Processing Systems}, pages
  6389--6399, 2018{\natexlab{b}}.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Orr, and M\"{u}ller]{lecun1998}
Yann LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M\"{u}ller.
\newblock Efficient backprop.
\newblock In \emph{Neural Networks: Tricks of the Trade, This Book is an
  Outgrowth of a 1996 NIPS Workshop}, pages 9--50, London, UK, UK, 1998.
  Springer-Verlag.
\newblock ISBN 3-540-65311-2.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=645754.668382}.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016}
Levent Sagun, L{\'e}on Bottou, and Yann LeCun.
\newblock Singularity of the hessian in deep learning.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{huang2017snapshot}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E. Hopcroft, and Kilian~Q.
  Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free, 2017.

\end{thebibliography}
