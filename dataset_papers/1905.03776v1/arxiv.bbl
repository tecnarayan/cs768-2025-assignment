\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6240--6249, 2017.

\bibitem[Chaudhari \& Soatto(2017)Chaudhari and Soatto]{SGDVinference}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock \emph{CoRR}, abs/1710.11029, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.11029}.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{Glorot}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{AISTATS 2010}, 2010.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'{a}}r, P., Girshick, R.~B., Noordhuis, P., Wesolowski, L.,
  Kyrola, A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock \emph{CoRR}, abs/1706.02677, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.02677}.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{harvey2017nearly}
Harvey, N., Liaw, C., and Mehrabian, A.
\newblock Nearly-tight vc-dimension bounds for piecewise linear neural
  networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  1064--1068, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock \emph{CoRR}, abs/1502.01852, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.01852}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NTK}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{CoRR}, abs/1806.07572, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.07572}.

\bibitem[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{threefactors}
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y.,
  and Storkey, A.~J.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{CoRR}, abs/1711.04623, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.04623}.

\bibitem[Karakida et~al.(2017)Karakida, Akaho, and ichi Amari]{fishermeanfield}
Karakida, R., Akaho, S., and ichi Amari, S.
\newblock Universal statistics of fisher information in deep neural networks:
  Mean field approach.
\newblock \emph{arXiv preprint arXiv:}, abs/1806.01316, 2017.
\newblock URL \url{http://arxiv.org/abs/1806.01316}.

\bibitem[Karras et~al.(2017)Karras, Aila, Laine, and Lehtinen]{NTKGAN1}
Karras, T., Aila, T., Laine, S., and Lehtinen, J.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock \emph{CoRR}, abs/1710.10196, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.10196}.

\bibitem[Karras et~al.(2018)Karras, Laine, and Aila]{NTKGAN2}
Karras, T., Laine, S., and Aila, T.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock \emph{CoRR}, abs/1812.04948, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.04948}.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krizhevsky(2009)]{cifar10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1996)LeCun, Bottou, Orr, and M{\"u}ller]{efficient}
LeCun, Y., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Effiicient backprop.
\newblock In \emph{Neural Networks: Tricks of the Trade}, 1996.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{lenet}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock In \emph{Proceedings of the IEEE}, pp.\  2278--2324, 1998.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{mnist}
LeCun, Y., Cortes, C., and Burges, C.~J.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-dickstein]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and
  Sohl-dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1EA-M-0Z}.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{SGDBinference}
Mandt, S., Hoffman, M.~D., and Blei, D.~M.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock \emph{arXiv preprint arXiv:}, abs/1704.04289, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.04289}.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{largebatch}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock \emph{CoRR}, abs/1812.06162, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.06162}.

\bibitem[Nagarajan \& Kolter(2018)Nagarajan and
  Kolter]{nagarajan2018deterministic}
Nagarajan, V. and Kolter, Z.
\newblock Deterministic pac-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock 2018.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{novak2018sensitivity}
Novak, R., Bahri, Y., Abolafia, D.~A., Pennington, J., and Sohl-Dickstein, J.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJC2SzZCW}.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Sagun, L., Evci, U., Guney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Schoenholz et~al.(2016)Schoenholz, Gilmer, Ganguli, and
  Sohl{-}Dickstein]{deepprop}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl{-}Dickstein, J.
\newblock Deep information propagation.
\newblock \emph{CoRR}, abs/1611.01232, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01232}.

\bibitem[Sculley et~al.(2018)Sculley, Snoek, Wiltschko, and
  Rahimi]{sculley2018iclr}
Sculley, D., Snoek, J., Wiltschko, A.~B., and Rahimi, A.
\newblock Winner's curse? on pace, progress, and empirical rigor.
\newblock In \emph{ICLR (Workshop)} \citet{sculley2018iclr}.
\newblock URL
  \url{http://dblp.uni-trier.de/db/conf/iclr/iclr2018w.html#SculleySWR18}.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl{-}Dickstein,
  Frostig, and Dahl]{minibatch}
Shallue, C.~J., Lee, J., Antognini, J.~M., Sohl{-}Dickstein, J., Frostig, R.,
  and Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{CoRR}, abs/1811.03600, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.03600}.

\bibitem[Smith \& Le(2017)Smith and Le]{smithLe}
Smith, S.~L. and Le, Q.~V.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock \emph{CoRR}, abs/1710.06451, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.06451}.

\bibitem[Smith et~al.(2017)Smith, Kindermans, and Le]{smith2017dont}
Smith, S.~L., Kindermans, P., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{CoRR}, abs/1711.00489, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.00489}.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[van Laarhoven(2017)]{NTKL2}
van Laarhoven, T.
\newblock {L2} regularization versus batch and weight normalization.
\newblock \emph{CoRR}, abs/1706.05350, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.05350}.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4148--4158, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fmnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{CoRR}, abs/1708.07747, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.07747}.

\bibitem[{Yaida}(2018)]{yaida2018fluctuation}
{Yaida}, S.
\newblock {Fluctuation-dissipation relations for stochastic gradient descent}.
\newblock \emph{arXiv preprint arXiv:}, abs/1810.00004, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.00004}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{WideRN}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{CoRR}, abs/1605.07146, 2016.
\newblock URL \url{http://arxiv.org/abs/1605.07146}.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangBHRV16}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{CoRR}, abs/1611.03530, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.03530}.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock 2018.

\end{thebibliography}
