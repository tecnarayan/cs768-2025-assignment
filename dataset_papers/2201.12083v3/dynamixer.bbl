\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.

\bibitem[Bai et~al.(2022)Bai, Yuan, Xia, Yan, Li, and Liu]{bai2022improving}
Bai, J., Yuan, L., Xia, S.-T., Yan, S., Li, Z., and Liu, W.
\newblock Improving vision transformers by revisiting high-frequency
  components.
\newblock \emph{arXiv preprint arXiv:2204.00993}, 2022.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Fan, and Panda]{chen2021crossvit}
Chen, C.-F., Fan, Q., and Panda, R.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock \emph{arXiv preprint arXiv:2103.14899}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Xie, Ge, Liang, and
  Luo]{cyclemlp}
Chen, S., Xie, E., Ge, C., Liang, D., and Luo, P.
\newblock Cyclemlp: A mlp-like architecture for dense prediction.
\newblock \emph{arXiv preprint arXiv:2107.10224}, 2021{\natexlab{b}}.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{randaugment}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  702--703, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and
  Feichtenhofer]{fan2021multiscale}
Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and
  Feichtenhofer, C.
\newblock Multiscale vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.11227}, 2021.

\bibitem[Ge et~al.(2021)Ge, Liang, Song, Jiao, Wang, and
  Luo]{ge2021revitalizing}
Ge, C., Liang, Y., Song, Y., Jiao, J., Wang, J., and Luo, P.
\newblock Revitalizing cnn attention via transformers in self-supervised visual
  representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Guo et~al.(2021)Guo, Tang, Han, Chen, Wu, Xu, Xu, and Wang]{hire-mlp}
Guo, J., Tang, Y., Han, K., Chen, X., Wu, H., Xu, C., Xu, C., and Wang, Y.
\newblock Hire-mlp: Vision mlp via hierarchical rearrangement.
\newblock \emph{arXiv preprint arXiv:2108.13341}, 2021.

\bibitem[Han et~al.(2020)Han, Wang, Chen, Chen, Guo, Liu, Tang, Xiao, Xu, Xu,
  et~al.]{han2020survey}
Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A.,
  Xu, C., Xu, Y., et~al.
\newblock A survey on visual transformer.
\newblock \emph{arXiv preprint arXiv:2012.12556}, 2020.

\bibitem[Han et~al.(2021)Han, Xiao, Wu, Guo, Xu, and Wang]{han2021transformer}
Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y.
\newblock Transformer in transformer.
\newblock \emph{arXiv preprint arXiv:2103.00112}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hou et~al.(2021)Hou, Jiang, Yuan, Cheng, Yan, and Feng]{vip}
Hou, Q., Jiang, Z., Yuan, L., Cheng, M.-M., Yan, S., and Feng, J.
\newblock Vision permutator: A permutable mlp-like architecture for visual
  recognition.
\newblock \emph{arXiv preprint arXiv:2106.12368}, 2021.

\bibitem[Hu et~al.(2019)Hu, Zhang, Xie, and Lin]{hu2019local}
Hu, H., Zhang, Z., Xie, Z., and Lin, S.
\newblock Local relation networks for image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  3464--3473, 2019.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{hu2018squeeze}
Hu, J., Shen, L., and Sun, G.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7132--7141, 2018.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pp.\  646--661.
  Springer, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4700--4708, 2017.

\bibitem[Jiang et~al.(2021)Jiang, Hou, Yuan, Zhou, Shi, Jin, Wang, and
  Feng]{token-labeling}
Jiang, Z.-H., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., and
  Feng, J.
\newblock All tokens matter: Token labeling for training better vision
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Khan et~al.(2021)Khan, Naseer, Hayat, Zamir, Khan, and
  Shah]{khan2021transformers}
Khan, S., Naseer, M., Hayat, M., Zamir, S.~W., Khan, F.~S., and Shah, M.
\newblock Transformers in vision: A survey.
\newblock \emph{arXiv preprint arXiv:2101.01169}, 2021.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[Li et~al.(2021)Li, Hassani, Walton, and Shi]{convmlp}
Li, J., Hassani, A., Walton, S., and Shi, H.
\newblock Convmlp: Hierarchical convolutional mlps for vision.
\newblock \emph{arXiv preprint arXiv:2109.04454}, 2021.

\bibitem[Lian et~al.(2021)Lian, Yu, Sun, and Gao]{asmlp}
Lian, D., Yu, Z., Sun, X., and Gao, S.
\newblock As-mlp: An axial shifted mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2107.08391}, 2021.

\bibitem[Liang et~al.(2022)Liang, Ge, Tong, Song, Wang, and Xie]{liang2022not}
Liang, Y., Ge, C., Tong, Z., Song, Y., Wang, J., and Xie, P.
\newblock Not all patches are what you need: Expediting vision transformers via
  token reorganizations.
\newblock \emph{arXiv preprint arXiv:2202.07800}, 2022.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Dai, So, and Le]{gmlp}
Liu, H., Dai, Z., So, D.~R., and Le, Q.~V.
\newblock Pay attention to mlps.
\newblock \emph{arXiv preprint arXiv:2105.08050}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{arXiv preprint arXiv:2103.14030}, 2021{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. 2019.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{regnet}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10428--10436, 2020.

\bibitem[Rao et~al.(2021)Rao, Zhao, Zhu, Lu, and Zhou]{gfnet}
Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J.
\newblock Global filter networks for image classification.
\newblock \emph{arXiv preprint arXiv:2107.00645}, 2021.

\bibitem[Srinivas et~al.(2021)Srinivas, Lin, Parmar, Shlens, Abbeel, and
  Vaswani]{bot}
Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A.
\newblock Bottleneck transformers for visual recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16519--16529, 2021.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1--9, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.~A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{Thirty-first AAAI conference on artificial intelligence},
  2017.

\bibitem[Tan \& Le(2019)Tan and Le]{efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6105--6114. PMLR, 2019.

\bibitem[Tang et~al.(2021)Tang, Zhao, Wang, Luo, Xie, and Zeng]{smlpnet}
Tang, C., Zhao, Y., Wang, G., Luo, C., Xie, W., and Zeng, W.
\newblock Sparse mlp for image recognition: Is self-attention really necessary?
\newblock \emph{arXiv preprint arXiv:2109.05422}, 2021.

\bibitem[Tatsunami \& Taki(2021)Tatsunami and Taki]{raftmlp}
Tatsunami, Y. and Taki, M.
\newblock Raftmlp: Do mlp-based models dream of winning over computer vision?
\newblock \emph{CoRR}, abs/2108.04384, 2021.

\bibitem[Tay et~al.(2021)Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{synthesizer}
Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and Zheng, C.
\newblock Synthesizer: Rethinking self-attention for transformer models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10183--10192. PMLR, 2021.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{mlp-mixer}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,
  T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2105.01601}, 2021.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{tong2022videomae}
Tong, Z., Song, Y., Wang, J., and Wang, L.
\newblock Videomae: Masked autoencoders are data-efficient learners for
  self-supervised video pre-training.
\newblock \emph{arXiv preprint arXiv:2203.12602}, 2022.

\bibitem[Touvron et~al.(2021{\natexlab{a}})Touvron, Bojanowski, Caron, Cord,
  El-Nouby, Grave, Izacard, Joulin, Synnaeve, Verbeek, et~al.]{resmlp}
Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E.,
  Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et~al.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock \emph{arXiv preprint arXiv:2105.03404}, 2021{\natexlab{a}}.

\bibitem[Touvron et~al.(2021{\natexlab{b}})Touvron, Cord, Douze, Massa,
  Sablayrolles, and J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10347--10357. PMLR, 2021{\natexlab{b}}.

\bibitem[Touvron et~al.(2021{\natexlab{c}})Touvron, Cord, Sablayrolles,
  Synnaeve, and J{\'e}gou]{cait}
Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and J{\'e}gou, H.
\newblock Going deeper with image transformers.
\newblock \emph{arXiv preprint arXiv:2103.17239}, 2021{\natexlab{c}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{pvt}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock \emph{arXiv preprint arXiv:2102.12122}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Yao, Chen, Lin, Cai, He, and
  Liu]{crossformer}
Wang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., and Liu, W.
\newblock Crossformer: A versatile vision transformer hinging on cross-scale
  attention.
\newblock \emph{arXiv preprint arXiv:2108.00154}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
Wang, X., Girshick, R., Gupta, A., and He, K.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7794--7803, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1492--1500, 2017.

\bibitem[Yu et~al.(2021{\natexlab{a}})Yu, Li, Cai, Sun, and Li]{s2-mlp}
Yu, T., Li, X., Cai, Y., Sun, M., and Li, P.
\newblock S$^{2}$-mlp: Spatial-shift mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2106.07477}, 2021{\natexlab{a}}.

\bibitem[Yu et~al.(2021{\natexlab{b}})Yu, Li, Cai, Sun, and Li]{s2-mlpv2}
Yu, T., Li, X., Cai, Y., Sun, M., and Li, P.
\newblock S$^{2}$-mlpv2: Improved spatial-shift mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2108.01072}, 2021{\natexlab{b}}.

\bibitem[Yuan et~al.(2021{\natexlab{a}})Yuan, Chen, Wang, Yu, Shi, Jiang, Tay,
  Feng, and Yan]{Yuan_2021_ICCV}
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F.~E., Feng,
  J., and Yan, S.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pp.\  558--567, October 2021{\natexlab{a}}.

\bibitem[Yuan et~al.(2021{\natexlab{b}})Yuan, Hou, Jiang, Feng, and
  Yan]{yuan2021volo}
Yuan, L., Hou, Q., Jiang, Z., Feng, J., and Yan, S.
\newblock Volo: Vision outlooker for visual recognition.
\newblock \emph{arXiv preprint arXiv:2106.13112}, 2021{\natexlab{b}}.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  6023--6032, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Wu, Zhang, Zhu, Lin, Zhang, Sun, He, Mueller,
  Manmatha, et~al.]{split-atten}
Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang, Z., Sun, Y., He, T.,
  Mueller, J., Manmatha, R., et~al.
\newblock Resnest: Split-attention networks.
\newblock \emph{arXiv preprint arXiv:2004.08955}, 2020.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{zhong2020random}
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y.
\newblock Random erasing data augmentation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  13001--13008, 2020.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Kang, Jin, Yang, Lian, Jiang,
  Hou, and Feng]{zhou2021deepvit}
Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., and Feng,
  J.
\newblock Deepvit: Towards deeper vision transformer.
\newblock \emph{arXiv preprint arXiv:2103.11886}, 2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, Shi, Kang, Yu, Jiang, Li, Jin,
  Hou, and Feng]{zhou2021refiner}
Zhou, D., Shi, Y., Kang, B., Yu, W., Jiang, Z., Li, Y., Jin, X., Hou, Q., and
  Feng, J.
\newblock Refiner: Refining self-attention for vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.03714}, 2021{\natexlab{b}}.

\end{thebibliography}
