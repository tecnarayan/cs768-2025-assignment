\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Daneshmand, and Sra]{Ahn2023TransformersLT}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for in-context learning.
\newblock \emph{ArXiv}, abs/2306.00297, 2023.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{Akyrek2022WhatLA}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{ArXiv}, abs/2211.15661, 2022.

\bibitem[Arjevani et~al.(2016)Arjevani, Shalev-Shwartz, and Shamir]{Arjevani2016OnLower}
Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On lower and upper bounds in smooth and strongly convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0 (126):\penalty0 1--51, 2016.
\newblock URL \url{http://jmlr.org/papers/v17/15-106.html}.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{Bai2023TransformersAS}
Yu~Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock \emph{ArXiv}, abs/2306.04637, 2023.

\bibitem[Ben-Israel(1965)]{adi1965ai}
Adi Ben-Israel.
\newblock An iterative method for computing the generalized inverse of an arbitrary matrix.
\newblock \emph{Mathematics of Computation}, 19\penalty0 (91):\penalty0 452--455, 1965.
\newblock ISSN 00255718, 10886842.
\newblock URL \url{http://www.jstor.org/stable/2003676}.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen~P Boyd and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020lm}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Cai et~al.(2022)Cai, Gan, and Han]{Cai2022EfficientViTEL}
Han Cai, Chuang Gan, and Song Han.
\newblock Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition.
\newblock \emph{ArXiv}, abs/2205.14756, 2022.

\bibitem[Chang and Jia(2023)]{chang-jia-2023-data}
Ting-Yun Chang and Robin Jia.
\newblock Data curation alone can stabilize in-context learning.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8123--8144, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.452}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.452}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Chuang et~al.(2023)Chuang, Xie, Luo, Kim, Glass, and He]{chuang2023dola}
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He.
\newblock Dola: Decoding by contrasting layers improves factuality in large language models, 2023.

\bibitem[Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and Garriga-Alonso]{conmy2023automated}
Arthur Conmy, Augustine~N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso.
\newblock Towards automated circuit discovery for mechanistic interpretability, 2023.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Sui, and Wei]{Dai2023WhyCG}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei.
\newblock Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers.
\newblock \emph{ArXiv}, abs/2212.10559, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{Garg2022WhatCT}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{ArXiv}, abs/2208.01066, 2022.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva-etal-2022-transformer}
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 30--45, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.3}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.3}.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{pmlr-v202-giannou23a}
Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason~D. Lee, and Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 11398--11442. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/giannou23a.html}.

\bibitem[Han et~al.(2023)Han, Wang, Zhao, and Ji]{han2023incontext}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji.
\newblock In-context learning of large language models explained as kernel regression, 2023.

\bibitem[Hassid et~al.(2022)Hassid, Peng, Rotem, Kasai, Montero, Smith, and Schwartz]{hassid2022does}
Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah~A. Smith, and Roy Schwartz.
\newblock How much does attention actually attend? questioning the importance of attention in pretrained transformers, 2022.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997lstm}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock {Long Short-Term Memory}.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 11 1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.8.1735}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.8.1735}.

\bibitem[Lee et~al.(2023)Lee, Jiang, and Berg-Kirkpatrick]{lee2023exploring}
Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick.
\newblock Exploring the relationship between model architecture and in-context learning ability, 2023.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and Oymak]{Li2023TransformersAA}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in in-context learning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Liu and Nocedal(1989)]{Liu1989OnTL}
Dong~C. Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical Programming}, 45:\penalty0 503--528, 1989.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:5681609}.

\bibitem[Liu et~al.(2022)Liu, Shen, Zhang, Dolan, Carin, and Chen]{liu-etal-2022-makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
\newblock What makes good in-context examples for {GPT}-3?
\newblock In \emph{Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures}, pages 100--114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.deelio-1.10}.
\newblock URL \url{https://aclanthology.org/2022.deelio-1.10}.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and Stenetorp]{lu-etal-2022-fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8086--8098, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.556}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.556}.

\bibitem[Mahankali et~al.(2024)Mahankali, Hashimoto, and Ma]{mahankali2024one}
Arvind~V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=8p3fu56lKc}.

\bibitem[Min et~al.(2022{\natexlab{a}})Min, Lewis, Hajishirzi, and Zettlemoyer]{min-etal-2022-noisy}
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Noisy channel language model prompting for few-shot text classification.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5316--5330, Dublin, Ireland, May 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.365}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.365}.

\bibitem[Min et~al.(2022{\natexlab{b}})Min, Lewis, Zettlemoyer, and Hajishirzi]{min-etal-2022-metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {M}eta{ICL}: Learning to learn in context.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2791--2809, Seattle, United States, July 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.201}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.201}.

\bibitem[Min et~al.(2022{\natexlab{c}})Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min-etal-2022-rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, Abu Dhabi, United Arab Emirates, December 2022{\natexlab{c}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.759}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.759}.

\bibitem[Moore(1920)]{moonre1920pseudo}
E.H Moore.
\newblock On the reciprocal of the general algebraic matrix.
\newblock \emph{Bulletin of American Mathematical Society}, 26:\penalty0 394--395, 1920.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability, 2023.

\bibitem[Nemirovski and Yudin(1983)]{nemirovskii1983problem}
A.S. Nemirovski and D.B Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nguyen and Wong(2023)]{nguyen2023context}
Tai Nguyen and Eric Wong.
\newblock In-context example selection with influences.
\newblock \emph{arXiv preprint arXiv:2302.11042}, 2023.

\bibitem[Nocedal and Wright(1999)]{nocedal1999numerical}
Jorge Nocedal and Stephen~J Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer, 1999.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI.
\newblock Gpt-4 technical report, 2023.
\newblock URL \url{http://arxiv.org/abs/2303.08774v3}.

\bibitem[Pan and Schreiber(1991)]{Pan1991AnIN}
Victor~Y. Pan and Robert~S. Schreiber.
\newblock An improved newton iteration for the generalized inverse of a matrix, with applications.
\newblock \emph{SIAM J. Sci. Comput.}, 12:\penalty0 1109--1130, 1991.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rae et~al.(2022)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{rae2022scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones,
  James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training gopher, 2022.

\bibitem[Raventós et~al.(2023)Raventós, Paul, Chen, and Ganguli]{raventós2023pretraining}
Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli.
\newblock Pretraining task diversity and the emergence of non-bayesian in-context learning for regression, 2023.

\bibitem[Rubin et~al.(2022)Rubin, Herzig, and Berant]{rubin-etal-2022-learning}
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
\newblock Learning to retrieve prompts for in-context learning.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2655--2671, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.191}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.191}.

\bibitem[Schulz(1933)]{schulz1933inverse}
G\"unther Schulz.
\newblock Iterative berechung der reziproken matrix.
\newblock \emph{Zeitschrift f\"ur Angewandte Mathematik und Mechanik (Journal of Applied Mathematics and Mechanics)}, 13:\penalty0 57--59, 1933.

\bibitem[Sharan et~al.(2019)Sharan, Sidford, and Valiant]{sharan2019memory}
Vatsal Sharan, Aaron Sidford, and Gregory Valiant.
\newblock Memory-sample tradeoffs for linear regression with small error.
\newblock In \emph{Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing}, pages 890--901, 2019.

\bibitem[Shen et~al.(2023{\natexlab{a}})Shen, Guo, Tan, Tang, Wang, and Bian]{shen2023study}
Kai Shen, Junliang Guo, Xu~Tan, Siliang Tang, Rui Wang, and Jiang Bian.
\newblock A study on relu and softmax in transformer, 2023{\natexlab{a}}.

\bibitem[Shen et~al.(2023{\natexlab{b}})Shen, Mishra, and Khashabi]{shen2023pretrained}
Lingfeng Shen, Aayush Mishra, and Daniel Khashabi.
\newblock Do pretrained transformers really learn in-context by gradient descent?, 2023{\natexlab{b}}.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi, Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza~Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022.

\bibitem[Soderstrom and Stewart(1974)]{soderstrom1974onp}
Torsten Soderstrom and G.~W. Stewart.
\newblock On the numerical properties of an iterative method for computing the moore- penrose generalized inverse.
\newblock \emph{SIAM Journal on Numerical Analysis}, 11\penalty0 (1):\penalty0 61--74, 1974.
\newblock ISSN 00361429.
\newblock URL \url{http://www.jstor.org/stable/2156431}.

\bibitem[Su et~al.(2023)Su, Kasai, Wu, Shi, Wang, Xin, Zhang, Ostendorf, Zettlemoyer, Smith, and Yu]{su2023selective}
Hongjin Su, Jungo Kasai, Chen~Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah~A. Smith, and Tao Yu.
\newblock Selective annotation makes language models better few-shot learners.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=qY1hlv7gwg}.

\bibitem[Tarzanagh et~al.(2023{\natexlab{a}})Tarzanagh, Li, Thrampoulidis, and Oymak]{AtaeeTarzanagh2023TransformersAS}
Davoud~Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak.
\newblock Transformers as support vector machines.
\newblock \emph{ArXiv}, abs/2308.16898, 2023{\natexlab{a}}.

\bibitem[Tarzanagh et~al.(2023{\natexlab{b}})Tarzanagh, Li, Zhang, and Oymak]{tarzanagh2023maxmargin}
Davoud~Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak.
\newblock Max-margin token selection in attention mechanism, 2023{\natexlab{b}}.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang, Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang, Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi, Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson, Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and Le]{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani2017AttentionIA}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vladymyrov et~al.(2024)Vladymyrov, von Oswald, Sandler, and Ge]{vladymyrov2024linear}
Max Vladymyrov, Johannes von Oswald, Mark Sandler, and Rong Ge.
\newblock Linear transformers are versatile in-context learners, 2024.

\bibitem[von Oswald et~al.(2022)von Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{Oswald2022TransformersLI}
Johannes von Oswald, Eyvind Niklasson, E.~Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Schlegel, Kobayashi, Zucchet, Scherrer, Miller, Sandler, y~Arcas, Vladymyrov, Pascanu, and Sacramento]{Oswald2023UncoveringMA}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise~Ag{\"u}era y~Arcas, Max Vladymyrov, Razvan Pascanu, and Joao Sacramento.
\newblock Uncovering mesa-optimization algorithms in transformers.
\newblock \emph{ArXiv}, abs/2309.05858, 2023.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022.

\bibitem[Wei et~al.(2023)Wei, Wei, Tay, Tran, Webson, Lu, Chen, Liu, Huang, Zhou, and Ma]{wei2023larger}
Jerry Wei, Jason Wei, Yi~Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da~Huang, Denny Zhou, and Tengyu Ma.
\newblock Larger language models do in-context learning differently, 2023.

\bibitem[Wortsman et~al.(2023)Wortsman, Lee, Gilmer, and Kornblith]{wortsman2023replacing}
Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Replacing softmax with relu in vision transformers, 2023.

\bibitem[Yoo et~al.(2022)Yoo, Kim, Kim, Cho, Jo, Lee, Lee, and Kim]{yoo-etal-2022-ground}
Kang~Min Yoo, Junyeob Kim, Hyuhng~Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim.
\newblock Ground-truth labels matter: A deeper look into input-label demonstrations.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 2422--2437, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.155}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.155}.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{Zhang2023TrainedTL}
Ruiqi Zhang, Spencer Frei, and Peter~L. Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{ArXiv}, abs/2306.09927, 2023.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In \emph{International Conference on Machine Learning}, pages 12697--12706. PMLR, 2021.

\end{thebibliography}
