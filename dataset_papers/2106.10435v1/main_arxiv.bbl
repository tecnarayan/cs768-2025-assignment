% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Konevcny_Arxiv_2016}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, D.~Ramage, and P.~Richt{\'a}rik,
  ``Federated optimization: Distributed machine learning for on-device
  intelligence,'' \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{Li_Smola_NIPS_2014_CommunicationEfficient}
M.~Li, D.~G. Andersen, A.~J. Smola, and K.~Yu, ``Communication efficient
  distributed machine learning with the parameter server,'' in \emph{Advances
  in Neural Information Processing Systems 27}, Z.~Ghahramani, M.~Welling,
  C.~Cortes, N.~D. Lawrence, and K.~Q. Weinberger, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Curran Associates, Inc., 2014, pp. 19--27.

\bibitem{Dean_NIPS_2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, M.~Ranzato,
  A.~Senior, P.~Tucker, K.~Yang \emph{et~al.}, ``Large scale distributed deep
  networks,'' in \emph{Advances in neural information processing systems},
  2012, pp. 1223--1231.

\bibitem{Leaute_JAIR_2013}
T.~L{\'e}aut{\'e} and B.~Faltings, ``Protecting privacy through distributed
  computation in multi-agent decision making,'' \emph{Journal of Artificial
  Intelligence Research}, vol.~47, pp. 649--695, 2013.

\bibitem{Fang_NIPS_2018_spider}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang, ``{S}pider: Near-optimal non-convex
  optimization via stochastic path-integrated differential estimator,'' in
  \emph{Advances in Neural Information Processing Systems}, 2018, pp. 689--699.

\bibitem{Zhou_NIPS_2018_SNVRG}
D.~Zhou, P.~Xu, and Q.~Gu, ``Stochastic nested variance reduction for nonconvex
  optimization,'' \emph{arXiv preprint arXiv:1806.07811}, 2018.

\bibitem{Cutkosky_NIPS2019}
A.~Cutkosky and F.~Orabona, ``Momentum-based variance reduction in non-convex
  {SGD},'' in \emph{Advances in Neural Information Processing Systems
  32}.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2019,
  pp. 15\,236--15\,245.

\bibitem{Dinh_Arxiv_2019}
Q.~Tran-Dinh, N.~H. Pham, D.~T. Phan, and L.~M. Nguyen, ``Hybrid stochastic
  gradient descent algorithms for stochastic nonconvex optimization,''
  \emph{arXiv preprint arXiv:1905.05920}, 2019.

\bibitem{Zhang_FedPD_Arxiv_2020}
X.~Zhang, M.~Hong, S.~Dhople, W.~Yin, and Y.~Liu, ``Fedpd: A federated learning
  framework with optimal rates and adaptivity to non-iid data,'' 2020.

\bibitem{Li_Arxiv_2018_FedProx}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith,
  ``Federated optimization in heterogeneous networks,'' \emph{arXiv preprint
  arXiv:1812.06127}, 2018.

\bibitem{Mcmahan_PMLR_2017}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas,
  ``Communication-efficient learning of deep networks from decentralized
  data,'' in \emph{Artificial Intelligence and Statistics}.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 2017, pp. 1273--1282.

\bibitem{Yu_Zhu_2018parallel}
H.~Yu, S.~Yang, and S.~Zhu, ``Parallel restarted sgd with faster convergence
  and less communication: Demystifying why model averaging works for deep
  learning,'' 2018.

\bibitem{Woodworth_Minibatch_Arxiv_2020}
B.~Woodworth, K.~K. Patel, and N.~Srebro, ``Minibatch vs local sgd for
  heterogeneous distributed learning,'' 2020.

\bibitem{Yu_Jin_Arxiv_2019linear}
H.~Yu, R.~Jin, and S.~Yang, ``On the linear speedup analysis of communication
  efficient momentum sgd for distributed non-convex optimization,'' 2019.

\bibitem{Karimireddy_scaffold_2020}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~Reddi, S.~Stich, and A.~T. Suresh,
  ``Scaffold: Stochastic controlled averaging for federated learning,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2020, pp. 5132--5143.

\bibitem{Yang_ICLR_2021}
H.~Yang, M.~Fang, and J.~Liu, ``Achieving linear speedup with partial worker
  participation in non-iid federated learning,'' 2021.

\bibitem{Karimireddy_Arxiv_2020mime}
S.~P. Karimireddy, M.~Jaggi, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and
  A.~T. Suresh, ``Mime: Mimicking centralized stochastic algorithms in
  federated learning,'' \emph{arXiv preprint arXiv:2008.03606}, 2020.

\bibitem{Sanghvi_FedSTEPH_2020}
R.~Das, A.~Hashemi, S.~Sanghavi, and I.~S. Dhillon, ``Improved convergence
  rates for non-convex federated learning with compression,'' \emph{arXiv
  preprint arXiv:2012.04061}, 2020.

\bibitem{Woodworth_Local_Arxiv_2020}
B.~Woodworth, K.~K. Patel, S.~U. Stich, Z.~Dai, B.~Bullins, H.~B. McMahan,
  O.~Shamir, and N.~Srebro, ``Is local sgd better than minibatch sgd?''
  \emph{arXiv preprint arXiv:2002.07839}, 2020.

\bibitem{Yu_Jin_PMLR_2019dynamicbatches}
H.~Yu and R.~Jin, ``On the computation and communication complexity of parallel
  sgd with dynamic batch sizes for stochastic non-convex optimization,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 7174--7183.

\bibitem{Wang_Joshi_Arxiv_2018cooperative}
J.~Wang and G.~Joshi, ``Cooperative sgd: A unified framework for the design and
  analysis of communication-efficient sgd algorithms,'' 2018.

\bibitem{Khaled_Arxiv_2019}
A.~Khaled, K.~Mishchenko, and P.~Richt{\'a}rik, ``Better communication
  complexity for local sgd,'' \emph{arXiv}, 2019.

\bibitem{Stich_Arxiv_Local_2018}
S.~U. Stich, ``Local sgd converges fast and communicates little,'' \emph{arXiv
  preprint arXiv:1805.09767}, 2018.

\bibitem{Lin_Don't_Arxiv_2018}
T.~Lin, S.~U. Stich, K.~K. Patel, and M.~Jaggi, ``Don't use large mini-batches,
  use local sgd,'' 2018.

\bibitem{Zhou_KStep_IJCAI_2018}
\BIBentryALTinterwordspacing
F.~Zhou and G.~Cong, ``On the convergence properties of a k-step averaging
  stochastic gradient descent algorithm for nonconvex optimization,'' in
  \emph{Proceedings of the Twenty-Seventh International Joint Conference on
  Artificial Intelligence, {IJCAI-18}}.\hskip 1em plus 0.5em minus 0.4em\relax
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018, pp. 3219--3227. [Online]. Available:
  \url{https://doi.org/10.24963/ijcai.2018/447}
\BIBentrySTDinterwordspacing

\bibitem{Sattler_IEEETNN_2019}
F.~Sattler, S.~Wiedemann, K.-R. M{\"u}ller, and W.~Samek, ``Robust and
  communication-efficient federated learning from non-iid data,'' \emph{IEEE
  transactions on neural networks and learning systems}, vol.~31, no.~9, pp.
  3400--3413, 2019.

\bibitem{Zhao_Arxiv_FedNonIID_2018}
Y.~Zhao, M.~Li, L.~Lai, N.~Suda, D.~Civin, and V.~Chandra, ``Federated learning
  with non-iid data,'' \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{Wang_ICLR_2019slowmo}
J.~Wang, V.~Tantia, N.~Ballas, and M.~Rabbat, ``Slowmo: Improving
  communication-efficient distributed sgd with slow momentum,'' \emph{arXiv
  preprint arXiv:1910.00643}, 2019.

\bibitem{Liang_Arxiv_2019_VRL-SGD}
X.~Liang, S.~Shen, J.~Liu, Z.~Pan, E.~Chen, and Y.~Cheng, ``Variance reduced
  local sgd with lower communication complexity,'' \emph{arXiv preprint
  arXiv:1912.12844}, 2019.

\bibitem{Sharma_Arxiv_2019}
P.~Sharma, P.~Khanduri, S.~Bulusu, K.~Rajawat, and P.~K. Varshney, ``Parallel
  restarted {SPIDER} -- communication efficient distributed nonconvex
  optimization with optimal computation complexity,'' \emph{arXiv preprint
  arXiv:1912.06036}, 2019.

\bibitem{Reddi_Arxiv_2019_Adam}
S.~J. Reddi, S.~Kale, and S.~Kumar, ``On the convergence of adam and beyond,''
  \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem{Koloskova_PMLR_2020}
A.~Koloskova, N.~Loizou, S.~Boreiri, M.~Jaggi, and S.~Stich, ``A unified theory
  of decentralized sgd with changing topology and local updates,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2020, pp. 5381--5393.

\bibitem{Johnson_NIPS_2013}
R.~Johnson and T.~Zhang, ``Accelerating stochastic gradient descent using
  predictive variance reduction,'' in \emph{Advances in Neural Information
  Processing Systems 26}.\hskip 1em plus 0.5em minus 0.4em\relax Curran
  Associates, Inc., 2013, pp. 315--323.

\bibitem{Bottou_SIAM_2018_Review}
L.~Bottou, F.~E. Curtis, and J.~Nocedal, ``Optimization methods for large-scale
  machine learning,'' \emph{SIAM Review}, vol.~60, no.~2, pp. 223--311, 2018.

\bibitem{Drori_ICML_2020complexity}
Y.~Drori and O.~Shamir, ``The complexity of finding stationary points with
  stochastic gradient descent,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 2658--2667.

\end{thebibliography}
