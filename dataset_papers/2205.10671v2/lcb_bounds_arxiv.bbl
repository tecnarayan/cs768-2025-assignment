\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bottou et~al.(2013)Bottou, Peters, Qui{\~n}onero-Candela, Charles,
  Chickering, Portugaly, Ray, Simard, and Snelson]{bottou2013counterfactual}
L{\'e}on Bottou, Jonas Peters, Joaquin Qui{\~n}onero-Candela, Denis~X Charles,
  D~Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and
  Ed~Snelson.
\newblock Counterfactual reasoning and learning systems: The example of
  computational advertising.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (11), 2013.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.00360}, 2019.

\bibitem[Chen and Jiang(2022)]{chen2022offline}
Jinglin Chen and Nan Jiang.
\newblock Offline reinforcement learning under value and density-ratio
  realizability: the power of gaps.
\newblock \emph{arXiv preprint arXiv:2203.13935}, 2022.

\bibitem[Cheng et~al.(2022)Cheng, Xie, Jiang, and
  Agarwal]{cheng2022adversarially}
Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
\newblock Adversarially trained actor critic for offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2202.02446}, 2022.

\bibitem[Duan et~al.(2021)Duan, Jin, and Li]{duan2021risk}
Yaqi Duan, Chi Jin, and Zhiyuan Li.
\newblock Risk bounds and rademacher complexity in batch reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2103.13883}, 2021.

\bibitem[Foster et~al.(2020)Foster, Rakhlin, Simchi-Levi, and
  Xu]{foster2020instance}
Dylan~J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock \emph{arXiv preprint arXiv:2010.03104}, 2020.

\bibitem[Foster et~al.(2021{\natexlab{a}})Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021{\natexlab{a}}.

\bibitem[Foster et~al.(2021{\natexlab{b}})Foster, Krishnamurthy, Simchi-Levi,
  and Xu]{foster2021offline}
Dylan~J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu.
\newblock Offline reinforcement learning: Fundamental barriers for value
  function approximation.
\newblock \emph{arXiv preprint arXiv:2111.10919}, 2021{\natexlab{b}}.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[Khamaru et~al.(2020)Khamaru, Pananjady, Ruan, Wainwright, and
  Jordan]{khamaru2020temporal}
Koulik Khamaru, Ashwin Pananjady, Feng Ruan, Martin~J Wainwright, and Michael~I
  Jordan.
\newblock Is temporal difference learning optimal? an instance-dependent
  analysis.
\newblock \emph{arXiv preprint arXiv:2003.07337}, 2020.

\bibitem[Khamaru et~al.(2021)Khamaru, Xia, Wainwright, and
  Jordan]{khamaru2021instance}
Koulik Khamaru, Eric Xia, Martin~J Wainwright, and Michael~I Jordan.
\newblock Instance-optimality in optimal value estimation: Adaptivity via
  variance-reduced q-learning.
\newblock \emph{arXiv preprint arXiv:2106.14352}, 2021.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock {MOReL}: {M}odel-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock \emph{arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: {T}utorial, review, and perspectives
  on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2022)Li, Shi, Chen, Chi, and Wei]{li2022settling}
Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei.
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2204.05275}, 2022.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and Schapire]{li2010contextual}
Lihong Li, Wei Chu, John Langford, and Robert~E Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \emph{Proceedings of the 19th international conference on World
  wide web}, pages 661--670, 2010.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Mania et~al.(2019)Mania, Tu, and Recht]{mania2019certainty}
Horia Mania, Stephen Tu, and Benjamin Recht.
\newblock Certainty equivalence is efficient for linear quadratic control.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Pananjady and Wainwright(2020)]{pananjady2020instance}
Ashwin Pananjady and Martin~J Wainwright.
\newblock Instance-dependent $\ell_\infty$-bounds for policy evaluation in
  tabular reinforcement learning.
\newblock \emph{IEEE Transactions on Information Theory}, 67\penalty0
  (1):\penalty0 566--585, 2020.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[Shi et~al.(2022)Shi, Li, Wei, Chen, and Chi]{shi2022pessimistic}
Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi.
\newblock Pessimistic q-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock \emph{arXiv preprint arXiv:2202.13890}, 2022.

\bibitem[Simchowitz and Foster(2020)]{simchowitz2020naive}
Max Simchowitz and Dylan Foster.
\newblock Naive exploration is optimal for online lqr.
\newblock In \emph{International Conference on Machine Learning}, pages
  8937--8948. PMLR, 2020.

\bibitem[Strehl et~al.(2010)Strehl, Langford, Kakade, and
  Li]{strehl2010learning}
Alex Strehl, John Langford, Sham Kakade, and Lihong Li.
\newblock Learning from logged implicit exploration data.
\newblock \emph{arXiv preprint arXiv:1003.0120}, 2010.

\bibitem[Swaminathan and
  Joachims(2015{\natexlab{a}})]{swaminathan2015counterfactual}
Adith Swaminathan and Thorsten Joachims.
\newblock Counterfactual risk minimization: Learning from logged bandit
  feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  814--823. PMLR, 2015{\natexlab{a}}.

\bibitem[Swaminathan and Joachims(2015{\natexlab{b}})]{swaminathan2015self}
Adith Swaminathan and Thorsten Joachims.
\newblock The self-normalized estimator for counterfactual learning.
\newblock \emph{advances in neural information processing systems}, 28,
  2015{\natexlab{b}}.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{uehara2021representation}
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[Wagenmaker and Jamieson(2022)]{wagenmaker2022instance}
Andrew Wagenmaker and Kevin Jamieson.
\newblock Instance-dependent near-optimal policy identification in linear mdps
  via online experiment design.
\newblock \emph{arXiv preprint arXiv:2207.02575}, 2022.

\bibitem[Wang et~al.(2020)Wang, Foster, and Kakade]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Wu(2020)]{wunotes}
Yihong Wu.
\newblock Lecture notes on: information-theoreticl methods for high-dimensional
  statistics.
\newblock 2020.

\bibitem[Xiao et~al.(2021)Xiao, Wu, Mei, Dai, Lattimore, Li, Szepesvari, and
  Schuurmans]{xiao2021optimality}
Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo~Dai, Tor Lattimore, Lihong Li, Csaba
  Szepesvari, and Dale Schuurmans.
\newblock On the optimality of batch policy optimization algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  11362--11371. PMLR, 2021.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06926}, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Yan et~al.(2022)Yan, Li, Chen, and Fan]{yan2022efficacy}
Yuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan.
\newblock The efficacy of pessimism in asynchronous q-learning.
\newblock \emph{arXiv preprint arXiv:2203.07368}, 2022.

\bibitem[Yin and Wang(2021)]{yin2021towards}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock \emph{arXiv preprint arXiv:2110.08695}, 2021.

\bibitem[Yin et~al.(2022)Yin, Duan, Wang, and Wang]{yin2022near}
Ming Yin, Yaqi Duan, Mengdi Wang, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock \emph{arXiv preprint arXiv:2203.05804}, 2022.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock {MOPO: M}odel-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[Yurtsever et~al.(2020)Yurtsever, Lambert, Carballo, and
  Takeda]{yurtsever2020survey}
Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda.
\newblock A survey of autonomous driving: {C}ommon practices and emerging
  technologies.
\newblock \emph{IEEE Access}, 8:\penalty0 58443--58469, 2020.

\bibitem[Zanette(2021)]{zanette2021exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online rl.
\newblock In \emph{International Conference on Machine Learning}, pages
  12287--12297. PMLR, 2021.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2108.08812}, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline}
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason~D Lee.
\newblock Offline reinforcement learning with realizability and single-policy
  concentrability.
\newblock \emph{arXiv preprint arXiv:2202.04634}, 2022.

\end{thebibliography}
