\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cai et~al.(2019)Cai, Zhu, and Han]{cai2018proxylessnas}
Cai, H., Zhu, L., and Han, S.
\newblock Proxyless{NAS}: Direct neural architecture search on target task and
  hardware.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HylVB3AqYm}.

\bibitem[Chen et~al.(2014)Chen, Luo, Liu, Zhang, He, Wang, Li, Chen, Xu, Sun,
  et~al.]{chen2014dadiannao}
Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu,
  Z., Sun, N., et~al.
\newblock Dadiannao: A machine-learning supercomputer.
\newblock In \emph{2014 47th Annual IEEE/ACM International Symposium on
  Microarchitecture}, pp.\  609--622. IEEE, 2014.

\bibitem[Chollet(2017)]{chollet2017xception}
Chollet, F.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1251--1258, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{denton2014exploiting}
Denton, E.~L., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1269--1277, 2014.

\bibitem[Ding et~al.(2019{\natexlab{a}})Ding, Ding, Guo, and
  Han]{ding2019centripetal}
Ding, X., Ding, G., Guo, Y., and Han, J.
\newblock Centripetal sgd for pruning very deep convolutional networks with
  complicated structure.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4943--4953, 2019{\natexlab{a}}.

\bibitem[Ding et~al.(2019{\natexlab{b}})Ding, Ding, Guo, Han, and
  Yan]{ding2019approximated}
Ding, X., Ding, G., Guo, Y., Han, J., and Yan, C.
\newblock Approximated oracle filter pruning for destructive cnn width
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1607--1616, 2019{\natexlab{b}}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, B. and Stork, D.~G.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  164--171, 1993.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and Yang]{he2018soft}
He, Y., Kang, G., Dong, X., Fu, Y., and Yang, Y.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1808.06866}, 2018{\natexlab{a}}.

\bibitem[He et~al.(2018{\natexlab{b}})He, Lin, Liu, Wang, Li, and
  Han]{he2018amc}
He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  784--800, 2018{\natexlab{b}}.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019filter}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4340--4349, 2019.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Iandola, F.~N., Han, S., Moskewicz, M.~W., Ashraf, K., Dally, W.~J., and
  Keutzer, K.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Kim et~al.(2015)Kim, Park, Yoo, Choi, Yang, and
  Shin]{kim2015compression}
Kim, Y.-D., Park, E., Yoo, S., Choi, T., Yang, L., and Shin, D.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock \emph{arXiv preprint arXiv:1511.06530}, 2015.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Lebedev et~al.(2014)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev2014speeding}
Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., and Lempitsky, V.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock \emph{arXiv preprint arXiv:1412.6553}, 2014.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Lin et~al.(2013)Lin, Chen, and Yan]{lin2013network}
Lin, M., Chen, Q., and Yan, S.
\newblock Network in network.
\newblock \emph{arXiv preprint arXiv:1312.4400}, 2013.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS}: Differentiable architecture search.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=S1eYHoC5FX}.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2736--2744, 2017.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Mu, Zhang, Guo, Yang, Cheng, and
  Sun]{liu2019metapruning}
Liu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, K.-T., and Sun, J.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  3296--3305, 2019{\natexlab{b}}.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{arXiv preprint arXiv:1712.01312}, 2017.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Luo, J.-H., Wu, J., and Lin, W.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5058--5066, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Py{T}orch(2019)]{pytorchmodel}
Py{T}orch.
\newblock Torchvision models, 2019.
\newblock URL
  \url{https://github.com/pytorch/vision/tree/master/torchvision/models}.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4510--4520, 2018.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1--9, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Tai et~al.(2016)Tai, Xiao, Zhang, Wang, and
  Weinan]{tai2016convolutional}
Tai, C., Xiao, T., Zhang, Y., Wang, X., and Weinan, E.
\newblock Convolutional neural networks with low-rank regularization.
\newblock In \emph{4th International Conference on Learning Representations,
  ICLR 2016}, 2016.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6105--6114, 2019.

\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and
  Le]{tan2019mnasnet}
Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le,
  Q.~V.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2820--2828, 2019.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
Ullrich, K., Meeds, E., and Welling, M.
\newblock Soft weight-sharing for neural network compression.
\newblock \emph{arXiv preprint arXiv:1702.04008}, 2017.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2074--2082, 2016.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1492--1500, 2017.

\bibitem[Yang et~al.(2019)Yang, Wen, and Li]{yang2019deephoyer}
Yang, H., Wen, W., and Li, H.
\newblock Deephoyer: Learning sparser neural network with differentiable
  scale-invariant sparsity measures.
\newblock \emph{arXiv preprint arXiv:1908.09979}, 2019.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Wang, Figueiredo, and
  Balzano]{zhang2018learning}
Zhang, D., Wang, H., Figueiredo, M., and Balzano, L.
\newblock Learning to {S}hare: {S}imultaneous {P}arameter {T}ying and
  {S}parsification in {D}eep {L}earning.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=rypT3fb0b}.

\bibitem[Zhang et~al.(2015)Zhang, Zou, He, and Sun]{zhang2015accelerating}
Zhang, X., Zou, J., He, K., and Sun, J.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (10):\penalty0 1943--1955, 2015.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Zhou, Lin, and
  Sun]{zhang2018shufflenet}
Zhang, X., Zhou, X., Lin, M., and Sun, J.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  6848--6856, 2018{\natexlab{b}}.

\end{thebibliography}
