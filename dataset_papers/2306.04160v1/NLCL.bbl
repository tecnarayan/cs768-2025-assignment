\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acharya et~al.(2022)Acharya, Sanghavi, Jing, Bhushanam, Choudhary,
  Rabbat, and Dhillon]{acharya2022positive}
Acharya, A., Sanghavi, S., Jing, L., Bhushanam, B., Choudhary, D., Rabbat, M.,
  and Dhillon, I.
\newblock Positive unlabeled contrastive learning.
\newblock \emph{arXiv preprint arXiv:2206.01206}, 2022.

\bibitem[Aitchison(2021)]{aitchison2021infonce}
Aitchison, L.
\newblock Infonce is a variational autoencoder.
\newblock \emph{arXiv preprint arXiv:2107.02495}, 2021.

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Ash et~al.(2022)Ash, Goel, Krishnamurthy, and
  Misra]{ash2022investigating}
Ash, J., Goel, S., Krishnamurthy, A., and Misra, D.
\newblock Investigating the role of negatives in contrastive representation
  learning.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Assran et~al.(2020)Assran, Ballas, Castrejon, and
  Rabbat]{assran2020supervision}
Assran, M., Ballas, N., Castrejon, L., and Rabbat, M.
\newblock Supervision accelerates pre-training in contrastive semi-supervised
  learning of visual representations.
\newblock \emph{arXiv preprint arXiv:2006.10803}, 2020.

\bibitem[Bao et~al.(2022)Bao, Nagano, and Nozawa]{bao2022surrogate}
Bao, H., Nagano, Y., and Nozawa, K.
\newblock On the surrogate gap between contrastive and supervised losses.
\newblock In \emph{ICML}, 2022.

\bibitem[Chen et~al.(2022)Chen, Fu, Narayan, Zhang, Song, Fatahalian, and
  R{\'e}]{chen2022perfectly}
Chen, M., Fu, D.~Y., Narayan, A., Zhang, M., Song, Z., Fatahalian, K., and
  R{\'e}, C.
\newblock Perfectly balanced: Improving transfer and robustness of supervised
  contrastive learning.
\newblock In \emph{ICML}, 2022.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020.

\bibitem[Chen \& He(2021)Chen and He]{chen2021exploring}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Cheng et~al.(2021)Cheng, Zhu, Sun, and Liu]{cheng2021demystifying}
Cheng, H., Zhu, Z., Sun, X., and Liu, Y.
\newblock Demystifying how self-supervised features improve training from noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2110.09022}, 2021.

\bibitem[Chuang et~al.(2022)Chuang, Hjelm, Wang, Vineet, Joshi, Torralba,
  Jegelka, and Song]{chuang2022robust}
Chuang, C.-Y., Hjelm, R.~D., Wang, X., Vineet, V., Joshi, N., Torralba, A.,
  Jegelka, S., and Song, Y.
\newblock Robust contrastive learning against noisy views.
\newblock In \emph{CVPR}, 2022.

\bibitem[Fulton(2000)]{fulton2000eigenvalues}
Fulton, W.
\newblock Eigenvalues, invariant factors, highest weights, and schubert
  calculus.
\newblock \emph{Bulletin of the American Mathematical Society}, 37\penalty0
  (3):\penalty0 209--249, 2000.

\bibitem[Ghosh \& Lan(2021)Ghosh and Lan]{ghosh2021contrastive}
Ghosh, A. and Lan, A.
\newblock Contrastive learning improves model robustness under label noise.
\newblock In \emph{CVPR}, 2021.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Hu et~al.(2023)Hu, Liu, Zhou, Wang, and Huang]{hu2022your}
Hu, T., Liu, Z., Zhou, F., Wang, W., and Huang, W.
\newblock Your contrastive learning is secretly doing stochastic neighbor
  embedding.
\newblock In \emph{ICLR}, 2023.

\bibitem[Huang et~al.(2023)Huang, Yi, Zhao, and Jiang]{huang2023towards}
Huang, W., Yi, M., Zhao, X., and Jiang, Z.
\newblock Towards the generalization of contrastive self-supervised learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Islam et~al.(2021)Islam, Chen, Panda, Karlinsky, Radke, and
  Feris]{islam2021broad}
Islam, A., Chen, C.-F.~R., Panda, R., Karlinsky, L., Radke, R., and Feris, R.
\newblock A broad study on the transferability of visual representations with
  contrastive learning.
\newblock In \emph{ICCV}, pp.\  8845--8855, 2021.

\bibitem[Johnson et~al.(2023)Johnson, Hanchi, and
  Maddison]{johnson2022contrastive}
Johnson, D.~D., Hanchi, A.~E., and Maddison, C.~J.
\newblock Contrastive learning can find an optimal basis for approximately
  view-invariant functions.
\newblock In \emph{ICLR}, 2023.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
  A., Liu, C., and Krishnan, D.
\newblock Supervised contrastive learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Kim et~al.(2021{\natexlab{a}})Kim, Choo, Kwon, Joe, Min, and
  Gwon]{kim2021selfmatch}
Kim, B., Choo, J., Kwon, Y.-D., Joe, S., Min, S., and Gwon, Y.
\newblock Selfmatch: Combining contrastive self-supervision and consistency for
  semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2101.06480}, 2021{\natexlab{a}}.

\bibitem[Kim et~al.(2019)Kim, Yim, Yun, and Kim]{kim2019nlnl}
Kim, Y., Yim, J., Yun, J., and Kim, J.
\newblock Nlnl: Negative learning for noisy labels.
\newblock In \emph{ICCV}, 2019.

\bibitem[Kim et~al.(2021{\natexlab{b}})Kim, Yun, Shon, and Kim]{kim2021joint}
Kim, Y., Yun, J., Shon, H., and Kim, J.
\newblock Joint negative and positive learning for noisy labels.
\newblock In \emph{CVPR}, 2021{\natexlab{b}}.

\bibitem[Ko et~al.(2022)Ko, Mohapatra, Liu, Chen, Daniel, and
  Weng]{ko2022revisiting}
Ko, C.-Y., Mohapatra, J., Liu, S., Chen, P.-Y., Daniel, L., and Weng, L.
\newblock Revisiting contrastive learning through the lens of neighborhood
  component analysis: an integrated framework.
\newblock In \emph{ICML}, 2022.

\bibitem[Lee et~al.(2022)Lee, Kim, Kim, Cheon, Cho, and
  Han]{lee2022contrastive}
Lee, D., Kim, S., Kim, I., Cheon, Y., Cho, M., and Han, W.-S.
\newblock Contrastive regularization for semi-supervised learning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Li et~al.(2022)Li, Xia, Ge, and Liu]{li2022selective}
Li, S., Xia, X., Ge, S., and Liu, T.
\newblock Selective-supervised contrastive learning with noisy labels.
\newblock In \emph{CVPR}, 2022.

\bibitem[Navaneet et~al.(2022)Navaneet, Abbasi~Koohpayegani, Tejankar,
  Pourahmadi, Subramanya, and Pirsiavash]{navaneet2022constrained}
Navaneet, K., Abbasi~Koohpayegani, S., Tejankar, A., Pourahmadi, K.,
  Subramanya, A., and Pirsiavash, H.
\newblock Constrained mean shift using distant yet related neighbors for
  representation learning.
\newblock In \emph{ECCV 2022}, 2022.

\bibitem[Nozawa \& Sato(2021)Nozawa and Sato]{nozawa2021understanding}
Nozawa, K. and Sato, I.
\newblock Understanding negative samples in instance discriminative
  self-supervised representation learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ortego et~al.(2021)Ortego, Arazo, Albert, O'Connor, and
  McGuinness]{ortego2021multi}
Ortego, D., Arazo, E., Albert, P., O'Connor, N.~E., and McGuinness, K.
\newblock Multi-objective interpolation training for robustness to label noise.
\newblock In \emph{CVPR}, 2021.

\bibitem[Shen et~al.(2022)Shen, Jones, Kumar, Xie, HaoChen, Ma, and
  Liang]{shen2022connect}
Shen, K., Jones, R.~M., Kumar, A., Xie, S.~M., HaoChen, J.~Z., Ma, T., and
  Liang, P.
\newblock Connect, not collapse: Explaining contrastive learning for
  unsupervised domain adaptation.
\newblock In \emph{ICML}, 2022.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Geng, Jiang, Li, Wang, Yang, and
  Lin]{wang2021residual}
Wang, Y., Geng, Z., Jiang, F., Li, C., Wang, Y., Yang, J., and Lin, Z.
\newblock Residual relaxation for multi-view representation learning.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Zhang, Wang, Yang, and
  Lin]{wang2021chaos}
Wang, Y., Zhang, Q., Wang, Y., Yang, J., and Lin, Z.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Zhang, Du, Yang, Lin, and
  Wang]{wang2023message}
Wang, Y., Zhang, Q., Du, T., Yang, J., Lin, Z., and Wang, Y.
\newblock A message passing perspective on learning dynamics of contrastive
  learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Xue et~al.(2022)Xue, Whitecross, and
  Mirzasoleiman]{xue2022investigating}
Xue, Y., Whitecross, K., and Mirzasoleiman, B.
\newblock Investigating why contrastive learning benefits robustness against
  label noise.
\newblock In \emph{ICML}, 2022.

\bibitem[Yan et~al.(2022)Yan, Luo, Xu, Deng, and Huang]{yan2022noise}
Yan, J., Luo, L., Xu, C., Deng, C., and Huang, H.
\newblock Noise is also useful: Negative correlation-steered latent contrastive
  learning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yang et~al.(2022)Yang, Wu, Zhang, Jiang, Liu, Zheng, Zhang, Wang, and
  Zeng]{yang2022class}
Yang, F., Wu, K., Zhang, S., Jiang, G., Liu, Y., Zheng, F., Zhang, W., Wang,
  C., and Zeng, L.
\newblock Class-aware contrastive semi-supervised learning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yao et~al.(2021)Yao, Sun, Zhang, Shen, Wu, Zhang, and Tang]{yao2021jo}
Yao, Y., Sun, Z., Zhang, C., Shen, F., Wu, Q., Zhang, J., and Tang, Z.
\newblock Jo-src: A contrastive approach for combating noisy labels.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Yuan, Yao, and
  Huang]{zhang2022learning}
Zhang, M., Yuan, C., Yao, J., and Huang, W.
\newblock Learning with noisily-labeled class-imbalanced data.
\newblock \emph{arXiv preprint arXiv:2211.10955}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Zhang, Li, Qiu, Xu, and
  Tian]{zhang2022semi}
Zhang, Y., Zhang, X., Li, J., Qiu, R., Xu, H., and Tian, Q.
\newblock Semi-supervised contrastive learning with similarity co-calibration.
\newblock \emph{IEEE Transactions on Multimedia}, 2022{\natexlab{b}}.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
Zimmermann, R.~S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W.
\newblock Contrastive learning inverts the data generating process.
\newblock In \emph{ICML}, 2021.

\end{thebibliography}
