\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, and Yang]{agarwal2020model}
Alekh Agarwal, Sham Kakade, and Lin~F Yang.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In \emph{Conference on Learning Theory}, pages 67--83. PMLR, 2020.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2020learning}
OpenAI:~Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob
  McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
  Ray, et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{The International Journal of Robotics Research}, 39\penalty0
  (1):\penalty0 3--20, 2020.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert~J Kappen.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Chen and Jiang(2019)]{pmlr-v97-chen19e}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  1042--1051. PMLR, 09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/chen19e.html}.

\bibitem[Chen et~al.(2019)Chen, Beutel, Covington, Jain, Belletti, and
  Chi]{chen2019top}
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and
  Ed~H Chi.
\newblock Top-k off-policy correction for a reinforce recommender system.
\newblock In \emph{Proceedings of the Twelfth ACM International Conference on
  Web Search and Data Mining}, pages 456--464, 2019.

\bibitem[Cui and Yang(2020)]{cui2020plug}
Qiwen Cui and Lin~F Yang.
\newblock Is plug-in solver sample-efficient for feature-based reinforcement
  learning?
\newblock \emph{arXiv preprint arXiv:2010.05673}, 2020.

\bibitem[Dai et~al.(2017)Dai, Khalil, Zhang, Dilkina, and
  Song]{dai2017learning}
Hanjun Dai, Elias~B Khalil, Yuyu Zhang, Bistra Dilkina, and Le~Song.
\newblock Learning combinatorial optimization algorithms over graphs.
\newblock \emph{arXiv preprint arXiv:1704.01665}, 2017.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 5717--5727, 2017.

\bibitem[Duan and Wang(2020)]{duan2020minimax}
Yaqi Duan and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2002.09516}, 2020.

\bibitem[Feng et~al.(2020)Feng, Ren, Tang, and Liu]{feng2020accountable}
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu.
\newblock Accountable off-policy evaluation with kernel bellman statistics.
\newblock In \emph{International Conference on Machine Learning}, pages
  3102--3111. PMLR, 2020.

\bibitem[Freedman(1975)]{freedman1975tail}
David~A Freedman.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pages 100--118, 1975.

\bibitem[Gelada and Bellemare(2019)]{gelada2019off}
Carles Gelada and Marc~G Bellemare.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3647--3655, 2019.

\bibitem[Ghandeharioun et~al.(2019)Ghandeharioun, Shen, Jaques, Ferguson,
  Jones, Lapedriza, and Picard]{ghandeharioun2019approximating}
Asma Ghandeharioun, Judy~Hanwen Shen, Natasha Jaques, Craig Ferguson, Noah
  Jones, Agata Lapedriza, and Rosalind Picard.
\newblock Approximating interactive human evaluation with self-play for
  open-domain dialog systems.
\newblock \emph{arXiv preprint arXiv:1906.09308}, 2019.

\bibitem[Gottesman et~al.(2019)Gottesman, Johansson, Komorowski, Faisal,
  Sontag, Doshi-Velez, and Celi]{gottesman2019guidelines}
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David
  Sontag, Finale Doshi-Velez, and Leo~Anthony Celi.
\newblock Guidelines for reinforcement learning in healthcare.
\newblock \emph{Nature medicine}, 25\penalty0 (1):\penalty0 16--18, 2019.

\bibitem[Hallak and Mannor(2017)]{hallak2017consistent}
Assaf Hallak and Shie Mannor.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1372--1383. PMLR, 2017.

\bibitem[Jiang and Agarwal(2018)]{jiang2018open}
Nan Jiang and Alekh Agarwal.
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In \emph{Conference On Learning Theory}, pages 3395--3398. PMLR,
  2018.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018qlearning}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, pages 4863--4873. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf}.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kallus and Uehara(2019)]{kallus2019efficiently}
Nathan Kallus and Masatoshi Uehara.
\newblock Efficiently breaking the curse of horizon in off-policy evaluation
  with double reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1909.05850}, 2019.

\bibitem[Kallus and Uehara(2020)]{kallus2020double}
Nathan Kallus and Masatoshi Uehara.
\newblock Double reinforcement learning for efficient off-policy evaluation in
  markov decision processes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (167):\penalty0 1--63, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lattimore and Hutter(2014)]{lattimore2014near}
Tor Lattimore and Marcus Hutter.
\newblock Near-optimal pac bounds for discounted mdps.
\newblock \emph{Theoretical Computer Science}, 558:\penalty0 125--143, 2014.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \emph{arXiv preprint arXiv:2005.12900}, 2020.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015toward}
Lihong Li, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 608--616.
  PMLR, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock \emph{arXiv preprint arXiv:1810.12429}, 2018.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and Brunskill]{liu2019off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with state distribution correction.
\newblock \emph{arXiv preprint arXiv:1904.08473}, 2019.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with stationary distribution correction.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1180--1190.
  PMLR, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020{\natexlab{b}}.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and
  Popovic]{mandel2014offline}
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic.
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In \emph{AAMAS}, pages 1077--1084, 2014.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and Talwalkar]{mohri2012book}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock The MIT Press, 2012.
\newblock ISBN 026201825X.

\bibitem[Munos(2003)]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{ICML}, volume~3, pages 560--567, 2003.

\bibitem[Murphy et~al.(2001)Murphy, van~der Laan, Robins, and
  Group]{murphy2001marginal}
Susan~A Murphy, Mark~J van~der Laan, James~M Robins, and Conduct Problems
  Prevention~Research Group.
\newblock Marginal mean models for dynamic regimes.
\newblock \emph{Journal of the American Statistical Association}, 96\penalty0
  (456):\penalty0 1410--1423, 2001.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2318--2328, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Pananjady and Wainwright(2020)]{pananjady2020instance}
Ashwin Pananjady and Martin~J Wainwright.
\newblock Instance-dependent l-infinity-bounds for policy evaluation in tabular
  reinforcement learning.
\newblock \emph{IEEE Transactions on Information Theory}, 67\penalty0
  (1):\penalty0 566--585, 2020.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  9659--9668. PMLR, 2020.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Sham~M Kakade.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Xie and Jiang(2020{\natexlab{a}})]{pmlr-v124-xie20a}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In Jonas Peters and David Sontag, editors, \emph{Proceedings of the
  36th Conference on Uncertainty in Artificial Intelligence (UAI)}, volume 124
  of \emph{Proceedings of Machine Learning Research}, pages 550--559. PMLR,
  03--06 Aug 2020{\natexlab{a}}.
\newblock URL \url{http://proceedings.mlr.press/v124/xie20a.html}.

\bibitem[Xie and Jiang(2020{\natexlab{b}})]{xie2020batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock \emph{arXiv preprint arXiv:2008.04990}, 2020{\natexlab{b}}.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock \emph{arXiv preprint arXiv:1906.03393}, 2019.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004. PMLR, 2019.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{yang2020off}
Mengjiao Yang, Ofir Nachum, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock \emph{arXiv preprint arXiv:2007.03438}, 2020.

\bibitem[Yin and Wang(2020)]{yin2020asymptotically}
Ming Yin and Yu-Xiang Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 3948--3958. PMLR, 26--28 Aug 2020.

\bibitem[Yin et~al.(2020)Yin, Bai, and Wang]{yin2020near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021nearoptimal}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction, 2021.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock \emph{arXiv preprint arXiv:2002.09072}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Ji, and
  Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020{\natexlab{b}}.

\end{thebibliography}
