\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baevski \& Auli(2019)Baevski and Auli]{baevski2018adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Bahrick et~al.(2008)Bahrick, Hall, and Da~Costa]{bahrick2008fifty}
Bahrick, H.~P., Hall, L.~K., and Da~Costa, L.~A.
\newblock Fifty years of memory of college grades: Accuracy and distortions.
\newblock \emph{Emotion}, 8\penalty0 (1):\penalty0 13, 2008.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deep}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock Deep equilibrium models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  688--699, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song,
  Davis, Sarlos, Belanger, Colwell, and Weller]{choromanski2020masked}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Davis, J., Sarlos, T.,
  Belanger, D., Colwell, L., and Weller, A.
\newblock Masked language modeling for proteins via linearly scalable
  long-context transformers.
\newblock \emph{arXiv preprint arXiv:2006.03555}, 2020.

\bibitem[Correia et~al.(2019)Correia, Niculae, and
  Martins]{correia2019adaptively}
Correia, G.~M., Niculae, V., and Martins, A.~F.
\newblock Adaptively sparse transformers.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2174--2184, 2019.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J.~G., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{{ACL} {(1)}}, pp.\  2978--2988. Association for
  Computational Linguistics, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT (1)}, 2019.

\bibitem[Elman(1990)]{Elman1990FindingSI}
Elman, J.
\newblock Finding structure in time.
\newblock \emph{Cogn. Sci.}, 14:\penalty0 179--211, 1990.

\bibitem[Fan et~al.(2019{\natexlab{a}})Fan, Gardent, Braud, and
  Bordes]{fan2019using}
Fan, A., Gardent, C., Braud, C., and Bordes, A.
\newblock Using local knowledge graph construction to scale seq2seq models to
  multi-document inputs.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4177--4187,
  2019{\natexlab{a}}.

\bibitem[Fan et~al.(2019{\natexlab{b}})Fan, Lewis, and
  Dauphin]{fan2019strategies}
Fan, A., Lewis, M., and Dauphin, Y.
\newblock Strategies for structuring story generation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2650--2660, 2019{\natexlab{b}}.

\bibitem[Fan et~al.(2020{\natexlab{a}})Fan, Grave, and Joulin]{Fan2020Reducing}
Fan, A., Grave, E., and Joulin, A.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Fan et~al.(2020{\natexlab{b}})Fan, Lavril, Grave, Joulin, and
  Sukhbaatar]{fan2020accessing}
Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar, S.
\newblock Addressing some limitations of transformers with feedback memory.
\newblock \emph{arXiv preprint arXiv:2002.09402}, 2020{\natexlab{b}}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Grave et~al.(2017)Grave, Joulin, Ciss{\'e}, and
  J{\'e}gou]{grave2017efficient}
Grave, E., Joulin, A., Ciss{\'e}, M., and J{\'e}gou, H.
\newblock Efficient softmax approximation for gpus.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Izacard \& Grave(2020)Izacard and Grave]{izacard2020leveraging}
Izacard, G. and Grave, E.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2007.01282}, 2020.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2019)Kitaev, Kaiser, and Levskaya]{kitaev2019reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lample et~al.(2019)Lample, Sablayrolles, Ranzato, Denoyer, and
  J{\'e}gou]{lample2019large}
Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and J{\'e}gou, H.
\newblock Large memory layers with product keys.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8548--8559, 2019.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Mahoney(2011)]{mahoney2011large}
Mahoney, M.
\newblock Large text compression benchmark.
\newblock \emph{URL: http://www. mattmahoney. net/text/text. html}, 2011.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Mikolov, T., Karafi{\'a}t, M., Burget, L., {\v{C}}ernock{\`y}, J., and
  Khudanpur, S.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh annual conference of the international speech
  communication association}, 2010.

\bibitem[Murre \& Dros(2015)Murre and Dros]{Murre2015ReplicationAA}
Murre, J. and Dros, J.
\newblock Replication and analysis of ebbinghausâ€™ forgetting curve.
\newblock \emph{PLoS ONE}, 10, 2015.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu,
  G{\"{u}}l{\c{c}}ehre, Jayakumar, Jaderberg, Kaufman, Clark, Noury, Botvinick,
  Heess, and Hadsell]{Parisotto2019StabilizingTF}
Parisotto, E., Song, H.~F., Rae, J.~W., Pascanu, R., G{\"{u}}l{\c{c}}ehre,
  {\c{C}}., Jayakumar, S.~M., Jaderberg, M., Kaufman, R.~L., Clark, A., Noury,
  S., Botvinick, M., Heess, N., and Hadsell, R.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021random}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.~A., and Kong, L.
\newblock Random feature attention.
\newblock \emph{arXiv preprint arXiv:2103.02143}, 2021.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and
  Lillicrap]{rae2020compressive}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Rives et~al.(2019)Rives, Goyal, Meier, Guo, Ott, Zitnick, Ma, and
  Fergus]{Rives2019BiologicalSA}
Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.~L., Ma, J., and
  Fergus, R.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{bioRxiv}, 2019.

\bibitem[Roller et~al.(2020)Roller, Dinan, Goyal, Ju, Williamson, Liu, Xu, Ott,
  Shuster, Smith, et~al.]{roller2020recipes}
Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott,
  M., Shuster, K., Smith, E.~M., et~al.
\newblock Recipes for building an open-domain chatbot.
\newblock \emph{arXiv preprint arXiv:2004.13637}, 2020.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{roy2020efficient}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{arXiv preprint arXiv:2003.05997}, 2020.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Schlag, I., Irie, K., and Schmidhuber, J.
\newblock Linear transformers are secretly fast weight memory systems.
\newblock \emph{arXiv preprint arXiv:2102.11174}, 2021.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock In \emph{{NAACL-HLT} {(2)}}, 2018.

\bibitem[Sukhbaatar et~al.(2015{\natexlab{a}})Sukhbaatar, Szlam, Synnaeve,
  Chintala, and Fergus]{Sukhbaatar2015MazeBaseAS}
Sukhbaatar, S., Szlam, A., Synnaeve, G., Chintala, S., and Fergus, R.
\newblock Mazebase: A sandbox for learning from games.
\newblock \emph{ArXiv}, abs/1511.07401, 2015{\natexlab{a}}.

\bibitem[Sukhbaatar et~al.(2015{\natexlab{b}})Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R.
\newblock End-to-end memory networks.
\newblock In \emph{{NIPS}}, 2015{\natexlab{b}}.

\bibitem[Sukhbaatar et~al.(2019{\natexlab{a}})Sukhbaatar, Grave, Bojanowski,
  and Joulin]{sukhbaatar2019adaptive}
Sukhbaatar, S., Grave, {\'E}., Bojanowski, P., and Joulin, A.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  331--335, 2019{\natexlab{a}}.

\bibitem[Sukhbaatar et~al.(2019{\natexlab{b}})Sukhbaatar, Grave, Lample, Jegou,
  and Joulin]{sukhbaatar2019augmenting}
Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin, A.
\newblock Augmenting self-attention with persistent memory.
\newblock \emph{arXiv preprint arXiv:1907.01470}, 2019{\natexlab{b}}.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{tay2020efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Urbanek et~al.(2019)Urbanek, Fan, Karamcheti, Jain, Humeau, Dinan,
  Rockt{\"a}schel, Kiela, Szlam, and Weston]{urbanek2019learning}
Urbanek, J., Fan, A., Karamcheti, S., Jain, S., Humeau, S., Dinan, E.,
  Rockt{\"a}schel, T., Kiela, D., Szlam, A., and Weston, J.
\newblock Learning to speak and act in a fantasy text adventure game.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  673--683, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wixted(2004)]{wixted2004psychology}
Wixted, J.~T.
\newblock The psychology and neuroscience of forgetting.
\newblock \emph{Annu. Rev. Psychol.}, 55:\penalty0 235--269, 2004.

\bibitem[Wu et~al.(2019)Wu, Feichtenhofer, Fan, He, Krahenbuhl, and
  Girshick]{wu2019long}
Wu, C.-Y., Feichtenhofer, C., Fan, H., He, K., Krahenbuhl, P., and Girshick, R.
\newblock Long-term feature banks for detailed video understanding.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  284--293, 2019.

\bibitem[Wu et~al.(2018)Wu, Fan, Baevski, Dauphin, and Auli]{wu2018pay}
Wu, F., Fan, A., Baevski, A., Dauphin, Y., and Auli, M.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ye et~al.(2019)Ye, Guo, Gan, Qiu, and Zhang]{ye2019bp}
Ye, Z., Guo, Q., Gan, Q., Qiu, X., and Zhang, Z.
\newblock Bp-transformer: Modelling long-range context via binary partitioning.
\newblock \emph{arXiv preprint arXiv:1911.04070}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Luan, Sun, Zhai, Xu, Zhang, and
  Liu]{zhang2018improving}
Zhang, J., Luan, H., Sun, M., Zhai, F., Xu, J., Zhang, M., and Liu, Y.
\newblock Improving the transformer translation model with document-level
  context.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  533--542, 2018.

\end{thebibliography}
