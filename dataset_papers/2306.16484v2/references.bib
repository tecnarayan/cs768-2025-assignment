@Article{Hydra,
  author  = {Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal = {Journal of Machine Learning Research},
  title   = {Distributed coordinate descent method for learning with big data},
  year    = {2016},
  number  = {75},
  pages   = {1--25},
  volume  = {17},
  groups  = {richtap:1},
}

@article{zhang2019gradient,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1905.11881},
  year={2019}
}


@article{zhang2020improved,
  title={Improved analysis of clipping algorithms for non-convex optimization},
  author={Zhang, Bohang and Jin, Jikai and Fang, Cong and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15511--15521},
  year={2020}
}



@article{jin2021non,
  title={Non-convex distributionally robust optimization: Non-asymptotic analysis},
  author={Jin, Jikai and Zhang, Bohang and Wang, Haiyang and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2771--2782},
  year={2021}
}


@article{yang2022normalized,
  title={Normalized/Clipped {SGD} with Perturbation for Differentially Private Non-Convex Optimization},
  author={Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2206.13033},
  year={2022}
}



@article{wu2022motley,
  title={Motley: Benchmarking Heterogeneity and Personalization in Federated Learning},
  author={Wu, Shanshan and Li, Tian and Charles, Zachary and Xiao, Yu and Liu, Ziyu and Xu, Zheng and Smith, Virginia},
  journal={arXiv preprint arXiv:2206.09262},
  year={2022}
}

@article{yu2020salvaging,
  title={Salvaging federated learning by local adaptation},
  author={Yu, Tao and Bagdasaryan, Eugene and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2002.04758},
  year={2020}
}

@inproceedings{gasanov2022flix,
  author    = {Elnur Gasanov and
               Ahmed Khaled and
               Samuel Horv{\'{a}}th and
               Peter Richt{\'{a}}rik},
  title     = {{FLIX:} {A} Simple and Communication-Efficient Alternative to Local
               Methods in Federated Learning},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v151/gasanov22a/gasanov22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/gasanov22a.html}
}

@article{hanzely2020mixture,
  author    = {Filip Hanzely and
               Peter Richt{\'{a}}rik},
  title     = {Federated Learning of a Mixture of Global and Local Models},
  journal={arXiv preprint arXiv:2002.05516},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.05516},
  eprinttype = {arXiv},
  eprint    = {2002.05516},
  timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-05516.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hanzely2020lower,
  title={Lower bounds and optimal algorithms for personalized federated learning},
  author={Hanzely, Filip and Hanzely, Slavom{\'\i}r and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}

@InProceedings{konecny2019improving,
  author    = {Jiang, Yihan and Kone\v{c}n\'{y}, Jakub and Rush, Keith and Kannan, Sreeram},
  title     = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}

@InProceedings{konecny2016fedlearn,
  author    = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Yu, Felix and Richt\'{a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  booktitle = {NIPS Private Multi-Party Machine Learning Workshop},
  title     = {Federated learning: strategies for improving communication efficiency},
  year      = {2016},
}

@article{konevcny2016federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\'y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}

@article{jiang2019improving,
  title={Improving federated learning personalization via model agnostic meta learning},
  author={Jiang, Yihan and Kone{\v{c}}n{\'y}, Jakub and Rush, Keith and Kannan, Sreeram},
  journal={arXiv preprint arXiv:1909.12488},
  year={2019}
}
@article{mansour2020three,
  title={Three approaches for personalization with applications to federated learning},
  author={Mansour, Yishay and Mohri, Mehryar and Ro, Jae and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:2002.10619},
  year={2020}
}

@article{kairouz2021advances_,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@article{kairouz2021advances,
  author    = {Peter Kairouz and
               H. Brendan McMahan and
               Brendan Avent and
               Aur{\'{e}}lien Bellet and
               Mehdi Bennis and
               Arjun Nitin Bhagoji and
               Kallista A. Bonawitz and
               Zachary Charles and
               Graham Cormode and
               Rachel Cummings and
               Rafael G. L. D'Oliveira and
               Hubert Eichner and
               Salim El Rouayheb and
               David Evans and
               Josh Gardner and
               Zachary Garrett and
               Adri{\`{a}} Gasc{\'{o}}n and
               Badih Ghazi and
               Phillip B. Gibbons and
               Marco Gruteser and
               Za{\"{\i}}d Harchaoui and
               Chaoyang He and
               Lie He and
               Zhouyuan Huo and
               Ben Hutchinson and
               Justin Hsu and
               Martin Jaggi and
               Tara Javidi and
               Gauri Joshi and
               Mikhail Khodak and
               Jakub Kone{\v{c}}n{\'y} and
               Aleksandra Korolova and
               Farinaz Koushanfar and
               Sanmi Koyejo and
               Tancr{\`{e}}de Lepoint and
               Yang Liu and
               Prateek Mittal and
               Mehryar Mohri and
               Richard Nock and
               Ayfer {\"{O}}zg{\"{u}}r and
               Rasmus Pagh and
               Hang Qi and
               Daniel Ramage and
               Ramesh Raskar and
               Mariana Raykova and
               Dawn Song and
               Weikang Song and
               Sebastian U. Stich and
               Ziteng Sun and
               Ananda Theertha Suresh and
               Florian Tram{\`{e}}r and
               Praneeth Vepakomma and
               Jianyu Wang and
               Li Xiong and
               Zheng Xu and
               Qiang Yang and
               Felix X. Yu and
               Han Yu and
               Sen Zhao},
  title     = {Advances and Open Problems in Federated Learning},
  journal   = {Found. Trends Mach. Learn.},
  volume    = {14},
  number    = {1-2},
  pages     = {1--210},
  year      = {2021},
  url       = {https://doi.org/10.1561/2200000083},
  doi       = {10.1561/2200000083},
  timestamp = {Sat, 09 Apr 2022 12:27:56 +0200},
  biburl    = {https://dblp.org/rec/journals/ftml/KairouzMABBBBCC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smith2017federated,
  title={Federated multi-task learning},
  author={Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  journal={Advances in neural information processing systems (NeurIPS)},
  year={2017}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local {SGD} on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@article{woodworth2020minibatch,
  title={Minibatch vs local sgd for heterogeneous distributed learning},
  author={Woodworth, Blake E and Patel, Kumar Kshitij and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={6281--6292},
  year={2020}
}

@inproceedings{glasgow2022sharp,
  title={Sharp Bounds for Federated Averaging (Local {SGD}) and Continuous Perspective},
  author={Glasgow, Margalit R and Yuan, Honglin and Ma, Tengyu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9050--9090},
  year={2022},
  organization={PMLR}
}

@article{malinovsky2022server,
  title={Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization},
  author={Malinovsky, Grigory and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2201.11066},
  year={2022}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{reddi2020adaptive,
  title={Adaptive Federated Optimization},
  author={Reddi, Sashank J and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, Hugh Brendan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{karimireddy2021breaking,
  title={Breaking the centralized barrier for cross-device federated learning},
  author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28663--28676},
  year={2021}
}
per
@article{li2020federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@article{deng2012mnist, 
  title={The mnist database of handwritten digit images for machine learning research}, 
  author={Deng, Li}, 
  journal={IEEE Signal Processing Magazine}, 
  volume={29}, 
  number={6}, 
  pages={141--142}, 
  year={2012}, 
  publisher={IEEE} 
}
@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}


@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@article{paulik2021federated,
  title={Federated evaluation and tuning for on-device personalization: System design \& applications},
  author={Paulik, Matthias and Seigel, Matt and Mason, Henry and Telaar, Dominic and Kluivers, Joris and van Dalen, Rogier and Lau, Chi Wai and Carlson, Luke and Granqvist, Filip and Vandevelde, Chris and others},
  journal={arXiv preprint arXiv:2102.08503},
  year={2021}
}

@article{wang2019federated,
  title={Federated evaluation of on-device personalization},
  author={Wang, Kangkang and Mathews, Rajiv and Kiddon, Chlo{\'e} and Eichner, Hubert and Beaufays, Fran{\c{c}}oise and Ramage, Daniel},
  journal={arXiv preprint arXiv:1910.10252},
  year={2019}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@misc{apple19wwdc,
    title = {Designing for Privacy (video and slide deck)},
    howpublished = {Apple WWDC, \url{https://developer.apple.com/videos/play/wwdc2019/708}},
    author = {Apple},
    year = 2019
}

@misc{nvidia2020,
  title         ={Triaging COVID-19 Patients: 20 Hospitals in 20 Days Build AI Model that Predicts Oxygen Needs},
  howpublished  ={\url{https://blogs.nvidia.com/blog/2020/10/05/federated-learning-covid-oxygen-needs/}},
  author        ={NVIDIA},
  year          ={2020},
  month         ={October},
}

@inproceedings{cohen2017emnist,
  title={{EMNIST}: Extending {MNIST} to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@article{krizhevsky2009cifar,
  title={Cifar-10 and cifar-100 datasets},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  journal={URl: https://www. cs. toronto. edu/kriz/cifar. html},
  volume={6},
  number={1},
  pages={1},
  year={2009}
}

@article{yuan2019distributed,
  title={Distributed learning of deep neural networks using independent subnet training},
  author={Yuan, Binhang and Wolfe, Cameron R and Dun, Chen and Tang, Yuxin and Kyrillidis, Anastasios and Jermaine, Christopher M},
  journal={arXiv preprint arXiv:1910.02120},
  year={2019}
}

@article{yuan2022distributed,
  title={Distributed learning of fully connected neural networks using independent subnet training},
  author={Yuan, Binhang and Wolfe, Cameron R and Dun, Chen and Tang, Yuxin and Kyrillidis, Anastasios and Jermaine, Chris},
  journal={Proceedings of the VLDB Endowment},
  volume={15},
  number={8},
  pages={1581--1590},
  year={2022},
  publisher={VLDB Endowment}
}

@article{wolfe2021gist,
  title={{GIST}: Distributed training for large-scale graph convolutional networks},
  author={Wolfe, Cameron R and Yang, Jingkang and Liao, Fangshuo and Chowdhury, Arindam and Dun, Chen and Bayer, Artun and Segarra, Santiago and Kyrillidis, Anastasios},
  journal={Journal of Applied and Computational Topology},
  pages={1--53},
  year={2023},
  publisher={Springer}
}

@article{liao2021convergence,
  title={On the convergence of shallow neural network training with randomly masked neurons},
  author={Liao, Fangshuo and Kyrillidis, Anastasios},
  journal={arXiv preprint arXiv:2112.02668},
  year={2021}
}

@inproceedings{dun2022resist,
  title={{ResIST}: Layer-wise decomposition of ResNets for distributed training},
  author={Dun, Chen and Wolfe, Cameron R and Jermaine, Christopher M and Kyrillidis, Anastasios},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={610--620},
  year={2022},
  organization={PMLR}
}

@article{dun2022efficient_arxiv,
  title={Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout},
  author={Dun, Chen and Hipolito, Mirian and Jermaine, Chris and Dimitriadis, Dimitrios and Kyrillidis, Anastasios},
  journal={arXiv preprint arXiv:2210.16105},
  year={2022}
}

@inproceedings{dun2023efficient,
  title={Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout},
  author={Dun, Chen and Hipolito, Mirian and Jermaine, Chris and Dimitriadis, Dimitrios and Kyrillidis, Anastasios},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6630--6660},
  year={2023},
  organization={PMLR}
}

@inproceedings{dun2022efficient,
  title={Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout},
  author={Dun, Chen and Garcia, Mirian Del Carmen Hipolito and Jermaine, Christopher and Dimitriadis, Dimitrios and Kyrillidis, Anastasios},
  booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)}
}

@inproceedings{zhou2022federated,
    title={Federated Learning with Online Adaptive Heterogeneous Local Models},
    author={Hanhan Zhou and Tian Lan and Guru Prasadh Venkataramani and Wenbo Ding},
    booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)},
    year={2022},
    url={https://openreview.net/forum?id=p3EhUXVMeyn}
}

@inproceedings{mohtashami2022masked,
  title={Masked Training of Neural Networks with Partial Gradients},
  author={Mohtashami, Amirkeivan and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5876--5890},
  year={2022},
  organization={PMLR}
}

@article{richtarik2021ef21,
  title={{EF}21: A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4384--4396},
  year={2021}
}

@article{tyurin2022dasha,
  title={{DASHA}: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization},
  author={Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2202.01268},
  year={2022}
}

@article{chayti2022optimization,
  title={Optimization with access to auxiliary information},
  author={Chayti, El Mahdi and Karimireddy, Sai Praneeth},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=kxYqgSkH8I}
}

@article{khaled2019gradient,
  title={Gradient descent with compressed iterates},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1909.04716},
  booktitle={NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality},
  year={2019}
}

@article{chraibi2019distributed,
      title={Distributed Fixed Point Methods with Compressed Iterates}, 
      author={Sélim Chraibi and Ahmed Khaled and Dmitry Kovalev and Peter Richt{\'a}rik and Adil Salim and Martin Takáč},
      year={2019},
      journal={arXiv preprint arXiv:2102.07245},
      eprint={1912.09925},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{shulgin2022shifted,
  title={Shifted Compression Framework: Generalizations and Improvements},
  author={Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
  year={2022}
}

@article{horvath2021fjord,
  title={{FjORD}: Fair and accurate federated learning under heterogeneous targets with ordered dropout},
  author={Horvath, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos and Lane, Nicholas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12876--12889},
  year={2021}
}

@article{zhou2022convergence,
  title={On the Convergence of Heterogeneous Federated Learning with Arbitrary Adaptive Online Model Pruning},
  author={Zhou, Hanhan and Lan, Tian and Venkataramani, Guru and Ding, Wenbo},
  journal={arXiv preprint arXiv:2201.11803},
  booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)},
  year={2022}
}

@article{liao2022on,
  title={On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons},
  author={Fangshuo Liao and Anastasios Kyrillidis},
  journal={Transactions on Machine Learning Research},
  year={2022},
  url={https://openreview.net/forum?id=e7mYYMSyZH}
}

@inproceedings{li2021page,
  title={{PAGE}: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={6286--6295},
  year={2021},
  organization={PMLR}
}

@article{alistarh2017qsgd,
  title={{QSGD}: Communication-efficient {SGD} via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{khirirat2018distributed,
      title={Distributed learning with compressed gradients}, 
      author={Sarit Khirirat and Hamid Reza Feyzmahdavian and Mikael Johansson},
      year={2018},
      journal={arXiv preprint arXiv:1806.06573},
      eprint={1806.06573},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{ajalloeian2020convergence,
  title={On the convergence of {SGD} with biased gradients},
  author={Ajalloeian, Ahmad and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2008.00051},
  year={2020}
}

@article{gower2019sgd,
  title={{SGD}: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={Proceedings of the 36th International Conference on Machine
Learning, Long Beach, California},
  year={2019}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@inproceedings{szlendak2022permutation,
  title={Permutation Compressors for Provably Faster Distributed Nonconvex Optimization},
  author={Rafa{\l} Szlendak and Alexander Tyurin and Peter Richt{\'a}rik},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=GugZ5DzzAu}
}

@inproceedings{lin2019dynamic,
  title={Dynamic Model Pruning with Feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{peste2021ac,
  title={{AC/DC}: Alternating compressed/decompressed training of deep neural networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8557--8570},
  year={2021}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{han2017dsd,
  title={{DSD}: Dense-Sparse-Dense Training for Deep Neural Networks},
  author={Song Han and Jeff Pool and Sharan Narang and Huizi Mao and Enhao Gong and Shijian Tang and Erich Elsen and Peter Vajda and Manohar Paluri and John Tran and Bryan Catanzaro and William J. Dally},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=HyoST_9xl}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{wortsman2022fi,
  title={lo-fi: distributed fine-tuning without communication},
  author={Wortsman, Mitchell and Gururangan, Suchin and Li, Shen and Farhadi, Ali and Schmidt, Ludwig and Rabbat, Michael and Morcos, Ari S},
  journal={arXiv preprint arXiv:2210.11948},
  year={2022}
}

@article{hanson1990stochastic,
  title={A stochastic version of the delta rule},
  author={Hanson, Stephen Jos{\'e}},
  journal={Physica D: Nonlinear Phenomena},
  volume={42},
  number={1-3},
  pages={265--272},
  year={1990},
  publisher={Elsevier}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{wan2013regularization,
  title={Regularization of neural networks using dropconnect},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle={International conference on machine learning},
  pages={1058--1066},
  year={2013},
  organization={PMLR}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of {SGD}: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={680--690},
  year={2020},
  organization={PMLR}
}

@article{caldas2018expanding,
  title={Expanding the reach of federated learning by reducing client resource requirements},
  author={Caldas, Sebastian and Kone{\v{c}}ny, Jakub and McMahan, H Brendan and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.07210},
  year={2018}
}

@inproceedings{
    diao2020heterofl,
    title={Hetero{FL}: Computation and Communication Efficient Federated Learning for Heterogeneous Clients},
    author={Enmao Diao and Jie Ding and Vahid Tarokh},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=TNkPBBYFkXg}
}

@inproceedings{buyukates2021timely,
  title={Timely communication in federated learning},
  author={Buyukates, Baturalp and Ulukus, Sennur},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}

@article{sidahmed2021efficient,
  title={Efficient and Private Federated Learning with Partially Trainable Networks},
  author={Sidahmed, Hakim and Xu, Zheng and Garg, Ankush and Cao, Yuan and Chen, Mingqing},
  journal={arXiv preprint arXiv:2110.03450},
  year={2021}
}

@inproceedings{qiu2022zerofl,
  title={Zero{FL}: Efficient On-Device Training for  Federated Learning with Local Sparsity},
  author={Xinchi Qiu and Javier Fernandez-Marques and Pedro PB Gusmao and Yan Gao and Titouan Parcollet and Nicholas Donald Lane},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=2sDQwC_hmnM}
}

@inproceedings{yang2022partial,
  title={Partial variable training for efficient on-device federated learning},
  author={Yang, Tien-Ju and Guliani, Dhruv and Beaufays, Fran{\c{c}}oise and Motta, Giovanni},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4348--4352},
  year={2022},
  organization={IEEE}
}

@article{wen2022federated,
  title={Federated Dropout—A Simple Approach for Enabling Federated Learning on Resource Constrained Devices},
  author={Wen, Dingzhu and Jeon, Ki-Jun and Huang, Kaibin},
  journal={IEEE Wireless Communications Letters},
  volume={11},
  number={5},
  pages={923--927},
  year={2022},
  publisher={IEEE}
}

@inproceedings{cheng2022does,
  title={Does Federated Dropout Actually Work?},
  author={Cheng, Gary and Charles, Zachary and Garrett, Zachary and Rush, Keith},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3387--3395},
  year={2022}
}

@inproceedings{chen2022fedobd,
  title={{FEDOBD}: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning},
  author={Chen, Yuanyuan and Chen, Zichen and Wu, Pengcheng and Yu, Han},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  pages={3541--3549},
  year={2023}
}

@article{jiang2022model,
  title={Model pruning enables efficient federated learning on edge devices},
  author={Jiang, Yuang and Wang, Shiqiang and Valls, Victor and Ko, Bong Jun and Lee, Wei-Han and Leung, Kin K and Tassiulas, Leandros},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{lin2022federated,
  title={Federated Pruning: Improving Neural Network Efficiency with Federated Learning},
  author={Lin, Rongmei and Xiao, Yonghui and Yang, Tien-Ju and Zhao, Ding and Xiong, Li and Motta, Giovanni and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:2209.06359},
  year={2022}
}

@article{charles2022federated,
  title={Federated select: A primitive for communication-and memory-efficient federated learning},
  author={Charles, Zachary and Bonawitz, Kallista and Chiknavaryan, Stanislav and McMahan, Brendan and others},
  journal={arXiv preprint arXiv:2208.09432},
  year={2022}
}

@inproceedings{alam2022fedrolex,
  title={{FedRolex}: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction},
  author={Samiul Alam and Luyang Liu and Ming Yan and Mi Zhang},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=OtxyysUdBE}
}

@article{xu2020compressed,
	year = {2020},
	title = {Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation},
	author = {Xu, Hang and Ho, Chen-Yu and Abdelmoniem, Ahmed M. and Dutta, Aritra and Bergou, El Houcine and Karatsenidis, Konstantinos and Canini, Marco and Kalnis, Panos},
	url = {http://hdl.handle.net/10754/662495},
	journal = {Technical report}
}

@article{safaryan2021uncertainty,
    author = {Safaryan, Mher and Shulgin, Egor and Richt{\'a}rik, Peter},
    title = "{Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor}",
    journal = {Information and Inference: A Journal of the IMA},
    year = {2021},
    month = {04},
    issn = {2049-8772},
    doi = {10.1093/imaiai/iaab006},
    url = {https://doi.org/10.1093/imaiai/iaab006},
    note = {iaab006},
    eprint = {https://academic.oup.com/imaiai/advance-article-pdf/doi/10.1093/imaiai/iaab006/37022980/iaab006.pdf}
}

@article{albasyoni2020optimal,
      title={Optimal Gradient Compression for Distributed and Federated Learning}, 
      author={Alyazeed Albasyoni and Mher Safaryan and Laurent Condat and Peter Richt{\'a}rik},
      year={2020},
      journal={arXiv preprint arXiv:2010.03246},
      eprint={2010.03246},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{huang2022lower,
  title={Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression},
  author={Xinmeng Huang and Yiming Chen and Wotao Yin and Kun Yuan},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=Xm9iN3UsdpH}
}

@article{beznosikov2020biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={arXiv preprint arXiv:2002.12410},
  year={2020}
}

@InProceedings{safaryan2021fednl,
  title = 	 {{F}ed{NL}: Making {N}ewton-Type Methods Applicable to Federated Learning},
  author =       {Safaryan, Mher and Islamov, Rustem and Qian, Xun and Richtarik, Peter},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18959--19010},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/safaryan22a/safaryan22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/safaryan22a.html},
}

@inproceedings{arjevani2020tight,
  title={A tight convergence analysis for stochastic gradient descent with delayed updates},
  author={Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={111--132},
  year={2020},
  organization={PMLR}
}

@inproceedings{goujaud2022super,
  title={Super-acceleration with cyclical step-sizes},
  author={Goujaud, Baptiste and Scieur, Damien and Dieuleveut, Aymeric and Taylor, Adrien B and Pedregosa, Fabian},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3028--3065},
  year={2022},
  organization={PMLR}
}

@inproceedings{cunha2022only,
  title={Only tails matter: Average-Case Universality and Robustness in the Convex Regime},
  author={Cunha, Leonardo and Gidel, Gauthier and Pedregosa, Fabian and Scieur, Damien and Paquette, Courtney},
  booktitle={International Conference on Machine Learning},
  pages={4474--4491},
  year={2022},
  organization={PMLR}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{zhu2022quadratic,
  title={Quadratic models for understanding neural network dynamics},
  author={Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNNs}},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@article{konecny2017federated,
      title={Federated Learning: Strategies for Improving Communication Efficiency}, 
      author={Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richt{\'a}rik and Ananda Theertha Suresh and Dave Bacon},
      year={2016},
      journal={NIPS Private Multi-Party Machine Learning Workshop}
}

@article{khaled2022better,
    title={Better Theory for {SGD} in the  Nonconvex World},
    author={Ahmed Khaled and Peter Richt{\'a}rik},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=AU4qHN2VkS},
    note={Survey Certification}
}

@article{khaled2020better_arxiv,
  title={Better theory for {SGD} in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@inproceedings{rajbhandari2020zero,
  title={{ZeRO}: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@inproceedings{rasley2020deepspeed,
  title={{DeepSpeed}: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}

@article{dean2012large,
  title={Large Scale Distributed Deep Networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}

@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{farber1997parallel,
  title={Parallel neural network training on multi-spert},
  author={Farber, Philipp and Asanovic, Krste},
  booktitle={Proceedings of 3rd International Conference on Algorithms and Architectures for Parallel Processing},
  pages={659--666},
  year={1997},
  organization={IEEE}
}

@article{zhang1989efficient,
  title={An efficient implementation of the back-propagation algorithm on the connection machine {CM-2}},
  author={Zhang, Xiru and Mckenna, Michael and Mesirov, Jill and Waltz, David},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@article{safaryan2021smoothness,
  title={Smoothness matrices beat smoothness constants: Better communication compression techniques for distributed optimization},
  author={Safaryan, Mher and Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25688--25702},
  year={2021}
}

@article{wang2022theoretically,
  title={Theoretically better and numerically faster distributed optimization with smoothness-aware quantization techniques},
  author={Wang, Bokun and Safaryan, Mher and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9841--9852},
  year={2022}
}

@article{nesterov2012efficiency,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}

@article{richtarik2014iteration,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Mathematical Programming},
  volume={144},
  number={1-2},
  pages={1--38},
  year={2014},
  publisher={Springer}
}

@inproceedings{mishchenko2022proxskip,
  title={{ProxSkip}: Yes! {L}ocal gradient steps provably lead to communication acceleration! {F}inally!},
  author={Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={15750--15769},
  year={2022},
  organization={PMLR}
}

@article{goyal2018accurate,
      title={Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour}, 
      author={Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
      year={2018},
      journal={arXiv preprint arXiv:1706.02677},
      eprint={1706.02677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{shulgin2023towards,
  title={Towards a Better Theoretical Understanding of Independent Subnetwork Training},
  author={Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2306.16484},
  year={2023}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{bian2023does,
  title={Does compressing activations help model parallel training?},
  author={Bian, Song and Li, Dacheng and Wang, Hongyi and Xing, Eric P and Venkataraman, Shivaram},
  journal={arXiv preprint arXiv:2301.02654},
  year={2023}
}

@article{demidovich2023mast,
  title={{MAST}: Model-Agnostic Sparsified Training},
  author={Demidovich, Yury and Malinovsky, Grigory and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2311.16086},
  year={2023}
}