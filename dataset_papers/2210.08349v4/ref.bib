@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@inproceedings{atari2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2013}
}

@article{li2014event,
  title={Event-triggered robust model predictive control of continuous-time nonlinear systems},
  author={Li, Huiping and Shi, Yang},
  journal={Automatica},
  volume={50},
  number={5},
  pages={1507--1513},
  year={2014},
  publisher={Elsevier}
}

//Theory Related works
//CPI
@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning},
  year={2002}
}


@inproceedings{sun2018dual,
  title={Dual policy iteration},
  author={Sun, Wen and Gordon, Geoffrey J and Boots, Byron and Bagnell, J},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2018}
}

@inproceedings{farahmand2017value,
  title={Value-aware loss function for model-based reinforcement learning},
  author={Farahmand, Amir-massoud and Barreto, Andre and Nikovski, Daniel},
  booktitle={Artificial Intelligence and Statistics},
  year={2017},
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  year={2020},
}

@inproceedings{efroni2019tight,
  title={Tight regret bounds for model-based reinforcement learning with greedy policies},
  author={Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2019}
}

@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert},
  journal={Machine Learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013}
}

@inproceedings{kakade2020information,
  title={Information theoretic regret bounds for online nonlinear control},
  author={Kakade, Sham and Krishnamurthy, Akshay and Lowrey, Kendall and Ohnishi, Motoya and Sun, Wen},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

//related works
@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@article{wang2019benchmarking,
  title={Benchmarking model-based reinforcement learning},
  author={Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  journal={arXiv preprint arXiv:1907.02057},
  year={2019}
}

@inproceedings{sutton1990integrated,
  title={Integrated architecture for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={International Conference on Machine Learning},
  year={1990}
}

@article{sutton1991dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sutton1991planning,
  title={Planning by incremental dynamic programming},
  author={Sutton, Richard S},
  booktitle={International Conference on Machine Learning},
  year={1991}
}

@article{feinberg2018model,
  title={Model-based value estimation for efficient model-free reinforcement learning},
  author={Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I and Gonzalez, Joseph E and Levine, Sergey},
  journal={arXiv preprint arXiv:1803.00101},
  year={2018}
}

@inproceedings{kalweit2017uncertainty,
  title={Uncertainty-driven imagination for continuous deep reinforcement learning},
  author={Kalweit, Gabriel and Boedecker, Joschka},
  booktitle={Conference on Robot Learning},
  year={2017},
}


@inproceedings{kearns1998finite,
  title={Finite-sample convergence rates for Q-learning and indirect algorithms},
  author={Kearns, Michael and Singh, Satinder},
  booktitle={Advances in Neural Information Processing Systems},
  year={1998}
}

@inproceedings{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

//Gaussian Processes
@article{deisenroth2011learning,
  title={Learning to control a low-cost manipulator using data-efficient reinforcement learning},
  author={Deisenroth, Marc Peter and Rasmussen, Carl Edward and Fox, Dieter},
  journal={Robotics: Science and Systems VII},
  volume={7},
  pages={57--64},
  year={2011},
  publisher={MIT Cambridge}
}

@article{ko2009gp,
  title={GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models},
  author={Ko, Jonathan and Fox, Dieter},
  journal={Autonomous Robots},
  volume={27},
  number={1},
  pages={75--90},
  year={2009},
  publisher={Springer}
}

//time-varying linear dynamics
@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  year={2013},
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

//Neural Network
@inproceedings{gal2016improving,
  title={Improving PILCO with Bayesian neural network dynamics models},
  author={Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle={ICML Workshop on Data-EfÔ¨Åcient Machine Learning Workshop},
  year={2016}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={International Conference on Robotics and Automation},
  year={2018},
}


//model ensemble
@inproceedings{kurutach2018model,
  title={Model-Ensemble Trust-Region Policy Optimization},
  author={Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

//SAC
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  year={2018},
}

//TRPO
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  year={2015},
}

//Dkitty
@inproceedings{ahn2020robel,
  title={Robel: Robotics benchmarks for learning with low-cost robots},
  author={Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
  booktitle={Conference on robot learning},
  year={2020}
}

//Pandas
@article{james2019pyrep,
  title={PyRep: Bringing V-REP to Deep Robot Learning},
  author={James, Stephen and Freese, Marc and Davison, Andrew J.},
  journal={arXiv preprint arXiv:1906.11176},
  year={2019}
}


//PPO
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

//TD3
@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  year={2018},
}

//SLBO
@inproceedings{luo2018algorithmic,
  title={Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees},
  author={Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

//MBPO
@inproceedings{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

//M2AC
@inproceedings{pan2020trust,
  title={Trust the model when it is confident: Masked model-based actor-critic},
  author={Pan, Feiyang and He, Jia and Tu, Dandan and He, Qing},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2020}
}

//AutoMBPO
@inproceedings{lai2021effective,
  title={On Effective Scheduling of Model-based Reinforcement Learning},
  author={Lai, Hang and Shen, Jian and Zhang, Weinan and Huang, Yimin and Zhang, Xing and Tang, Ruiming and Yu, Yong and Li, Zhenguo},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}


//STEVE
@inproceedings{buckman2018sample,
  title={Sample-efficient reinforcement learning with stochastic ensemble value expansion},
  author={Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2018}
}

//MAX
@inproceedings{shyam2019model,
  title={Model-based active exploration},
  author={Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  booktitle={International Conference on Machine Learning},
  year={2019},
}


//MOPO
@inproceedings{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

//PETS
@inproceedings{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2018}
}

//AMPO
@inproceedings{shen2020model,
  title={Model-based policy optimization with unsupervised model adaptation},
  author={Shen, Jian and Zhao, Han and Zhang, Weinan and Yu, Yong},
  booktitle={Advances in Neural Information Processing Systems}},
  year={2020}
}

//Mujoco
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={International Conference on Intelligent Robots and Systems},
  year={2012},
}

//gym
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}


//MBRLLIB
@article{Pineda2021MBRL,
  author  = {Luis Pineda and Brandon Amos and Amy Zhang and Nathan O. Lambert and Roberto Calandra},
  journal = {Arxiv},
  title   = {MBRL-Lib: A Modular Library for Model-based Reinforcement Learning},
  year    = {2021},
  url     = {https://arxiv.org/abs/2104.10159},
}



@article{kearns2002sparse,
  title={A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
  author={Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={193--208},
  year={2002},
  publisher={Springer}
}

//iLQR
@inproceedings{amos2018differentiable,
  title={Differentiable mpc for end-to-end planning and control},
  author={Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

//Lipschitz
@inproceedings{asadi2018lipschitz,
  title={Lipschitz continuity in model-based reinforcement learning},
  author={Asadi, Kavosh and Misra, Dipendra and Littman, Michael},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

//suttonbook
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press, Cambridge, MA}
}

@inproceedings{morimoto2002minimax,
  title={Minimax differential dynamic programming: An application to robust biped walking},
  author={Morimoto, Jun and Atkeson, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2002}
}

@inproceedings{hester2012rtmba,
  title={RTMBA: A real-time model-based reinforcement learning architecture for robot control},
  author={Hester, Todd and Quinlan, Michael and Stone, Peter},
  booktitle={International Conference on Robotics and Automation},
  year={2012}
}

@article{polydoros2017survey,
  title={Survey of model-based reinforcement learning: Applications on robotics},
  author={Polydoros, Athanasios S and Nalpantidis, Lazaros},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={86},
  number={2},
  pages={153--173},
  year={2017},
  publisher={Springer}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}
@inproceedings{lillicrap2016continuous,
  title={Continuous control with deep reinforcement learning.},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

//SACLIB
@misc{saclib,
  author = {pranz24},
  title = {pytorch-soft-actor-critic},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/pranz24/pytorch-soft-actor-critic}},
  year = {2018}
}

@inproceedings{heemels2012introduction,
  title={An introduction to event-triggered and self-triggered control},
  author={Heemels, Wilhelmus PMH and Johansson, Karl Henrik and Tabuada, Paulo},
  booktitle={2012 ieee 51st ieee conference on decision and control (cdc)},
  pages={3270--3285},
  year={2012},
  organization={IEEE}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@book{thomas2006elements,
  title={Elements of information theory},
  author={Thomas, MTCAJ and Joy, A Thomas},
  year={2006},
  publisher={Wiley-Interscience}
}

@inproceedings{fan2021model,
  title={Model-based Reinforcement Learning for Continuous Control with Posterior Sampling},
  author={Fan, Ying and Ming, Yifei},
  booktitle={International Conference on Machine Learning},
  year={2021}
}