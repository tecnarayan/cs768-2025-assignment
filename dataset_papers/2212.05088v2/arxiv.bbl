\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alacaoglu et~al.(2017)Alacaoglu, Tran~Dinh, Fercoq, and
  Cevher]{alacaoglu2017smooth}
Ahmet Alacaoglu, Quoc Tran~Dinh, Olivier Fercoq, and Volkan Cevher.
\newblock Smooth primal-dual coordinate descent algorithms for nonsmooth convex
  optimization.
\newblock In \emph{Proc.~NeurIPS'17}, 2017.

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8194--8244, 2017.

\bibitem[Allen-Zhu et~al.(2016)Allen-Zhu, Qu, Richt{\'a}rik, and
  Yuan]{allen2016even}
Zeyuan Allen-Zhu, Zheng Qu, Peter Richt{\'a}rik, and Yang Yuan.
\newblock Even faster accelerated coordinate descent using non-uniform
  sampling.
\newblock In \emph{Proc.~ICML'16}, 2016.

\bibitem[Beck and Tetruashvili(2013)]{beck2013convergence}
Amir Beck and Luba Tetruashvili.
\newblock On the convergence of block coordinate descent type methods.
\newblock \emph{SIAM journal on Optimization}, 23\penalty0 (4):\penalty0
  2037--2060, 2013.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Chauhan et~al.(2017)Chauhan, Dahiya, and Sharma]{chauhan2017mini}
Vinod~Kumar Chauhan, Kalpana Dahiya, and Anuj Sharma.
\newblock Mini-batch block-coordinate based stochastic average adjusted
  gradient methods to solve big data problems.
\newblock In \emph{Proc.~ACML'16}, 2017.

\bibitem[Chen and Gu(2016)]{chen2016accelerated}
Jinghui Chen and Quanquan Gu.
\newblock Accelerated stochastic block coordinate gradient descent for sparsity
  constrained nonconvex optimization.
\newblock In \emph{Proc.~UAI'16}, 2016.

\bibitem[Chen et~al.(2021)Chen, Li, and Lu]{chen2021global}
Ziang Chen, Yingzhou Li, and Jianfeng Lu.
\newblock On the global convergence of randomized coordinate gradient descent
  for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2101.01323}, 2021.

\bibitem[Chow et~al.(2017)Chow, Wu, and Yin]{chow2017cyclic}
Yat~Tin Chow, Tianyu Wu, and Wotao Yin.
\newblock Cyclic coordinate-update algorithms for fixed-point problems:
  Analysis and applications.
\newblock \emph{SIAM Journal on Scientific Computing}, 39\penalty0
  (4):\penalty0 A1280--A1300, 2017.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Proc.~NeurIPS'19}, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Proc.~NeurIPS'14}, 2014.

\bibitem[Diakonikolas and Orecchia(2018)]{diakonikolas2018alternating}
Jelena Diakonikolas and Lorenzo Orecchia.
\newblock Alternating randomized block coordinate descent.
\newblock In \emph{Proc.~ICML'18}, 2018.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Proc.~NeurIPS'18}, 2018.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{friedman2010regularization}
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{Journal of Statistical Software}, 33\penalty0 (1):\penalty0 1,
  2010.

\bibitem[Fu et~al.(2020)Fu, Ibrahim, Wai, Gao, and Huang]{fu2020block}
Xiao Fu, Shahana Ibrahim, Hoi-To Wai, Cheng Gao, and Kejun Huang.
\newblock Block-randomized stochastic proximal gradient for low-rank tensor
  factorization.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  2170--2185, 2020.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Hanzely, and
  Richt{\'a}rik]{gorbunov2020unified}
Eduard Gorbunov, Filip Hanzely, and Peter Richt{\'a}rik.
\newblock A unified theory of sgd: Variance reduction, sampling, quantization
  and coordinate descent.
\newblock In \emph{Proc.~AISTATS'20}, 2020.

\bibitem[Grippof and Sciandrone(1999)]{grippof1999globally}
Luigi Grippof and Marco Sciandrone.
\newblock Globally convergent block-coordinate techniques for unconstrained
  optimization.
\newblock \emph{Optimization Methods and Software}, 10\penalty0 (4):\penalty0
  587--637, 1999.

\bibitem[Gurbuzbalaban et~al.(2017)Gurbuzbalaban, Ozdaglar, Parrilo, and
  Vanli]{gurbuzbalaban2017cyclic}
Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo~A Parrilo, and Nuri Vanli.
\newblock When cyclic coordinate descent outperforms randomized coordinate
  descent.
\newblock In \emph{Proc.~NeurIPS'17}, 2017.

\bibitem[Hong et~al.(2017)Hong, Wang, Razaviyayn, and Luo]{hong2017iteration}
Mingyi Hong, Xiangfeng Wang, Meisam Razaviyayn, and Zhi-Quan Luo.
\newblock Iteration complexity analysis of block coordinate descent methods.
\newblock \emph{Mathematical Programming}, 163\penalty0 (1):\penalty0 85--114,
  2017.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Proc.~NeurIPS'13}, 2013.

\bibitem[Kamri et~al.(2022)Kamri, Hendrickx, and Glineur]{kamri2022worst}
Yassine Kamri, Julien~M Hendrickx, and Fran{\c{c}}ois Glineur.
\newblock On the worst-case analysis of cyclic coordinate-wise algorithms on
  smooth convex functions.
\newblock \emph{arXiv preprint arXiv:2211.17018}, 2022.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017on}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{Proc.~ICLR'17}, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee and Wright(2019)]{lee2019random}
Ching-Pei Lee and Stephen~J Wright.
\newblock Random permutations fix a worst case for cyclic coordinate descent.
\newblock \emph{IMA Journal of Numerical Analysis}, 39\penalty0 (3):\penalty0
  1246--1275, 2019.

\bibitem[Lei and Shanbhag(2020)]{lei2020asynchronous}
Jinlong Lei and Uday~V Shanbhag.
\newblock Asynchronous variance-reduced block schemes for composite non-convex
  stochastic optimization: block-specific steplengths and adapted batch-sizes.
\newblock \emph{Optimization Methods and Software}, pages 1--31, 2020.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Proc.~NeurIPS'17}, 2017.

\bibitem[Li et~al.(2017)Li, Zhao, Arora, Liu, and Hong]{li2017faster}
Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Mingyi Hong.
\newblock On faster convergence of cyclic block coordinate descent-type methods
  for strongly convex minimization.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6741--6764, 2017.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2020page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt{\'a}rik.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{Proc.~ICML21}, 2021.

\bibitem[Lin et~al.(2015)Lin, Lu, and Xiao]{lin2015accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated randomized proximal coordinate gradient method and its
  application to regularized empirical risk minimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  2244--2273, 2015.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR:} stochastic gradient descent with warm restarts.
\newblock In \emph{Proc.~ICLR'17,}, 2017.

\bibitem[Mazumder et~al.(2011)Mazumder, Friedman, and
  Hastie]{mazumder2011sparsenet}
Rahul Mazumder, Jerome~H Friedman, and Trevor Hastie.
\newblock Sparsenet: Coordinate descent with nonconvex penalties.
\newblock \emph{Journal of the American Statistical Association}, 106\penalty0
  (495):\penalty0 1125--1138, 2011.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{Proc.~ICLR'18}, 2018.

\bibitem[Nakamura et~al.(2021)Nakamura, Soatto, and Hong]{nakamura2021block}
Kensuke Nakamura, Stefano Soatto, and Byung-Woo Hong.
\newblock Block-cyclic stochastic coordinate descent for deep neural networks.
\newblock \emph{Neural Networks}, 139:\penalty0 348--357, 2021.

\bibitem[Nesterov(2012)]{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Nesterov and Stich(2017)]{nesterov2017efficiency}
Yurii Nesterov and Sebastian~U Stich.
\newblock Efficiency of the accelerated coordinate descent method on structured
  optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (1):\penalty0
  110--123, 2017.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{Proc.~ICML'17}, 2017.

\bibitem[Nie et~al.(2021)Nie, Xue, Wu, Wang, Li, and Li]{nie2021coordinate}
Feiping Nie, Jingjing Xue, Danyang Wu, Rong Wang, Hui Li, and Xuelong Li.
\newblock Coordinate descent method for $ k $ k-means.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44\penalty0 (5):\penalty0 2371--2385, 2021.

\bibitem[Nutini et~al.(2015)Nutini, Schmidt, Laradji, Friedlander, and
  Koepke]{nutini2015coordinate}
Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt
  Koepke.
\newblock Coordinate descent converges faster with the {G}auss-{S}outhwell rule
  than random selection.
\newblock In \emph{Proc.~ICML'15}, 2015.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Proc.~NeurIPS'19}, 2019.

\bibitem[Plummer et~al.(2020)Plummer, Pati, and
  Bhattacharya]{plummer2020dynamics}
Sean Plummer, Debdeep Pati, and Anirban Bhattacharya.
\newblock Dynamics of coordinate ascent variational inference: A case study in
  2d ising models.
\newblock \emph{Entropy}, 22\penalty0 (11):\penalty0 1263, 2020.

\bibitem[Razaviyayn et~al.(2013)Razaviyayn, Hong, and
  Luo]{razaviyayn2013unified}
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo.
\newblock A unified convergence analysis of block successive minimization
  methods for nonsmooth optimization.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (2):\penalty0
  1126--1153, 2013.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proc.~ICML'16}, 2016.

\bibitem[Saha and Tewari(2013)]{saha2013nonasymptotic}
Ankan Saha and Ambuj Tewari.
\newblock On the nonasymptotic convergence of cyclic coordinate descent
  methods.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (1):\penalty0
  576--601, 2013.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  2017.

\bibitem[Song and Diakonikolas(2021)]{song2021fast}
Chaobing Song and Jelena Diakonikolas.
\newblock Fast cyclic coordinate dual averaging with extrapolation for
  generalized variational inequalities.
\newblock \emph{arXiv preprint arXiv:2102.13244}, 2021.

\bibitem[Song et~al.(2020)Song, Jiang, and Ma]{song2020variance}
Chaobing Song, Yong Jiang, and Yi~Ma.
\newblock Variance reduction via accelerated dual averaging for finite-sum
  optimization.
\newblock In \emph{Proc.~NeurIPS'20}, 2020.

\bibitem[Sun and Ye(2021)]{sun2021worst}
Ruoyu Sun and Yinyu Ye.
\newblock Worst-case complexity of cyclic coordinate descent: {$ O (n^2) $} gap
  with randomized version.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1):\penalty0 487--520,
  2021.

\bibitem[Tseng(2001)]{tseng2001convergence}
Paul Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 109\penalty0
  (3):\penalty0 475--494, 2001.

\bibitem[Vandaele et~al.(2016)Vandaele, Gillis, Lei, Zhong, and
  Dhillon]{vandaele2016efficient}
Arnaud Vandaele, Nicolas Gillis, Qi~Lei, Kai Zhong, and Inderjit Dhillon.
\newblock Efficient and non-convex coordinate descent for symmetric nonnegative
  matrix factorization.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (21):\penalty0 5571--5584, 2016.

\bibitem[Wang et~al.(2016)Wang, Wan, and Chang]{wang2016randomized}
Wenyu Wang, Hong Wan, and Kuo-Hao Chang.
\newblock Randomized block coordinate descendant strong for large-scale
  stochastic optimization.
\newblock In \emph{Proc.~WSC'16}, 2016.

\bibitem[Wright and Lee(2020)]{wright2020analyzing}
Stephen Wright and Ching-pei Lee.
\newblock Analyzing random permutations for cyclic coordinate descent.
\newblock \emph{Mathematics of Computation}, 89\penalty0 (325):\penalty0
  2217--2248, 2020.

\bibitem[Wright(2015)]{wright2015coordinate}
Stephen~J Wright.
\newblock Coordinate descent algorithms.
\newblock \emph{Mathematical Programming}, 151\penalty0 (1):\penalty0 3--34,
  2015.

\bibitem[Wu and Lange(2008)]{wu2008coordinate}
Tong~Tong Wu and Kenneth Lange.
\newblock Coordinate descent algorithms for lasso penalized regression.
\newblock \emph{The Annals of Applied Statistics}, 2\penalty0 (1):\penalty0
  224--244, 2008.

\bibitem[Xu and Yin(2013)]{xu2013block}
Yangyang Xu and Wotao Yin.
\newblock A block coordinate descent method for regularized multiconvex
  optimization with applications to nonnegative tensor factorization and
  completion.
\newblock \emph{SIAM Journal on Imaging Sciences}, 6\penalty0 (3):\penalty0
  1758--1789, 2013.

\bibitem[Xu and Yin(2015)]{xu2015block}
Yangyang Xu and Wotao Yin.
\newblock Block stochastic gradient iteration for convex and nonconvex
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (3):\penalty0
  1686--1716, 2015.

\bibitem[Xu and Yin(2017)]{xu2017globally}
Yangyang Xu and Wotao Yin.
\newblock A globally convergent algorithm for nonconvex optimization based on
  block coordinate update.
\newblock \emph{Journal of Scientific Computing}, 72\penalty0 (2):\penalty0
  700--734, 2017.

\bibitem[Zeng et~al.(2014)Zeng, Peng, Lin, and Xu]{zeng2014cyclic}
Jinshan Zeng, Zhimin Peng, Shaobo Lin, and Zongben Xu.
\newblock A cyclic coordinate descent algorithm for lq regularization.
\newblock \emph{arXiv preprint arXiv:1408.0578}, 2014.

\bibitem[Zeng and So(2020)]{zeng2020coordinate}
Wen-Jun Zeng and Hing-Cheung So.
\newblock Coordinate descent algorithms for phase retrieval.
\newblock \emph{Signal Processing}, 169:\penalty0 107418, 2020.

\bibitem[Zheng and Kwok(2016)]{zheng2016fast}
Shuai Zheng and James~T Kwok.
\newblock Fast-and-light stochastic admm.
\newblock In \emph{Proc.~IJCAI'16}, 2016.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Xu, and Gu]{zhou2018finding}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Finding local minima via stochastic nested variance reduction.
\newblock \emph{arXiv preprint arXiv:1806.08782}, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Xu, and Gu]{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock In \emph{Proc.~NeurIPS'18}, 2018{\natexlab{b}}.

\end{thebibliography}
