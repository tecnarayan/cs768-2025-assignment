\newcommand{\noopsort}[1]{} \newcommand{\printfirst}[2]{#1}
  \newcommand{\singleletter}[1]{#1} \newcommand{\switchargs}[2]{#2#1}
\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bhandari and Russo(2020)]{bhandari2020note}
Jalaj Bhandari and Daniel Russo.
\newblock A note on the linear convergence of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2007.11120}, 2020.

\bibitem[Borkar and Konda(1997)]{borkar1997actor}
Vivek~S Borkar and Vijaymohan~R Konda.
\newblock The actor-critic algorithm as multi-time-scale stochastic
  approximation.
\newblock \emph{Sadhana}, 22:\penalty0 525--543, 1997.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{1606.01540}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Cen et~al.(2021)Cen, Cheng, Chen, Wei, and Chi]{cen2021fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{Operations Research}, 2021.

\bibitem[Chan and van~der Schaar(2021)]{chan2021scalable}
Alex~J Chan and Mihaela van~der Schaar.
\newblock Scalable bayesian inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.06483}, 2021.

\bibitem[Choi and Kim(2012)]{choi2012nonparametric}
Jaedeug Choi and Kee-Eung Kim.
\newblock Nonparametric bayesian inverse reinforcement learning for multiple
  reward functions.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Choi and Kim(2014)]{choi2014hierarchical}
Jaedeug Choi and Kee-Eung Kim.
\newblock Hierarchical bayesian inverse reinforcement learning.
\newblock \emph{IEEE transactions on cybernetics}, 45\penalty0 (4):\penalty0
  793--805, 2014.

\bibitem[Duan et~al.(2017)Duan, Andrychowicz, Stadie, Jonathan~Ho, Schneider,
  Sutskever, Abbeel, and Zaremba]{duan2017one}
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan~Ho, Jonas
  Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba.
\newblock One-shot imitation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Garg et~al.(2021)Garg, Chakraborty, Cundy, Song, and
  Ermon]{garg2021iq}
Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano
  Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4028--4039, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem[Hirakawa et~al.(2018)Hirakawa, Yamashita, Tamaki, Fujiyoshi, Umezu,
  Takeuchi, Matsumoto, and Yoda]{hirakawa2018can}
Tsubasa Hirakawa, Takayoshi Yamashita, Toru Tamaki, Hironobu Fujiyoshi, Yuta
  Umezu, Ichiro Takeuchi, Sakiko Matsumoto, and Ken Yoda.
\newblock Can ai predict animal movements? filling gaps in animal trajectories
  using inverse reinforcement learning.
\newblock \emph{Ecosphere}, 9\penalty0 (10):\penalty0 e02447, 2018.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Khodadadian et~al.(2021)Khodadadian, Chen, and
  Maguluri]{khodadadian2021finite}
Sajad Khodadadian, Zaiwei Chen, and Siva~Theja Maguluri.
\newblock Finite-sample analysis of off-policy natural actor-critic algorithm.
\newblock In \emph{International Conference on Machine Learning}, pages
  5420--5431. PMLR, 2021.

\bibitem[Lan(2022)]{lan2022policy}
Guanghui Lan.
\newblock Policy optimization over general state and action spaces.
\newblock \emph{arXiv preprint arXiv:2211.16715}, 2022.

\bibitem[Lan(2023)]{lan2023policy}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical programming}, 198\penalty0 (1):\penalty0
  1059--1106, 2023.

\bibitem[Li et~al.(2022)Li, Wu, and Lan]{li2022stochastic}
Tianjiao Li, Feiyang Wu, and Guanghui Lan.
\newblock Stochastic first-order methods for average-reward markov decision
  processes.
\newblock \emph{arXiv preprint arXiv:2205.05800}, 2022.

\bibitem[Metelli et~al.(2023)Metelli, Lazzati, and
  Restelli]{metelli2023towards}
Alberto~Maria Metelli, Filippo Lazzati, and Marcello Restelli.
\newblock Towards theoretical understanding of inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2304.12966}, 2023.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Ni et~al.(2021)Ni, Sikchi, Wang, Gupta, Lee, and Eysenbach]{ni2021f}
Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Ben
  Eysenbach.
\newblock f-irl: Inverse reinforcement learning via state marginal matching.
\newblock In \emph{Conference on Robot Learning}, pages 529--551. PMLR, 2021.

\bibitem[Pflug(1990)]{pflug1990line}
G~Ch Pflug.
\newblock On-line optimization of simulated markovian processes.
\newblock \emph{Mathematics of Operations Research}, 15\penalty0 (3):\penalty0
  381--395, 1990.

\bibitem[Pflug(2006)]{pflug2006derivatives}
Georg~Ch Pflug.
\newblock Derivatives of probability measures-concepts and applications to the
  optimization of stochastic systems.
\newblock In \emph{Discrete Event Systems: Models and Applications: IIASA
  Conference Sopron, Hungary, August 3--7, 1987}, pages 252--274. Springer,
  2006.

\bibitem[Pinsler et~al.(2018)Pinsler, Maag, Arenz, and
  Neumann]{pinsler2018inverse}
Robert Pinsler, Max Maag, Oleg Arenz, and Gerhard Neumann.
\newblock Inverse reinforcement learning of bird flocking behavior.
\newblock In \emph{ICRA Swarms Workshop}, 2018.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{{M}arkov decision processes: Discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian
  Ernestus, and Noah Dormann.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (268):\penalty0 1--8, 2021.

\bibitem[Schaal(1999)]{schaal1999imitation}
Stefan Schaal.
\newblock Is imitation learning the route to humanoid robots?
\newblock \emph{Trends in cognitive sciences}, 3\penalty0 (6):\penalty0
  233--242, 1999.

\bibitem[Sezener et~al.(2014)Sezener, Uchibe, and Doya]{sezenerobtaining}
Can~Eren Sezener, Eiji Uchibe, and Kenji Doya.
\newblock Obtaining reward functions of rats using inverse reinforcement
  learning.
\newblock 2014.

\bibitem[Wang et~al.(2017)Wang, Merel, Reed, de~Freitas, Wayne, and
  Heess]{wang2017robust}
Ziyu Wang, Josh~S Merel, Scott~E Reed, Nando de~Freitas, Gregory Wayne, and
  Nicolas Heess.
\newblock Robust imitation of diverse behaviors.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Xu et~al.(2020)Xu, Wang, and Liang]{xu2020improving}
Tengyu Xu, Zhe Wang, and Yingbin Liang.
\newblock Improving sample complexity bounds for (natural) actor-critic
  algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4358--4369, 2020.

\bibitem[Yamaguchi et~al.(2018)Yamaguchi, Naoki, Ikeda, Tsukada, Nakano, Mori,
  and Ishii]{yamaguchi2018identification}
Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki Tsukada, Shunji Nakano,
  Ikue Mori, and Shin Ishii.
\newblock Identification of animal behavioral strategies by inverse
  reinforcement learning.
\newblock \emph{PLoS computational biology}, 14\penalty0 (5):\penalty0
  e1006122, 2018.

\bibitem[Zeng et~al.(2022)Zeng, Li, Garcia, and Hong]{zeng2022maximum}
Siliang Zeng, Chenliang Li, Alfredo Garcia, and Mingyi Hong.
\newblock Maximum-likelihood inverse reinforcement learning with finite-time
  guarantees.
\newblock \emph{arXiv preprint arXiv:2210.01808}, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Li, Garcia, and Hong]{zeng2023understanding}
Siliang Zeng, Chenliang Li, Alfredo Garcia, and Mingyi Hong.
\newblock Understanding expertise through demonstrations: A maximum likelihood
  framework for offline inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2302.07457}, 2023.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2008.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}
Brian~D Ziebart, J~Andrew Bagnell, and Anind~K Dey.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{ICML}, 2010.

\bibitem[Zolna et~al.(2021)Zolna, Reed, Novikov, Colmenarejo, Budden, Cabi,
  Denil, de~Freitas, and Wang]{zolna2021task}
Konrad Zolna, Scott Reed, Alexander Novikov, Sergio~Gomez Colmenarejo, David
  Budden, Serkan Cabi, Misha Denil, Nando de~Freitas, and Ziyu Wang.
\newblock Task-relevant adversarial imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 247--263. PMLR, 2021.

\end{thebibliography}
