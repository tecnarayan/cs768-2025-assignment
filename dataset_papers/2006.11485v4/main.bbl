\begin{thebibliography}{10}

\bibitem{andrychowicz_hindsight_2017}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Hindsight experience replay.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2017.

\bibitem{barto_recent_2003}
Andrew~G. Barto and Sridhar Mahadevan.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock {\em Discrete event dynamic systems}, 13(1-2):41--77, 2003.

\bibitem{castro_scalable_2019}
Pablo~Samuel Castro.
\newblock Scalable methods for computing state similarity in deterministic
  {Markov} {Decision} {Processes}.
\newblock In {\em AAAI}, 2020.

\bibitem{dayan_improving_1993}
Peter Dayan.
\newblock Improving generalization for temporal difference learning: {The}
  successor representation.
\newblock {\em Neural Computation}, 5(4):613--624, 1993.

\bibitem{dayan_feudal_1993}
Peter Dayan and Geoffrey~E Hinton.
\newblock Feudal reinforcement learning.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  1993.

\bibitem{duan_benchmarking_2016}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em {ICML}}, 2016.

\bibitem{ecoffet_go-explore:_2019}
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth~O. Stanley, and Jeff
  Clune.
\newblock Go-{Explore}: {A} new approach for hard-exploration problems.
\newblock {\em arXiv preprint arXiv:1901.10995}, 2019.

\bibitem{eysenbach_search_2019}
Ben Eysenbach, Russ~R Salakhutdinov, and Sergey Levine.
\newblock Search on the replay buffer: {Bridging} planning and reinforcement
  learning.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2019.

\bibitem{ferns_metrics_2004}
Norm Ferns, Prakash Panangaden, and Doina Precup.
\newblock Metrics for finite {Markov} {Decision} {Processes}.
\newblock In {\em {AAAI}}, 2004.

\bibitem{florensa_self-supervised_2019}
Carlos Florensa, Jonas Degrave, Nicolas Heess, Jost~Tobias Springenberg, and
  Martin Riedmiller.
\newblock Self-supervised learning of image embedding for continuous control.
\newblock {\em arXiv preprint arXiv:1901.00943}, 2019.

\bibitem{florensa_stochastic_2017}
Carlos Florensa, Yan Duan, and Pieter Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock In {\em {ICLR}}, 2017.

\bibitem{florensa_automatic_2018}
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In {\em {ICML}}, 2018.

\bibitem{fujimoto_addressing_2018}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em {ICML}}, 2018.

\bibitem{hadsell_dimensionality_2006}
R.~Hadsell, S.~Chopra, and Y.~LeCun.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock In {\em {CVPR}}, 2006.

\bibitem{hartikainen_dynamical_2020}
Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine.
\newblock Dynamical distance learning for semi-supervised and unsupervised
  skill discovery.
\newblock In {\em {ICLR}}, 2020.

\bibitem{higgins_beta-vae:_2017}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock Beta-{VAE}: {Learning} basic visual concepts with a constrained
  variational framework.
\newblock In {\em {ICLR}}, 2017.

\bibitem{huang_mapping_2019}
Zhiao Huang, Fangchen Liu, and Hao Su.
\newblock Mapping state space using landmarks for universal goal reaching.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2019.

\bibitem{simsek_identifying_2005}
Özgür Şimşek, Alicia~P. Wolfe, and Andrew~G. Barto.
\newblock Identifying useful subgoals in reinforcement learning by local graph
  partitioning.
\newblock In {\em {ICML}}, 2005.

\bibitem{khetarpal_what_2020}
Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina
  Precup.
\newblock What can {I} do here? {A} theory of affordances in reinforcement
  learning.
\newblock In {\em {ICML}}, 2020.

\bibitem{kulkarni_hierarchical_2016}
Tejas~D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: {Integrating} temporal
  abstraction and intrinsic motivation.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2016.

\bibitem{kulkarni_deep_2016}
Tejas~D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel~J. Gershman.
\newblock Deep successor reinforcement learning.
\newblock {\em arXiv preprint arXiv:1606.02396}, 2016.

\bibitem{levy_learning_2019}
Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock In {\em {ICLR}}, 2019.

\bibitem{mcgovern_automatic_2001}
Amy McGovern and Andrew~G. Barto.
\newblock Automatic discovery of subgoals in reinforcement learning using
  diverse density.
\newblock In {\em {ICML}}, 2001.

\bibitem{mnih_asynchronous_2016}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em {ICML}}, 2016.

\bibitem{nachum_near-optimal_2019}
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine.
\newblock Near-optimal representation learning for hierarchical reinforcement
  learning.
\newblock In {\em {ICLR}}, 2019.

\bibitem{nachum_data-efficient_2018}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2018.

\bibitem{nachum_why_2019}
Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey
  Levine.
\newblock Why does hierarchy (sometimes) work so well in reinforcement
  learning?
\newblock {\em arXiv preprint arXiv:1909.10618}, 2019.

\bibitem{nair_visual_2018}
Ashvin~V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and
  Sergey Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2018.

\bibitem{nasiriany_planning_2019}
Soroush Nasiriany, Vitchyr~H. Pong, Steven Lin, and Sergey Levine.
\newblock Planning with goal-conditioned policies.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2019.

\bibitem{pong_temporal_2018}
Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine.
\newblock Temporal difference models: {Model}-free deep {RL} for model-based
  control.
\newblock In {\em {ICLR}}, 2018.

\bibitem{precup_temporal_2000}
Doina Precup.
\newblock {\em Temporal abstraction in reinforcement learning}.
\newblock PhD thesis, University of Massachusetts, Amherst, 2000.

\bibitem{rafati_unsupervised_2019}
Jacob Rafati and David~C Noelle.
\newblock Unsupervised methods for subgoal discovery during intrinsic
  motivation in model-free hierarchical reinforcement learning.
\newblock In {\em {AAAI}}, 2019.

\bibitem{ren_exploration_2019}
Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng.
\newblock Exploration via hindsight goal generation.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2019.

\bibitem{savinov_semi-parametric_2018}
Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun.
\newblock Semi-parametric topological memory for navigation.
\newblock In {\em {ICLR}}, 2018.

\bibitem{savinov_episodic_2019}
Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc
  Pollefeys, Timothy Lillicrap, and Sylvain Gelly.
\newblock Episodic curiosity through reachability.
\newblock In {\em {ICLR}}, 2019.

\bibitem{schaul_universal_2015}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In {\em {ICML}}, 2015.

\bibitem{schmidhuber_planning_1993}
Jürgen Schmidhuber and Reiner Wahnsiedler.
\newblock Planning simple trajectories using neural subgoal generators.
\newblock In {\em From {Animals} to {Animats} 2: {Proceedings} of the {Second}
  {International} {Conference} on {Simulation} of {Adaptive} {Behavior}},
  volume~2, page 196. MIT Press, 1993.

\bibitem{srinivas_curl_2020}
Aravind Srinivas, Michael Laskin, and Pieter Abbeel.
\newblock {CURL}: {Contrastive} unsupervised representations for reinforcement
  learning.
\newblock In {\em {ICML}}, 2020.

\bibitem{sutton_between_1999}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDPs} and semi-{MDPs}: {A} framework for temporal
  abstraction in reinforcement learning.
\newblock {\em Artificial Intelligence}, 112(1-2):181--211, 1999.

\bibitem{todorov_mujoco_2012}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In {\em International {Conference} on {Intelligent} {Robots} and
  {Systems}}, 2012.

\bibitem{van_de_wiele_q-learning_2020}
Tom Van~de Wiele, David Warde-Farley, Andriy Mnih, and Volodymyr Mnih.
\newblock Q-learning in enormous action spaces via amortized approximate
  maximization.
\newblock In {\em {ICLR}}, 2020.

\bibitem{vezhnevets_feudal_2017}
Alexander~Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max
  Jaderberg, David Silver, and Koray Kavukcuoglu.
\newblock {FeUdal} networks for hierarchical reinforcement learning.
\newblock In {\em {ICML}}, 2017.

\bibitem{zahavy_learn_nodate}
Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel~J Mankowitz, and Shie Mannor.
\newblock Learn what not to learn: Action elimination with deep reinforcement
  learning.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  2018.

\bibitem{zhang_composable_2018}
Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, and Arthur Szlam.
\newblock Composable planning with attributes.
\newblock In {\em {ICML}}, 2018.

\end{thebibliography}
