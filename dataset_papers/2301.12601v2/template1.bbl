\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Agarwal, Jiang, Kakade, and Sun}{Agarwal
  et~al.}{2021}]{agarwal2019reinforcement}
Agarwal, A., N.~Jiang, S.~M. Kakade, and W.~Sun (2021).
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em \url{https://rltheorybook.github.io/rltheorybook_AJKS.pdf}\/}.

\bibitem[\protect\citeauthoryear{Artzner, Delbaen, Eber, Heath, and Ku}{Artzner
  et~al.}{2007}]{artzner2007coherent}
Artzner, P., F.~Delbaen, J.-M. Eber, D.~Heath, and H.~Ku (2007).
\newblock Coherent multiperiod risk adjusted values and bellmanâ€™s principle.
\newblock {\em Annals of Operations Research\/}~{\em 152\/}(1), 5--22.

\bibitem[\protect\citeauthoryear{Azar, Osband, and Munos}{Azar
  et~al.}{2017}]{azar2017minimax}
Azar, M.~G., I.~Osband, and R.~Munos (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pp.\
  263--272. PMLR.

\bibitem[\protect\citeauthoryear{Bastani, Ma, Shen, and Xu}{Bastani
  et~al.}{2022}]{bastani2022regret}
Bastani, O., Y.~J. Ma, E.~Shen, and W.~Xu (2022).
\newblock Regret bounds for risk-sensitive reinforcement learning.
\newblock {\em arXiv preprint arXiv:2210.05650\/}.

\bibitem[\protect\citeauthoryear{B{\"a}uerle and Glauner}{B{\"a}uerle and
  Glauner}{2022}]{bauerle2022markov}
B{\"a}uerle, N. and A.~Glauner (2022).
\newblock Markov decision processes with recursive risk measures.
\newblock {\em European Journal of Operational Research\/}~{\em 296\/}(3),
  953--966.

\bibitem[\protect\citeauthoryear{Bellemare, Dabney, and Munos}{Bellemare
  et~al.}{2017}]{bellemare2017distributional}
Bellemare, M.~G., W.~Dabney, and R.~Munos (2017).
\newblock A distributional perspective on reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pp.\
  449--458. PMLR.

\bibitem[\protect\citeauthoryear{Ben-Tal and Teboulle}{Ben-Tal and
  Teboulle}{1986}]{ben1986expected}
Ben-Tal, A. and M.~Teboulle (1986).
\newblock Expected utility, penalty functions, and duality in stochastic
  nonlinear programming.
\newblock {\em Management Science\/}~{\em 32\/}(11), 1445--1466.

\bibitem[\protect\citeauthoryear{Ben-Tal and Teboulle}{Ben-Tal and
  Teboulle}{2007}]{ben2007old}
Ben-Tal, A. and M.~Teboulle (2007).
\newblock An old-new concept of convex risk measures: the optimized certainty
  equivalent.
\newblock {\em Mathematical Finance\/}~{\em 17\/}(3), 449--476.

\bibitem[\protect\citeauthoryear{Chow, Ghavamzadeh, Janson, and Pavone}{Chow
  et~al.}{2017}]{chow2017risk}
Chow, Y., M.~Ghavamzadeh, L.~Janson, and M.~Pavone (2017).
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock {\em The Journal of Machine Learning Research\/}~{\em 18\/}(1),
  6070--6120.

\bibitem[\protect\citeauthoryear{Dann}{Dann}{2019}]{dann2019strategic}
Dann, C. (2019).
\newblock {\em Strategic Exploration in Reinforcement Learning-New Algorithms
  and Learning Guarantees}.
\newblock Ph.\ D. thesis, Carnegie Mellon University.

\bibitem[\protect\citeauthoryear{Ding, Jin, and Lavaei}{Ding
  et~al.}{2022}]{ding2022non}
Ding, Y., M.~Jin, and J.~Lavaei (2022).
\newblock Non-stationary risk-sensitive reinforcement learning: Near-optimal
  dynamic regret, adaptive detection, and separation design.
\newblock {\em arXiv preprint arXiv:2211.10815\/}.

\bibitem[\protect\citeauthoryear{Domingues, M{\'e}nard, Kaufmann, and
  Valko}{Domingues et~al.}{2021}]{domingues2021episodic}
Domingues, O.~D., P.~M{\'e}nard, E.~Kaufmann, and M.~Valko (2021).
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In {\em Algorithmic Learning Theory}, pp.\  578--598. PMLR.

\bibitem[\protect\citeauthoryear{Du, Wang, and Huang}{Du
  et~al.}{2022}]{du2022risk}
Du, Y., S.~Wang, and L.~Huang (2022).
\newblock Risk-sensitive reinforcement learning: Iterated cvar and the worst
  path.
\newblock {\em arXiv preprint arXiv:2206.02678\/}.

\bibitem[\protect\citeauthoryear{Epstein and Zin}{Epstein and
  Zin}{1989}]{epstein1989substitution}
Epstein, L.~G. and S.~E. Zin (1989).
\newblock Substitution, risk aversion, and the temporal behavior of consumption
  and asset returns: a theoretical framework.
\newblock {\em Econometrica\/}~{\em 57}, 937--969.

\bibitem[\protect\citeauthoryear{Fei, Yang, Chen, and Wang}{Fei
  et~al.}{2021}]{fei2021exponential}
Fei, Y., Z.~Yang, Y.~Chen, and Z.~Wang (2021).
\newblock Exponential bellman equation and improved regret bounds for
  risk-sensitive reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems\/}~{\em 34},
  20436--20446.

\bibitem[\protect\citeauthoryear{Fei, Yang, Chen, Wang, and Xie}{Fei
  et~al.}{2020}]{fei2020risk}
Fei, Y., Z.~Yang, Y.~Chen, Z.~Wang, and Q.~Xie (2020).
\newblock Risk-sensitive reinforcement learning: Near-optimal risk-sample
  tradeoff in regret.
\newblock {\em Advances in Neural Information Processing Systems\/}~{\em 33},
  22384--22395.

\bibitem[\protect\citeauthoryear{Fei, Yang, and Wang}{Fei
  et~al.}{2021}]{fei2021risk}
Fei, Y., Z.~Yang, and Z.~Wang (2021).
\newblock Risk-sensitive reinforcement learning with function approximation: A
  debiasing approach.
\newblock In {\em International Conference on Machine Learning}, pp.\
  3198--3207. PMLR.

\bibitem[\protect\citeauthoryear{Garc{\i}a and Fern{\'a}ndez}{Garc{\i}a and
  Fern{\'a}ndez}{2015}]{garcia2015comprehensive}
Garc{\i}a, J. and F.~Fern{\'a}ndez (2015).
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research\/}~{\em 16\/}(1),
  1437--1480.

\bibitem[\protect\citeauthoryear{Garivier, M{\'e}nard, and Stoltz}{Garivier
  et~al.}{2019}]{garivier2019explore}
Garivier, A., P.~M{\'e}nard, and G.~Stoltz (2019).
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock {\em Mathematics of Operations Research\/}~{\em 44\/}(2), 377--399.

\bibitem[\protect\citeauthoryear{Howard and Matheson}{Howard and
  Matheson}{1972}]{howard1972risk}
Howard, R.~A. and J.~E. Matheson (1972).
\newblock Risk-sensitive markov decision processes.
\newblock {\em Management science\/}~{\em 18\/}(7), 356--369.

\bibitem[\protect\citeauthoryear{Jaksch, Ortner, and Auer}{Jaksch
  et~al.}{2010}]{jaksch10a}
Jaksch, T., R.~Ortner, and P.~Auer (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research\/}~{\em 11\/}(51),
  1563--1600.

\bibitem[\protect\citeauthoryear{Jin, Allen-Zhu, Bubeck, and Jordan}{Jin
  et~al.}{2018}]{jin2018q}
Jin, C., Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan (2018).
\newblock Is q-learning provably efficient?
\newblock {\em Advances in neural information processing systems\/}~{\em 31}.

\bibitem[\protect\citeauthoryear{Kreps and Porteus}{Kreps and
  Porteus}{1978}]{kreps1978temporal}
Kreps, D.~M. and E.~L. Porteus (1978).
\newblock Temporal resolution of uncertainty and dynamic choice theory.
\newblock {\em Econometrica: journal of the Econometric Society\/}, 185--200.

\bibitem[\protect\citeauthoryear{LA and Bhat}{LA and
  Bhat}{2022}]{la2022wasserstein}
LA, P. and S.~P. Bhat (2022).
\newblock A wasserstein distance approach for concentration of empirical risk
  estimates.
\newblock {\em Journal of Machine Learning Research\/}~{\em 23}, 1--61.

\bibitem[\protect\citeauthoryear{Lee, Park, and Shin}{Lee
  et~al.}{2020}]{lee2020learning}
Lee, J., S.~Park, and J.~Shin (2020).
\newblock Learning bounds for risk-sensitive learning.
\newblock {\em Advances in Neural Information Processing Systems\/}~{\em 33},
  13867--13879.

\bibitem[\protect\citeauthoryear{Liang and Luo}{Liang and
  Luo}{2022}]{liang2022bridging}
Liang, H. and Z.-Q. Luo (2022).
\newblock Bridging distributional and risk-sensitive reinforcement learning
  with provable regret bounds.
\newblock {\em arXiv preprint arXiv:2210.14051\/}.

\bibitem[\protect\citeauthoryear{Mannor and Tsitsiklis}{Mannor and
  Tsitsiklis}{2011}]{mannor2011mean}
Mannor, S. and J.~N. Tsitsiklis (2011).
\newblock Mean-variance optimization in markov decision processes.
\newblock In {\em Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pp.\  177--184.

\bibitem[\protect\citeauthoryear{Marcus, Fern{\'a}ndez-Gaucherand,
  Hern{\'a}ndez-Hernandez, Coraluppi, and Fard}{Marcus
  et~al.}{1997}]{marcus1997risk}
Marcus, S.~I., E.~Fern{\'a}ndez-Gaucherand, D.~Hern{\'a}ndez-Hernandez,
  S.~Coraluppi, and P.~Fard (1997).
\newblock Risk sensitive markov decision processes.
\newblock In {\em Systems and control in the twenty-first century}, pp.\
  263--279. Springer.

\bibitem[\protect\citeauthoryear{Markowitz}{Markowitz}{1952}]{markowitz1952}
Markowitz, H. (1952).
\newblock Portfolio selection.
\newblock {\em The Journal of Finance\/}~{\em 7\/}(1), 77--91.

\bibitem[\protect\citeauthoryear{Parikh, Boyd, et~al.}{Parikh
  et~al.}{2014}]{parikh2014proximal}
Parikh, N., S.~Boyd, et~al. (2014).
\newblock Proximal algorithms.
\newblock {\em Foundations and trends{\textregistered} in Optimization\/}~{\em
  1\/}(3), 127--239.

\bibitem[\protect\citeauthoryear{Prashanth~L and Fu}{Prashanth~L and
  Fu}{2018}]{prashanth2018risk}
Prashanth~L, A. and M.~Fu (2018).
\newblock Risk-sensitive reinforcement learning.
\newblock {\em arXiv e-prints\/}, arXiv:1810.09126.

\bibitem[\protect\citeauthoryear{Puterman}{Puterman}{2014}]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[\protect\citeauthoryear{Ruszczy{\'n}ski}{Ruszczy{\'n}ski}{2010}]{ruszczynski2010risk}
Ruszczy{\'n}ski, A. (2010).
\newblock Risk-averse dynamic programming for markov decision processes.
\newblock {\em Mathematical programming\/}~{\em 125\/}(2), 235--261.

\bibitem[\protect\citeauthoryear{Ruszczy{\'n}ski and Shapiro}{Ruszczy{\'n}ski
  and Shapiro}{2006}]{ruszczynski2006optimization}
Ruszczy{\'n}ski, A. and A.~Shapiro (2006).
\newblock Optimization of convex risk functions.
\newblock {\em Mathematics of operations research\/}~{\em 31\/}(3), 433--452.

\bibitem[\protect\citeauthoryear{Sarver}{Sarver}{2018}]{sarver2018dynamic}
Sarver, T. (2018).
\newblock Dynamic mixture-averse preferences.
\newblock {\em Econometrica\/}~{\em 86\/}(4), 1347--1382.

\bibitem[\protect\citeauthoryear{Shen, Stannat, and Obermayer}{Shen
  et~al.}{2013}]{shen2013risk}
Shen, Y., W.~Stannat, and K.~Obermayer (2013).
\newblock Risk-sensitive markov control processes.
\newblock {\em SIAM Journal on Control and Optimization\/}~{\em 51\/}(5),
  3652--3672.

\bibitem[\protect\citeauthoryear{Shen, Tobia, Sommer, and Obermayer}{Shen
  et~al.}{2014}]{shen2014risk}
Shen, Y., M.~J. Tobia, T.~Sommer, and K.~Obermayer (2014).
\newblock Risk-sensitive reinforcement learning.
\newblock {\em Neural computation\/}~{\em 26\/}(7), 1298--1328.

\bibitem[\protect\citeauthoryear{Sutton and Barto}{Sutton and
  Barto}{2018}]{sutton2018reinforcement}
Sutton, R.~S. and A.~G. Barto (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[\protect\citeauthoryear{Tamar, Di~Castro, and Mannor}{Tamar
  et~al.}{2016}]{tamar2016learning}
Tamar, A., D.~Di~Castro, and S.~Mannor (2016).
\newblock Learning the variance of the reward-to-go.
\newblock {\em The Journal of Machine Learning Research\/}~{\em 17\/}(1),
  361--396.

\bibitem[\protect\citeauthoryear{Zanette and Brunskill}{Zanette and
  Brunskill}{2019}]{zanette2019tighter}
Zanette, A. and E.~Brunskill (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning}, pp.\
  7304--7312. PMLR.

\end{thebibliography}
