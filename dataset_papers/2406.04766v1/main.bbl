\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alsabbagh et~al.(2019)Alsabbagh, Yin, and Ma]{alsabbaghDistributedChargingManagement2019}
Alsabbagh, A., Yin, H., and Ma, C.
\newblock Distributed charging management of multi-class electric vehicles with different charging priorities.
\newblock \emph{IET Generation, Transmission \& Distribution}, 13\penalty0 (22):\penalty0 5257--5264, 2019.

\bibitem[Anderson et~al.(1999)Anderson, Bai, Bischof, Blackford, Demmel, Dongarra, Du~Croz, Greenbaum, Hammarling, McKenney, and Sorensen]{andersonLAPACKUsersGuide1999}
Anderson, E., Bai, Z., Bischof, C., Blackford, L.~S., Demmel, J., Dongarra, J., Du~Croz, J., Greenbaum, A., Hammarling, S., McKenney, A., and Sorensen, D.
\newblock \emph{{{LAPACK Users}}' {{Guide}}}.
\newblock Software, {{Environments}}, and {{Tools}}. {Society for Industrial and Applied Mathematics}, January 1999.

\bibitem[Anselmi et~al.(2022)Anselmi, Gaujal, and Rebuffi]{anselmiReinforcementLearningBirth2022}
Anselmi, J., Gaujal, B., and Rebuffi, L.-S.
\newblock Reinforcement {{Learning}} in a {{Birth}} and {{Death Process}}: {{Breaking}} the {{Dependence}} on the {{State Space}}.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14464--14474, December 2022.

\bibitem[Auer \& Ortner(2006)Auer and Ortner]{auerLogarithmicOnlineRegret2006}
Auer, P. and Ortner, R.
\newblock Logarithmic {{Online Regret Bounds}} for {{Undiscounted Reinforcement Learning}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~19, pp.\  49--56. MIT Press, 2006.

\bibitem[Bourel et~al.(2020)Bourel, Maillard, and Talebi]{bourelTighteningExplorationUpper2020a}
Bourel, H., Maillard, O., and Talebi, M.~S.
\newblock Tightening {{Exploration}} in {{Upper Confidence Reinforcement Learning}}.
\newblock In \emph{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}, pp.\  1056--1066. PMLR, November 2020.

\bibitem[Bubeck et~al.(2013)Bubeck, {Cesa-Bianchi}, and Lugosi]{bubeckBanditsHeavyTail2013}
Bubeck, S., {Cesa-Bianchi}, N., and Lugosi, G.
\newblock Bandits {{With Heavy Tail}}.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0 (11):\penalty0 7711--7717, November 2013.

\bibitem[Carpenter \& Nichols(2002)Carpenter and Nichols]{carpenterDifferentiatedServicesInternet2002}
Carpenter, B. and Nichols, K.
\newblock Differentiated services in the {{Internet}}.
\newblock \emph{Proceedings of the IEEE}, 90\penalty0 (9):\penalty0 1479--1494, September 2002.

\bibitem[Cohen et~al.(2024)Cohen, Subramanian, and Zhang]{cohenLearningBasedOptimalAdmission2024}
Cohen, A., Subramanian, V., and Zhang, Y.
\newblock Learning-{{Based Optimal Admission Control}} in a {{Single-Server Queuing System}}.
\newblock \emph{Stochastic Systems}, January 2024.

\bibitem[Della~Vecchia et~al.(2012)Della~Vecchia, Di~Marco, and {Jean-Marie}]{dellavecchiaIllustratedReviewConvergence2012}
Della~Vecchia, E., Di~Marco, S., and {Jean-Marie}, A.
\newblock Illustrated review of convergence conditions of the value iteration algorithm and the rolling horizon procedure for average-cost {{MDPs}}.
\newblock \emph{Annals of Operations Research}, 199\penalty0 (1):\penalty0 193--214, October 2012.

\bibitem[Feinberg \& Reiman(1994)Feinberg and Reiman]{feinbergOptimalityRandomizedTrunk1994a}
Feinberg, E.~A. and Reiman, M.~I.
\newblock Optimality of {{Randomized Trunk Reservation}}.
\newblock \emph{Probability in the Engineering and Informational Sciences}, 8\penalty0 (4):\penalty0 463--489, October 1994.

\bibitem[Feinberg \& Yang(2011)Feinberg and Yang]{feinbergOptimalityTrunkReservation2011}
Feinberg, E.~A. and Yang, F.
\newblock Optimality of trunk reservation for an {{M}}/{{M}}/k/{{N}} queue with several customer types and holding costs.
\newblock \emph{Probability in the Engineering and Informational Sciences}, 25\penalty0 (4):\penalty0 537--560, October 2011.

\bibitem[Gao \& Zhou(2023)Gao and Zhou]{gaoLogarithmicRegretBounds2023}
Gao, X. and Zhou, X.~Y.
\newblock Logarithmic regret bounds for continuous-time average-reward {{Markov}} decision processes, July 2023.

\bibitem[Haviv \& Puterman(1998)Haviv and Puterman]{havivBiasOptimalityControlled1998}
Haviv, M. and Puterman, M.~L.
\newblock Bias {{Optimality}} in {{Controlled Queueing Systems}}.
\newblock \emph{Journal of Applied Probability}, 35\penalty0 (1):\penalty0 136--150, 1998.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JMLR:v11:jaksch10a}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (51):\penalty0 1563--1600, 2010.

\bibitem[Kemp(1987)]{kempStochasticModellingAnalysis1987}
Kemp, A.~W.
\newblock Stochastic {{Modelling}} and {{Analysis}}: {{A Computational Approach}}.
\newblock \emph{Journal of the Royal Statistical Society Series D: The Statistician}, 36\penalty0 (4):\penalty0 417, October 1987.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and Szepesv{\'a}ri]{lattimoreBanditAlgorithms2020}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit {{Algorithms}}}.
\newblock Cambridge University Press, July 2020.

\bibitem[Lewis \& Puterman(2002)Lewis and Puterman]{lewisBiasOptimality2002}
Lewis, M.~E. and Puterman, M.~L.
\newblock Bias {{Optimality}}.
\newblock In Feinberg, E.~A. and Shwartz, A. (eds.), \emph{Handbook of {{Markov Decision Processes}}: {{Methods}} and {{Applications}}}, International {{Series}} in {{Operations Research}} \& {{Management Science}}, pp.\  89--111. Springer US, Boston, MA, 2002.

\bibitem[Lewis et~al.(1999)Lewis, Ayhan, and Foley]{lewisBIASOPTIMALITYQUEUE1999}
Lewis, M.~E., Ayhan, H., and Foley, R.~D.
\newblock Bias {{Optimality In A Queue With Admission Control}}.
\newblock \emph{Probability in the Engineering and Informational Sciences}, 13\penalty0 (3):\penalty0 309--327, 1999.

\bibitem[Manne(1960)]{manneLinearProgrammingSequential1960}
Manne, A.~S.
\newblock Linear {{Programming}} and {{Sequential Decisions}}.
\newblock \emph{Management Science}, 6\penalty0 (3):\penalty0 259--267, 1960.

\bibitem[Massaro et~al.(2019)Massaro, De~Pellegrini, and Maggi]{massaroOptimalTrunkReservationPolicy2019}
Massaro, A., De~Pellegrini, F., and Maggi, L.
\newblock Optimal {{Trunk-Reservation}} by {{Policy Learning}}.
\newblock In \emph{{{IEEE INFOCOM}} 2019 - {{IEEE Conference}} on {{Computer Communications}}}, pp.\  127--135, April 2019.

\bibitem[Miller(1969)]{millerQueueingRewardSystem1969a}
Miller, B.~L.
\newblock A {{Queueing Reward System}} with {{Several Customer Classes}}.
\newblock \emph{Management Science}, 16\penalty0 (3):\penalty0 234--245, 1969.

\bibitem[Osband \& Van~Roy(2016)Osband and Van~Roy]{osbandPosteriorSamplingReinforcement2016a}
Osband, I. and Van~Roy, B.
\newblock Posterior {{Sampling}} for {{Reinforcement Learning Without Episodes}}.
\newblock \emph{arXiv preprint arXiv:1608.02731}, August 2016.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osbandMoreEfficientReinforcement2013}
Osband, I., Russo, D., and Van~Roy, B.
\newblock ({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~26. Curran Associates, Inc., 2013.

\bibitem[Puterman(2005)]{putermanMarkovDecisionProcesses2005a}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic Programming}.
\newblock Wiley Series in Probability and Statistics. Wiley-Interscience, Hoboken, NJ, 2005.

\bibitem[Roy et~al.(2022)Roy, Borkar, Karandikar, and Chaporkar]{royOnlineReinforcementLearning2022}
Roy, A., Borkar, V., Karandikar, A., and Chaporkar, P.
\newblock Online {{Reinforcement Learning}} of {{Optimal Threshold Policies}} for {{Markov Decision Processes}}.
\newblock \emph{IEEE Transactions on Automatic Control}, 67\penalty0 (7):\penalty0 3722--3729, July 2022.

\bibitem[Talebi \& Maillard(2018)Talebi and Maillard]{talebiVarianceAwareRegretBounds2018b}
Talebi, M.~S. and Maillard, O.-A.
\newblock Variance-{{Aware Regret Bounds}} for {{Undiscounted Reinforcement Learning}} in {{MDPs}}.
\newblock In \emph{Proceedings of {{Algorithmic Learning Theory}}}, pp.\  770--805. PMLR, April 2018.

\bibitem[Tossou et~al.(2019)Tossou, Basu, and Dimitrakakis]{tossouNearoptimalOptimisticReinforcement2019}
Tossou, A., Basu, D., and Dimitrakakis, C.
\newblock Near-optimal {{Optimistic Reinforcement Learning}} using {{Empirical Bernstein Inequalities}}.
\newblock \emph{arXiv preprint arXiv:1905.12425}, December 2019.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verd{\'u}, and Weinberger]{weissmanInequalitiesL1Deviation2003}
Weissman, T., Ordentlich, E., Seroussi, G., Verd{\'u}, S., and Weinberger, M.
\newblock Inequalities for the {{L1 Deviation}} of the {{Empirical Distribution}}.
\newblock \emph{Hewlett-{{Packard Labs}}, {{Tech}}. {{Rep}}}, 2003.

\bibitem[Zhang \& Ji(2019)Zhang and Ji]{zhangRegretMinimizationReinforcement2019a}
Zhang, Z. and Ji, X.
\newblock Regret {{Minimization}} for {{Reinforcement Learning}} by {{Evaluating}} the {{Optimal Bias Function}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32, pp.\  2827--2836. Curran Associates, Inc., 2019.

\bibitem[Zhou et~al.(2017)Zhou, Wang, Jia, and Guo]{zhouReinforcementLearningbasedAdaptive2017}
Zhou, X., Wang, K., Jia, W., and Guo, M.
\newblock Reinforcement learning-based adaptive resource management of differentiated services in geo-distributed data centers.
\newblock In \emph{2017 {{IEEE}}/{{ACM}} 25th {{International Symposium}} on {{Quality}} of {{Service}} ({{IWQoS}})}, pp.\  1--6, June 2017.

\end{thebibliography}
