\begin{thebibliography}{}

\bibitem[Abachi et~al., 2020]{paml}
Abachi, R., Ghavamzadeh, M., and Farahmand, A. (2020).
\newblock Policy-aware model learning for policy gradient methods.
\newblock {\em CoRR}, abs/2003.00030.

\bibitem[Ayoub et~al., 2020]{Ayoub}
Ayoub, A., Jia, Z., Szepesv\'ari, C., Wang, M., and Yang, L. (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In {\em ICML}.

\bibitem[Buesing et~al., 2019]{woulda_coulda_shoulda}
Buesing, L., Weber, T., Zwols, Y., Heess, N., Racani{\`{e}}re, S., Guez, A.,
  and Lespiau, J. (2019).
\newblock Woulda, coulda, shoulda: Counterfactually-guided policy search.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[Deisenroth et~al., 2015]{deisenroth}
Deisenroth, M.~P., Fox, D., and Rasmussen, C.~E. (2015).
\newblock Gaussian processes for data-efficient learning in robotics and
  control.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 37(2):408--423.

\bibitem[D'Oro et~al., 2019]{doro}
D'Oro, P., Metelli, A.~M., Tirinzoni, A., Papini, M., and Restelli, M. (2019).
\newblock Gradient-aware model-based policy search.
\newblock {\em CoRR}, abs/1909.04115.

\bibitem[Farahmand, 2018]{iter_vaml}
Farahmand, A. (2018).
\newblock Iterative value-aware model learning.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al,
  Canada}, pages 9090--9101.

\bibitem[Farahmand et~al., 2017]{vaml}
Farahmand, A.~M., Barreto, A., and Nikovski, D. (2017).
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In Singh, A. and Zhu, X.~J., editors, {\em Proceedings of the 20th
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}}, volume~54 of {\em
  Proceedings of Machine Learning Research}, pages 1486--1494. {PMLR}.

\bibitem[Farquhar et~al., 2017]{Farquhar}
Farquhar, G., Rockt{\"{a}}schel, T., Igl, M., and Whiteson, S. (2017).
\newblock Treeqn and atreec: Differentiable tree planning for deep
  reinforcement learning.
\newblock {\em CoRR}, abs/1710.11417.

\bibitem[Gelada and Bellemare, 2019]{Gelada2019OffPolicyDR}
Gelada, C. and Bellemare, M.~G. (2019).
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In {\em AAAI}.

\bibitem[Goyal et~al., 2019]{recall_traces}
Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap, T.~P., Levine, S.,
  Larochelle, H., and Bengio, Y. (2019).
\newblock Recall traces: Backtracking models for efficient reinforcement
  learning.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[Ha and Schmidhuber, 2018]{has}
Ha, D. and Schmidhuber, J. (2018).
\newblock Recurrent world models facilitate policy evolution.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al,
  Canada}, pages 2455--2467.

\bibitem[Hallak and Mannor, 2017]{Hallak2017ConsistentOO}
Hallak, A. and Mannor, S. (2017).
\newblock Consistent on-line off-policy evaluation.
\newblock {\em ArXiv}, abs/1702.07121.

\bibitem[Harutyunyan et~al., 2019a]{termination_critic}
Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., and Precup, D.
  (2019a).
\newblock The termination critic.
\newblock In Chaudhuri, K. and Sugiyama, M., editors, {\em The 22nd
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2019, 16-18 April 2019, Naha, Okinawa, Japan}, volume~89 of {\em Proceedings
  of Machine Learning Research}, pages 2231--2240. {PMLR}.

\bibitem[Harutyunyan et~al., 2019b]{hindsight_credit_assignment}
Harutyunyan, A., Dabney, W., Mesnard, T., Azar, M.~G., Piot, B., Heess, N.,
  {van Hasselt}, H., Wayne, G., Singh, S., Precup, D., and Munos, R. (2019b).
\newblock Hindsight credit assignment.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R., editors, {\em Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
  Vancouver, BC, Canada}, pages 12467--12476.

\bibitem[Hester and Stone, 2013]{hester_and_stone}
Hester, T. and Stone, P. (2013).
\newblock {TEXPLORE:} real-time sample-efficient reinforcement learning for
  robots.
\newblock {\em Mach. Learn.}, 90(3):385--429.

\bibitem[Hung et~al., 2019]{Hung2019OptimizingAB}
Hung, C.-C., Lillicrap, T.~P., Abramson, J., Wu, Y., Mirza, M., Carnevale, F.,
  Ahuja, A., and Wayne, G. (2019).
\newblock Optimizing agent behavior over long time scales by transporting
  value.
\newblock {\em Nature Communications}, 10.

\bibitem[Jafferjee et~al., 2020]{hallucinating_value}
Jafferjee, T., Imani, E., Talvitie, E., White, M., and Bowling, M. (2020).
\newblock Hallucinating value: {A} pitfall of dyna-style planning with
  imperfect environment models.
\newblock {\em CoRR}, abs/2006.04363.

\bibitem[Joseph et~al., 2013]{joseph}
Joseph, J.~M., Geramifard, A., Roberts, J.~W., How, J.~P., and Roy, N. (2013).
\newblock Reinforcement learning with misspecified model classes.
\newblock In {\em 2013 {IEEE} International Conference on Robotics and
  Automation, Karlsruhe, Germany, May 6-10, 2013}, pages 939--946. {IEEE}.

\bibitem[Ke et~al., 2018]{Ke2018SparseAB}
Ke, N.~R., Goyal, A., Bilaniuk, O., Binas, J., Mozer, M., Pal, C., and Bengio,
  Y. (2018).
\newblock Sparse attentive backtracking: Temporal creditassignment through
  reminding.
\newblock In {\em NeurIPS}.

\bibitem[LaValle, 2006]{Lav06}
LaValle, S.~M. (2006).
\newblock {\em Planning Algorithms}.
\newblock Cambridge University Press, Cambridge, U.K.
\newblock Available at http://planning.cs.uiuc.edu/.

\bibitem[Lengyel and Dayan, 2007]{dayan_and_Lengyel}
Lengyel, M. and Dayan, P. (2007).
\newblock Hippocampal contributions to control: The third way.
\newblock In {\em Proceedings of the 20th International Conference on Neural
  Information Processing Systems}, NIPS’07, page 889–896, Red Hook, NY,
  USA. Curran Associates Inc.

\bibitem[Lin, 1992]{lin}
Lin, L.-J. (1992).
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine Learning}, 8(3):293--321.

\bibitem[Luo et~al., 2019]{luo}
Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2019).
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[MacKay, 2002]{mackay}
MacKay, D. J.~C. (2002).
\newblock {\em Information Theory, Inference \& Learning Algorithms}.
\newblock Cambridge University Press, USA.

\bibitem[Mahmood et~al., 2015]{Mahmood2015EmphaticTL}
Mahmood, A.~R., Yu, H., White, M., and Sutton, R.~S. (2015).
\newblock Emphatic temporal-difference learning.
\newblock {\em ArXiv}, abs/1507.01569.

\bibitem[Mattar and Daw, 2018]{mattar}
Mattar, M.~G. and Daw, N.~D. (2018).
\newblock Prioritized memory access explains planning and hippocampal replay.
\newblock {\em Nature Neuroscience}, 21(11):1609--1617.

\bibitem[McMahan and Gordon, 2005]{prioritized_sweeping_MG}
McMahan, H.~B. and Gordon, G.~J. (2005).
\newblock Fast exact planning in markov decision processes.
\newblock In Biundo, S., Myers, K.~L., and Rajan, K., editors, {\em Proceedings
  of the Fifteenth International Conference on Automated Planning and
  Scheduling {(ICAPS} 2005), June 5-10 2005, Monterey, California, {USA}},
  pages 151--160. {AAAI}.

\bibitem[Mnih et~al., 2015]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M.~A., Fidjeland, A., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D. (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533.

\bibitem[Moore and Atkeson, 1993]{prioritized_sweeping_MA}
Moore, A.~W. and Atkeson, C.~G. (1993).
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  time.
\newblock {\em Mach. Learn.}, 13:103--130.

\bibitem[Morimura et~al., 2010]{morimura}
Morimura, T., Uchibe, E., Yoshimoto, J., Peters, J., and Doya, K. (2010).
\newblock Derivatives of logarithmic stationary distributions for policy
  gradient reinforcement learning.
\newblock {\em Neural Comput.}, 22(2):342–376.

\bibitem[Oh et~al., 2020]{Oh2020DiscoveringRL}
Oh, J., Hessel, M., Czarnecki, W., Xu, Z., Hasselt, H.~V., Singh, S., and
  Silver, D. (2020).
\newblock Discovering reinforcement learning algorithms.
\newblock {\em ArXiv}, abs/2007.08794.

\bibitem[Oh et~al., 2017]{value_pred_net}
Oh, J., Singh, S., and Lee, H. (2017).
\newblock Value prediction network.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}}, pages
  6118--6128.

\bibitem[Pan et~al., 2018]{organizing_experience}
Pan, Y., Zaheer, M., White, A., Patterson, A., and White, M. (2018).
\newblock Organizing experience: {A} deeper look at replay mechanisms for
  sample-based planning in continuous state domains.
\newblock {\em CoRR}, abs/1806.04624.

\bibitem[Parr et~al., 2008]{Parr}
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., and Littman, M.~L. (2008).
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In {\em Proceedings of the 25th International Conference on Machine
  Learning}, ICML '08, pages 752--759, New York, NY, USA. ACM.

\bibitem[Peng and Williams, 1993]{DynaQ}
Peng, J. and Williams, R.~J. (1993).
\newblock Efficient learning and planning within the dyna framework.
\newblock {\em Adaptive Behavior}, 1(4):437--454.

\bibitem[Pitis, 2019]{source_traces}
Pitis, S. (2019).
\newblock Source traces for temporal difference learning.
\newblock {\em CoRR}, abs/1902.02907.

\bibitem[Precup et~al., 1998]{option_models}
Precup, D., Sutton, R.~S., and Singh, S.~P. (1998).
\newblock Theoretical results on reinforcement learning with temporally
  abstract options.
\newblock In {\em Machine Learning: ECML-98, 10th European Conference on
  Machine Learning, Chemnitz, Germany, April 21-23, 1998, Proceedings}, pages
  382--393.

\bibitem[Puterman, 1994]{puterman}
Puterman, M.~L. (1994).
\newblock {\em Markov Decision Processes}.
\newblock Wiley.

\bibitem[Satija et~al., 2020]{satija2020constrained}
Satija, H., Amortila, P., and Pineau, J. (2020).
\newblock Constrained markov decision processes via backward value functions.

\bibitem[Schaul et~al., 2016]{prioritized_er}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016).
\newblock Prioritized experience replay.
\newblock In Bengio, Y. and LeCun, Y., editors, {\em 4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}.

\bibitem[Schrittwieser et~al., 2019]{muzero}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap,
  T.~P., and Silver, D. (2019).
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock {\em CoRR}, abs/1911.08265.

\bibitem[Schroecker et~al., 2019]{generative_pred_models}
Schroecker, Y., Vecer{\'{\i}}k, M., and Scholz, J. (2019).
\newblock Generative predecessor models for sample-efficient imitation
  learning.
\newblock {\em CoRR}, abs/1904.01139.

\bibitem[Silver et~al., 2017]{predictron}
Silver, D., {van Hasselt}, H., Hessel, M., Schaul, T., Guez, A., Harley, T.,
  Dulac{-}Arnold, G., Reichert, D.~P., Rabinowitz, N.~C., Barreto, A., and
  Degris, T. (2017).
\newblock The predictron: End-to-end learning and planning.
\newblock In Precup, D. and Teh, Y.~W., editors, {\em Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of {\em Proceedings of Machine
  Learning Research}, pages 3191--3199. {PMLR}.

\bibitem[Sutton and Barto, 1981]{Sutton1981AnAN}
Sutton, R. and Barto, A.~G. (1981).
\newblock An adaptive network that constructs and uses an internal model of its
  world.

\bibitem[Sutton et~al., 1999]{Sutton1999BetweenMA}
Sutton, R., Precup, D., and Singh, S. (1999).
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock {\em Artif. Intell.}, 112:181--211.

\bibitem[Sutton, 1988]{TD}
Sutton, R.~S. (1988).
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44.

\bibitem[Sutton, 1990a]{dyna}
Sutton, R.~S. (1990a).
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In {\em In Proceedings of the Seventh International Conference on
  Machine Learning}, pages 216--224. Morgan Kaufmann.

\bibitem[Sutton, 1990b]{Sutton90}
Sutton, R.~S. (1990b).
\newblock Integrated modeling and control based on reinforcement learning.
\newblock In Lippmann, R., Moody, J.~E., and Touretzky, D.~S., editors, {\em
  Advances in Neural Information Processing Systems 3, {[NIPS} Conference,
  Denver, Colorado, USA, November 26-29, 1990]}, pages 471--478. Morgan
  Kaufmann.

\bibitem[Sutton, 1995]{TD_models}
Sutton, R.~S. (1995).
\newblock {TD} models: Modeling the world at a mixture of time scales.
\newblock In {\em Machine Learning, Proceedings of the Twelfth International
  Conference on Machine Learning, Tahoe City, California, USA, July 9-12,
  1995}, pages 531--539.

\bibitem[Sutton and Barto, 2018]{rl_book}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Sutton et~al., 2016]{Sutton2016AnEA}
Sutton, R.~S., Mahmood, A.~R., and White, M. (2016).
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock {\em ArXiv}, abs/1503.04269.

\bibitem[Sutton et~al., 2011]{horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D. (2011).
\newblock Horde: a scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In {\em 10th International Conference on Autonomous Agents and
  Multiagent Systems {(AAMAS} 2011), Taipei, Taiwan, May 2-6, 2011, Volume
  1-3}, pages 761--768.

\bibitem[Sutton et~al., 2012]{linear_dyna}
Sutton, R.~S., Szepesv{\'{a}}ri, C., Geramifard, A., and Bowling, M. (2012).
\newblock Dyna-style planning with linear function approximation and
  prioritized sweeping.
\newblock {\em CoRR}, abs/1206.3285.

\bibitem[{van Hasselt} et~al., 2019]{hado_models}
{van Hasselt}, H., Hessel, M., and Aslanides, J. (2019).
\newblock When to use parametric models in reinforcement learning?
\newblock {\em CoRR}, abs/1906.05243.

\bibitem[{van Hasselt} et~al., 2020]{Hasselt2020ExpectedET}
{van Hasselt}, H., Madjiheurem, S., Hessel, M., Silver, D., Barreto, A.~N., and
  Borsa, D. (2020).
\newblock Expected eligibility traces.
\newblock {\em ArXiv}, abs/2007.01839.

\bibitem[van Seijen and Sutton, 2013]{small_backups}
van Seijen, H. and Sutton, R.~S. (2013).
\newblock Planning by prioritized sweeping with small backups.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013}, volume~28 of {\em
  {JMLR} Workshop and Conference Proceedings}, pages 361--369. JMLR.org.

\bibitem[Van~Seijen and Sutton, 2015]{Harm}
Van~Seijen, H. and Sutton, R.~S. (2015).
\newblock A deeper look at planning as learning from replay.
\newblock In {\em Proceedings of the 32Nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, pages
  2314--2322. JMLR.org.

\bibitem[Wan et~al., 2019]{planning_expectation}
Wan, Y., Zaheer, M., White, A., White, M., and Sutton, R.~S. (2019).
\newblock Planning with expectation models.
\newblock {\em CoRR}, abs/1904.01191.

\bibitem[Watkins and Dayan, 1992]{watkins}
Watkins, C. J. C.~H. and Dayan, P. (1992).
\newblock Technical note: \cal q -learning.
\newblock {\em Mach. Learn.}, 8(3–4):279–292.

\bibitem[Xu et~al., 2020]{Xu2020MetaGradientRL}
Xu, Z., Hasselt, H.~V., Hessel, M., Oh, J., Singh, S., and Silver, D. (2020).
\newblock Meta-gradient reinforcement learning with an objective discovered
  online.
\newblock {\em ArXiv}, abs/2007.08433.

\bibitem[Xu et~al., 2018]{meta_gradient}
Xu, Z., {van Hasselt}, H., and Silver, D. (2018).
\newblock Meta-gradient reinforcement learning.
\newblock {\em CoRR}, abs/1805.09801.

\bibitem[Zhang et~al., 2020a]{Zhang2020ProvablyCT}
Zhang, S., Liu, B., Yao, H., and Whiteson, S. (2020a).
\newblock Provably convergent two-timescale off-policy actor-critic with
  function approximation.
\newblock {\em arXiv: Learning}.

\bibitem[Zhang et~al., 2020b]{Zhang2020LearningRK}
Zhang, S., Veeriah, V., and Whiteson, S. (2020b).
\newblock Learning retrospective knowledge with reverse reinforcement learning.

\end{thebibliography}
