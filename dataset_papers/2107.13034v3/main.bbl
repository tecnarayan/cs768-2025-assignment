\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Bohdal et~al.(2020)Bohdal, Yang, and Hospedales]{bohdal2020flexible}
Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock \emph{arXiv preprint arXiv:2006.08572}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Chen, and Lee]{nguyen2021dataset}
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhao and Bilen(2021)]{zhao2021dataset}
Bo~Zhao and Hakan Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features, 2019.

\bibitem[Huh et~al.(2016)Huh, Agrawal, and Efros]{huh2016makes}
Minyoung Huh, Pulkit Agrawal, and Alexei~A Efros.
\newblock What makes imagenet good for transfer learning?
\newblock \emph{arXiv preprint arXiv:1608.08614}, 2016.

\bibitem[Hermann and Lampinen(2020)]{hermann2020shapes}
Katherine~L Hermann and Andrew~K Lampinen.
\newblock What shapes feature representations? exploring datasets,
  architectures, and training.
\newblock \emph{arXiv preprint arXiv:2006.12433}, 2020.

\bibitem[Borsos et~al.(2020)Borsos, Mutn{\`y}, and Krause]{borsos2020coresets}
Zal{\'a}n Borsos, Mojm{\'\i}r Mutn{\`y}, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock \emph{arXiv preprint arXiv:2006.03875}, 2020.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2020dataset}
Bo~Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2018bayesian}
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron,
  Daniel~A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Aitchison, and
  Rasmussen]{garriga2018deep}
Adri{\`a} Garriga-Alonso, Laurence Aitchison, and Carl~Edward Rasmussen.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019on}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8141--8150. Curran Associates, Inc., 2019.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov,
  and Sanjeev Arora.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Shankar et~al.(2020)Shankar, Fang, Guo, Fridovich-Keil, Schmidt,
  Ragan-Kelley, and Recht]{shankar2020neural}
Vaishaal Shankar, Alex~Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig
  Schmidt, Jonathan Ragan-Kelley, and Benjamin Recht.
\newblock Neural kernels without tangents.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Bietti(2021)]{bietti2021approximation}
Alberto Bietti.
\newblock Approximation and learning with deep convolutional models: a kernel
  perspective.
\newblock \emph{arXiv preprint arXiv:2102.10032}, 2021.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{Arora2020Harnessing}
Sanjeev Arora, Simon~S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkl8sJBYvH}.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Jaehoon Lee, Samuel~S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{svhn}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Krizhevsky(2009)]{cifar10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot2018ntk}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S. Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Huang and Yau(2020)]{huang2019dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Dyer and Gur-Ari(2020)]{Dyer2020Asymptotics}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1gFvANKDS}.

\bibitem[Andreassen and Dyer(2020)]{andreassen2020}
Anders Andreassen and Ethan Dyer.
\newblock Asymptotics of wide convolutional neural networks.
\newblock \emph{arxiv preprint arXiv:2008.08675}, 2020.

\bibitem[Yaida(2020)]{yaida2019non}
Sho Yaida.
\newblock Non-{G}aussian processes and neural networks at finite widths.
\newblock In \emph{Mathematical and Scientific Machine Learning Conference},
  2020.

\bibitem[Bennett(1969)]{1054365}
R.~Bennett.
\newblock The intrinsic dimensionality of signal collections.
\newblock \emph{IEEE Transactions on Information Theory}, 15\penalty0
  (5):\penalty0 517--525, 1969.
\newblock \doi{10.1109/TIT.1969.1054365}.

\bibitem[Camastra and Staiano(2016)]{CAMASTRA201626}
Francesco Camastra and Antonino Staiano.
\newblock Intrinsic dimension estimation: Advances and open problems.
\newblock \emph{Information Sciences}, 328:\penalty0 26--41, 2016.
\newblock ISSN 0020-0255.
\newblock \doi{https://doi.org/10.1016/j.ins.2015.08.029}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0020025515006179}.

\bibitem[Facco et~al.(2017)Facco, d’Errico, Rodriguez, and
  Laio]{facco2017estimating}
Elena Facco, Maria d’Errico, Alex Rodriguez, and Alessandro Laio.
\newblock Estimating the intrinsic dimension of datasets by a minimal
  neighborhood information.
\newblock \emph{Scientific reports}, 7\penalty0 (1):\penalty0 1--8, 2017.

\bibitem[Ansuini et~al.(2019)Ansuini, Laio, Macke, and
  Zoccolan]{Ansuini2019IntrinsicDO}
A.~Ansuini, A.~Laio, J.~Macke, and D.~Zoccolan.
\newblock Intrinsic dimension of data representations in deep neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Sucholutsky and Schonlau(2019)]{sucholutsky2019softlabel}
Ilia Sucholutsky and Matthias Schonlau.
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock \emph{arXiv preprint arXiv:1910.02551}, 2019.

\bibitem[Neal(1994)]{neal}
Radford~M. Neal.
\newblock Priors for infinite networks (tech. rep. no. crg-tr-94-1).
\newblock \emph{University of Toronto}, 1994.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-dickstein]{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington,
  and Jascha Sohl-dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Matthews et~al.(2018)Matthews, Hron, Rowland, Turner, and
  Ghahramani]{matthews2018}
Alexander{\ }G.{\ }de{\ }G. Matthews, Jiri Hron, Mark Rowland, Richard~E.
  Turner, and Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yang(2019{\natexlab{a}})]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019{\natexlab{a}}.

\bibitem[Yang(2019{\natexlab{b}})]{yang2019wide}
Greg Yang.
\newblock Wide feedforward or recurrent neural networks of any architecture are
  gaussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and Novak]{hron2020}
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and
  Schoenholz]{xiao2019disentangling}
Lechao Xiao, Jeffrey Pennington, and Samuel~S Schoenholz.
\newblock Disentangling trainability and generalization in deep learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Adlam and Pennington(2020)]{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{International Conference on Machine Learning}, pages 74--84.
  PMLR, 2020.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Lewkowycz and Gur-Ari(2020)]{lewkowycz2020l2reg}
Aitor Lewkowycz and Guy Gur-Ari.
\newblock On the training dynamics of deep networks with l\_2 regularization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 4790--4799, 2020.

\bibitem[Adlam et~al.(2021)Adlam, Lee, Xiao, Pennington, and
  Snoek]{adlam2021exploring}
Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek.
\newblock Exploring the uncertainty properties of neural
  networks{\textquoteright} implicit priors in the infinite-width limit.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Park et~al.(2020)Park, Lee, Peng, Cao, and
  Sohl-Dickstein]{park2020towards}
Daniel~S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein.
\newblock Towards nngp-guided neural architecture search.
\newblock \emph{arXiv preprint arXiv:2011.06006}, 2020.

\bibitem[Chen et~al.(2021)Chen, Gong, and Wang]{chen2021neural}
Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
\newblock Neural architecture search on imagenet in four {\{}gpu{\}} hours: A
  theoretically inspired perspective.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zandieh et~al.(2021)Zandieh, Han, Avron, Shoham, Kim, and
  Shin]{zandieh2021scaling}
Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin.
\newblock Scaling neural tangent kernels via sketching and random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Yang, and Zhou]{DBLP:journals/corr/abs-1712-00409}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory~F. Diamos, Heewoo Jun,
  Hassan Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{CoRR}, abs/1712.00409, 2017.
\newblock URL \url{http://arxiv.org/abs/1712.00409}.

\bibitem[Rosenfeld et~al.(2020)Rosenfeld, Rosenfeld, Belinkov, and
  Shavit]{rosenfeld2020a}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and Le]{cubuk2019cvpr}
Ekin~D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2019.

\bibitem[Sohl-Dickstein et~al.(2020)Sohl-Dickstein, Novak, Schoenholz, and
  Lee]{sohl2020infinite}
Jascha Sohl-Dickstein, Roman Novak, Samuel~S Schoenholz, and Jaehoon Lee.
\newblock On the infinite width limit of neural networks with a standard
  parameterization.
\newblock \emph{arXiv preprint arXiv:2001.07301}, 2020.

\end{thebibliography}
