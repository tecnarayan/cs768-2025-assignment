% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
 \newcommand*{\annalstat}{Annals of Statistics} \newcommand*{\jasa}{Journal of
  the American Statistical Association (JASA)} \newcommand*{\tacl}{Transactions
  of the Association for Computational Linguistics (TACL)}
  \newcommand*{\coling}{International Conference on Computational Linguistics
  (COLING)} \newcommand*{\acl}{Association for Computational Linguistics (ACL)}
  \newcommand*{\naacl}{North American Association for Computational Linguistics
  (NAACL)} \newcommand*{\aclijcnlp}{Association for Computational Linguistics
  and International Joint Conference on Natural Language Processing
  (ACL-IJCNLP)} \newcommand*{\emnlpconll}{Empirical Methods in Natural Language
  Processing and Computational Natural Language Learning (EMNLP/CoNLL)}
  \newcommand*{\emnlpijnlp}{Empirical Methods in Natural Language Processing
  and International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)} \newcommand*{\emnlp}{Empirical Methods in Natural Language
  Processing} \newcommand*{\hltnaacl}{Human Language Technology and North
  American Association for Computational Linguistics (HLT/NAACL)}
  \newcommand*{\eacl}{European Association for Computational Linguistics
  (EACL)}  \newcommand*{\icml}{International Conference on Machine Learning
  (ICML)} \newcommand*{\neurips}{Advances in Neural Information Processing
  Systems (NeurIPS)} \newcommand*{\nips}{Advances in Neural Information
  Processing Systems} \newcommand*{\iclr}{International Conference on Learning
  Representations (ICLR)} \newcommand*{\iclrworkshop}{International Conference
  on Learning Representations Workshop (ICLR)} \newcommand*{\jmlr}{Journal of
  Machine Learning Research (JMLR)} \newcommand*{\fatml}{Conference on
  Fairness, Accountability, and Transparency} \newcommand*{\aistats}{Artificial
  Intelligence and Statistics (AISTATS)} \newcommand*{\cvpr}{Conference on
  Computer Vision and Pattern Recognition (CVPR)}
  \newcommand*{\iccv}{International Conference on Computer Vision (ICCV)}
  \newcommand*{\icpr}{International Conference on Pattern Recognition (ICPR)}
  \newcommand*{\eccv}{European Conference on Computer Vision (ECCV)}
  \newcommand*{\uai}{Conference on Uncertainty in Artificial Intelligence
  (UAI)}  \newcommand*{\ecai}{European Conference on Artificial Intelligence}
  \newcommand*{\aaai}{AAAI Conference on Artificial Intelligence}
  \newcommand*{\packdd}{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining} \newcommand*{\kdd}{International Conference on Knowledge Discovery
  and Data Mining (KDD)} \newcommand*{\neurcom}{Neural Computation}
  \newcommand*{\msml}{Mathematical and Scientific Machine Learning Conference
  (MSML)} \newcommand*{\ijcnn}{International Joint Conference on Neural
  Networks (IJCNN)} \newcommand*{\ieeesigproc}{IEEE Transactions on Signal
  Processing} \newcommand*{\ieeeec}{IEEE Transactions on Electronic Computers}
  \newcommand*{\procieee}{Proceedings of the IEEE}
  \newcommand*{\pnas}{Proceedings of the National Academy of Sciences}
  \newcommand*{\chiconf}{Conference on Human Factors in Computing Systems
  (CHI)} \newcommand*{\ieeecp}{IEEE Symposium on Security and Privacy (SP)}
  \newcommand*{\stoc}{Symposium on Theory of Computing (STOC)}
  \newcommand*{\pods}{Symposium on Principles of Database Systems (PODS)}
  \newcommand*{\colt}{Conference on Learning Theory (COLT)}
  \newcommand*{\www}{The World Wide Web Conference (WWW)}
  \newcommand*{\soda}{Symposium on Discrete Algorithms (SODA)}
  \newcommand*{\focs}{Symposium on Foundations of Computer Science (FOCS)}
  \newcommand*{\acm}{Communications of the Association for Computing Machinery
  (ACM)} \newcommand*{\ieeeaccess}{IEEE Access}
  \newcommand*{\ijcv}{International Journal of Computer Vision (IJCV)}
  \newcommand*{\ieeetpami}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence} \newcommand*{\ieeetit}{IEEE Transactions on Information Theory}
  \newcommand*{\alt}{Conference on Algorithmic Learning Theory (ALT)}
  \newcommand*{\cocoon}{International Computing and Combinatorics Conference
  (COCOON)} \newcommand*{\arxiv}[1]{arXiv preprint arXiv:#1}
\begin{thebibliography}{135}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen_simple_2020}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{\icml}, 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron_unsupervised_2020}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin,
  ``Unsupervised learning of visual features by contrasting cluster
  assignments,'' in \emph{\neurips}, 2020.

\bibitem[Saunshi et~al.(2019)Saunshi, Plevrakis, Arora, Khodak, and
  Khandeparkar]{saunshi_theoretical_2019}
N.~Saunshi, O.~Plevrakis, S.~Arora, M.~Khodak, and H.~Khandeparkar, ``A
  theoretical analysis of contrastive unsupervised representation learning,''
  in \emph{\icml}, 2019.

\bibitem[Bansal et~al.(2021)Bansal, Kaplun, and Barak]{bansal_for_2021}
Y.~Bansal, G.~Kaplun, and B.~Barak, ``For self-supervised learning, rationality
  implies generalization, provably,'' in \emph{\iclr}, 2021.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee_predicting_2021}
J.~D. Lee, Q.~Lei, N.~Saunshi, and J.~Zhuo, ``Predicting what you already know
  helps: Provable self-supervised learning,'' in \emph{\neurips}, 2021.

\bibitem[Tosh et~al.(2021{\natexlab{a}})Tosh, Krishnamurthy, and
  Hsu]{tosh_contrastive_2021}
C.~Tosh, A.~Krishnamurthy, and D.~Hsu, ``Contrastive learning, multi-view
  redundancy, and linear models,'' in \emph{\alt}, 2021.

\bibitem[Wen and Li(2021)]{wen_toward_2021}
Z.~Wen and Y.~Li, ``Toward understanding the feature learning process of
  self-supervised contrastive learning,'' in \emph{\icml}, 2021.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and
  Ma]{haochen_provable_2021}
J.~Z. HaoChen, C.~Wei, A.~Gaidon, and T.~Ma, ``Provable guarantees for
  self-supervised deep learning with spectral contrastive loss,'' in
  \emph{\neurips}, 2021.

\bibitem[Leen(1994)]{leen_from_1994}
T.~Leen, ``From data distributions to regularization in invariant learning,''
  in \emph{\neurips}, 1994.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Dobriban, and
  Lee]{chen_group-theoretic_2020}
S.~Chen, E.~Dobriban, and J.~H. Lee, ``A group-theoretic framework for data
  augmentation,'' in \emph{\neurips}, 2020.

\bibitem[Lyle et~al.(2020)Lyle, van~der Wilk, Kwiatkowska, Gal, and
  Bloem-Reddy]{lyle_on_2020}
C.~Lyle, M.~van~der Wilk, M.~Kwiatkowska, Y.~Gal, and B.~Bloem-Reddy, ``On the
  benefits of invariance in neural networks,'' \emph{\arxiv{2005.00178}}, 2020.

\bibitem[Dubois et~al.(2021)Dubois, Bloem-Reddy, Ullrich, and
  Maddison]{dubois_lossy_2021}
Y.~Dubois, B.~Bloem-Reddy, K.~Ullrich, and C.~J. Maddison, ``Lossy compression
  for lossless prediction,'' \emph{\neurips}, 2021.

\bibitem[Vapnik and Chervonenkis(1971)]{vapnik_on_1971}
V.~N. Vapnik and A.~Y. Chervonenkis, ``On uniform convergence of the
  frequencies of events to their probabilities,'' \emph{Teoriya Veroyatnostei i
  ee Primeneniya}, vol.~16, no.~2, pp. 264--279, 1971.

\bibitem[Burges(1998)]{burges_tutorial_1998}
C.~J.~C. Burges, ``A tutorial on support vector machines for pattern
  recognition,'' \emph{Data mining and knowledge discovery}, vol.~2, no.~2, pp.
  121--167, 1998.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever]{radford_learning_2021}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever, ``Learning
  transferable visual models from natural language supervision,'' in
  \emph{\icml}, 2021.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan_prevalence_2020}
V.~Papyan, X.~Y. Han, and D.~L. Donoho, ``Prevalence of neural collapse during
  the terminal phase of deep learning training,'' \emph{\pnas}, vol. 117, pp.
  24\,652 -- 24\,663, 2020.

\bibitem[Ergen and Pilanci(2021)]{ergen_revealing_2021}
T.~Ergen and M.~Pilanci, ``Revealing the structure of deep neural networks via
  convex duality,'' in \emph{\icml}, 2021.

\bibitem[Lu and Steinerberger(2022)]{lu_neural_2022}
J.~Lu and S.~Steinerberger, ``Neural collapse under cross-entropy loss,''
  \emph{Applied and Computational Harmonic Analysis}, vol.~59, pp. 224--241,
  2022.

\bibitem[Graf et~al.(2021)Graf, Hofer, Niethammer, and
  Kwitt]{graf_dissecting_2021}
F.~Graf, C.~Hofer, M.~Niethammer, and R.~Kwitt, ``Dissecting supervised
  contrastive learning,'' in \emph{\icml}, 2021.

\bibitem[Zarka et~al.(2021)Zarka, Guth, and Mallat]{zarka_separation_2021}
J.~Zarka, F.~Guth, and S.~Mallat, ``Separation and concentration in deep
  networks,'' in \emph{\iclr}, 2021.

\bibitem[E and Wojtowytsch(2021)]{e_on_2021}
W.~E and S.~Wojtowytsch, ``On the emergence of simplex symmetry in the final
  and penultimate layers of neural network classifiers,'' in \emph{\msml},
  2021.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and
  Qu]{zhu_geometric_2021}
Z.~Zhu, T.~Ding, J.~Zhou, X.~Li, C.~You, J.~Sulam, and Q.~Qu, ``A geometric
  analysis of neural collapse with unconstrained features,'' in
  \emph{\neurips}, 2021.

\bibitem[Ji et~al.(2022)Ji, Lu, Zhang, Deng, and Su]{ji_unconstrained_2022}
W.~Ji, Y.~Lu, Y.~Zhang, Z.~Deng, and W.~J. Su, ``An unconstrained layer-peeled
  perspective on neural collapse,'' in \emph{\iclr}, 2022.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{'e}gou, Mairal, Bojanowski,
  and Joulin]{caron_emerging_2021}
M.~Caron, H.~Touvron, I.~Misra, H.~J{'e}gou, J.~Mairal, P.~Bojanowski, and
  A.~Joulin, ``Emerging properties in self-supervised vision transformers,'' in
  \emph{\iccv}, 2021.

\bibitem[Chen and He(2021)]{chen_exploring_2021}
X.~Chen and K.~He, ``Exploring simple siamese representation learning,'' in
  \emph{\cvpr}, 2021.

\bibitem[Mnih and Kavukcuoglu(2013)]{mnih_learning_2013}
A.~Mnih and K.~Kavukcuoglu, ``Learning word embeddings efficiently with
  noise-contrastive estimation,'' in \emph{\neurips}, 2013.

\bibitem[van~den Oord et~al.(2019)van~den Oord, Li, and
  Vinyals]{oord_representation_2019}
A.~van~den Oord, Y.~Li, and O.~Vinyals, ``Representation learning with
  contrastive predictive coding,'' \emph{\arxiv{2110.02796}}, 2019.

\bibitem[Misra and van~der Maaten(2020)]{misra_self-supervised_2020}
I.~Misra and L.~van~der Maaten, ``Self-supervised learning of pretext-invariant
  representations,'' in \emph{\cvpr}, 2020.

\bibitem[Gutmann and Hyv{\"{a}}rinen(2010)]{gutmann_noise-contrastive_2010}
M.~Gutmann and A.~Hyv{\"{a}}rinen, ``Noise-contrastive estimation: {A} new
  estimation principle for unnormalized statistical models,'' in
  \emph{\aistats}, 2010.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{jozefowicz_exploring_2016}
R.~Jozefowicz, O.~Vinyals, M.~Schuster, N.~Shazeer, and Y.~Wu, ``Exploring the
  limits of language modeling,'' \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Ma and Collins(2018)]{ma_noise_2018}
Z.~Ma and M.~Collins, ``Noise contrastive estimation and negative sampling for
  conditional models: Consistency and statistical efficiency,'' in
  \emph{\emnlp}, 2018.

\bibitem[Fang et~al.(2021{\natexlab{a}})Fang, Wang, Wang, Zhang, Yang, and
  Liu]{fang_seed_2021}
Z.~Fang, J.~Wang, L.~Wang, L.~Zhang, Y.~Yang, and Z.~Liu, ``{SEED}:
  {S}elf-supervised distillation for visual representation,'' in \emph{\iclr},
  2021.

\bibitem[Jaynes(1957)]{jaynes_information_1957}
E.~T. Jaynes, ``Information theory and statistical mechanics,'' \emph{Physical
  Review}, vol. 106, no.~4, pp. 620--630, 1957.

\bibitem[Ermolov et~al.(2021)Ermolov, Siarohin, Sangineto, and
  Sebe]{ermolov_whitening_2021}
A.~Ermolov, A.~Siarohin, E.~Sangineto, and N.~Sebe, ``Whitening for
  self-supervised representation learning,'' in \emph{\icml}, 2021.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he_momentum_2020}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{\cvpr}, 2020.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'{e}}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{grill_bootstrap_2020}
J.~B. Grill, F.~Strub, F.~Altch{\'{e}}, C.~Tallec, P.~H. Richemond,
  E.~Buchatskaya, C.~Doersch, B.~A. Pires, Z.~Guo, M.~G. Azar, B.~Piot,
  K.~Kavukcuoglu, R.~Munos, and M.~Valko, ``Bootstrap {Your} {Own} {Latent} - a
  new approach to self-supervised learning,'' in \emph{\neurips}, 2020.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and
  Vincent]{bengio_representation_2013}
Y.~Bengio, A.~C. Courville, and P.~Vincent, ``Representation learning: {A}
  review and new perspectives,'' \emph{\ieeetpami}, vol.~35, no.~8, pp.
  1798--1828, 2013.

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{zhang_colorful_2016}
R.~Zhang, P.~Isola, and A.~A. Efros, ``Colorful image colorization,'' in
  \emph{\eccv}, 2016.

\bibitem[Cover(1965)]{cover_geometrical_1965}
T.~M. Cover, ``Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition,'' \emph{\ieeeec}, vol.
  EC-14, no.~3, pp. 326--334, Jun. 1965.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar_barlow_2021}
J.~Zbontar, L.~Jing, I.~Misra, Y.~LeCun, and S.~Deny, ``Barlow {Twins}:
  Self-supervised learning via redundancy reduction,'' in \emph{\icml}, 2021.

\bibitem[Hua et~al.(2021)Hua, Wang, Xue, Ren, Wang, and Zhao]{hua_on_2021}
T.~Hua, W.~Wang, Z.~Xue, S.~Ren, Y.~Wang, and H.~Zhao, ``On feature
  decorrelation in self-supervised learning,'' in \emph{\iccv}, 2021.

\bibitem[Jing et~al.(2022)Jing, Vincent, LeCun, and
  Tian]{jing_understanding_2022}
L.~Jing, P.~Vincent, Y.~LeCun, and Y.~Tian, ``Understanding dimensional
  collapse in contrastive self-supervised learning,'' in \emph{\iclr}, 2022.

\bibitem[Chen et~al.(2021)Chen, Luo, and Li]{chen_intriguing_2021}
T.~Chen, C.~Luo, and L.~Li, ``Intriguing properties of contrastive losses,'' in
  \emph{\neurips}, 2021.

\bibitem[Tsai et~al.(2021)Tsai, Wu, Salakhutdinov, and
  Morency]{tsai_self-supervised_2021}
Y.~H. Tsai, Y.~Wu, R.~R. Salakhutdinov, and L.~Morency, ``Self-supervised
  learning from a multi-view perspective,'' in \emph{\iclr}, 2021.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian_what_2020}
Y.~Tian, C.~Sun, B.~Poole, D.~Krishnan, C.~Schmid, and P.~Isola, ``What makes
  for good views for contrastive learning?'' in \emph{\neurips}, 2020.

\bibitem[Federici et~al.(2020)Federici, Dutta, Forr{\'{e}}, Kushman, and
  Akata]{federici_learning_2020*a}
M.~Federici, A.~Dutta, P.~Forr{\'{e}}, N.~Kushman, and Z.~Akata, ``Learning
  robust representations via multi-view information bottleneck,'' in
  \emph{\iclr}, 2020.

\bibitem[Mitrovic et~al.(2021)Mitrovic, McWilliams, Walker, Buesing, and
  Blundell]{mitrovic_representation_2021}
J.~Mitrovic, B.~McWilliams, J.~Walker, L.~Buesing, and C.~Blundell,
  ``Representation learning via invariant causal mechanisms,'' in \emph{\iclr},
  2021.

\bibitem[Wu et~al.(2021)Wu, Zhuang, Mosse, Yamins, and Goodman]{wu_on_2021}
M.~Wu, C.~Zhuang, M.~Mosse, D.~L.~K. Yamins, and N.~D. Goodman, ``On mutual
  information in contrastive learning for visual representations,''
  \emph{\arxiv{2005.13149}}, 2021.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and
  Douze]{caron_deep_2018}
M.~Caron, P.~Bojanowski, A.~Joulin, and M.~Douze, ``Deep clustering for
  unsupervised learning of visual features,'' in \emph{\eccv}, 2018.

\bibitem[Asano et~al.(2020)Asano, Rupprecht, and
  Vedaldi]{asano_self-labelling_2020}
Y.~M. Asano, C.~Rupprecht, and A.~Vedaldi, ``Self-labelling via simultaneous
  clustering and representation learning,'' in \emph{\iclr}, 2020.

\bibitem[Yang et~al.(2016)Yang, Parikh, and Batra]{yang_joint_2016}
J.~Yang, D.~Parikh, and D.~Batra, ``Joint unsupervised learning of deep
  representations and image clusters,'' in \emph{\cvpr}, 2016.

\bibitem[Sinkhorn and Knopp(1967)]{sinkhorn_concerning_1967}
R.~Sinkhorn and P.~Knopp, ``Concerning nonnegative matrices and doubly
  stochastic matrices,'' \emph{Pacific Journal of Mathematics}, vol.~21, no.~2,
  pp. 343--348, 1967.

\bibitem[Tosh et~al.(2021{\natexlab{b}})Tosh, Krishnamurthy, and
  Hsu]{tosh_contrastive_2021*a}
C.~Tosh, A.~Krishnamurthy, and D.~Hsu, ``Contrastive estimation reveals topic
  posterior information to linear models,'' \emph{\jmlr}, vol.~22, pp.
  281:1--281:31, 2021.

\bibitem[Nozawa and Sato(2021)]{nozawa_understanding_2021}
K.~Nozawa and I.~Sato, ``Understanding negative samples in instance
  discriminative self-supervised representation learning,'' in \emph{\neurips},
  2021.

\bibitem[Tian et~al.(2021{\natexlab{a}})Tian, Chen, and
  Ganguli]{tian_understanding_2021}
Y.~Tian, X.~Chen, and S.~Ganguli, ``Understanding self-supervised learning
  dynamics without contrastive pairs,'' in \emph{\icml}, 2021.

\bibitem[cs231n(2015)]{csn_tinyimagenet_2015}
cs231n, ``Tinyimagenet,'' 2015.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi_understanding_2022}
N.~Saunshi, J.~T. Ash, S.~Goel, D.~Misra, C.~Zhang, S.~Arora, S.~M. Kakade, and
  A.~Krishnamurthy, ``Understanding contrastive learning requires incorporating
  inductive biases,'' in \emph{\icml}, 2022.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu_unsupervised_2018}
Z.~Wu, Y.~Xiong, S.~X. Yu, and D.~Lin, ``Unsupervised feature learning via
  non-parametric instance discrimination,'' in \emph{\cvpr}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng_imagenet_2009}
J.~Deng, W.~Dong, R.~Socher, L.~Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{\cvpr}, 2009.

\bibitem[Goyal et~al.(2021{\natexlab{a}})Goyal, Duval, Reizenstein, Leavitt,
  Xu, Lefaudeux, Singh, Reis, Caron, Bojanowski, Joulin, and
  Misra]{goyal_vissl_2021}
P.~Goyal, Q.~Duval, J.~Reizenstein, M.~Leavitt, M.~Xu, B.~Lefaudeux, M.~Singh,
  V.~Reis, M.~Caron, P.~Bojanowski, A.~Joulin, and I.~Misra, ``{VISSL},'' 2021.

\bibitem[Kornblith et~al.(2021)Kornblith, Chen, Lee, and
  Norouzi]{kornblith_why_2021}
S.~Kornblith, T.~Chen, H.~Lee, and M.~Norouzi, ``Why do better loss functions
  lead to less transferable features?'' in \emph{\neurips}, 2021.

\bibitem[Dubois et~al.(2020)Dubois, Kiela, Schwab, and
  Vedantam]{dubois_learning_2020}
Y.~Dubois, D.~Kiela, D.~J. Schwab, and R.~Vedantam, ``Learning optimal
  representations with the decodable information bottleneck,'' in
  \emph{\neurips}, 2020.

\bibitem[Ruan et~al.(2022)Ruan, Dubois, and Maddison]{ruan_optimal_2022}
Y.~Ruan, Y.~Dubois, and C.~J. Maddison, ``Optimal representations for covariate
  shift,'' in \emph{\iclr}, 2022.

\bibitem[Goyal et~al.(2021{\natexlab{b}})Goyal, Caron, Lefaudeux, Xu, Wang,
  Pai, Singh, Liptchinsky, Misra, Joulin, and
  Bojanowski]{goyal_self-supervised_2021}
P.~Goyal, M.~Caron, B.~Lefaudeux, M.~Xu, P.~Wang, V.~Pai, M.~Singh,
  V.~Liptchinsky, I.~Misra, A.~Joulin, and P.~Bojanowski, ``Self-supervised
  pretraining of visual features in the wild,'' \emph{\arxiv{2103.01988}},
  2021.

\bibitem[Bommasani et~al.(2021)]{bommasani_on_2021}
R.~Bommasani \emph{et~al.}, ``On the opportunities and risks of foundation
  models,'' \emph{\arxiv{2108.07258}}, 2021.

\bibitem[Eaton(1989)]{eaton_group_1989}
M.~L. Eaton, ``Group invariance applications in statistics,'' \emph{Regional
  Conference Series in Probability and Statistics}, vol.~1, pp. i--133, 1989.

\bibitem[Lehmann and Romano(2008)]{lehmann_testing_2008}
E.~L. Lehmann and J.~P. Romano, \emph{Testing Statistical Hypotheses, Third
  Edition}, 3rd~ed., ser. Springer texts in statistics.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2008.

\bibitem[Zalinescu(2002)]{zalinescu_convex_2002}
C.~Zalinescu, \emph{Convex Analysis in General Vector Spaces}.\hskip 1em plus
  0.5em minus 0.4em\relax World Scientific, 2002.

\bibitem[Boyd and Vandenberghe(2004)]{boyd_convex_2004}
S.~Boyd and L.~Vandenberghe, \emph{Convex {Optimization}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Cambridge University Press, 2004.

\bibitem[Narici and Beckenstein(2010)]{narici_topological_2010}
L.~Narici and E.~Beckenstein, \emph{Topological Vector Spaces}, 2nd~ed.\hskip
  1em plus 0.5em minus 0.4em\relax Chapman and Hall/CRC, 2010.

\bibitem[Br{\o}ndsted(1983)]{brndsted_introduction_1983}
A.~Br{\o}ndsted, \emph{An introduction to convex polytopes}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer Science \& Business Media, 1983, no.~90.

\bibitem[Ziegler(2012)]{ziegler_lectures_2012}
G.~M. Ziegler, \emph{Lectures on polytopes}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer Science \& Business Media, 2012, vol. 152.

\bibitem[Gr{\"u}nbaum et~al.(1967)Gr{\"u}nbaum, Klee, Perles, and
  Shephard]{grunbaum_convex_1967}
B.~Gr{\"u}nbaum, V.~Klee, M.~A. Perles, and G.~C. Shephard, \emph{Convex
  polytopes}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1967, vol.~16.

\bibitem[Straszewicz(1935)]{straszewicz_over_1935}
S.~Straszewicz, ``Over exposed points of closed point sets,'' \emph{Fundamenta
  Mathematicae}, vol.~24, pp. 139--143, 1935.

\bibitem[Rockafellar(1970)]{rockafellar_convex_1970}
R.~T. Rockafellar, \emph{Convex analysis}.\hskip 1em plus 0.5em minus
  0.4em\relax Princeton university press, 1970.

\bibitem[Shalev-Shwartz and Zhang(2014)]{shalevshwartz_accelerated_2014}
S.~Shalev-Shwartz and T.~Zhang, ``Accelerated proximal stochastic dual
  coordinate ascent for regularized loss minimization,'' \emph{Mathematical
  Programming}, pp. 1--41, 2014.

\bibitem[Dutta and Goswami(2010)]{dutta_mode_2010}
S.~Dutta and A.~Goswami, ``Mode estimation for discrete distributions,''
  \emph{Mathematical Methods of Statistics}, vol.~19, no.~4, pp. 374--384,
  2010.

\bibitem[Flajolet et~al.(1992)Flajolet, Gardy, and
  Thimonier]{flajolet_birthday_1992}
P.~Flajolet, D.~Gardy, and L.~Thimonier, ``Birthday paradox, coupon collectors,
  caching algorithms and self-organizing search,'' \emph{Discrete Applied
  Mathematics}, vol.~39, no.~3, pp. 207--229, 1992.

\bibitem[Berenbrink and Sauerwald(2009)]{berenbrink_weighted_2009}
P.~Berenbrink and T.~Sauerwald, ``The weighted coupon collector's problem and
  applications,'' in \emph{\cocoon}, 2009.

\bibitem[Feller(2008)]{feller_introduction_2008}
W.~Feller, \emph{An introduction to probability theory and its
  applications}.\hskip 1em plus 0.5em minus 0.4em\relax John Wiley \& Sons,
  2008.

\bibitem[O'Neill(2021)]{oneill_classical_2021}
B.~O'Neill, ``The classical occupancy distribution: Computation and
  approximation,'' \emph{The American Statistician}, vol.~75, no.~4, pp.
  364--375, 2021.

\bibitem[Fang et~al.(2021{\natexlab{b}})Fang, He, Long, and
  Su]{fang_exploring_2021}
C.~Fang, H.~He, Q.~Long, and W.~J. Su, ``Exploring deep neural networks via
  layer-peeled model: Minority collapse in imbalanced training,'' \emph{\pnas},
  vol. 118, 2021.

\bibitem[MacKay(2003)]{mackay_information_2003}
D.~J.~C. MacKay, \emph{Information theory, inference, and learning
  algorithms}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University
  Press, 2003.

\bibitem[Kowalczyk(1997{\natexlab{a}})]{kowalczyk_dense_1997}
A.~Kowalczyk, ``Dense shattering and teaching dimensions for differentiable
  families (extended abstract),'' in \emph{\colt}, 1997.

\bibitem[Sontag(1990)]{sontag_remarks_1990}
E.~D. Sontag, ``Remarks on interpolation and recognition using neural nets,''
  in \emph{\neurips}, 1990.

\bibitem[Sontag(1997)]{sontag_shattering_1997}
------, ``Shattering all sets of k points in 'general position' requires (k -
  1)/2 parameters,'' \emph{\neurcom}, vol.~9, no.~2, pp. 337--348, 1997.

\bibitem[Baum(1988)]{baum_on_1988}
E.~B. Baum, ``On the capabilities of multilayer perceptrons,'' \emph{Journal of
  Complexity}, vol.~4, no.~3, pp. 193--215, 1988.

\bibitem[Sakurai(1992)]{sakurai_n-h-1_1992}
A.~Sakurai, ``n-h-1 networks store no less n*h+1 examples, but sometimes no
  more,'' in \emph{\ijcnn}, vol.~3, 1992.

\bibitem[Mitchison and Durbin(1989)]{mitchison_bounds_1989}
G.~J. Mitchison and R.~M. Durbin, ``Bounds on the learning capacity of some
  multi-layer networks,'' \emph{Biological Cybernetics}, vol.~60, no.~5, pp.
  345--365, 1989.

\bibitem[Cover(1968)]{cover_capacity_1968}
T.~M. Cover, ``Capacity problems for linear machines,'' \emph{Pattern
  recognition}, pp. 283--289, 1968.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett_nearly-tight_2019}
P.~L. Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian, ``Nearly-tight
  vc-dimension and pseudodimension bounds for piecewise linear neural
  networks,'' \emph{{JMLR}}, vol.~20, pp. 63:1--63:17, 2019.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalevshwartz_understanding_2014}
S.~Shalev-Shwartz and S.~Ben-David, \emph{Understanding Machine Learning: From
  Theory to Algorithms}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge
  University Press, 2014.

\bibitem[Kowalczyk(1997{\natexlab{b}})]{kowalczyk_estimates_1997}
A.~Kowalczyk, ``Estimates of storage capacity of multilayer perceptron with
  threshold logic hidden units,'' \emph{Neural Networks}, vol.~10, no.~8, pp.
  1417--1433, 1997.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun_small_2019}
C.~Yun, S.~Sra, and A.~Jadbabaie, ``Small relu networks are powerful
  memorizers: a tight analysis of memorization capacity,'' in \emph{\neurips},
  2019.

\bibitem[Baldi and Vershynin(2019)]{baldi_capacity_2019}
P.~Baldi and R.~Vershynin, ``The capacity of feedforward neural networks,''
  \emph{Neural Networks}, vol. 116, pp. 288--311, 2019.

\bibitem[Xu and Raginsky(2022)]{xu_minimum_2022}
A.~Xu and M.~Raginsky, ``Minimum excess risk in bayesian learning,''
  \emph{\ieeetit}, 2022.

\bibitem[Kallenberg(1997)]{kallenberg_foundations_1997}
O.~Kallenberg, \emph{Foundations of modern probability}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 1997.

\bibitem[Bao et~al.(2021)Bao, Nagano, and Nozawa]{bao_sharp_2021}
H.~Bao, Y.~Nagano, and K.~Nozawa, ``Sharp learning bounds for contrastive
  unsupervised representation learning,'' \emph{\arxiv{2110.02501}}, 2021.

\bibitem[Nozawa et~al.(2020)Nozawa, Germain, and
  Guedj]{nozawa_pac-bayesian_2020}
K.~Nozawa, P.~Germain, and B.~Guedj, ``Pac-bayesian contrastive unsupervised
  representation learning,'' in \emph{\uai}, 2020.

\bibitem[Ash et~al.(2022)Ash, Goel, Krishnamurthy, and
  Misra]{ash_investigating_2022}
J.~T. Ash, S.~Goel, A.~Krishnamurthy, and D.~Misra, ``Investigating the role of
  negatives in contrastive representation learning,'' in \emph{\aistats}, 2022.

\bibitem[Awasthi et~al.(2022)Awasthi, Dikkala, and Kamath]{awasthi_do_2022}
P.~Awasthi, N.~Dikkala, and P.~Kamath, ``Do more negative samples necessarily
  hurt in contrastive learning?'' in \emph{\icml}, 2022.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang_chaos_2022}
Y.~Wang, Y.~Zhang, Y.~Wang, J.~Yang, and Z.~Lin, ``Chaos is a ladder: A new
  theoretical understanding of contrastive learning via augmentation overlap,''
  in \emph{\iclr}, 2022.

\bibitem[Tian et~al.(2021{\natexlab{b}})Tian, Yu, Chen, and
  Ganguli]{tian_understanding_2021*a}
Y.~Tian, L.~Yu, X.~Chen, and S.~Ganguli, ``Understanding self-supervised
  learning with dual deep networks,'' \emph{\arxiv{2010.00578}}, 2021.

\bibitem[Mixon et~al.(2022)Mixon, Parshall, and Pi]{mixon_neural_2022}
D.~G. Mixon, H.~Parshall, and J.~Pi, ``Neural collapse with unconstrained
  features,'' \emph{Sampling Theory, Signal Processing, and Data Analysis},
  vol.~20, no.~2, pp. 1--13, 2022.

\bibitem[Wang and Isola(2020)]{wang_understanding_2020}
T.~Wang and P.~Isola, ``Understanding contrastive representation learning
  through alignment and uniformity on the hypersphere,'' in \emph{\icml}, 2020.

\bibitem[Wang and Liu(2021)]{wang_understanding_2021}
F.~Wang and H.~Liu, ``Understanding the behaviour of contrastive loss,'' in
  \emph{\cvpr}, 2021.

\bibitem[Richemond et~al.(2020)Richemond, Grill, Altch{\'{e}}, Tallec, Strub,
  Brock, Smith, De, Pascanu, Piot, and Valko]{richemond_byol_2020}
P.~H. Richemond, J.-B. Grill, F.~Altch{\'{e}}, C.~Tallec, F.~Strub, A.~Brock,
  S.~L. Smith, S.~De, R.~Pascanu, B.~Piot, and M.~Valko, ``{BYOL} works even
  without batch statistics,'' \emph{\arxiv{2010.10241}}, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Zhang, Pham, Yoo, and
  Kweon]{zhang_how_2022}
C.~Zhang, K.~Zhang, C.~Zhang, T.~X. Pham, C.~D. Yoo, and I.~S. Kweon, ``How
  does simsiam avoid collapse without negative samples? a unified understanding
  with self-supervised contrastive learning,'' in \emph{{ICLR}}, 2022.

\bibitem[Ericsson et~al.(2021)Ericsson, Gouk, and
  Hospedales]{ericsson_why_2021}
L.~Ericsson, H.~Gouk, and T.~M. Hospedales, ``Why do self-supervised models
  transfer? investigating the impact of invariance on downstream tasks,''
  \emph{\arxiv{abs/2111.11398}}, 2021.

\bibitem[Foster et~al.(2021)Foster, Pukdee, and
  Rainforth]{foster_improving_2021}
A.~Foster, R.~Pukdee, and T.~Rainforth, ``Improving transformation invariance
  in contrastive representation learning,'' in \emph{\iclr}, 2021.

\bibitem[von K{\"{u}}gelgen et~al.(2021)von K{\"{u}}gelgen, Sharma, Gresele,
  Brendel, Sch{\"{o}}lkopf, Besserve, and
  Locatello]{kugelgen_self-supervised_2021}
J.~von K{\"{u}}gelgen, Y.~Sharma, L.~Gresele, W.~Brendel, B.~Sch{\"{o}}lkopf,
  M.~Besserve, and F.~Locatello, ``Self-supervised learning with data
  augmentations provably isolates content from style,'' in \emph{NeurIPS},
  2021.

\bibitem[Galanti et~al.(2022)Galanti, Gy{\"o}rgy, and Hutter]{galanti_on_2022}
T.~Galanti, A.~Gy{\"o}rgy, and M.~Hutter, ``On the role of neural collapse in
  transfer learning,'' in \emph{\iclr}, 2022.

\bibitem[Hui et~al.(2022)Hui, Belkin, and Nakkiran]{hui_limitations_2022}
L.~Hui, M.~Belkin, and P.~Nakkiran, ``Limitations of neural collapse for
  understanding generalization in deep learning,'' \emph{\arxiv{2202.08384}},
  2022.

\bibitem[Bardes et~al.(2022)Bardes, Ponce, and LeCun]{bardes_vicreg_2022}
A.~Bardes, J.~Ponce, and Y.~LeCun, ``{VICR}eg: Variance-invariance-covariance
  regularization for self-supervised learning,'' in \emph{\iclr}, 2022.

\bibitem[Yan et~al.(2020)Yan, Misra, Gupta, Ghadiyaram, and
  Mahajan]{yan_clusterfit_2020}
X.~Yan, I.~Misra, A.~Gupta, D.~Ghadiyaram, and D.~Mahajan, ``{ClusterFit}:
  Improving generalization of visual representations,'' in \emph{\cvpr}, 2020.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie{-}Marchildon, Grewal,
  Bachman, Trischler, and Bengio]{hjelm_learning_2019}
R.~D. Hjelm, A.~Fedorov, S.~Lavoie{-}Marchildon, K.~Grewal, P.~Bachman,
  A.~Trischler, and Y.~Bengio, ``Learning deep representations by mutual
  information estimation and maximization,'' in \emph{\iclr}, 2019.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman_learning_2019}
P.~Bachman, R.~D. Hjelm, and W.~Buchwalter, ``Learning representations by
  maximizing mutual information across views,'' in \emph{\neurips}, 2019.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Krishnan, and
  Isola]{tian_contrastive_2020}
Y.~Tian, D.~Krishnan, and P.~Isola, ``Contrastive multiview coding,'' in
  \emph{\eccv}, 2020.

\bibitem[Tschannen et~al.(2020)Tschannen, Djolonga, Rubenstein, Gelly, and
  Lucic]{tschannen_on_2020}
M.~Tschannen, J.~Djolonga, P.~K. Rubenstein, S.~Gelly, and M.~Lucic, ``On
  mutual information maximization for representation learning,'' in
  \emph{\iclr}, 2020.

\bibitem[Xu et~al.(2020)Xu, Zhao, Song, Stewart, and Ermon]{xu_theory_2020}
Y.~Xu, S.~Zhao, J.~Song, R.~Stewart, and S.~Ermon, ``A theory of usable
  information under computational constraints,'' in \emph{\iclr}, 2020.

\bibitem[Pokle et~al.(2022)Pokle, Tian, Li, and
  Risteski]{pokle_contrasting_2022}
A.~Pokle, J.~Tian, Y.~Li, and A.~Risteski, ``Contrasting the landscape of
  contrastive and non-contrastive learning,'' in \emph{\aistats}, 2022.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and
  Gool]{bossard_food-101_2014}
L.~Bossard, M.~Guillaumin, and L.~V. Gool, ``Food-101 -- mining discriminative
  components with random forests,'' in \emph{\eccv}, 2014.

\bibitem[Krizhevsky(2009)]{krizhevsky_learning_2009}
A.~Krizhevsky, ``Learning multiple layers of features from tiny images,''
  University of Toronto, Tech. Rep., 2009.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{krause_3d_2013}
J.~Krause, M.~Stark, J.~Deng, and L.~Fei-Fei, ``3d object representations for
  fine-grained categorization,'' in \emph{ICCV Wokshop on 3D Representation and
  Recognition}, 2013.

\bibitem[Maji et~al.(2013)Maji, Kannala, Rahtu, Blaschko, and
  Vedaldi]{maji_fine-grained_2013}
S.~Maji, J.~Kannala, E.~Rahtu, M.~Blaschko, and A.~Vedaldi, ``Fine-grained
  visual classification of aircraft,'' \emph{\arxiv{1306.5151}}, 2013.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, , and
  Vedaldi]{cimpoi_describing_2014}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, , and A.~Vedaldi, ``Describing
  textures in the wild,'' in \emph{\cvpr}, 2014.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi_cats_2012}
O.~M. Parkhi, A.~Vedaldi, A.~Zisserman, and C.~V. Jawahar, ``Cats and dogs,''
  in \emph{\cvpr}, 2012.

\bibitem[Li et~al.(2022)Li, Andreeto, Ranzato, and Perona]{li_caltech_2022}
F.-F. Li, M.~Andreeto, M.~A. Ranzato, and P.~Perona,
  ``\BIBforeignlanguage{en}{Caltech 101},'' 2022.

\bibitem[Nilsback and Zisserman(2008)]{nilsback_automated_2008}
M.-E. Nilsback and A.~Zisserman, ``Automated flower classification over a large
  number of classes,'' in \emph{{Indian} {Conference} on {Computer} {Vision},
  {Graphics} \& {Image} {Processing} ({ICVGIP})}, 2008.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Computer Vision and Pattern Recognition (CVPR)},
  2016, pp. 770--778.

\bibitem[Ioffe and Szegedy(2015)]{ioffe_batch_2015}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{International
  Conference on Machine Learning (ICML)}, 2015, pp. 448--456.

\bibitem[Kingma and Ba(2015)]{kingma_adam_2015}
D.~P. Kingma and J.~Ba, ``{Adam}: A method for stochastic optimization,'' in
  \emph{\iclr}, 2015.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{pedregosa_scikit-learn_2011}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay, ``Scikit-learn:
  Machine learning in {P}ython,'' \emph{Journal of Machine Learning Research
  (JMLR)}, vol.~12, 2011.

\bibitem[Cortes and Vapnik(1995)]{cortes_support-vector_1995}
C.~Cortes and V.~Vapnik, ``Support-vector networks,'' \emph{Machine Learning},
  vol.~20, pp. 273--297, 1995.

\bibitem[McInnes et~al.(2018)McInnes, Healy, Saul, and
  Gro{\ss}berger]{mcinnes_umap-_2018}
L.~McInnes, J.~Healy, N.~Saul, and L.~Gro{\ss}berger, ``{UMAP:} uniform
  manifold approximation and projection,'' \emph{Journal of Open Source
  Software}, vol.~3, no.~29, p. 861, 2018.

\end{thebibliography}
