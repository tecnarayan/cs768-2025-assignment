\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam \& Sastry(2017)Achiam and Sastry]{achiam2017surprise}
Achiam, J. and Sastry, S.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.01732}, 2017.

\bibitem[Agre \& Rosenschein(1996)Agre and Rosenschein]{agre1996computational}
Agre, P. and Rosenschein, S.~J.
\newblock \emph{Computational theories of interaction and agency}.
\newblock Mit Press, 1996.

\bibitem[Baldassarre \& Mirolli(2013)Baldassarre and
  Mirolli]{baldassarre2013intrinsically}
Baldassarre, G. and Mirolli, M.
\newblock \emph{Intrinsically motivated learning in natural and artificial
  systems}.
\newblock Springer, 2013.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Chentanez et~al.(2005)Chentanez, Barto, and
  Singh]{chentanez2005intrinsically}
Chentanez, N., Barto, A.~G., and Singh, S.~P.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1281--1288, 2005.

\bibitem[Chinchuluun et~al.(2008)Chinchuluun, Pardalos, Migdalas, and
  Pitsoulis]{chinchuluun2008pareto}
Chinchuluun, A., Pardalos, P.~M., Migdalas, A., and Pitsoulis, L.
\newblock \emph{Pareto optimality, game theory and equilibria}.
\newblock Springer, 2008.

\bibitem[Cover \& Thomas(2006)Cover and Thomas]{Cover:book}
Cover, T.~M. and Thomas, J.~A.
\newblock \emph{Elements of Information Theory}.
\newblock Wiley, 2006.

\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{degris2012off}
Degris, T., White, M., and Sutton, R.~S.
\newblock Off-policy actor-critic.
\newblock \emph{arXiv preprint arXiv:1205.4839}, 2012.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fox et~al.(2015)Fox, Pakman, and Tishby]{fox2015taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock \emph{arXiv preprint arXiv:1512.08562}, 2015.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Ghahramani, Turner, and Levine]{gu2016q}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., and Levine, S.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock \emph{arXiv preprint arXiv:1611.02247}, 2016.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Turner, Ghahramani, Sch{\"o}lkopf, and
  Levine]{gu2017interpolated}
Gu, S.~S., Lillicrap, T., Turner, R.~E., Ghahramani, Z., Sch{\"o}lkopf, B., and
  Levine, S.
\newblock Interpolated policy gradient: Merging on-policy and off-policy
  gradient estimation for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3846--3855, 2017.

\bibitem[Guo et~al.(2018)Guo, Oh, Singh, and Lee]{guo2018generative}
Guo, Y., Oh, J., Singh, S., and Lee, H.
\newblock Generative adversarial self-imitation learning.
\newblock \emph{arXiv preprint arXiv:1812.00950}, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017rein}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock \emph{arXiv preprint arXiv:1702.08165}, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft2}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Han \& Sung(2019)Han and Sung]{han2019dimension}
Han, S. and Sung, Y.
\newblock Dimension-wise importance sampling weight clipping for
  sample-efficient reinforcement learning.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Hong et~al.(2018)Hong, Shann, Su, Chang, Fu, and
  Lee]{hong2018diversity}
Hong, Z.-W., Shann, T.-Y., Su, S.-Y., Chang, Y.-H., Fu, T.-J., and Lee, C.-Y.
\newblock Diversity-driven exploration strategy for deep reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10489--10500, 2018.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel, P.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1109--1117, 2016.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lopes et~al.(2012)Lopes, Lang, Toussaint, and
  Oudeyer]{lopes2012exploration}
Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y.
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  206--214, 2012.

\bibitem[Martin et~al.(2017)Martin, Sasikumar, Everitt, and
  Hutter]{martin2017count}
Martin, J., Sasikumar, S.~N., Everitt, T., and Hutter, M.
\newblock Count-based exploration in feature space for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1706.08090}, 2017.

\bibitem[Mazoure et~al.(2019)Mazoure, Doan, Durand, Hjelm, and
  Pineau]{mazoure2019leveraging}
Mazoure, B., Doan, T., Durand, A., Hjelm, R.~D., and Pineau, J.
\newblock Leveraging exploration in off-policy algorithms via normalizing
  flows.
\newblock \emph{arXiv preprint arXiv:1905.06893}, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nachum et~al.(2017{\natexlab{a}})Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2775--2785, 2017{\natexlab{a}}.

\bibitem[Nachum et~al.(2017{\natexlab{b}})Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017trust}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Trust-pcl: An off-policy trust region method for continuous control.
\newblock \emph{arXiv preprint arXiv:1707.01891}, 2017{\natexlab{b}}.

\bibitem[Nielsen(2019)]{nielsen2019jensen}
Nielsen, F.
\newblock On the jensen--shannon symmetrization of distances relying on
  abstract means.
\newblock \emph{Entropy}, 21\penalty0 (5):\penalty0 485, 2019.

\bibitem[O'Donoghue et~al.(2016)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{o2016combining}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock Combining policy gradient and q-learning.
\newblock \emph{arXiv preprint arXiv:1611.01626}, 2016.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  16--17, 2017.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Puterman \& Brumelle(1979)Puterman and
  Brumelle]{puterman1979convergence}
Puterman, M.~L. and Brumelle, S.~L.
\newblock On the convergence of policy iteration in stationary dynamic
  programming.
\newblock \emph{Mathematics of Operations Research}, 4\penalty0 (1):\penalty0
  60--69, 1979.

\bibitem[Rawlik et~al.(2013)Rawlik, Toussaint, and
  Vijayakumar]{rawlik2013stochastic}
Rawlik, K., Toussaint, M., and Vijayakumar, S.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In \emph{Twenty-Third International Joint Conference on Artificial
  Intelligence}, 2013.

\bibitem[Santos \& Rust(2004)Santos and Rust]{santos2004convergence}
Santos, M.~S. and Rust, J.
\newblock Convergence properties of policy iteration.
\newblock \emph{SIAM Journal on Control and Optimization}, 42\penalty0
  (6):\penalty0 2094--2115, 2004.

\bibitem[Schulman et~al.(2017{\natexlab{a}})Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017{\natexlab{a}}.

\bibitem[Schulman et~al.(2017{\natexlab{b}})Schulman, Wolski, Dhariwal,
  Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{b}}.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Suzuki, and
  Kanamori]{sugiyama2012density}
Sugiyama, M., Suzuki, T., and Kanamori, T.
\newblock \emph{Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock The MIT Press, Cambridge, MA, 1998.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  DeTurck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2753--2762, 2017.

\bibitem[Todorov(2008)]{todorov2008general}
Todorov, E.
\newblock General duality between optimal control and estimation.
\newblock In \emph{2008 47th IEEE Conference on Decision and Control}, pp.\
  4286--4292. IEEE, 2008.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Toussaint(2009)]{toussaint2009robot}
Toussaint, M.
\newblock Robot trajectory optimization using approximate inference.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  1049--1056. ACM, 2009.

\bibitem[Wang et~al.(2016)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{wang2016sample}
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and
  de~Freitas, N.
\newblock Sample efficient actor-critic with experience replay.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2016.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3):\penalty0 279--292, 1992.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{wu2017scalable}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5279--5288, 2017.

\bibitem[Zheng et~al.(2018)Zheng, Oh, and Singh]{zheng2018learning}
Zheng, Z., Oh, J., and Singh, S.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4644--4654, 2018.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock PhD thesis, figshare, 2010.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock 2008.

\end{thebibliography}
