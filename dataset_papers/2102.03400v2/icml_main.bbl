\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2312--2320, 2011.

\bibitem[Abeille et~al.(2017)Abeille, Lazaric, et~al.]{abeille2017linear}
Abeille, M., Lazaric, A., et~al.
\newblock Linear thompson sampling revisited.
\newblock \emph{Electronic Journal of Statistics}, 11\penalty0 (2):\penalty0
  5165--5197, 2017.

\bibitem[Agrawal \& Goyal(2012)Agrawal and Goyal]{agrawal2012analysis}
Agrawal, S. and Goyal, N.
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock In \emph{Conference on learning theory}, pp.\  39--1, 2012.

\bibitem[Agrawal \& Goyal(2013)Agrawal and Goyal]{agrawal2013thompson}
Agrawal, S. and Goyal, N.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  127--135, 2013.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.05449}, 2017.

\bibitem[Badanidiyuru et~al.(2013)Badanidiyuru, Kleinberg, and
  Slivkins]{badanidiyuru2013bandits}
Badanidiyuru, A., Kleinberg, R., and Slivkins, A.
\newblock Bandits with knapsacks.
\newblock In \emph{2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science}, pp.\  207--216. IEEE, 2013.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011contextual}
Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and Schapire, R.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  19--26, 2011.

\bibitem[Bretagnolle \& Huber(1979)Bretagnolle and
  Huber]{bretagnolle1979estimation}
Bretagnolle, J. and Huber, C.
\newblock Estimation des densit{\'e}s: risque minimax.
\newblock \emph{Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete}, 47\penalty0 (2):\penalty0 119--137, 1979.

\bibitem[Cohen et~al.(2020)Cohen, Kaplan, Mansour, and
  Rosenberg]{cohen2020near}
Cohen, A., Kaplan, H., Mansour, Y., and Rosenberg, A.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock \emph{arXiv preprint arXiv:2002.09869}, 2020.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Dani, V., Hayes, T.~P., and Kakade, S.~M.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \emph{Conference on Learning Theory}, pp.\  355--366, 2008.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516, 2019.

\bibitem[Degenne \& Perchet(2016)Degenne and Perchet]{degenne2016anytime}
Degenne, R. and Perchet, V.
\newblock Anytime optimal algorithms in stochastic multi-armed bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1587--1595. PMLR, 2016.

\bibitem[Efroni et~al.(2019)Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Efroni, Y., Merlis, N., Ghavamzadeh, M., and Mannor, S.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12224--12234, 2019.

\bibitem[Efroni et~al.(2020)Efroni, Merlis, and
  Mannor]{efroni2020reinforcement}
Efroni, Y., Merlis, N., and Mannor, S.
\newblock Reinforcement learning with trajectory feedback.
\newblock \emph{arXiv preprint arXiv:2008.06036}, 2020.

\bibitem[Foster et~al.(2019)Foster, Krishnamurthy, and Luo]{foster2019model}
Foster, D.~J., Krishnamurthy, A., and Luo, H.
\newblock Model selection for contextual bandits.
\newblock \emph{arXiv preprint arXiv:1906.00531}, 2019.

\bibitem[Foster et~al.(2020)Foster, Rakhlin, Simchi-Levi, and
  Xu]{foster2020instance}
Foster, D.~J., Rakhlin, A., Simchi-Levi, D., and Xu, Y.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock \emph{arXiv preprint arXiv:2010.03104}, 2020.

\bibitem[Garivier \& Capp{\'e}(2011)Garivier and Capp{\'e}]{garivier2011kl}
Garivier, A. and Capp{\'e}, O.
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In \emph{Conference on Learning Theory}, pp.\  359--376, 2011.

\bibitem[Garivier et~al.(2019)Garivier, M{\'e}nard, and
  Stoltz]{garivier2019explore}
Garivier, A., M{\'e}nard, P., and Stoltz, G.
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock \emph{Mathematics of Operations Research}, 44\penalty0 (2):\penalty0
  377--399, 2019.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4863--4873, 2018.

\bibitem[Jin \& Luo(2019)Jin and Luo]{jin2019learning}
Jin, T. and Luo, H.
\newblock Learning adversarial mdps with bandit feedback and unknown
  transition.
\newblock \emph{arXiv preprint arXiv:1912.01192}, 2019.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and Munos]{kaufmann2012thompson}
Kaufmann, E., Korda, N., and Munos, R.
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \emph{International conference on algorithmic learning theory},
  pp.\  199--213. Springer, 2012.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and Mannor]{maillard2014hard}
Maillard, O.-A., Mann, T.~A., and Mannor, S.
\newblock How hard is my mdp?" the distribution-norm to the rescue".
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 1835--1843, 2014.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{maurer2009empirical}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Merlis \& Mannor(2019)Merlis and Mannor]{merlis2019batch}
Merlis, N. and Mannor, S.
\newblock Batch-size independent regret bounds for the combinatorial
  multi-armed bandit problem.
\newblock In \emph{Conference on Learning Theory}, pp.\  2465--2489. PMLR,
  2019.

\bibitem[Merlis \& Mannor(2020)Merlis and Mannor]{merlis2020tight}
Merlis, N. and Mannor, S.
\newblock Tight lower bounds for combinatorial multi-armed bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  2830--2857. PMLR,
  2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Seldin et~al.(2014)Seldin, Bartlett, Crammer, and
  Abbasi-Yadkori]{seldin2014prediction}
Seldin, Y., Bartlett, P., Crammer, K., and Abbasi-Yadkori, Y.
\newblock Prediction with limited advice and multiarmed bandits with paid
  observations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  280--287. PMLR, 2014.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1153--1162, 2019.

\bibitem[Sinha et~al.(2021)Sinha, Sankararaman, Kazerouni, and
  Avadhanula]{sinha2021multi}
Sinha, D., Sankararaman, K.~A., Kazerouni, A., and Avadhanula, V.
\newblock Multi-armed bandits with cost subsidy.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3016--3024. PMLR, 2021.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Garcelon, Valko, Pirotta, and
  Lazaric]{tarbouriech2020no}
Tarbouriech, J., Garcelon, E., Valko, M., Pirotta, M., and Lazaric, A.
\newblock No-regret exploration in goal-oriented reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9428--9437. PMLR, 2020.

\bibitem[Yun et~al.(2018)Yun, Proutiere, Ahn, Shin, and Yi]{yun2018multi}
Yun, D., Proutiere, A., Ahn, S., Shin, J., and Yi, Y.
\newblock Multi-armed bandit with additional observations.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 2\penalty0 (1):\penalty0 1--22, 2018.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock \emph{arXiv preprint arXiv:1901.00210}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Ji, and Du]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020.

\end{thebibliography}
