@inproceedings{Bai23,
title={{Transformers as statisticians: provable in-context learning with in-context algorithm selection}},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={ICML Workshop on Efficient Systems for Foundation Models},
year={2023}
}

@inproceedings{Garg22,
author = {Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
title = {{What can Transformers learn in-context? A case study of simple function classes}},
year = {2022},
booktitle = {Advances in Neural Information Processing Systems}
}

@article{Zhang23,
author={Ruiqi Zhang and Spencer Frei and Peter L. Bartlett},
title={{Trained Transformers learn linear models in-context}}, 
year={2023},
journal={arXiv preprint arXiv:2306.09927}
}

@article{Ahn23a,
author={Kwangjun Ahn and Xiang Cheng and Hadi Daneshmand and Suvrit Sra},
title={{Transformers learn to implement preconditioned gradient descent for in-context learning}}, 
year={2023},
journal={arXiv preprint arXiv:2306.00297}
}

@article{Mahankali23,
author={Arvind Mahankali and Tatsunori B. Hashimoto and Tengyu Ma},
title={{One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention}}, 
year={2023},
journal={arXiv preprint arXiv:2307.03576}
}

@article{Huang23,
author={Yu Huang and Yuan Cheng and Yingbin Liang},
title={{In-context convergence of Transformers}}, 
year={2023},
journal={arXiv preprint arXiv:2310.05249}
}

@article{Lin23,
author={Licong Lin and Yu Bai and Song Mei},
title={{Transformers as decision makers: provable in-context reinforcement learning via supervised pretraining}}, 
year={2023},
journal={arXiv preprint arXiv:2310.08566}
}

@inproceedings{Wu24,
author={Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Peter L. Bartlett},
title={How many pretraining tasks are needed for in-context learning of linear regression?}, 
year={2024},
booktitle={International Conference on Learning Representations}
}

@article{Guo23,
author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
title={{How do Transformers learn in-context beyond simple functions? A case study on learning with representations}}, 
year={2023},
journal={arXiv preprint arXiv:2310.10616}
}

@article{Ahn23b,
author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
title={{Linear attention is (maybe) all you need (to understand Transformer optimization)}}, 
year={2023},
journal={arXiv preprint arXiv:2310.01082}
}

@inproceedings{Yasuda23,
title={Sequential attention for feature selection},
author={Taisuke Yasuda and Mohammadhossein Bateni and Lin Chen and Matthew Fahrbach and Gang Fu and Vahab Mirrokni},
booktitle={International Conference on Learning Representations},
year={2023}
}

@inproceedings{Oswald23,
title={Transformers learn in-context by gradient descent},
author= {Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
booktitle={International Conference on Machine Learning},
year={2023}
}

@article{Zhang23Bayes,
title={What and how does in-context learning learn? {B}ayesian model averaging, parameterization, and generalization}, 
author={Yufeng Zhang and Fengzhuo Zhang and Zhuoran Yang and Zhaoran Wang},
year={2023},
journal={arXiv preprint arXiv:2305.19420}
}

@inproceedings{Akyrek23,
title={{What learning algorithm is in-context learning? Investigations with linear models}},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
year={2023},
booktitle={International Conference on Learning Representations}
}

@article{Shen23,
title={Do pretrained {T}ransformers really learn in-context by gradient descent?}, 
author={Lingfeng Shen and Aayush Mishra and Daniel Khashabi},
year={2023},
journal={arXiv preprint arXiv:2310.08540}
}

@inproceedings{Tsai19,
author = {Tsai, Yao-Hung and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
year = {2019},
title = {Transformer dissection: an unified understanding for {T}ransformer's attention via the lens of kernel},
booktitle={Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing},
publisher={Association for Computational Linguistics}
}

@inproceedings{Katharopoulos20,
  title={{Transformers are RNNs: fast autoregressive Transformers with linear attention}},
  author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Franccois Fleuret},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@article{Yang23,
title={Gated linear attention {T}ransformers with hardware-efficient training}, 
author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
year={2023},
journal={arXiv preprint arXiv:2312.06635}
}

@article{Han23,
title={Explaining emergent in-context learning as kernel regression}, 
author={Chi Han and Ziqi Wang and Han Zhao and Heng Ji},
year={2023},
journal={arXiv preprint arXiv:2305.12766}
}

@article{Tian23,
title={{JoMA: demystifying multilayer Transformers via joint dynamics of MLP and attention}},
author={Yuandong Tian and Yiping Wang and Zhenyu Zhang and Beidi Chen and Simon Du},
year={2023},
journal={arXiv preprint arXiv:2310.00535}
}

@article{Schmidt20,
title={Nonparametric regression using deep neural networks with {ReLU} activation function},
volume={48},
number={4},
journal={The Annals of Statistics},
publisher={Institute of Mathematical Statistics},
author={Schmidt-Hieber, Johannes},
year={2020}
}

@article{Tropp15,
author = {Tropp, Joel A.},
title = {An Introduction to Matrix Concentration Inequalities},
year = {2015},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {8},
number = {1–2},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
month = {May},
pages = {1–230}
}

@article{Hayakawa20,
title = {On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces},
journal = {Neural Networks},
volume = {123},
pages = {343-361},
year = {2020},
author = {Satoshi Hayakawa and Taiji Suzuki}
}

@article{Szarek81,
title = {Nets of {G}rassmann manifold and orthogonal group},
journal = {Proceedings of Banach Space Workshop},
publisher={University of Iowa Press},
year = {1981},
pages = {169-186},
author = {Stanislaw J. Szarek}
}

@article{Zhang24,
title={{In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization}}, 
author={Ruiqi Zhang and Jingfeng Wu and Peter L. Bartlett},
year={2024},
journal={arXiv preprint arXiv:2402.14951}
}

@inproceedings{Suzuki19,
title={{Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality}},
author={Taiji Suzuki},
year={2019},
booktitle={International Conference on Learning Representations}
}

@book{Triebel83,
author = {Hans Triebel},
title = {Theory of function spaces},
series = {Monographs in mathematics},
publisher = {Birkh\"{a}user Verlag},
year = {1983}
}

@article{Dung11,
title = {Optimal adaptive sampling recovery},
journal = {Advances in Computational Mathematics},
year = {2011},
volume={34},
issue={1},
pages = {1-41},
author = {Dinh D{\~u}ng}
}

@article{Vybiral08,
title = {Widths of embeddings in function spaces},
journal = {Journal of Complexity},
volume = {24},
number = {4},
pages = {545-570},
year = {2008},
author = {Jan Vyb\'{i}ral}
}

@article{Devore88,
author = {Ronald A. De{V}ore and Vasil A. Popov},
journal = {Transactions of the American Mathematical Society},
number = {1},
pages = {397-414},
title = {Interpolation of {B}esov spaces},
volume = {305},
year = {1988}
}

@article{Li24,
title={{Training nonlinear Transformers for efficient in-context learning: a theoretical learning and generalization analysis}}, 
author={Hongkang Li and Meng Wang and Songtao Lu and Xiaodong Cui and Pin-Yu Chen},
journal={arXiv preprint arXiv:2402.15607},
year={2024}
}

@article{Donoho98,
author = {David L. Donoho and Iain M. Johnstone},
title = {Minimax estimation via wavelet shrinkage},
volume = {26},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {879-921},
year = {1998}
}

@article{Yarotsky16,
author = {Yarotsky, Dmitry},
year = {2016},
issue={8},
pages={103-114},
title = {{Error bounds for approximations with deep ReLU networks}},
volume = {94},
journal = {Neural Networks}
}

@article{Schmeisser87,
 title={An unconditional basis in periodic spaces with dominating mixed smoothness properties},
author={Schmeisser, H -J},
journal={Analysis Mathematica},
volume={13},
number={2},
pages={153-168},
year={1987}
}

@article{Sickel09,
author = {Sickel, Winfried and Ullrich, Tino},
title = {{Tensor products of Sobolev-Besov spaces and applications to approximation from the hyperbolic cross}},
year = {2009},
publisher = {Academic Press, Inc.},
volume = {161},
number = {2},
journal = {Journal of Approximation Theory},
pages = {748-786}
}

@article{Sickel11,
author = {Winfried Sickel and Tino Ullrich},
title = {Spline interpolation on sparse grids},
journal = {Applicable Analysis},
volume = {90},
number = {3-4},
pages = {337-383},
year = {2011},
publisher = {Taylor & Francis}
}

@book{Vybiral06,
author = {Jan Vyb\'{i}ral},
title = {Function spaces with dominating mixed smoothness},
volume = {30},
journal = {Dissertationes Math. (Rozprawy Mat.)},
year = {2006},
publisher={European Mathematical Society},
series={Lectures in Mathematics}
}

@article{Triebel11,
author = {Triebel, Hans},
year = {2011},
pages = {169-188},
title = {Entropy numbers in function spaces with mixed integrability},
volume = {24},
journal = {Revista Matematica Complutense}
}

@inproceedings{Suzuki21,
author = {Taiji Suzuki and Atsushi Nitanda},
title = {Deep learning is adaptive to intrinsic dimensionality
of model smoothness in anisotropic {B}esov space},
year = {2021},
booktitle = {Advances in Neural Information Processing Systems}
}

@book{Nikolskii75,
author = {Sergei Mihailovic Nikol'skii},
title = {Approximation of functions of several variables and imbedding theorems},
series={Grundlehren der mathematischen Wissenschaften},
volume={205},
year = {1975},
publisher={Springer Berlin}
}

@article{Dung11b,
title = {B-spline quasi-interpolant representations and sampling recovery of functions with mixed smoothness},
journal = {Journal of Complexity},
volume = {27},
number = {6},
pages = {541-567},
year = {2011},
author = {Dinh D{\~u}ng}
}

@book{Nickl15,
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={Mathematical foundations of infinite-dimensional statistical models},
publisher={Cambridge University Press},
author={Gin\'{e}, Evarist and Nickl, Richard},
year={2015}
}

@article{Yang99,
author = {Yuhong Yang and Andrew Barron},
title = {Information-theoretic determination of minimax rates of convergence},
volume = {27},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1564-1599},
year = {1999}
}

@article{Raskutti12,
author = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
title = {Minimax-optimal rates for sparse additive models over kernel classes via convex programming},
year = {2012},
volume = {13},
number = {1},
journal = {Journal of Machine Learning Research},
pages = {389–427}
}

@book{Wainwright19,
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={High-dimensional statistics: a non-asymptotic viewpoint},
publisher={Cambridge University Press},
author={Wainwright, Martin J.},
year={2019}
}

@article{Sickel99,
author = {Sickel, Winfried and Sprengel, Frauke},
year = {1999},
pages = {263-288},
title = {{Interpolation on sparse grids and tensor products of Nikol'skij–Besov spaces}},
volume = {1},
journal = {Journal of Computational Analysis and Applications}
}

@inproceedings{Takakura23,
title={Approximation and estimation ability of {T}ransformers for sequence-to-sequence functions with infinite dimensional input},
author={Shokichi Takakura and Taiji Suzuki},
booktitle={International Conference on Machine Learning},
year={2023}
}

@inproceedings{Okumoto22,
author={Sho Okumoto and Taiji Suzuki},
title={Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness}, 
year={2022},
booktitle={International Conference on Learning Representations}
}

@book{Book:VanDerVaart:WeakConvergence,
 author               = {A. W. van der Vaart and J. A. Wellner},
 publisher            = {Springer, New York},
 title                = {Weak convergence and empirical processes: with applications to statistics},
 year                 = {1996},
 }
@incollection{NIPS:Raskutti+Martin:2009,
 address              = {Cambridge, MA},
 author               = {Garvesh Raskutti and Martin Wainwright and Bin Yu},
 booktitle            = {Advances in Neural Information Processing Systems 22},
 pages                = {1563--1570},
 publisher            = {MIT Press},
 title                = {Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness},
 year                 = {2009},
 }

@article{raskutti2012minimax,
  title={Minimax-optimal rates for sparse additive models over kernel classes via convex programming},
  author={Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={389--427},
  year={2012}
}

@inproceedings{Kim24,
title={Transformers learn nonlinear features in context: nonconvex mean-field dynamics on the attention landscape}, 
author={Juno Kim and Taiji Suzuki},
year={2024},
booktitle={International Conference on Machine Learning}
}

@inproceedings{Garcia23,
  title={The unreasonable effectiveness of few-shot learning for machine translation},
  author={Xavier Garc{\'i}a and Yamini Bansal and Colin Cherry and George F. Foster and Maxim Krikun and Fan Feng and Melvin Johnson and Orhan Firat},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{Brown20,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language models are few-shot learners},
 year = {2020}
}

@inproceedings{
Raventos23,
title={Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression},
author={Allan Raventos and Mansheej Paul and Feng Chen and Surya Ganguli},
booktitle = {Advances in Neural Information Processing Systems},
year={2023}
}

@article{Dong23,
  title={A survey for in-context learning},
  author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
  journal={arXiv preprint arXiv:2301.00234},
  year={2023}
}

@article{Chen24,
title={Training dynamics of multi-head softmax attention for in-context learning: emergence, convergence, and optimality}, 
author={Siyu Chen and Heejune Sheen and Tianhao Wang and Zhuoran Yang},
year={2024},
journal={arXiv preprint arXiv:2402.19442}
}

@inproceedings{Chen21,
  title={Scatterbrain: unifying sparse and low-rank attention approximation},
  author={Beidi Chen and Tri Dao and Eric Winsor and Zhao Song and Atri Rudra and Chris R{\'e}},
  year={2021},
booktitle = {Advances in Neural Information Processing Systems}
}

@inproceedings{Bho20,
  title={Low-rank bottleneck in multi-head attention models},
  author={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{Tri20,
author = {Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael},
booktitle={International Conference on Machine Learning},
year = {2020},
title = {Provable meta-learning of linear representations}
}

@inproceedings{Du21,
title={Few-shot learning via learning the representation, provably},
author={Simon Du and Wei Hu and Sham Kakade and Jason Lee and Qi Lei},
booktitle={International Conference on Learning Representations},
year={2021}
}

@article{Meunier23,
title={Nonlinear meta-learning can guarantee faster rates}, 
      author={Dimitri Meunier and Zhu Li and Arthur Gretton and Samory Kpotufe},
year={2023},
journal={arXiv preprint arXiv:2307.10870}
}

@inproceedings{
nishimura2024minimax,
title={Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods},
author={Yuto Nishimura and Taiji Suzuki},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=EW8ZExRZkJ}
}

@misc{imaizumi2024,
  author       = {M. Imaizumi},
  title        = {Statistical analysis on in-context learning},
  note         = {Personal communication},
  year         = {2024}
}