\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{ABAB{\etalchar{+}}21}

\bibitem[AAM22]{abbe2022merged}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In {\em Conference on Learning Theory}, pages 4782--4887. PMLR, 2022.

\bibitem[ABAB{\etalchar{+}}21]{abbe2021staircase}
Emmanuel Abbe, Enric Boix-Adsera, Matthew~S Brennan, Guy Bresler, and Dheeraj Nagaraj.
\newblock The staircase property: How hierarchical structure can guide deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 26989--27002, 2021.

\bibitem[ALP22]{adlam2019random}
Ben Adlam, Jake~A Levinson, and Jeffrey Pennington.
\newblock A random matrix perspective on mixtures of nonlinearities in high dimensions.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, 2022.

\bibitem[AP20a]{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem[AP20b]{adlam2020understanding}
Ben Adlam and Jeffrey Pennington.
\newblock Understanding double descent requires a fine-grained bias-variance decomposition.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem[AS68]{abramowitz1968handbook}
Milton Abramowitz and Irene~A Stegun.
\newblock {\em Handbook of mathematical functions with formulas, graphs, and mathematical tables}, volume~55.
\newblock US Government printing office, 1968.

\bibitem[ASH19]{abbasi2019universality}
Ehsan Abbasi, Fariborz Salehi, and Babak Hassibi.
\newblock Universality in learning from linear measurements.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[BAGJ21]{arous2021online}
Gerard Ben~Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from high-dimensional inference.
\newblock {\em Journal of Machine Learning Research}, 22(106):1--51, 2021.

\bibitem[BCV13]{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 35(8):1798--1828, 2013.

\bibitem[BES{\etalchar{+}}22]{ba2022high}
Jimmy Ba, Murat~A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
\newblock High-dimensional asymptotics of feature learning: How one gradient step improves the representation.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[BKM23]{bombari23robustness}
Simone Bombari, Shayan Kiyani, and Marco Mondelli.
\newblock Beyond the universal law of robustness: Sharper laws for random features and neural tangent kernels.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem[BM23]{bombari2023stability}
Simone Bombari and Marco Mondelli.
\newblock Stability, generalization and privacy: Precise analysis for random and {NTK} features.
\newblock {\em arXiv preprint arXiv:2305.12100}, 2023.

\bibitem[BMP15]{banna2015limiting}
Marwa Banna, Florence Merlev{\`e}de, and Magda Peligrad.
\newblock On the limiting spectral distribution for a large class of symmetric random matrices with correlated entries.
\newblock {\em Stochastic Processes and their Applications}, 125(7):2700--2726, 2015.

\bibitem[BMZ23]{berthier2023learning}
Rapha{\"e}l Berthier, Andrea Montanari, and Kangjie Zhou.
\newblock Learning time-scales in two-layers neural networks.
\newblock {\em arXiv preprint arXiv:2303.00055}, 2023.

\bibitem[BNY20]{banna2020clt}
Marwa Banna, Jamal Najim, and Jianfeng Yao.
\newblock A {CLT} for linear spectral statistics of large random information-plus-noise matrices.
\newblock {\em Stochastic Processes and their Applications}, 130(4):2250--2281, 2020.

\bibitem[BP21]{benigni2021eigenvalue}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvalue distribution of some nonlinear models of random matrices.
\newblock {\em Electronic Journal of Probability}, 26:1--37, 2021.

\bibitem[BP22]{benigni2022largest}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock Largest eigenvalues of the conjugate kernel of single-layered neural networks.
\newblock {\em arXiv preprint arXiv:2201.04753}, 2022.

\bibitem[BPH23]{BoschPH23}
David Bosch, Ashkan Panahi, and Babak Hassibi.
\newblock Precise asymptotic analysis of deep random feature models.
\newblock In {\em Conference on Learning Theory}, 2023.

\bibitem[BS10]{bai2010spectral}
Zhidong Bai and Jack~W Silverstein.
\newblock {\em Spectral Analysis of Large Dimensional Random Matrices}, volume~20.
\newblock Springer, 2010.

\bibitem[Cap14]{capitaine2014exact}
Mireille Capitaine.
\newblock Exact separation phenomenon for the eigenvalues of large information-plus-noise type matrices, and an application to spiked models.
\newblock {\em Indiana University Mathematics Journal}, pages 1875--1910, 2014.

\bibitem[CB18]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock In {\em Advances in neural information processing systems}, 2018.

\bibitem[CCFM21]{chen2021spectral}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Spectral methods for data science: A statistical perspective.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning}, 14(5):566--806, 2021.

\bibitem[CD11]{couillet2011random}
Romain Couillet and Merouane Debbah.
\newblock {\em Random {M}atrix {M}ethods for {W}ireless {C}ommunications}.
\newblock Cambridge University Press, 2011.

\bibitem[CGKM22]{chen2022hardness}
Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka.
\newblock Hardness of noise-free learning for two-hidden-layer neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[CHS{\etalchar{+}}23]{collins2023provable}
Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Provable multi-task representation learning by two-layer relu neural networks.
\newblock {\em arXiv preprint arXiv:2307.06887}, 2023.

\bibitem[CKZ23]{cui23b}
Hugo Cui, Florent Krzakala, and Lenka Zdeborova.
\newblock {B}ayes-optimal learning of deep random networks of extensive-width.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem[CL22]{couillet2022random}
Romain Couillet and Zhenyu Liao.
\newblock {\em Random Matrix Methods for Machine Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[CLKZ23]{clarte2023double}
Lucas Clart{\'e}, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock On double-descent in uncertainty quantification in overparametrized models.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, 2023.

\bibitem[CS13]{cheng2013spectrum}
Xiuyuan Cheng and Amit Singer.
\newblock The spectrum of random inner-product kernel matrices.
\newblock {\em Random Matrices: Theory and Applications}, 2(04):1350010, 2013.

\bibitem[Dee70]{deev1970representation}
AD~Deev.
\newblock Representation of statistics of discriminant analysis and asymptotic expansion when space dimensions are comparable with sample size.
\newblock In {\em Sov. Math. Dokl.}, volume~11, pages 1547--1550, 1970.

\bibitem[dGSB21]{d2021interplay}
St{\'e}phane d'Ascoli, Marylou Gabri{\'e}, Levent Sagun, and Giulio Biroli.
\newblock On the interplay between data structure and loss function in classification problems.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem[DKD16]{donahue2016adversarial}
Jeff Donahue, Philipp Kr{\"a}henb{\"u}hl, and Trevor Darrell.
\newblock Adversarial feature learning.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem[DKL{\etalchar{+}}23]{dandi2023learning}
Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan.
\newblock Learning two-layer neural networks, one (giant) step at a time.
\newblock {\em arXiv preprint arXiv:2305.18270}, 2023.

\bibitem[DLS22]{damian2022neural}
Alex Damian, Jason Lee, and Mahdi Soltanolkotabi.
\newblock Neural networks can learn representations with gradient descent.
\newblock In {\em Conference on Learning Theory}, 2022.

\bibitem[DS07]{dozier2007empirical}
R~Brent Dozier and Jack~W Silverstein.
\newblock On the empirical distribution of eigenvalues of large dimensional information-plus-noise-type matrices.
\newblock {\em Journal of Multivariate Analysis}, 98(4):678--694, 2007.

\bibitem[DW18]{dobriban2018high}
Edgar Dobriban and Stefan Wager.
\newblock High-dimensional asymptotics of prediction: Ridge regression and classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279, 2018.

\bibitem[Dys62]{dyson1962brownian}
Freeman~J Dyson.
\newblock A {B}rownian-motion model for the eigenvalues of a random matrix.
\newblock {\em Journal of Mathematical Physics}, 3(6):1191--1198, 1962.

\bibitem[DZPS19]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[EK10]{el2010spectrum}
Noureddine El~Karoui.
\newblock The spectrum of kernel random matrices.
\newblock {\em The Annals of Statistics}, 38(1):1--50, 2010.

\bibitem[EPR{\etalchar{+}}10]{erdHos2010bulk}
L{\'a}szl{\'o} Erd{\"{o}}s, Sandrine P{\'e}ch{\'e}, Jos{\'e}~A Ram{\'\i}rez, Benjamin Schlein, and Horng-Tzer Yau.
\newblock Bulk universality for {W}igner matrices.
\newblock {\em Communications on Pure and Applied Mathematics}, 63(7):895--925, 2010.

\bibitem[Erd19]{erdos2019matrix}
Laszlo Erd\"os.
\newblock The matrix {D}yson equation and its applications for random matrices.
\newblock {\em arXiv preprint arXiv:1903.10060}, 2019.

\bibitem[EVdB01]{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem[EYY12]{erdHos2012bulk}
L{\'a}szl{\'o} Erd{\"{o}}s, Horng-Tzer Yau, and Jun Yin.
\newblock Bulk universality for generalized {W}igner matrices.
\newblock {\em Probability Theory and Related Fields}, 154(1):341--407, 2012.

\bibitem[FM19]{fan2019spectral}
Zhou Fan and Andrea Montanari.
\newblock The spectral norm of random inner-product kernel matrices.
\newblock {\em Probability Theory and Related Fields}, 173(1):27--85, 2019.

\bibitem[FW20]{fan2020spectra}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem[Gau61]{gaudin1961loi}
Michel Gaudin.
\newblock Sur la loi limite de l'espacement des valeurs propres d'une matrice al\'{e} atoire.
\newblock {\em Nuclear Physics}, 25:447--458, 1961.

\bibitem[GK19]{goel2019learning}
Surbhi Goel and Adam~R Klivans.
\newblock Learning neural networks with two nonlinear layers in polynomial time.
\newblock In {\em Conference on Learning Theory}, 2019.

\bibitem[GLR{\etalchar{+}}22]{goldt2022gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock The {G}aussian equivalence of generative models for learning with shallow neural networks.
\newblock In {\em Mathematical and Scientific Machine Learning}, pages 426--471, 2022.

\bibitem[GMMM21]{ghorbani2021linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em The Annals of Statistics}, 49(2):1029--1054, 2021.

\bibitem[GT90]{gyorgyi1990statistical}
G{\'e}za Gy{\"o}rgyi and Naftali Tishby.
\newblock Statistical theory of learning a rule.
\newblock {\em Neural Networks and Spin Glasses}, pages 3--36, 1990.

\bibitem[HJ22]{hassani2022curse}
Hamed Hassani and Adel Javanmard.
\newblock The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression.
\newblock {\em arXiv preprint arXiv:2201.05149}, 2022.

\bibitem[HL23]{hu2022universality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock {\em IEEE Transactions on Information Theory}, 69(3), 2023.

\bibitem[HN19]{hanin2019finite}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[HY20]{huang2020dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem[KC22]{kuchibhotla2022moving}
Arun~Kumar Kuchibhotla and Abhishek Chakrabortty.
\newblock Moving beyond sub-gaussianity in high-dimensional statistics: Applications in covariance estimation and linear regression.
\newblock {\em Information and Inference: A Journal of the IMA}, 11(4):1389--1456, 2022.

\bibitem[LD21]{lin2021causes}
Licong Lin and Edgar Dobriban.
\newblock What causes the test error? going beyond bias-variance via {ANOVA}.
\newblock {\em Journal of Machine Learning Research}, 22:155--1, 2021.

\bibitem[LGC{\etalchar{+}}21]{loureiro2021learning}
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborov{\'a}.
\newblock Learning curves of generic features maps for realistic datasets with a teacher-student model.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lin22]{lindeberg1922neue}
Jarl~Waldemar Lindeberg.
\newblock Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung.
\newblock {\em Mathematische Zeitschrift}, 15(1):211--225, 1922.

\bibitem[LLC18]{louart2018random}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem[LMH{\etalchar{+}}23]{disagreement}
Donghwan Lee, Behrad Moniri, Xinmeng Huang, Edgar Dobriban, and Hamed Hassani.
\newblock Demystifying disagreement-on-the-line in high dimensions.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem[LXS{\etalchar{+}}19]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[LY22]{lu2022equivalence}
Yue~M Lu and Horng-Tzer Yau.
\newblock An equivalence principle for the spectrum of random inner-product kernel matrices.
\newblock {\em arXiv preprint arXiv:2205.06308}, 2022.

\bibitem[Meh04]{mehta2004random}
Madan~Lal Mehta.
\newblock {\em Random matrices}.
\newblock Elsevier, 2004.

\bibitem[Mis22]{misiakiewicz2022spectrum}
Theodor Misiakiewicz.
\newblock Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression.
\newblock {\em arXiv preprint arXiv:2204.10425}, 2022.

\bibitem[MM22]{mei2022generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise asymptotics and the double descent curve.
\newblock {\em Communications on Pure and Applied Mathematics}, 75(4):667--766, 2022.

\bibitem[MMM19]{pmlr-v99-mei19a}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In {\em Conference on Learning Theory}, 2019.

\bibitem[MMN18]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 115(33):E7665--E7671, 2018.

\bibitem[MP21]{mel2021anisotropic}
Gabriel Mel and Jeffrey Pennington.
\newblock Anisotropic random feature regression in high dimensions.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[MRSY19]{montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}, 2019.

\bibitem[MS22]{montanari2022universality}
Andrea Montanari and Basil~N Saeed.
\newblock Universality of empirical risk minimization.
\newblock In {\em Conference on Learning Theory}, 2022.

\bibitem[NDL23]{nichani2023provable}
Eshaan Nichani, Alex Damian, and Jason~D Lee.
\newblock Provable guarantees for nonlinear feature learning in three-layer neural networks.
\newblock {\em arXiv preprint arXiv:2305.06986}, 2023.

\bibitem[NR21]{naveh2021self}
Gadi Naveh and Zohar Ringel.
\newblock A self consistent theory of gaussian processes captures feature learning effects in finite cnns.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem[O'D14]{o2014analysis}
Ryan O'Donnell.
\newblock {\em Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[OK96]{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of Neural Networks III}, pages 151--209. Springer, 1996.

\bibitem[Opp95]{opper1995statistical}
Manfred Opper.
\newblock Statistical mechanics of learning: Generalization.
\newblock {\em The Handbook of Brain Theory and Neural Networks,}, pages 922--925, 1995.

\bibitem[PA14]{paul2014random}
Debashis Paul and Alexander Aue.
\newblock Random matrix theory in statistics: A review.
\newblock {\em Journal of Statistical Planning and Inference}, 150:1--29, 2014.

\bibitem[P{\'e}c19]{peche}
Sandrine P{\'e}ch{\'e}.
\newblock {A note on the Pennington-Worah distribution}.
\newblock {\em Electronic Communications in Probability}, 24:1--7, 2019.

\bibitem[PW17]{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem[Rau67]{raudys1967determining}
{\v{S}}ar{\=u}nas Raudys.
\newblock On determining training sample size of linear classifier.
\newblock {\em Computing Systems (in Russian)}, 28:79–87, 1967.

\bibitem[Rau72]{raudys1972amount}
{\v{S}}ar{\=u}nas Raudys.
\newblock On the amount of a priori information in designing the classification algorithm.
\newblock {\em Technical Cybernetics (in Russian)}, 4:168--174, 1972.

\bibitem[RBPB22]{radhakrishnan2022feature}
Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
\newblock Feature learning in neural networks and kernel machines that recursively learn features.
\newblock {\em arXiv preprint arXiv:2212.13881}, 2022.

\bibitem[RR07]{RahimiRecht}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, 2007.

\bibitem[RVE22]{rotskoff2022trainability}
Grant Rotskoff and Eric Vanden-Eijnden.
\newblock Trainability and accuracy of artificial neural networks: An interacting particle system approach.
\newblock {\em Communications on Pure and Applied Mathematics}, 75(9):1889--1935, 2022.

\bibitem[RY04]{raudys2004results}
{\v{S}}ar{\=u}nas Raudys and Dean~M Young.
\newblock Results in statistical discriminant analysis: A review of the former {S}oviet {U}nion literature.
\newblock {\em Journal of Multivariate Analysis}, 89(1):1--35, 2004.

\bibitem[Sam23]{sambale2023some}
Holger Sambale.
\newblock Some notes on concentration for $\alpha$-subexponential random variables.
\newblock In {\em High Dimensional Probability IX: The Ethereal Volume}, pages 167--192. Springer, 2023.

\bibitem[Ser07]{serdobolskii2007multiparametric}
Vadim~Ivanovich Serdobolskii.
\newblock {\em Multiparametric {S}tatistics}.
\newblock Elsevier, 2007.

\bibitem[SNR23]{seroussi2023separation}
Inbar Seroussi, Gadi Naveh, and Zohar Ringel.
\newblock Separation of scales and a thermodynamic description of feature learning in some cnns.
\newblock {\em Nature Communications}, 14(1):908, 2023.

\bibitem[SS90]{stewart1990matrix}
Gilbert~W Stewart and Ji-guang Sun.
\newblock {\em Matrix perturbation theory}.
\newblock Elsevier, 1990.

\bibitem[SS20]{sirignano2020mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock {\em Stochastic Processes and their Applications}, 130(3):1820--1852, 2020.

\bibitem[SWL22]{shi2022theoretical}
Zhenmei Shi, Junyi Wei, and Yingyu Liang.
\newblock A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem[TAP21]{tripuraneni2021covariate}
Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington.
\newblock Overparameterization improves robustness to covariate shift in high dimensions.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem[TV04]{tulino2004random}
Antonio~M Tulino and Sergio Verd{\'u}.
\newblock Random matrix theory and wireless communications.
\newblock {\em Communications and Information Theory}, 1(1):1--182, 2004.

\bibitem[TV11]{tao2011random}
Terence Tao and Van Vu.
\newblock Random matrices: universality of local eigenvalue statistics.
\newblock {\em Acta mathematica}, 206(1):127--204, 2011.

\bibitem[vdVW13]{wellner2013weak}
Aad van~der Vaart and Jon Wellner.
\newblock {\em Weak convergence and empirical processes: with applications to statistics}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Ver12]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock In Yonina~C. Eldar and Gitta Kutyniok, editors, {\em Compressed Sensing: Theory and Applications}, page 210–268. Cambridge University Press, 2012.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications in data science}.
\newblock Cambridge University Press, 2018.

\bibitem[VGNA20]{vladimirova2020sub}
Mariia Vladimirova, St{\'e}phane Girard, Hien Nguyen, and Julyan Arbel.
\newblock Sub-weibull distributions: Generalizing sub-gaussian and sub-exponential properties to heavier tailed distributions.
\newblock {\em Stat}, 9(1), 2020.

\bibitem[Wed72]{wedin1972perturbation}
Per-Ake Wedin.
\newblock Perturbation bounds in connection with singular value decomposition.
\newblock {\em BIT Numerical Mathematics}, 12:99--111, 1972.

\bibitem[WES{\etalchar{+}}22]{wang2022spectral}
Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang.
\newblock Spectral evolution and invariance in linear-width neural networks.
\newblock {\em arXiv preprint arXiv:2211.06506}, 2022.

\bibitem[Wig55]{wigner1955characteristic}
Eugene~P Wigner.
\newblock Characteristic vectors of bordered matrices with infinite dimensions.
\newblock {\em Annals of Mathematics}, pages 548--564, 1955.

\bibitem[WNL23]{wang2023learning}
Zihao Wang, Eshaan Nichani, and Jason~D Lee.
\newblock Learning hierarchical polynomials with three-layer neural networks.
\newblock {\em arXiv preprint arXiv:2311.13774}, 2023.

\bibitem[XHM{\etalchar{+}}22]{xiao2022precise}
Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington.
\newblock Precise learning curves and higher-order scalings for dot-product kernel regression.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yai20]{yaida2020non}
Sho Yaida.
\newblock Non-gaussian processes and neural networks at finite widths.
\newblock In {\em Mathematical and Scientific Machine Learning}, pages 165--192. PMLR, 2020.

\bibitem[YBZ15]{yao2015large}
Jianfeng Yao, Zhidong Bai, and Shurong Zheng.
\newblock {\em Large Sample Covariance Matrices and High-Dimensional Data Analysis}.
\newblock Cambridge University Press, 2015.

\bibitem[YH21]{yang2020feature_learn}
Greg Yang and Edward~J Hu.
\newblock Feature learning in infinite-width neural networks.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem[ZC21]{zhang2020concentration}
Huiming Zhang and Songxi Chen.
\newblock Concentration inequalities for statistical inference.
\newblock {\em Communications in Mathematical Research}, 37(1):1--85, 2021.

\bibitem[ZWL22]{zhenmei2022theoretical}
Shi Zhenmei, Junyi Wei, and Yingyu Liang.
\newblock A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features.
\newblock In {\em International Conference on Learning Representations}, 2022.

\end{thebibliography}
