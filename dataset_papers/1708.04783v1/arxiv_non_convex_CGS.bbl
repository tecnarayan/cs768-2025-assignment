\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 699--707, 2016.

\bibitem[Allen-Zhu and Yuan(2016)]{allen2016improved}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 1080--1089, 2016.

\bibitem[Bertsekas(1999)]{bertsekas1999nonlinear}
Dimitri~P Bertsekas.
\newblock \emph{Nonlinear programming}.
\newblock Athena scientific Belmont, 1999.

\bibitem[Chandrasekaran et~al.(2009)Chandrasekaran, Sanghavi, Parrilo, and
  Willsky]{chandrasekaran2009sparse}
Venkat Chandrasekaran, Sujay Sanghavi, Pablo~A Parrilo, and Alan~S Willsky.
\newblock Sparse and low-rank matrix decompositions.
\newblock \emph{IFAC Proceedings Volumes}, 42\penalty0 (10):\penalty0
  1493--1498, 2009.

\bibitem[Collins et~al.(2008)Collins, Globerson, Koo, Carreras, and
  Bartlett]{collins2008exponentiated}
Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter~L
  Bartlett.
\newblock Exponentiated gradient algorithms for conditional random fields and
  max-margin markov networks.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (Aug):\penalty0 1775--1822, 2008.

\bibitem[Frank and Wolfe(1956)]{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Garber and Hazan(2013)]{garber2013linearly}
Dan Garber and Elad Hazan.
\newblock A linearly convergent conditional gradient algorithm with
  applications to online and stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1301.4666}, 2013.

\bibitem[Garber and Hazan(2015)]{garber2015faster}
Dan Garber and Elad Hazan.
\newblock Faster rates for the frank-wolfe method over strongly-convex sets.
\newblock In \emph{ICML}, pages 541--549, 2015.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi and Lan(2016)]{ghadimi2016accelerated}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0 59--99,
  2016.

\bibitem[Guzm{\'a}n and Nemirovski(2015)]{guzman2015lower}
Crist{\'o}bal Guzm{\'a}n and Arkadi Nemirovski.
\newblock On lower complexity bounds for large-scale smooth convex
  optimization.
\newblock \emph{Journal of Complexity}, 31\penalty0 (1):\penalty0 1--14, 2015.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{ICML (1)}, pages 427--435, 2013.

\bibitem[Lacoste-Julien(2016)]{lacoste2016convergence}
Simon Lacoste-Julien.
\newblock Convergence rate of frank-wolfe for non-convex objectives.
\newblock \emph{arXiv preprint arXiv:1607.00345}, 2016.

\bibitem[Lan(2013)]{lan2013complexity}
G~Lan.
\newblock The complexity of large-scale convex programming under a linear
  optimization oracle. department of industrial and systems engineering,
  university of florida, gainesville.
\newblock Technical report, Florida. Technical Report, 2013.

\bibitem[Lan and Zhou(2016)]{lan2016conditional}
Guanghui Lan and Yi~Zhou.
\newblock Conditional gradient sliding for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1379--1409, 2016.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Reddi et~al.(2016{\natexlab{a}})Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 314--323, 2016{\natexlab{a}}.

\bibitem[Reddi et~al.(2016{\natexlab{b}})Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016stochasticfw}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Stochastic frank-wolfe methods for nonconvex optimization.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2016 54th
  Annual Allerton Conference on}, pages 1244--1251. IEEE, 2016{\natexlab{b}}.

\bibitem[Shalev-Shwartz(2016)]{shalev2016sdca}
Shai Shalev-Shwartz.
\newblock Sdca without duality, regularization, and individual convexity.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 747--754, 2016.

\end{thebibliography}
