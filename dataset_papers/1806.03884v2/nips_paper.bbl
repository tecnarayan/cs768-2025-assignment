\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 1998.

\bibitem[Ba et~al.(2017)Ba, Grosse, and Martens]{ba2016distributed}
Jimmy Ba, Roger Grosse, and James Martens.
\newblock Distributed second-order optimization using kronecker-factored
  approximations.
\newblock In \emph{ICLR}, 2017.

\bibitem[Becker et~al.(1988)Becker, Le~Cun, et~al.]{becker1988improving}
Sue Becker, Yann Le~Cun, et~al.
\newblock Improving the convergence of back-propagation learning with second
  order methods.
\newblock In \emph{Proceedings of the 1988 connectionist models summer school}.
  San Matteo, CA: Morgan Kaufmann, 1988.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint}, 2016.

\bibitem[Desjardins et~al.(2015)Desjardins, Simonyan, Pascanu,
  et~al.]{desjardins2015natural}
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et~al.
\newblock Natural neural networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Fujimoto \& Ohira(2018)Fujimoto and Ohira]{fujimoto2018neural}
Yuki Fujimoto and Toru Ohira.
\newblock A neural network model with bidirectional whitening.
\newblock In \emph{International Conference on Artificial Intelligence and Soft
  Computing}, pp.\  47--57. Springer, 2018.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[George(2021)]{george_nngeometry}
Thomas George.
\newblock {NNGeometry: Easy and Fast Fisher Information Matrices and Neural
  Tangent Kernels in PyTorch}, February 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.4532597}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Grosse \& Martens(2016)Grosse and Martens]{grosse2016kronecker}
Roger Grosse and James Martens.
\newblock A kronecker-factored approximate fisher matrix for convolution
  layers.
\newblock In \emph{ICML}, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{ICCV}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Heskes(2000)]{heskes2000natural}
Tom Heskes.
\newblock On “natural” learning and pruning in multilayered perceptrons.
\newblock \emph{Neural Computation}, 12\penalty0 (4):\penalty0 881--901, 2000.

\bibitem[Hinton \& Salakhutdinov(2006)Hinton and
  Salakhutdinov]{hinton2006reducing}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Laurent et~al.(2018)Laurent, George, Bouthillier, Ballas, and
  Vincent]{laurent2018evaluation}
C\'{e}sar Laurent, Thomas George, Xavier Bouthillier, Nicolas Ballas, and
  Pascal Vincent.
\newblock An evaluation of fisher approximations beyond kronecker
  factorization.
\newblock \emph{ICLR Workshop}, 2018.

\bibitem[Le~Roux et~al.(2008)Le~Roux, Manzagol, and
  Bengio]{roux2008topmoumoute}
Nicolas Le~Roux, Pierre-Antoine Manzagol, and Yoshua Bengio.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{NIPS}, 2008.

\bibitem[Liu \& Nocedal(1989)Liu and Nocedal]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 1989.

\bibitem[Liu et~al.(2018)Liu, Masana, Herranz, Van~de Weijer, Lopez, and
  Bagdanov]{liu2018rotate}
Xialei Liu, Marc Masana, Luis Herranz, Joost Van~de Weijer, Antonio~M Lopez,
  and Andrew~D Bagdanov.
\newblock Rotate your networks: Better weight consolidation and less
  catastrophic forgetting.
\newblock In \emph{2018 24th International Conference on Pattern Recognition
  (ICPR)}, pp.\  2262--2268. IEEE, 2018.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{ICML}, 2015.

\bibitem[Ollivier(2015)]{ollivier2015riemannian}
Yann Ollivier.
\newblock Riemannian metrics for neural networks i: feedforward networks.
\newblock \emph{Information and Inference: A Journal of the IMA}, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Schraudolph(2001)]{schraudolph2001fast}
Nicol~N Schraudolph.
\newblock Fast curvature matrix-vector products.
\newblock In \emph{International Conference on Artificial Neural Networks}.
  Springer, 2001.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 2012.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
