\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barp et~al.(2019)Barp, Briol, Duncan, Girolami, and Mackey]{barp2019}
Barp, A., Briol, F.-X., Duncan, A., Girolami, M., and Mackey, L.
\newblock Minimum stein discrepancy estimators.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, 2019.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chen et~al.(2011)Chen, Goldstein, and Shao]{chen2011}
Chen, L.~H., Goldstein, L., and Shao, Q.-M.
\newblock \emph{Normal approximation by Stein's method}, volume~2.
\newblock Springer, 2011.

\bibitem[Chwialkowski et~al.(2016)Chwialkowski, Strathmann, and Gretton]{kernelgoodness}
Chwialkowski, K., Strathmann, H., and Gretton, A.
\newblock A kernel test of goodness of fit.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  2606--2615. PMLR, 2016.

\bibitem[Gorham \& Mackey(2015)Gorham and Mackey]{gorham2015}
Gorham, J. and Mackey, L.
\newblock Measuring sample quality with stein\textquotesingle s method.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~28. Curran Associates, Inc., 2015.

\bibitem[Gorham et~al.(2020)Gorham, Raj, and Mackey]{Gorham2020}
Gorham, J., Raj, A., and Mackey, L.
\newblock Stochastic stein discrepancies.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  17931--17942. Curran Associates, Inc., 2020.

\bibitem[Gutmann \& Hyvärinen(2010)Gutmann and Hyvärinen]{gutmann2010}
Gutmann, M. and Hyvärinen, A.
\newblock Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.
\newblock In \emph{Proceedings of the 13th International Conference on Artificial Intelligence and Statistics}, volume~9 of \emph{Proceedings of Machine Learning Research}, pp.\  297--304. PMLR, 2010.

\bibitem[Gutmann \& Hyv{{\"a}}rinen(2012)Gutmann and Hyv{{\"a}}rinen]{gutmann2012}
Gutmann, M.~U. and Hyv{{\"a}}rinen, A.
\newblock Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (11):\penalty0 307--361, 2012.

\bibitem[Hyv{{\"a}}rinen(2005)]{scorematching1}
Hyv{{\"a}}rinen, A.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (24):\penalty0 695--709, 2005.

\bibitem[Hyv{\"a}rinen(2007)]{scorematching2}
Hyv{\"a}rinen, A.
\newblock Some extensions of score matching.
\newblock \emph{Computational statistics \& data analysis}, 51\penalty0 (5):\penalty0 2499--2512, 2007.

\bibitem[Jitkrittum et~al.(2017)Jitkrittum, Xu, Szabo, Fukumizu, and Gretton]{jitkrittum2017}
Jitkrittum, W., Xu, W., Szabo, Z., Fukumizu, K., and Gretton, A.
\newblock A linear-time kernel goodness-of-fit test.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Kalos \& Whitlock(2009)Kalos and Whitlock]{kalos2009monte}
Kalos, M.~H. and Whitlock, P.~A.
\newblock \emph{Monte carlo methods}.
\newblock John Wiley \& Sons, 2009.

\bibitem[Kanagawa et~al.(2019)Kanagawa, Jitkrittum, Mackey, Fukumizu, and Gretton]{kanagawa2019}
Kanagawa, H., Jitkrittum, W., Mackey, L., Fukumizu, K., and Gretton, A.
\newblock A kernel stein test for comparing latent variable models.
\newblock \emph{arXiv preprint arXiv:1907.00586}, 2019.

\bibitem[Krishnamoorthy \& Menon(2013)Krishnamoorthy and Menon]{cholesky}
Krishnamoorthy, A. and Menon, D.
\newblock Matrix inversion using cholesky decomposition.
\newblock In \emph{2013 signal processing: Algorithms, architectures, arrangements, and applications (SPA)}, pp.\  70--72. IEEE, 2013.

\bibitem[Lin et~al.(2016)Lin, Drton, and Shojaie]{highdgraphical}
Lin, L., Drton, M., and Shojaie, A.
\newblock Estimation of high-dimensional graphical models using regularized score matching.
\newblock \emph{Electronic journal of statistics}, 10\penalty0 (1):\penalty0 806, 2016.

\bibitem[Liu et~al.(2016)Liu, Lee, and Jordan]{ksd}
Liu, Q., Lee, J., and Jordan, M.
\newblock A kernelized stein discrepancy for goodness-of-fit tests.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  276--284. PMLR, 2016.

\bibitem[Liu et~al.(2022)Liu, Kanamori, and Williams]{song}
Liu, S., Kanamori, T., and Williams, D.~J.
\newblock Estimating density models with truncation boundaries using score matching.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (186):\penalty0 1--38, 2022.

\bibitem[Pang et~al.(2020)Pang, Xu, Li, Song, Ermon, and Zhu]{pang2020}
Pang, T., Xu, K., Li, C., Song, Y., Ermon, S., and Zhu, J.
\newblock Efficient learning of generative models via finite-difference score matching.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  19175--19188, 2020.

\bibitem[Serfling(2009)]{serfling2009}
Serfling, R.~J.
\newblock \emph{Approximation theorems of mathematical statistics}.
\newblock John Wiley \& Sons, 2009.

\bibitem[Sharrock et~al.(2022)Sharrock, Simons, Liu, and Beaumont]{jack}
Sharrock, L., Simons, J., Liu, S., and Beaumont, M.
\newblock Sequential neural score estimation: Likelihood-free inference with conditional score based diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.04872}, 2022.

\bibitem[Shi et~al.(2021)Shi, Liu, and Mackey]{shi2021}
Shi, J., Liu, C., and Mackey, L.
\newblock Sampling with mirrored stein operators.
\newblock \emph{arXiv preprint arXiv:2106.12506}, 2021.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, 2019.

\bibitem[Song \& Kingma(2021)Song and Kingma]{trainebm}
Song, Y. and Kingma, D.~P.
\newblock How to train your energy-based models.
\newblock \emph{arXiv preprint arXiv:2101.03288}, 2021.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Stein(1972)]{stein1972}
Stein, C.
\newblock A bound for the error in the normal approximation to the distribution of a sum of dependent random variables.
\newblock In \emph{Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 2: Probability theory}, pp.\  583--602. University of California Press, 1972.

\bibitem[Steinwart \& Christmann(2008)Steinwart and Christmann]{steinwart2008support}
Steinwart, I. and Christmann, A.
\newblock \emph{Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[{UCLA: Statistical Consulting Group}(2022)]{statatruncated}
{UCLA: Statistical Consulting Group}.
\newblock Truncated regression | stata data analysis examples, 2022.
\newblock URL \url{https://stats.oarc.ucla.edu/stata/dae/truncated-regression/}.
\newblock Accessed March 17, 2023.

\bibitem[Williams \& Liu(2022)Williams and Liu]{williams2022}
Williams, D.~J. and Liu, S.
\newblock Score matching for truncated density estimation on a manifold.
\newblock In \emph{Topological, Algebraic and Geometric Learning Workshops 2022}, pp.\  312--321. PMLR, 2022.

\bibitem[Wu et~al.(2022)Wu, Diao, Elkhalil, Ding, and Tarokh]{Wu2022}
Wu, S., Diao, E., Elkhalil, K., Ding, J., and Tarokh, V.
\newblock Score-based hypothesis testing for unnormalized models.
\newblock \emph{IEEE Access}, 10:\penalty0 71936--71950, 2022.

\bibitem[Xu(2022)]{bdksd}
Xu, W.
\newblock Standardisation-function kernel stein discrepancy: A unifying view on kernel stein discrepancy tests for goodness-of-fit.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  1575--1597. PMLR, 2022.

\bibitem[Xu \& Matsuda(2021)Xu and Matsuda]{xu2021}
Xu, W. and Matsuda, T.
\newblock Interpretable stein goodness-of-fit tests on riemannian manifold.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  11502--11513. PMLR, 2021.

\bibitem[Yang et~al.(2018)Yang, Liu, Rao, and Neville]{yang2018}
Yang, J., Liu, Q., Rao, V., and Neville, J.
\newblock Goodness-of-fit testing for discrete distributions via stein discrepancy.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  5561--5570. PMLR, 2018.

\bibitem[Yu et~al.(2016)Yu, Kolar, and Gupta]{pairwisegraphical}
Yu, M., Kolar, M., and Gupta, V.
\newblock Statistical inference for pairwise graphical models using score matching.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~29, 2016.

\bibitem[Yu et~al.(2019)Yu, Drton, and Shojaie]{yu2019}
Yu, S., Drton, M., and Shojaie, A.
\newblock Generalized score matching for non-negative data.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0 (76):\penalty0 1--70, 2019.

\bibitem[Yu et~al.(2021)Yu, Drton, and Shojaie]{yu2021}
Yu, S., Drton, M., and Shojaie, A.
\newblock Generalized score matching for general domains.
\newblock \emph{Information and Inference: A Journal of the IMA}, 2021.

\end{thebibliography}
