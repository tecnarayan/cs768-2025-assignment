% NIPS, NeurIPS
% Get bibtex from the proceeding site. 
% e.g.: https://proceedings.neurips.cc/paper/2021/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html
% I remove URL to reduce verbosity
@inproceedings{B2019deep,
 author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Equilibrium Models},
 volume = {32},
 year = {2019}
}

@inproceedings{L2020certified,
 author = {Liu, Xingchao and Han, Xing and Zhang, Na and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15427--15438},
 publisher = {Curran Associates, Inc.},
 title = {Certified Monotonic Neural Networks},
 volume = {33},
 year = {2020}
}

@inproceedings{T2020coupling,
 author = {Teshima, Takeshi and Ishikawa, Isao and Tojo, Koichi and Oono, Kenta and Ikeda, Masahiro and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3362--3373},
 publisher = {Curran Associates, Inc.},
 title = {Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators},
 volume = {33},
 year = {2020}
}


@inproceedings{H2017principles,
 author = {Hauser, Michael and Ray, Asok},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Principles of {R}iemannian Geometry in Neural Networks},
 volume = {30},
 year = {2017}
}
@inproceedings{V2017neural,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 volume = {30},
 year = {2017}
}

@inproceedings{WBC2021,
 author = {Wang, Yixin and Blei, David and Cunningham, John P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {5443--5455},
 publisher = {Curran Associates, Inc.},
 title = {Posterior Collapse and Latent Variable Non-identifiability},
 volume = {34},
 year = {2021}
}
@inproceedings{L2017simple,
 author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 volume = {30},
 year = {2017}
}
@inproceedings{Z2022rethinking,
 author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {19398--19413},
 publisher = {Curran Associates, Inc.},
 title = {Rethinking {L}ipschitz Neural Networks and Certified Robustness: A Boolean Function Perspective},
 volume = {35},
 year = {2022}
}

@inproceedings{L2019certified,
 author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Certified Adversarial Robustness with Additive Noise},
 volume = {32},
 year = {2019}
}

@inproceedings{B2017safe,
 author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Safe Model-based Reinforcement Learning with Stability Guarantees},
 volume = {30},
 year = {2017}
}

@inproceedings{B2022agreementontheline,
 author = {Baek, Christina and Jiang, Yiding and Raghunathan, Aditi and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {19274--19289},
 publisher = {Curran Associates, Inc.},
 title = {Agreement-on-the-line: Predicting the Performance of Neural Networks under Distribution Shift},
 volume = {35},
 year = {2022}
}

@inproceedings{X2022lot,
 author = {Xu, Xiaojun and Li, Linyi and Li, Bo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {18904--18915},
 publisher = {Curran Associates, Inc.},
 title = {LOT: Layer-wise Orthogonal Training on Improving $L_2$ Certified Robustness},
 volume = {35},
 year = {2022}
}

@inproceedings{L2019preventing,
 author = {Li, Qiyang and Haque, Saminul and Anil, Cem and Lucas, James and Grosse, Roger B and Jacobsen, Joern-Henrik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Preventing Gradient Attenuation in {L}ipschitz Constrained Convolutional Networks},
 volume = {32},
 year = {2019}
}
@inproceedings{G2017improved,
 author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Training of {W}asserstein {GAN}s},
 volume = {30},
 year = {2017}
}

@inproceedings{F2023certified,
  title={Certified Robustness via Dynamic Margin Maximization and Improved {L}ipschitz Regularization},
  author={Fazlyab, Mahyar and Entesari, Taha and Roy, Aniket and Chellappa, Rama},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{F2019efficient,
 author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Efficient and Accurate Estimation of {L}ipschitz Constants for Deep Neural Networks},
 volume = {32},
 year = {2019}
}

@inproceedings{S2018lipschitz,
 author = {Scaman, Kevin and Virmaux, Aladin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Lipschitz regularity of deep neural networks: Analysis and efficient estimation},
 volume = {31},
 year = {2018}
}
@InProceedings{R2020case,
  title = 	 {A case for new neural network smoothness constraints},
  author =       {Rosca, Mihaela and Weber, Theophane and Gretton, Arthur and Mohamed, Shakir},
  booktitle = 	 {Proceedings on ``I Can't Believe It's Not Better!" at NeurIPS Workshops},
  pages = 	 {21--32},
  year = 	 {2020},
  editor = 	 {Zosa Forde, Jessica and Ruiz, Francisco and Pradier, Melanie F. and Schein, Aaron},
  volume = 	 {137},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v137/rosca20a/rosca20a.pdf},
  abstract = 	 {How sensitive should machine learning models be to input changes? We tackle the question of model smoothness and show that it is a useful inductive bias which aids generalization, adversarial robustness, generative modeling and reinforcement learning. We explore current methods of imposing smoothness constraints and observe they lack the flexibility to adapt to new tasks, they don’t account for data modalities, they interact with losses, architectures and optimization in ways not yet fully understood. We conclude that new advances in the field are hinging on finding ways to incorporate data, tasks and learning into our definitions of smoothness.}
}

@inproceedings{T2018lipschitz,
 author = {Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks},
 volume = {31},
 year = {2018}
}
@inproceedings{L2020simple,
 author = {Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7498--7512},
 publisher = {Curran Associates, Inc.},
 title = {Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness},
 volume = {33},
 year = {2020}
}

@inproceedings{YR2020,
 author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8588--8601},
 publisher = {Curran Associates, Inc.},
 title = {A Closer Look at Accuracy vs. Robustness},
 volume = {33},
 year = {2020}
}

@inproceedings{HTLC2018,
 author = {Huang, Chin-Wei and Tan, Shawn and Lacoste, Alexandre and Courville, Aaron C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improving Explorability in Variational Inference with Annealed Variational Objectives},
 volume = {31},
 year = {2018}
}
 % url = {https://proceedings.neurips.cc/paper/2018/file/65b0df23fd2d449ae1e4b2d27151d73b-Paper.pdf},

@inproceedings{VV2017,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 volume = {30},
 year = {2017}
}
 % url = {https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},

@inproceedings{LT2019,
 author = {Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Don\textquotesingle t Blame the {ELBO}! {A} linear {VAE} Perspective on Posterior Collapse},
 volume = {32},
 year = {2019}
}
 % url = {https://proceedings.neurips.cc/paper/2019/file/7e3315fe390974fcf25e44a9445bd821-Paper.pdf},

@inproceedings{B2017spectrally,
 author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Spectrally-normalized margin bounds for neural networks},
 volume = {30},
 year = {2017}
}

% arXiv -> NIPS 2016
@inproceedings{SRMSW2016,
 author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Ladder Variational Autoencoders},
 volume = {29},
 year = {2016}
}
 % url = {https://proceedings.neurips.cc/paper/2016/file/6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf},
% article{SRMSW2016,
%   title={How to train deep variational autoencoders and probabilistic ladder networks},
%   author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
%   journal={arXiv preprint arXiv:1602.02282},
%   volume={3},
%   number={2},
%   year={2016}
% }

% arxiv -> NeuIPS2022 (proceeding is not available yet)
@inproceedings{
WZ2022,
title={Posterior Collapse of a Linear Latent Variable Model},
author={Zihao Wang and Liu Ziyin},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}
% url={https://openreview.net/forum?id=zAc2a6_0aHb}
% article{WZ2022,
%   title={Posterior Collapse of a Linear Latent Variable Model},
%   author={Wang, Zihao and Ziyin, Liu},
%   journal={arXiv preprint arXiv:2205.04009},
%   year={2022}
% }
@inproceedings{LW2022,
  title={Alleviating ``Posterior Collapse'' in Deep Topic Models via Policy Gradient},
  author={Li, Yewen and Wang, Chaojie and Duan, Zhibin and Wang, Dongsheng and Chen, Bo and An, Bo and Zhou, Mingyuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
}
@inproceedings{VW2019,
  title={Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices},
  author={Vempala, Santosh and Wibisono, Andre},
  booktitle={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% -------------------------------------------------------------
% ICLR
@article{A2022amortizing,
  title={On amortizing convex conjugates for optimal transport},
  author={Amos, Brandon},
  journal={International Conference on Learning Representations},
  year={2023}
}
@article{M2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{K2023scalable,
  title={Scalable Monotonic Neural Networks},
  author={Kim, Hyunho and Lee, Jong-Seok},
  booktitle={International Conference on Learning Representations},
  year={2024}
}
@article{J2018excessive,
  title={Excessive invariance causes adversarial vulnerability},
  author={Jacobsen, J{\"o}rn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
  journal={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{N2023expressive,
  title={Expressive Monotonic Neural Networks},
  author={Nolte, Niklas and Kitouni, Ouail and Williams, Mike},
  booktitle={International Conference on Learning Representations},
  year={2023}
}
@article{D2016density,
  title={Density estimation using real {NVP}},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={International Conference on Learning Representations},
  year={2017}
}
@article{N2017variational,
  title={Variational continual learning},
  author={Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  journal={International Conference on Learning Representations},
  year={2018}
}
@article{K2015adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}
@inproceedings{Y2022constructing,
  title={Constructing orthogonal convolutions in an explicit manner},
  author={Yu, Tan and Li, Jun and Cai, Yunfeng and Li, Ping},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@article{T2021orthogonalizing,
  title={Orthogonalizing convolutional layers with the cayley transform},
  author={Trockman, Asher and Kolter, J Zico},
  journal={International Conference on Learning Representations},
  year={2021}
}
@article{M2018spectral,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}
@article{F2019generalizable,
  title={Generalizable adversarial training via spectral normalization},
  author={Farnia, Farzan and Zhang, Jesse M and Tse, David},
  journal={International Conference on Learning Representations},
  year={2019}
}
@article{A2023unified,
  title={A unified algebraic perspective on {L}ipschitz neural networks},
  author={Araujo, Alexandre and Havens, Aaron and Delattre, Blaise and Allauzen, Alexandre and Hu, Bin},
  journal={International Conference on Learning Representations},
  year={2023}
}
@article{H2020convex,
  title={Convex potential flows: Universal probability distributions with optimal transport and convex optimization},
  author={Huang, Chin-Wei and Chen, Ricky TQ and Tsirigotis, Christos and Courville, Aaron},
  journal={International Conference on Learning Representations},
  year={2021}
}
@article{KW2013,
  title={Auto-encoding variational {Bayes}},
  author={Kingma, Diederik P and Welling, Max},
  journal={International Conference on Learning Representations},
  year={2014}
}
@inproceedings{YK2019,
  title={Distributional concavity regularization for {GAN}s},
  author={Yamaguchi, Shoichiro and Koyama, Masanori},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{DS2016,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={International Conference on Learning Representations},
  year={2016}
}
@article{DM2016,
  title={Deep unsupervised clustering with gaussian mixture variational autoencoders},
  author={Dilokthanakul, Nat and Mediano, Pedro AM and Garnelo, Marta and Lee, Matthew CH and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  journal={International Conference on Learning Representations},
  year={2016}
}
% arXiv -> ICLR2016
@inproceedings{
CKSDDSSA2016,
title={Variational Lossy Autoencoder},
author={Xi Chen and Diederik P. Kingma and Tim Salimans and Yan Duan and Prafulla Dhariwal and John Schulman and Ilya Sutskever and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2017},
}
% url={https://openreview.net/forum?id=BysvGP5ee}
% article{CKSDDSSA2016,
%   title={Variational lossy autoencoder},
%   author={Chen, Xi and Kingma, Diederik P and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
%   journal={arXiv preprint arXiv:1611.02731},
%   year={2016}
% }

% arxiv -> ICLR2019
@inproceedings{
RO2019,
title={Preventing Posterior Collapse with delta-{VAE}s},
author={Ali Razavi and Aaron van den Oord and Ben Poole and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2019},
}
% url={https://openreview.net/forum?id=BJe0Gn0cY7},
% article{RO2019,
%   title={Preventing posterior collapse with delta-vaes},
%   author={Razavi, Ali and Oord, A{\"a}ron van den and Poole, Ben and Vinyals, Oriol},
%   journal={arXiv preprint arXiv:1901.03416},
%   year={2019}
% }

% arxiv -> ICLR2019
@inproceedings{
HS2019,
title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},
author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
booktitle={International Conference on Learning Representations},
year={2019},
}
% url={https://openreview.net/forum?id=rylDfnCqF7},
% article{HS2019,
%   title={Lagging inference networks and posterior collapse in variational autoencoders},
%   author={He, Junxian and Spokoyny, Daniel and Neubig, Graham and Berg-Kirkpatrick, Taylor},
%   journal={arXiv preprint arXiv:1901.05534},
%   year={2019}
% }

% arxiv -> ICLR2017
@inproceedings{
GK2016,
title={Pixel{VAE}: A Latent Variable Model for Natural Images},
author={Ishaan Gulrajani and Kundan Kumar and Faruk Ahmed and Adrien Ali Taiga and Francesco Visin and David Vazquez and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2017},
}
% url={https://openreview.net/forum?id=BJKYvt5lg}
% article{GK2016,
%   title={Pixelvae: A latent variable model for natural images},
%   author={Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
%   journal={arXiv preprint arXiv:1611.05013},
%   year={2016}
% }

@inproceedings{
CS2018,
title={Optimal Control Via Neural Networks: A Convex Approach},
author={Yize Chen and Yuanyuan Shi and Baosen Zhang},
booktitle={International Conference on Learning Representations},
year={2019},
}
% url={https://openreview.net/forum?id=H1MW72AcK7},
% article{CS2018,
%   title={Optimal control via neural networks: A convex approach},
%   author={Chen, Yize and Shi, Yuanyuan and Zhang, Baosen},
%   journal={arXiv preprint arXiv:1805.11835},
%   year={2018}
% }

% ??? ->ICLR2017
@inproceedings{
HM2016,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017},
}
% url={https://openreview.net/forum?id=Sy2fzU9gl}
% article{HM2016,
%   title={beta-vae: Learning basic visual concepts with a constrained variational framework},
%   author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
%   year={2016}
% }


% -------------------------------------------------------------
% ICML
% We get bibtex from JMLR site
% e.g.: https://proceedings.mlr.press/v32/rezende14.html
@InProceedings{W2024monotone,
  title = 	 {Monotone, Bi-Lipschitz, and Polyak-{Ł}ojasiewicz Networks},
  author =       {Wang, Ruigang and Dvijotham, Krishnamurthy Dj and Manchester, Ian},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {50379--50399},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24p/wang24p.pdf},
  abstract = 	 {This paper presents a new <em>bi-Lipschitz</em> invertible neural network, the BiLipNet, which has the ability to smoothly control both its <em>Lipschitzness</em> (output sensitivity to input perturbations) and <em>inverse Lipschitzness</em> (input distinguishability from different outputs). The second main contribution is a new scalar-output network, the PLNet, which is a composition of a BiLipNet and a quadratic potential. We show that PLNet satisfies the Polyak-Łojasiewicz condition and can be applied to learn non-convex surrogate losses with a unique and efficiently-computable global minimum. The central technical element in these networks is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build the BiLipNet. The certification of these properties is based on incremental quadratic constraints, resulting in much tighter bounds than can be achieved with spectral normalization. Moreover, we formulate the calculation of the inverse of a BiLipNet – and hence the minimum of a PLNet – as a series of three-operator splitting problems, for which fast algorithms can be applied.}
}
@InProceedings{R2023constrained,
  title = 	 {Constrained Monotonic Neural Networks},
  author =       {Runje, Davor and Shankaranarayana, Sharath M},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {29338--29353},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  abstract = 	 {Wider adoption of neural networks in many critical domains such as finance and healthcare is being hindered by the need to explain their predictions and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain signs on its weights. Unfortunately, this construction does not work with popular non-saturated activation functions as it can only approximate convex functions. We show this shortcoming can be fixed by constructing two additional activation functions from a typical unsaturated monotonic activation function and employing each of them on the part of neurons. Our experiments show this approach of building monotonic neural networks has better accuracy when compared to other state-of-the-art methods, while being the simplest one in the sense of having the least number of parameters, and not requiring any modifications to the learning procedure or post-learning steps. Finally, we prove it can approximate any continuous monotone function on a compact subset of $\mathbb{R}^n$.}
}


@InProceedings{Z2020approximation,
  title = 	 {Approximation Capabilities of Neural {ODE}s and Invertible Residual Networks},
  author =       {Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11086--11095},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20h/zhang20h.pdf},
  abstract = 	 {Recent interest in invertible models and normalizing flows has resulted in new architectures that ensure invertibility of the network model. Neural ODEs and i-ResNets are two recent techniques for constructing models that are invertible, but it is unclear if they can be used to approximate any continuous invertible mapping. Here, we show that out of the box, both of these architectures are limited in their approximation capabilities. We then show how to overcome this limitation: we prove that any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space. We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear layer is sufficient to turn the model into a universal approximator for non-invertible continuous functions.}
}

@InProceedings{G2016dropout,
  title = 	 {Dropout as a {B}ayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@InProceedings{R2015variational,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}

@InProceedings{W2023direct,
  title = 	 {Direct Parameterization of {L}ipschitz-Bounded Deep Networks},
  author =       {Wang, Ruigang and Manchester, Ian},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {36093--36110},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wang23v/wang23v.pdf},
  abstract = 	 {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed $\ell^2$ Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP). We provide a “direct” parameterization, i.e., a smooth mapping from $\mathbb R^N$ onto the set of weights satisfying the SDP-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the SDP bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the SDP constraint. The new parameterization can equivalently be thought of as either a new layer type (the <em>sandwich layer</em>), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. A comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on both empirical and certified robust accuracy. Code is available at https://github.com/acfr/LBDN.}
}

@InProceedings{A2019sorting,
  title = 	 {Sorting Out {L}ipschitz Function Approximation},
  author =       {Anil, Cem and Lucas, James and Grosse, Roger},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {291--301},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/anil19a/anil19a.pdf},
  abstract = 	 {Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy.}
}

@InProceedings{C2017parseval,
  title = 	 {Parseval Networks: Improving Robustness to Adversarial Examples},
  author =       {Moustapha Cisse and Piotr Bojanowski and Edouard Grave and Yann Dauphin and Nicolas Usunier},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {854--863},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/cisse17a/cisse17a.pdf},
  abstract = 	 {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than $1$. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.}
}


@InProceedings{L2021globally,
  title = 	 {Globally-Robust Neural Networks},
  author =       {Leino, Klas and Wang, Zifan and Fredrikson, Matt},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6212--6222},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/leino21a/leino21a.pdf},
  abstract = 	 {The threat of adversarial examples has motivated work on training certifiably robust neural networks to facilitate efficient verification of local robustness at inference time. We formalize a notion of global robustness, which captures the operational properties of on-line local robustness certification while yielding a natural learning objective for robust training. We show that widely-used architectures can be easily adapted to this objective by incorporating efficient global Lipschitz bounds into the network, yielding certifiably-robust models by construction that achieve state-of-the-art verifiable accuracy. Notably, this approach requires significantly less time and memory than recent certifiable training methods, and leads to negligible costs when certifying points on-line; for example, our evaluation shows that it is possible to train a large robust Tiny-Imagenet model in a matter of hours. Our models effectively leverage inexpensive global Lipschitz bounds for real-time certification, despite prior suggestions that tighter local bounds are needed for good performance; we posit this is possible because our models are specifically trained to achieve tighter global bounds. Namely, we prove that the maximum achievable verifiable accuracy for a given dataset is not improved by using a local bound.}
}


@InProceedings{S2021skew,
  title = 	 {Skew Orthogonal Convolutions},
  author =       {Singla, Sahil and Feizi, Soheil},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9756--9766},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/singla21a/singla21a.pdf},
  abstract = 	 {Training convolutional neural networks with a Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients, stable training, etc. While 1-Lipschitz networks can be designed by imposing a 1-Lipschitz constraint on each layer, training such networks requires each layer to be gradient norm preserving (GNP) to prevent gradients from vanishing. However, existing GNP convolutions suffer from slow training, lead to significant reduction in accuracy and provide no guarantees on their approximations. In this work, we propose a GNP convolution layer called \textbf{S}kew \textbf{O}rthogonal \textbf{C}onvolution (SOC) that uses the following mathematical property: when a matrix is {\it Skew-Symmetric}, its exponential function is an {\it orthogonal} matrix. To use this property, we first construct a convolution filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series expansion of the Jacobian exponential to construct the SOC layer that is orthogonal. To efficiently implement SOC, we keep a finite number of terms from the Taylor series and provide a provable guarantee on the approximation error. Our experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train provably Lipschitz, large convolutional neural networks significantly faster than prior works while achieving significant improvements for both standard and certified robust accuracies.}
}


@article{R2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle = 	 {Proceedings of the 29st International Conference on Machine Learning},
  year={2012},
  series = 	 {Proceedings of Machine Learning Research},
}

@InProceedings{M2022dynamical,
  title = 	 {A Dynamical System Perspective for {L}ipschitz Neural Networks},
  author =       {Meunier, Laurent and Delattre, Blaise J and Araujo, Alexandre and Allauzen, Alexandre},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15484--15500},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/meunier22a/meunier22a.pdf},
  abstract = 	 {The Lipschitz constant of neural networks has been established as a key quantity to enforce the robustness to adversarial examples. In this paper, we tackle the problem of building $1$-Lipschitz Neural Networks. By studying Residual Networks from a continuous time dynamical system perspective, we provide a generic method to build $1$-Lipschitz Neural Networks and show that some previous approaches are special cases of this framework. Then, we extend this reasoning and show that ResNet flows derived from convex potentials define $1$-Lipschitz transformations, that lead us to define the <em>Convex Potential Layer</em> (CPL). A comprehensive set of experiments on several datasets demonstrates the scalability of our architecture and the benefits as an $\ell_2$-provable defense against adversarial examples. Our code is available at \url{https://github.com/MILES-PSL/Convex-Potential-Layer}}
}

@InProceedings{A2017wasserstein,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}

@InProceedings{V2020uncertainty,
  title = 	 {Uncertainty Estimation Using a Single Deep Deterministic Neural Network},
  author =       {Van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9690--9700},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/van-amersfoort20a/van-amersfoort20a.pdf},
  abstract = 	 {We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.}
}


@InProceedings{RMW2014,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32(2)},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}
  % remove number field and add the number information to volume field because of the following error occurred:
  % > can't use both volume and number fields in RMW2014
  % number =       {2},
  % 
  % url = 	 {https://proceedings.mlr.press/v32/rezende14.html},
% inproceedings{RMW2014,
%   title={Stochastic backpropagation and approximate inference in deep generative models},
%   author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
%   booktitle={International conference on machine learning},
%   pages={1278--1286},
%   year={2014},
%   organization={PMLR}
% }

@InProceedings{K2023,
  title={Controlling Posterior Collapse by an Inverse {L}ipschitz Constraint on the Decoder Network},
  author={Kinoshita, Yuri and Oono, Kenta and Fukumizu, Kenji and Yoshida, Yuichi and Maeda, Shin-ichi},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  journal={arXiv preprint arXiv:2304.12770},
  year={2023},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR}
}

@InProceedings{S2013stochastic,
  title = 	 {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},
  author = 	 {Shamir, Ohad and Zhang, Tong},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {71--79},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  abstract = 	 {Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD \emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \emphlast SGD iterate scales as O(\log(T)/\sqrtT) for non-smooth convex objective functions, and O(\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.}
}

@InProceedings{DWW2020,
  title = 	 {The Usual Suspects? {R}eassessing Blame for {VAE} Posterior Collapse},
  author =       {Dai, Bin and Wang, Ziyu and Wipf, David},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2313--2322},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/dai20c/dai20c.pdf},
  abstract = 	 {In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.}
}
  % url = 	 {https://proceedings.mlr.press/v119/dai20c.html},
% inproceedings{DWW2020,
%   title={The usual suspects? Reassessing blame for VAE posterior collapse},
%   author={Dai, Bin and Wang, Ziyu and Wipf, David},
%   booktitle={International Conference on Machine Learning},
%   pages={2313--2322},
%   year={2020},
%   organization={PMLR}
% }


@InProceedings{KW2018,
  title = 	 {Semi-Amortized Variational Autoencoders},
  author =       {Kim, Yoon and Wiseman, Sam and Miller, Andrew and Sontag, David and Rush, Alexander},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2678--2687},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18e/kim18e.pdf},
  abstract = 	 {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.}
}
  % url = 	 {https://proceedings.mlr.press/v80/kim18e.html},
% inproceedings{KW2018,
%   title={Semi-amortized variational autoencoders},
%   author={Kim, Yoon and Wiseman, Sam and Miller, Andrew and Sontag, David and Rush, Alexander},
%   booktitle={International Conference on Machine Learning},
%   pages={2678--2687},
%   year={2018},
%   organization={PMLR}
% }

@InProceedings{YH2017,
  title = 	 {Improved Variational Autoencoders for Text Modeling using Dilated Convolutions},
  author =       {Zichao Yang and Zhiting Hu and Ruslan Salakhutdinov and Taylor Berg-Kirkpatrick},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3881--3890},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/yang17d/yang17d.pdf},
  abstract = 	 {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.}
}
  % url = 	 {https://proceedings.mlr.press/v70/yang17d.html},
% inproceedings{YH2017,
%   title={Improved variational autoencoders for text modeling using dilated convolutions},
%   author={Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
%   booktitle={International conference on machine learning},
%   pages={3881--3890},
%   year={2017},
%   organization={PMLR}
% }


@InProceedings{AX2017,
  title = 	 {Input Convex Neural Networks},
  author =       {Brandon Amos and Lei Xu and J. Zico Kolter},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {146--155},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/amos17b/amos17b.pdf},
  abstract = 	 {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.}
}
  % url = 	 {https://proceedings.mlr.press/v70/amos17b.html},
% inproceedings{AX2017,
%   title={Input convex neural networks},
%   author={Amos, Brandon and Xu, Lei and Kolter, J Zico},
%   booktitle={International Conference on Machine Learning},
%   pages={146--155},
%   year={2017},
%   organization={PMLR}
% }


@InProceedings{B2019invertible,
  title = 	 {Invertible Residual Networks},
  author =       {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {573--582},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.}
}

-------------------------------------------------------------
% SIGNLL
@inproceedings{BVVDJB2016,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
      Vilnis, Luke  and
      Vinyals, Oriol  and
      Dai, Andrew  and
      Jozefowicz, Rafal  and
      Bengio, Samy",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}
    % url = "https://aclanthology.org/K16-1002",
% inproceedings{BVVDJB2016,
%   title={Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning},
%   author={Bowman, SR and Vilnis, L and Vinyals, O and Dai, AM and Jozefowicz, R and Bengio, S},
%   year={2016},
%   organization={Association for Computational Linguistics}
% }

% -------------------------------------------------------------
% ACL
% arXiv -> ACL 2018
@inproceedings{ZLE2018,
    title = "Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation",
    author = "Zhao, Tiancheng  and
      Lee, Kyusong  and
      Eskenazi, Maxine",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P18-1101",
    pages = "1098--1107",
    abstract = "The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.",
}
    % url = "https://aclanthology.org/P18-1101",
% article{ZLE2018,
%   title={Unsupervised discrete sentence representation learning for interpretable neural dialog generation},
%   author={Zhao, Tiancheng and Lee, Kyusong and Eskenazi, Maxine},
%   journal={arXiv preprint arXiv:1804.08069},
%   year={2018}
% }


% -------------------------------------------------------------
% AISTATS
% Get bibtex from JMLR site
% e.g.: https://proceedings.mlr.press/v89/bauer19a.html

@InProceedings{X2010understanding,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@InProceedings{B2021understanding,
  title = 	 { Understanding and Mitigating Exploding Inverses in Invertible Neural Networks },
  author =       {Behrmann, Jens and Vicol, Paul and Wang, Kuan-Chieh and Grosse, Roger and Jacobsen, Joern-Henrik},
  booktitle = 	 {Proceedings of the 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1792--1800},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/behrmann21a/behrmann21a.pdf},
  abstract = 	 { Invertible neural networks (INNs) have been used to design generative models, implement memory-saving gradient computation, and solve inverse problems. In this work, we show that commonly-used INN architectures suffer from exploding inverses and are thus prone to becoming numerically non-invertible. Across a wide range of INN use-cases, we reveal failures including the non-applicability of the change-of-variables formula on in- and out-of-distribution (OOD) data, incorrect gradients for memory-saving backprop, and the inability to sample from normalizing flow models. We further derive bi-Lipschitz properties of atomic building blocks of common architectures. These insights into the stability of INNs then provide ways forward to remedy these failures. For tasks where local invertibility is sufficient, like memory-saving backprop, we propose a flexible and efficient regularizer. For problems where global invertibility is necessary, such as applying normalizing flows on OOD data, we show the importance of designing stable INN building blocks. }
}

@InProceedings{BM2019,
  title = 	 {Resampled Priors for Variational Autoencoders},
  author =       {Bauer, Matthias and Mnih, Andriy},
  booktitle = 	 {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics},
  pages = 	 {66--75},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/bauer19a/bauer19a.pdf},
  abstract = 	 {We propose Learned Accept/Reject Sampling (LARS), a method for constructing richer priors using rejection sampling with a learned acceptance function. This work is motivated by recent analyses of the VAE objective, which pointed out that commonly used simple priors can lead to underfitting. As the distribution induced by LARS involves an intractable normalizing constant, we show how to estimate it and its gradients efficiently. We demonstrate that LARS priors improve VAE performance on several standard datasets both when they are learned jointly with the rest of the model and when they are fitted to a pretrained model. Finally, we show that LARS can be combined with existing methods for defining flexible priors for an additional boost in performance.}
}
%   url = 	 {https://proceedings.mlr.press/v89/bauer19a.html},
% inproceedings{BM2019,
%   title={Resampled priors for variational autoencoders},
%   author={Bauer, Matthias and Mnih, Andriy},
%   booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
%   pages={66--75},
%   year={2019},
%   organization={PMLR}
% }

@InProceedings{TW2018,
  title = 	 {VAE with a {V}amp{P}rior},
  author = 	 {Tomczak, Jakub and Welling, Max},
  booktitle = 	 {Proceedings of the 21st International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1214--1223},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/tomczak18a/tomczak18a.pdf},
  abstract = 	 {Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.}
}
%   url = 	 {https://proceedings.mlr.press/v84/tomczak18a.html},
% inproceedings{TW2018,
%   title={VAE with a VampPrior},
%   author={Tomczak, Jakub and Welling, Max},
%   booktitle={International Conference on Artificial Intelligence and Statistics},
%   pages={1214--1223},
%   year={2018},
%   organization={PMLR}
% }

@InProceedings{DK2019,
  title = 	 {Avoiding Latent Variable Collapse with Generative Skip Models},
  author =       {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2397--2405},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/dieng19a/dieng19a.pdf},
  abstract = 	 {Variational autoencoders (VAEs) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.}
}
%   url = 	 {https://proceedings.mlr.press/v89/dieng19a.html},
% inproceedings{DK2019,
%   title={Avoiding latent variable collapse with generative skip models},
%   author={Dieng, Adji B and Kim, Yoon and Rush, Alexander M and Blei, David M},
%   booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
%   pages={2397--2405},
%   year={2019},
%   organization={PMLR}
% }

@InProceedings{EH2021,
  title = 	 { Fisher Auto-Encoders },
  author =       {Elkhalil, Khalil and Hasan, Ali and Ding, Jie and Farsiu, Sina and Tarokh, Vahid},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {352--360},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/elkhalil21a/elkhalil21a.pdf},
  abstract = 	 { It has been conjectured that the Fisher divergence is more robust to model uncertainty than the conventional Kullback-Leibler (KL) divergence. This motivates the design of a new class of robust generative auto-encoders (AE) referred to as Fisher auto-encoders. Our approach is to design Fisher AEs by minimizing the Fisher divergence between the intractable joint distribution of observed data and latent variables, with that of the postulated/modeled joint distribution. In contrast to KL-based variational AEs (VAEs), the Fisher AE can exactly quantify the distance between the true and the model-based posterior distributions. Qualitative and quantitative results are provided on both MNIST and celebA datasets demonstrating the competitive performance of Fisher AEs in terms of robustness compared to other AEs such as VAEs and Wasserstein AEs. }
}
%   url = 	 {https://proceedings.mlr.press/v130/elkhalil21a.html},
% inproceedings{EH2021,
%   title={Fisher Auto-Encoders},
%   author={Elkhalil, Khalil and Hasan, Ali and Ding, Jie and Farsiu, Sina and Tarokh, Vahid},
%   booktitle={International Conference on Artificial Intelligence and Statistics},
%   pages={352--360},
%   year={2021},
%   organization={PMLR}
% }


% -------------------------------------------------------------
% NAACL
% Get bibtes from ACL Anthology
% e.g.: https://aclanthology.org/N19-1021/

% arXiv -> NAACL
@inproceedings{FLLGCC2019,
    title = "Cyclical Annealing Schedule: A Simple Approach to Mitigating {KL} Vanishing",
    author = "Fu, Hao  and
      Li, Chunyuan  and
      Liu, Xiaodong  and
      Gao, Jianfeng  and
      Celikyilmaz, Asli  and
      Carin, Lawrence",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1021",
    pages = "240--250",
    abstract = "Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter $\beta$. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for $\beta$, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing $\beta$ multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.",
}
    % url = "https://aclanthology.org/N19-1021",
% article{FLLGCC2019,
%   title={Cyclical annealing schedule: A simple approach to mitigating kl vanishing},
%   author={Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
%   journal={arXiv preprint arXiv:1903.10145},
%   year={2019}
% }


% -------------------------------------------------------------
% EMNLP
% arxiv -> EMNLP2018
@inproceedings{XD2018,
    title = "Spherical Latent Spaces for Stable Variational Autoencoders",
    author = "Xu, Jiacheng  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D18-1480",
    pages = "4503--4513",
    abstract = "A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of {``}collapse{''} which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.",
}
    % url = "https://aclanthology.org/D18-1480",
% article{XD2018,
%   title={Spherical latent spaces for stable variational autoencoders},
%   author={Xu, Jiacheng and Durrett, Greg},
%   journal={arXiv preprint arXiv:1808.10805},
%   year={2018}
% }

% arxiv -> EMNLP2019
@inproceedings{LH2019,
    title = "A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text",
    author = "Li, Bohan  and
      He, Junxian  and
      Neubig, Graham  and
      Berg-Kirkpatrick, Taylor  and
      Yang, Yiming",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D19-1370",
    pages = "3603--3614",
    abstract = "When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.",
}
    % url = "https://aclanthology.org/D19-1370",
% article{LH2019,
%   title={A surprisingly effective fix for deep latent variable modeling of text},
%   author={Li, Bohan and He, Junxian and Neubig, Graham and Berg-Kirkpatrick, Taylor and Yang, Yiming},
%   journal={arXiv preprint arXiv:1909.00868},
%   year={2019}
% }


% -------------------------------------------------------------
% Elsevier
% Get bibtex from Elsevier site
% e.g.: https://www.sciencedirect.com/science/article/pii/S0925231222010591

@article{TL2022,
title = {Preventing oversmoothing in {VAE} via generalized variance parameterization},
journal = {Neurocomputing},
volume = {509},
pages = {137-156},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.08.067},
author = {Yuhta Takida and Wei-Hsiang Liao and Chieh-Hsin Lai and Toshimitsu Uesaka and Shusuke Takahashi and Yuki Mitsufuji},
keywords = {Bayesian inference, Gaussian model, Variational autoencoders, Posterior collapse, Decoder variance, Maximum likelihood estimation},
abstract = {Variational autoencoders (VAEs) often suffer from posterior collapse, which is a phenomenon in which the learned latent space becomes uninformative. This is often related to the hyperparameter resembling the data variance. It can be shown that an inappropriate choice of this hyperparameter causes the oversmoothness in the linearly approximated case and can be empirically verified for the general cases. Moreover, determining such appropriate choice becomes infeasible if the data variance is non-uniform or conditional. Therefore, we propose VAE extensions with generalized parameterizations of the data variance and incorporate maximum likelihood estimation into the objective function to adaptively regularize the decoder smoothness. The images generated from proposed VAE extensions show improved Fréchet inception distance (FID) on MNIST and CelebA datasets.}
}
% url = {https://www.sciencedirect.com/science/article/pii/S0925231222010591},
% article{TL2022,
%   title={Preventing oversmoothing in VAE via generalized variance parameterization},
%   author={Takida, Yuhta and Liao, Wei-Hsiang and Lai, Chieh-Hsin and Uesaka, Toshimitsu and Takahashi, Shusuke and Mitsufuji, Yuki},
%   journal={Neurocomputing},
%   volume={509},
%   pages={137--156},
%   year={2022},
%   publisher={Elsevier}
% }

@article{OV2000,
title = {Generalization of an Inequality by {T}alagrand and Links with the Logarithmic {S}obolev Inequality},
journal = {Journal of Functional Analysis},
volume = {173},
number = {2},
pages = {361-400},
year = {2000},
issn = {0022-1236},
doi = {https://doi.org/10.1006/jfan.1999.3557},
author = {F. Otto and C. Villani},
abstract = {We show that transport inequalities, similar to the one derived by M. Talagrand (1996, Geom. Funct. Anal.6, 587–600) for the Gaussian measure, are implied by logarithmic Sobolev inequalities. Conversely, Talagrand's inequality implies a logarithmic Sobolev inequality if the density of the measure is approximately log-concave, in a precise sense. All constants are independent of the dimension and optimal in certain cases. The proofs are based on partial differential equations and an interpolation inequality involving the Wasserstein distance, the entropy functional, and the Fisher information.}
}
% url = {https://www.sciencedirect.com/science/article/pii/S0022123699935577},
% article{OV2000,
%   title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
%   author={Otto, Felix and Villani, C{\'e}dric},
%   journal={Journal of Functional Analysis},
%   volume={173},
%   number={2},
%   pages={361--400},
%   year={2000},
%   publisher={Elsevier}
% }

@article{W2016,
title = {Bayesian information in an experiment and the {F}isher information distance},
journal = {Statistics \& Probability Letters},
volume = {112},
pages = {5-9},
year = {2016},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2016.01.014},
author = {Stephen G. Walker},
keywords = {Shannon entropy, Information, Bayesian design},
abstract = {There are two forms of Fisher information; for the parameter of a model and for the information in a density model. These two forms are shown to be fundamentally connected through a measure of gain in information from a Bayesian experiment.}
}
% url = {https://www.sciencedirect.com/science/article/pii/S0167715216000109},
% article{W2016,
%   title={Bayesian information in an experiment and the Fisher information distance},
%   author={Walker, Stephen G},
%   journal={Statistics \& Probability Letters},
%   volume={112},
%   pages={5--9},
%   year={2016},
%   publisher={Elsevier}
% }


% -------------------------------------------------------------
% Oxford University Press
% Get bibtex from OUP site
% e.g.: https://academic.oup.com/biomet/article-abstract/104/2/497/3074978?redirectedFrom=fulltext

@article{HW2017,
    author = {Holmes, C. C. and Walker, S. G.},
    title = "{Assigning a value to a power likelihood in a general {B}ayesian model}",
    journal = {Biometrika},
    volume = {104},
    number = {2},
    pages = {497-503},
    year = {2017},
    month = {03},
    abstract = "{Bayesian robustness under model misspecification is a current area of active research. Among recent ideas is that of raising the likelihood function to a power. In this paper we discuss the choice of appropriate power and provide examples.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asx010},
    eprint = {https://academic.oup.com/biomet/article-pdf/104/2/497/17239738/asx010.pdf},
}
%     url = {https://doi.org/10.1093/biomet/asx010},
% article{HW2017,
%   title={Assigning a value to a power likelihood in a general Bayesian model},
%   author={Holmes, Chris C and Walker, Stephen G},
%   journal={Biometrika},
%   volume={104},
%   number={2},
%   pages={497--503},
%   year={2017},
%   publisher={Oxford University Press}
% }


% -------------------------------------------------------------
% Springer
% Get bibtex from Springer site
% e.g.: https://link.springer.com/chapter/10.1007/978-3-540-44489-3_5#citeas

@Inbook{B2004,
author="Ball, K.",
title="An Elementary Introduction to Monotone Transportation",
bookTitle="Geometric Aspects of Functional Analysis: Israel Seminar 2002-2003",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="41--52",
abstract="1. A Construction of the Brenier Map2. The Brunn-Minkowski Inequality3. The Marton-Talagrand InequalityReferences",
isbn="978-3-540-44489-3",
doi="10.1007/978-3-540-44489-3_5",
}
% url="https://doi.org/10.1007/978-3-540-44489-3_5"
% incollection{B2004,
%   title={An elementary introduction to monotone transportation},
%   author={Ball, Keith},
%   booktitle={Geometric aspects of functional analysis},
%   pages={41--52},
%   year={2004},
%   publisher={Springer}
% }

@InProceedings{L1999,
author="Ledoux, Michel",
editor="Az{\'e}ma, Jacques
and {\'E}mery, Michel
and Ledoux, Michel
and Yor, Marc",
title="Concentration of measure and logarithmic {S}obolev inequalities",
booktitle="S{\'e}minaire de Probabilit{\'e}s XXXIII",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="120--216",
isbn="978-3-540-48407-3"
}
% incollection{L1999,
%   title={Concentration of measure and logarithmic Sobolev inequalities},
%   author={Ledoux, Michel},
%   booktitle={Seminaire de probabilites XXXIII},
%   pages={120--216},
%   year={1999},
%   publisher={Springer}
% }

@InProceedings{BE2006,
author="Bakry, D.
and {\'E}mery, M.",
editor="Az{\'e}ma, Jacques
and Yor, Marc",
title="Diffusions hypercontractives",
booktitle="S{\'e}minaire de Probabilit{\'e}s XIX 1983/84",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="177--206",
isbn="978-3-540-39397-9"
}
% -------------------------------------------------------------
% JSTOR
% e.g.: https://www.jstor.org/stable/2373688

@article{G1975,
 ISSN = {00029327, 10806377},
 author = {Leonard Gross},
 journal = {American Journal of Mathematics},
 number = {4},
 pages = {1061--1083},
 publisher = {Johns Hopkins University Press},
 title = {Logarithmic {S}obolev Inequalities},
 urldate = {2023-01-23},
 volume = {97},
 year = {1975}
}
 % URL = {http://www.jstor.org/stable/2373688},
% article{G1975,
%   title={Logarithmic sobolev inequalities},
%   author={Gross, Leonard},
%   journal={American Journal of Mathematics},
%   volume={97},
%   number={4},
%   pages={1061--1083},
%   year={1975},
%   publisher={JSTOR}
% }


% -------------------------------------------------------------
% JMLR
% e.g.: https://www.jmlr.org/papers/v6/hyvarinen05a.html

@article{HD2005,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
}

@article{DN2017,
  author  = {Christos Dimitrakakis and Blaine Nelson and Zuhe Zhang and Aikaterini Mitrokotsa and Benjamin I. P. Rubinstein},
  title   = {Differential Privacy for {B}ayesian Inference through Posterior Sampling},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {11},
  pages   = {1--39},
}
%   url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
% article{HD2005,
%   title={Estimation of non-normalized statistical models by score matching.},
%   author={Hyv{\"a}rinen, Aapo and Dayan, Peter},
%   journal={Journal of Machine Learning Research},
%   volume={6},
%   number={4},
%   year={2005}
% }

% -------------------------------------------------------------
% Other journals

% Get .ris file from Springer site and convert it to bibtex format
% https://link.springer.com/article/10.1007/BF01011161#citeas
@Article{HS1986,
author={Holley, Richard
and Stroock, Daniel},
title={Logarithmic {S}obolev inequalities and stochastic {I}sing models},
journal={Journal of Statistical Physics},
year={1987},
month={Mar},
day={01},
volume={46},
number={5},
pages={1159-1194},
abstract={We use logarithmic Sobolev inequalities to study the ergodic properties of stochastic Ising models both in terms of large deviations and in terms of convergence in distribution.},
issn={1572-9613},
doi={10.1007/BF01011161},
}
% url={https://doi.org/10.1007/BF01011161}
% article{HS1986,
%   title={Logarithmic Sobolev inequalities and stochastic Ising models},
%   author={Holley, Richard and Stroock, Daniel W},
%   year={1986},
%   publisher={Laboratory for Information and Decision Systems, Massachusetts Institute of~…}
% }


% -------------------------------------------------------------
% arXiv
@article{HT2020,
  title={Preventing posterior collapse with {L}evenshtein variational autoencoder},
  author={Havrylov, Serhii and Titov, Ivan},
  journal={arXiv preprint arXiv:2004.14758},
  year={2020}
}

@article{BGS2015,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}

@article{YK2017,
  title={Tackling over-pruning in variational autoencoders},
  author={Yeung, Serena and Kannan, Anitha and Dauphin, Yann and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1706.03643},
  year={2017}
}

@article{YM2019,
  title={Variational approximations using {F}isher divergence},
  author={Yang, Yue and Martin, Ryan and Bondell, Howard},
  journal={arXiv preprint arXiv:1905.05284},
  year={2019}
}

@article{HC2018,
  title={Practical bounds on the error of {B}ayesian posterior approximations: A nonasymptotic approach},
  author={Huggins, Jonathan H and Campbell, Trevor and Kasprzak, Miko{\l}aj and Broderick, Tamara},
  journal={arXiv preprint arXiv:1809.09505},
  year={2018}
}

% Title has been changed from arxiv v1 to v2
@article{ZY2020,
  title={Improve Variational Autoencoder for Text Generation with Discrete Latent Bottleneck},
  author={Zhao, Yang and Yu, Ping and Mahapatra, Suchismit and Su, Qinliang and Chen, Changyou},
  journal={arXiv preprint arXiv:2004.10603},
  year={2020}
}
% article{ZY2020,
%   title={Discretized Bottleneck: Posterior-Collapse-Free Sequence-to-Sequence Learning},
%   author={Zhao, Yang and Yu, Ping and Mahapatra, Suchismit and Su, Qinliang and Chen, Changyou},
%   year={2020}
% }

@article{V2021feature,
  title={On feature collapse and deep kernel learning for single forward pass uncertainty},
  author={van Amersfoort, Joost and Smith, Lewis and Jesson, Andrew and Key, Oscar and Gal, Yarin},
  journal={arXiv preprint arXiv:2102.11409},
  year={2021}
}

@article{XR2017,
  title={Fashion-{MNIST}: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}


@article{LS2015,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{HS1997,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@INPROCEEDINGS{AA2020,
  author={Agarap, Abien Fred and Azcarraga, Arnulfo P.},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Improving k-Means Clustering Performance with Disentangled Internal Representations}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN48605.2020.9207192}}

@InProceedings{BC2022,
  title = 	 { Certifiably Robust Variational Autoencoders },
  author =       {Barrett, Ben and Camuto, Alexander and Willetts, Matthew and Rainforth, Tom},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3663--3683},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  abstract = 	 { We introduce an approach for training variational autoencoders (VAEs) that are certifiably robust to adversarial attack. Specifically, we first derive actionable bounds on the minimal size of an input perturbation required to change a VAE’s reconstruction by more than an allowed amount, with these bounds depending on certain key parameters such as the Lipschitz constants of the encoder and decoder. We then show how these parameters can be controlled, thereby providing a mechanism to ensure a priori that a VAE will attain a desired level of robustness. Moreover, we extend this to a complete practical approach for training such VAEs to ensure our criteria are met. Critically, our method allows one to specify a desired level of robustness upfront and then train a VAE that is guaranteed to achieve this robustness. We further demonstrate that these Lipschitz-constrained VAEs are more robust to attack than standard VAEs in practice. }
}
@inproceedings{L2009,
author = {Lyu, Siwei},
title = {Interpretation and Generalization of Score Matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{HJ2016,
  title={Elbo surgery: yet another way to carve up the variational evidence lower bound},
  author={Hoffman, Matthew D and Johnson, Matthew J},
  booktitle={Workshop in Advances in Approximate Bayesian Inference},
  volume={1},
  year={2016}
}

@article{B2017,
  title={Potential-function proofs for first-order methods},
  author={Bansal, Nikhil and Gupta, Anupam},
  journal={arXiv preprint arXiv:1712.04581},
  year={2017}
}

@article{SZ2013,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
@inproceedings{P2022almost,
    author = {Prach, Bernd and Lampert, Christoph H.},
    title = {Almost-Orthogonal Layers for Efficient General-Purpose {L}ipschitz Networks},
    year = {2022},
    isbn = {978-3-031-19802-1},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    doi = {10.1007/978-3-031-19803-8_21},
    abstract = {It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously.Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation studies in the context of image classification with certified robust accuracy confirm that AOL layers achieve results that are on par with most existing methods. Yet, they are simpler to implement and more broadly applicable, because they do not require computationally expensive matrix orthogonalization or inversion steps as part of the network architecture.We provide code at .},
    booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI},
    pages = {350–365},
    numpages = {16},
    keywords = {Robustness, Orthogonality, Lipschitz networks},
    location = {Tel Aviv, Israel}
    }

@article{B2023dpsgd,
  title={DP-SGD Without Clipping: The {L}ipschitz Neural Network Way},
  author={B{\'e}thune, Louis and Mass{\'e}na, Thomas and Boissin, Thibaut and Prudent, Yannick and Friedrich, Corentin and Mamalet, Franck and Bellet, Aurelien and Serrurier, Mathieu and Vigouroux, David},
  journal={arXiv preprint arXiv:2305.16202},
  year={2023}
}
@article{B2022safe,
    author = {Brunke, Lukas and Greeff, Melissa and Hall, Adam W. and Yuan, Zhaocong and Zhou, Siqi and Panerati, Jacopo and Schoellig, Angela P.},
    title = {Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning},
    journal = {Annual Review of Control, Robotics, and Autonomous Systems},
    volume = {5},
    number = {1},
    pages = {411-444},
    year = {2022},
    doi = {10.1146/annurev-control-042920-020211},
    abstract = { The last half decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision-making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. It includes learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximityto humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches. }
}
@article{B2023universal,
    author = {Bubeck, S\'{e}bastien and Sellke, Mark},
    title = {A Universal Law of Robustness via Isoperimetry},
    year = {2023},
    issue_date = {April 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {70},
    number = {2},
    issn = {0004-5411},
    doi = {10.1145/3578580},
    abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a partial theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires d times more parameters than mere interpolation, where d is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry (or a mixture thereof). In the case of two-layer neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li, and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
    journal = {J. ACM},
    month = {mar},
    articleno = {10},
    numpages = {18},
    keywords = {neural networks, Adversarial robustness, isoperimetry}
}
@ARTICLE{D2023safe,
  author={Dawson, Charles and Gao, Sicun and Fan, Chuchu},
  journal={IEEE Transactions on Robotics}, 
  title={Safe Control With Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control}, 
  year={2023},
  volume={39},
  number={3},
  pages={1749-1767},
  doi={10.1109/TRO.2022.3232542}}

@inproceedings{L2022learning,
    author = {Liu, Hsueh-Ti Derek and Williams, Francis and Jacobson, Alec and Fidler, Sanja and Litany, Or},
    title = {Learning Smooth Neural Functions via {L}ipschitz Regularization},
    year = {2022},
    isbn = {9781450393379},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3528233.3530713},
    abstract = {Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field’s Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.},
    booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
    articleno = {31},
    numpages = {13},
    keywords = {geometric modeling, geometric learning, regularization, neural fields, machine learning},
    location = {Vancouver, BC, Canada},
    series = {SIGGRAPH '22}
    }

@inproceedings{R2021towards,
  title={Towards optimal attacks on reinforcement learning policies},
  author={Russo, Alessio and Proutiere, Alexandre},
  booktitle={2021 American Control Conference (ACC)},
  pages={4561--4567},
  year={2021},
  organization={IEEE}
}
@article{G2021regularisation,
  title={Regularisation of neural networks by enforcing {L}ipschitz continuity},
  author={Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J},
  journal={Machine Learning},
  volume={110},
  pages={393--416},
  year={2021},
  publisher={Springer}
}
@inproceedings{W2022yoularen,
  title={Youla-ren: Learning nonlinear feedback policies with robust stability guarantees},
  author={Wang, Ruigang and Manchester, Ian R},
  booktitle={2022 American Control Conference (ACC)},
  pages={2116--2123},
  year={2022},
  organization={IEEE}
}
@article{A2013provably,
    title = {Provably safe and robust learning-based model predictive control},
    journal = {Automatica},
    volume = {49},
    number = {5},
    pages = {1216-1226},
    year = {2013},
    issn = {0005-1098},
    doi = {https://doi.org/10.1016/j.automatica.2013.02.003},
    author = {Anil Aswani and Humberto Gonzalez and S. Shankar Sastry and Claire Tomlin},
    keywords = {Predictive control, Statistics, Robustness, Safety analysis, Learning control},
    abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics.}
    }
@article{J2020stability,
  title={Stability-certified reinforcement learning: A control-theoretic perspective},
  author={Jin, Ming and Lavaei, Javad},
  journal={IEEE Access},
  volume={8},
  pages={229086--229100},
  year={2020},
  publisher={IEEE}
}
@InProceedings{W2020orthogonal,
author = {Wang, Jiayun and Chen, Yubei and Chakraborty, Rudrasis and Yu, Stella X.},
title = {Orthogonal Convolutional Neural Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}
@inproceedings{H2020controllable,
title={Controllable orthogonalization in training {DNN}s},
author={Huang, Lei and Liu, Li and Zhu, Fan and Wan, Diwen and Yuan, Zehuan and Li, Bo and Shao, Ling},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
pages={6429--6438},
year={2020}
}
@article{A2023strongly,
  title={Strongly convex squared norms},
  author={Acu, Ana-Maria and Rasa, Ioan and {\c{S}}teopoaie, Ancu{\c{t}}a Emilia},
  journal={Dolomites Research Notes on Approximation},
  volume={16},
  number={2},
  year={2023}
}
@article{S2010equivalence,
  title={On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms},
  author={Shalev-Shwartz, Shai and Singer, Yoram},
  journal={Machine learning},
  volume={80},
  pages={141--163},
  year={2010},
  publisher={Springer}
}
@article{B2011spontaneous,
  title={Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment},
  author={Berkes, Pietro and Orb{\'a}n, Gerg{\H{o}} and Lengyel, M{\'a}t{\'e} and Fiser, J{\'o}zsef},
  journal={Science},
  volume={331},
  number={6013},
  pages={83--87},
  year={2011},
  publisher={American Association for the Advancement of Science}
}
@article{L2020markov,
  title={Markov-{L}ipschitz deep learning},
  author={Li, Stan Z and Zang, Zelin and Wu, Lirong},
  journal={arXiv preprint arXiv:2006.08256},
  year={2020}
}

@article{K2021benchmarking,
  title={Benchmarking invertible architectures on inverse problems},
  author={Kruse, Jakob and Ardizzone, Lynton and Rother, Carsten and K{\"o}the, Ullrich},
  journal={arXiv preprint arXiv:2101.10763},
  year={2021}
}
@article{H2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}
@article{D2011adaptive,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
}
@inproceedings{M2018nonlinear,
  title={Nonlinear dimension reduction via outer bi-lipschitz extensions},
  author={Mahabadi, Sepideh and Makarychev, Konstantin and Makarychev, Yury and Razenshteyn, Ilya},
  booktitle={Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1088--1101},
  year={2018}
}
@article{R2020residual,
  title={Residual networks as flows of diffeomorphisms},
  author={Rousseau, Fran{\c{c}}ois and Drumetz, Lucas and Fablet, Ronan},
  journal={Journal of Mathematical Imaging and Vision},
  volume={62},
  pages={365--375},
  year={2020},
  publisher={Springer}
}
@book{N2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{X2017fashion,
  title={Fashion-{MNIST}: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}
@article{L2010mnist,
  author = {LeCun, Yann and Cortes, Corinna},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  year = {2010}
}
@article{B2011notmnist,
  author = {Bulatov, Yaroslav},
  title = {{notMNIST} dataset},
  year = {2011},
  url = {https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html},
  urldate = {2023-10-10}
}
@article{Z2018fenchel,
  title={On the {F}enchel duality between strong convexity and {L}ipschitz continuous gradient},
  author={Zhou, Xingyu},
  journal={arXiv preprint arXiv:1803.06573},
  year={2018}
}
@book{R1997convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  volume={11},
  year={1997},
  publisher={Princeton university press}
}
@article{W2023comprehensive,
  title={A comprehensive survey of continual learning: Theory, method and application},
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2302.00487},
  year={2023}
}
@article{
K2017overcoming,
author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
title = {Overcoming catastrophic forgetting in neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {13},
pages = {3521-3526},
year = {2017},
doi = {10.1073/pnas.1611835114},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.}}

@incollection{M1989catastrophic,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
author = {Michael McCloskey and Neal J. Cohen},
abstract = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.}
}
@inproceedings{N1983method,
  title={A method of solving a convex programming problem with convergence rate O$(1/k^2)$},
  author={Nesterov, Yurii Evgen'evich},
  booktitle={Doklady Akademii Nauk},
  volume={269},
  number={3},
  pages={543--547},
  year={1983},
  organization={Russian Academy of Sciences}
}
@article{C2019tiny,
  title={On tiny episodic memories in continual learning},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.10486},
  year={2019}
}

@article{S2023normflows, 
  author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, 
  title = {Normflows: A PyTorch Package for Normalizing Flows}, 
  journal = {Journal of Open Source Software}, 
  volume = {8},
  number = {86}, 
  pages = {5361}, 
  publisher = {The Open Journal}, 
  doi = {10.21105/joss.05361}, 
  year = {2023}
} 


@article{R2019physics,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{S2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58--63},
  pages={94},
  year={2015},
  publisher={Springer}
}

@article{R1970maximal,
  title={On the maximal monotonicity of subdifferential mappings},
  author={Rockafellar, Ralph},
  journal={Pacific Journal of Mathematics},
  volume={33},
  number={1},
  pages={209--216},
  year={1970},
  publisher={Mathematical Sciences Publishers}
}
@article{Z2005liu,
  title={On the Liu--Floudas convexification of smooth programs},
  author={Zlobec, Sanjo},
  journal={Journal of Global Optimization},
  volume={32},
  number={3},
  pages={401--407},
  year={2005},
  publisher={Springer}
}
@article{A2016machine,
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  title = {Machine bias: There’s software used across the country to predict future criminals. {A}nd it’s biased against blacks.},
  journal = {Propublica},
  year = 2016,
}

@misc{Q1993auto,
  author       = {Quinlan, R.},
  title        = {{Auto MPG}},
  year         = {1993},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5859H}
}

@misc{J1998heart,
  author       = {Janosi, Andras and Steinbrunn, William and Pfisterer, Matthias and Detrano, Robert},
  title        = {{Heart Disease}},
  year         = {1988},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C52P4X}
}
@ARTICLE{M1997system,
  author={Megretski, A. and Rantzer, A.},
  journal={IEEE Transactions on Automatic Control}, 
  title={System analysis via integral quadratic constraints}, 
  year={1997},
  volume={42},
  number={6},
  pages={819-830},
  keywords={Robustness;Stability analysis;Power system modeling;Feedback;Control theory;Robust stability;Design engineering;Control system synthesis;Computational modeling;Parameter estimation},
  doi={10.1109/9.587335}}
% END OF DOC