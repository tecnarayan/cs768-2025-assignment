@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@inproceedings{Lillicrap2016DDPG,
  title={Continuous control with deep reinforcement learning},
  author={T. Lillicrap and Jonathan J. Hunt and A. Pritzel and N. Heess and T. Erez and Y. Tassa and D. Silver and Daan Wierstra},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2016},
  url={https://openreview.net/forum?id=tX_O8O-8Zl}
}

@InProceedings{dubey2017rational,
  title = {A rational analysis of curiosity},
  author = {Dubey, Rachit and Griffiths, Thomas L.},
  booktitle = {Proceedings of the 39th Annual Conference of the Cognitive Science Society},
  year = {2017},
  pages = {307--312},
  pdf2 = {https://cogsci.mindmodeling.org/2017/papers/0068/paper0068.pdf},
  url = {https://cogsci.mindmodeling.org/2017/papers/0068/index.html}
}

@InProceedings{sekar2020planning,
  title = {Planning to Explore via Self-Supervised World Models},
  author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages2 = {8583--8592},
  year = {2020},
  editor2 = {Daumé~III, Hal and Singh, Aarti},
  volume2 = {119},
  series2 = {Proceedings of Machine Learning Research},
  month2 = {13--18 Jul},
  publisher2 = {PMLR},
  pdf2 = {http://proceedings.mlr.press/v119/sekar20a/sekar20a.pdf},
  url = {https://proceedings.mlr.press/v119/sekar20a.html}
}

@InProceedings{pathak2019self,
  title = {Self-Supervised Exploration via Disagreement},
  author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages2 = {5062--5071},
  year = {2019},
  editor2 = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume2 = {97},
  series2 = {Proceedings of Machine Learning Research},
  month2 = {09--15 Jun},
  publisher2 = {PMLR},
  pdf2 = {http://proceedings.mlr.press/v97/pathak19a/pathak19a.pdf},
  url = {https://proceedings.mlr.press/v97/pathak19a.html}
}

@InProceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats},
  pages2={222--227},
  year={1991},
  url={https://dl.acm.org/doi/10.5555/116517.116542}
}

@InProceedings{BlaesVlastelicaZhuMartius2019:CWYC,
title = {Control {W}hat {Y}ou {C}an: {I}ntrinsically Motivated Task-Planning Agent},
author = {Sebastian Blaes and Marin Vlastelica and Jia-Jie Zhu and Georg Martius},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {},
year = {2019},
publisher2 = {Curran Associates, Inc.},
notes = {in press},
url = {https://proceedings.neurips.cc/paper/2019/hash/b6f97e6f0fd175613910d613d574d0cb-Abstract.html}
}


@InProceedings{Colas2019:CURIOUS,
  title = 	 {{CURIOUS}: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning},
  author =       {Colas, C{\'e}dric and Fournier, Pierre and Chetouani, Mohamed and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages2 = 	 {1331--1340},
  year = 	 {2019},
  editor2 = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume2 = 	 {97},
  series2 = 	 {Proceedings of Machine Learning Research},
  month2 = 	 {09--15 Jun},
  publisher2 =    {PMLR},
  pdf2 = 	 {http://proceedings.mlr.press/v97/colas19a/colas19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/colas19a.html},
  abstract = 	 {In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS , an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.}
}

@InProceedings{KlyubinPolani2005:Empowerment,  
author={Klyubin, A.S. and Polani, D. and Nehaniv, C.L.},  
booktitle={IEEE Congress on Evolutionary Computation},   
title={Empowerment: a universal agent-centric measure of control},  
year={2005},  
volume={1},  
number={},  
pages={128-135 Vol.1}, 
doi2={10.1109/CEC.2005.1554676},
url={https://ieeexplore.ieee.org/document/1554676}
}

@article{forestier2017intrinsically,
 author = {Forestier, S{\'e}bastien and Mollard, Yoan and Oudeyer, Pierre-Yves},
 category = {related},
 file = {:./related/forestier2017intrinsically.pdf},
 journal = {arXiv:1708.02190},
 title = {Intrinsically motivated goal exploration processes with automatic curriculum learning},
 year = {2017}
}

@article{oudeyer2009intrinsic,
 author = {Oudeyer, Pierre-Yves and Kaplan, Frederic},
 category = {related},
 file = {:./related/oudeyer2009intrinsic.pdf},
 journal = {Frontiers in Neurorobotics},
 pages = {6},
 publisher = {Frontiers},
 title = {What is intrinsic motivation? {A} typology of computational approaches},
 volume = {1},
 year = {2009}
}

@inproceedings{PaoloEtAl2021:NoveltySearchSparseReward,
author = {Paolo, Giuseppe and Coninx, Alexandre and Doncieux, Stephane and Laflaqui\`{e}re, Alban},
title = {Sparse Reward Exploration via Novelty Search and Emitters},
year = {2021},
isbn2 = {9781450383509},
publisher2 = {Association for Computing Machinery},
address2 = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459314},
doi2 = {10.1145/3449639.3459314},
abstract = {Reward-based optimization algorithms require both exploration, to find rewards, and exploitation, to maximize performance. The need for efficient exploration is even more significant in sparse reward settings, in which performance feedback is given sparingly, thus rendering it unsuitable for guiding the search process. In this work, we introduce the SparsE Reward Exploration via Novelty and Emitters (SERENE) algorithm, capable of efficiently exploring a search space, as well as optimizing rewards found in potentially disparate areas. Contrary to existing emitters-based approaches, SERENE separates the search space exploration and reward exploitation into two alternating processes. The first process performs exploration through Novelty Search, a divergent search algorithm. The second one exploits discovered reward areas through emitters, i.e. local instances of population-based optimization algorithms. A meta-scheduler allocates a global computational budget by alternating between the two processes, ensuring the discovery and efficient exploitation of disjoint reward areas. SERENE returns both a collection of diverse solutions covering the search space and a collection of high-performing solutions for each distinct reward area. We evaluate SERENE on various sparse reward environments and show it compares favorably to existing baselines.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {154–162},
numpages2 = {9},
keywords = {sparse rewards, emitters, evolutionary algorithm, novelty search, quality diversity},
location2 = {Lille, France},
series2 = {GECCO '21}
}


@InProceedings{pathak2017curiosity,
  title = {Curiosity-driven Exploration by Self-supervised Prediction},
  author = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages2 = 	 {2778--2787},
  year = 	 {2017},
  editor2 = 	 {Precup, Doina and Teh, Yee Whye},
  volume2 = 	 {70},
  series2 = 	 {Proceedings of Machine Learning Research},
  month2 = 	 {06--11 Aug},
  publisher2 =    {PMLR},
  pdf2 = 	 {http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pathak17a.html}
}

@InProceedings{burda2018exploration,
title={Exploration by random network distillation},
author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
booktitle={International Conference on Learning Representations (ICLR)},
year={2019},
url={https://openreview.net/forum?id=H1lJJnR5Ym},
}

@InProceedings{mohamed2015variational,
  title={Variational information maximisation for intrinsically motivated reinforcement learning},
  author={Mohamed, Shakir and Jimenez Rezende, Danilo},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume2={28},
  year={2015},
  url={https://arxiv.org/abs/1509.08731}
}

@inproceedings{Mendonca2021:LEXA,
title={Discovering and Achieving Goals via World Models},
author={Russell Mendonca and Oleh Rybkin and Kostas Daniilidis and Danijar Hafner and Deepak Pathak},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
editor2={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6vWuYzkp8d}
}

@article{openai2021:assymmetric-selfplay,
  author    = {OpenAI and
               Matthias Plappert and
               Raul Sampedro and
               Tao Xu and
               Ilge Akkaya and
               Vineet Kosaraju and
               Peter Welinder and
               Ruben D'Sa and
               Arthur Petron and
               Henrique Ponde de Oliveira Pinto and
               Alex Paino and
               Hyeonwoo Noh and
               Lilian Weng and
               Qiming Yuan and
               Casey Chu and
               Wojciech Zaremba},
  title     = {Asymmetric self-play for automatic goal discovery in robotic manipulation},
  year      = {2021},
  journal = {arXiv:2101.04882},
  url       = {https://arxiv.org/abs/2101.04882},
  eprinttype = {arXiv},
  eprint    = {2101.04882},
  timestamp = {Fri, 22 Jan 2021 15:16:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-04882.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pinneri2020:iCEM,
  title = {Sample-efficient Cross-Entropy Method for Real-time Planning},
  author = {Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Achterhold, Jan and Stueckler, Joerg and Rolinek, Michal and Martius, Georg},
  booktitle = {Conference on Robot Learning (CoRL)},
  year = {2020},
  doi = {},
  url2 = {https://corlconf.github.io/corl2020/paper_217/ },
  pdf2 = {https://proceedings.mlr.press/v155/pinneri21a/pinneri21a.pdf},
  url = {https://proceedings.mlr.press/v155/pinneri21a.html}
}

@inproceedings{VlastelicaBlaesEtal2021:riskaverse,
  title = {Risk-Averse Zero-Order Trajectory Optimization},
  author = {Vlastelica, Marin and Blaes, Sebastian and Pinneri, Cristina and Martius, Georg},
  booktitle = {Conference on Robot Learning (CoRL)},
  year = {2021},
  note2 = {*Equal Contribution},
  doi = {},
  url = {https://openreview.net/forum?id=WqUl7sNkDre}
}

% Relational networks
@InProceedings{kipf2019contrastive,
  title={Contrastive Learning of Structured World Models}, 
  author={Thomas Kipf and Elise van~der~Pol and Max Welling},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  url={https://openreview.net/forum?id=H1gax6VtDB}
}

@article{battaglia2018relational,
  title = {Relational inductive biases, deep learning, and graph networks}, 
  author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  year = {2018},
  journal = {arXiv:1806.01261},
  url={https://arxiv.org/abs/1806.01261}
}

@InProceedings{watters_NIPS2017,
  author    = {Nicholas Watters and
               Daniel Zoran and
               Theophane Weber and
               Peter W. Battaglia and
               Razvan Pascanu and
               Andrea Tacchetti},
  editor2    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Visual Interaction Networks: Learning a Physics Simulator from Video},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages2     = {4539--4547},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/8cbd005a556ccd4211ce43f309bc0eac-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/WattersZWBPT17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{sanchez2020learning,
  title = {Learning to Simulate Complex Physics with Graph Networks},
  author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages2 = {8459--8468},
  year = {2020},
  editor2 = {Daumé~III, Hal and Singh, Aarti},
  volume2 = {119},
  series2 = {Proceedings of Machine Learning Research},
  month2 = {13--18 Jul},
  publisher2 = {PMLR},
  pdf2 = {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html}
}

@article{tsividis2021human,
  title={Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning}, 
  author={Pedro A. Tsividis and Joao Loula and Jake Burga and Nathan Foss and Andres Campero and Thomas Pouncy and Samuel J. Gershman and Joshua B. Tenenbaum},
  year={2021},
  journal={arXiv:2107.12544},
  url={https://arxiv.org/abs/2107.12544}
}

% Stacking and task-oriented GNNs
@inproceedings{li2020towards,
  title={Towards practical multi-object manipulation using relational reinforcement learning},
  author={Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={4051--4058},
  year={2020},
  organization={IEEE},
  url={https://arxiv.org/abs/1912.11032}
}

@article{lin2022efficient,
  title={Efficient and interpretable robot manipulation with graph neural networks},
  author={Lin, Yixin and Wang, Austin S and Undersander, Eric and Rai, Akshara},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE}
}

% object centric curiosity
@article{watters2019cobra,
  title={Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration},
  author={Watters, Nicholas and Matthey, Loic and Bosnjak, Matko and Burgess, Christopher P and Lerchner, Alexander},
  url = {https://arxiv.org/abs/1905.09275},
  journal={arXiv preprint arXiv:1905.09275},
  year={2019}
}

@article{parisi2021interesting,
  title={Interesting Object, Curious Agent: Learning Task-Agnostic Exploration},
  author={Parisi, Simone and Dean, Victoria and Pathak, Deepak and Gupta, Abhinav},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume2={34},
  year={2021}
}

% Psychology references
@book{james1983talks,
  title={Talks to Teachers on Psychology and to Students on Some of Life's Ideals},
  author={James, William and Skrupskelis, Ignas K},
  volume={12},
  year={1983},
  publisher={Harvard University Press}
}

@article{loewenstein1994psychology,
  title={The psychology of curiosity: A review and reinterpretation.},
  author={Loewenstein, George},
  journal={Psychological Bulletin},
  volume={116},
  number={1},
  pages={75--98},
  year={1994},
  publisher={American Psychological Association},
  url = {https://doi.org/10.1037/0033-2909.116.1.75}
}

@article{stahl2015observing,
  title={Observing the unexpected enhances infants’ learning and exploration},
  author={Stahl, Aimee E and Feigenson, Lisa},
  journal={Science},
  volume={348},
  number={6230},
  pages={91--94},
  year={2015},
  publisher={American Association for the Advancement of Science},
  url={https://doi.org/10.1126/science.aaa3799}
}

@article{legare_inconsistency_2010,
	title = {Inconsistency {With} {Prior} {Knowledge} {Triggers} {Children}’s {Causal} {Explanatory} {Reasoning}},
	volume = {81},
	issn2 = {1467-8624},
	url = {https://doi.org/10.1111/j.1467-8624.2010.01443.x},
	doi2 = {10.1111/j.1467-8624.2010.01443.x},
	language = {en},
	number = {3},
	urldate = {2022-04-30},
	journal = {Child Development},
	author = {Legare, Cristine H. and Gelman, Susan A. and Wellman, Henry M.},
	year = {2010},
	note2 = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8624.2010.01443.x},
	keywords = {Novelty},
	pages = {929--944},
}

@article{bonawitz_children_2012,
	title = {Children balance theories and evidence in exploration, explanation, and learning},
	volume = {64},
	issn2 = {0010-0285},
	url = {https://doi.org/10.1016/j.cogpsych.2011.12.002},
	doi2 = {10.1016/j.cogpsych.2011.12.002},
	language = {en},
	number = {4},
	urldate = {2022-04-30},
	journal = {Cognitive Psychology},
	author = {Bonawitz, Elizabeth Baraff and van Schijndel, Tessa J. P. and Friel, Daniel and Schulz, Laura},
	month = jun,
	year = {2012},
	keywords = {Beliefs, Evidence, Explanation, Learning, Novelty, Play},
	pages = {215--234},
}

@article{kidd2015psychology,
  title={The psychology and neuroscience of curiosity},
  author={Kidd, Celeste and Hayden, Benjamin Y},
  journal={Neuron},
  volume={88},
  number={3},
  pages={449--460},
  year={2015},
  publisher={Elsevier},
  url={https://doi.org/10.1016/j.neuron.2015.09.010}
}

@article{poli2020infants,
  title={Infants tailor their attention to maximize learning},
  author={Poli, F and Serino, G and Mars, RB and Hunnius, S},
  journal={Science Advances},
  volume={6},
  number={39},
  pages={eabb5053},
  year={2020},
  publisher={American Association for the Advancement of Science},
  URL = {https://www.science.org/doi/abs/10.1126/sciadv.abb5053},
}

% References for bias in humans

@article{murphy1985role,
  title={The role of theories in conceptual coherence.},
  author={Murphy, Gregory L and Medin, Douglas L},
  journal={Psychological review},
  volume={92},
  number={3},
  pages={289},
  year={1985},
  publisher={American Psychological Association},
  url={https://doi.org/10.1037/0033-295X.92.3.289}
}

@book{haith1980rules,
  title={Rules that babies look by: The organization of newborn visual activity},
  author={Haith, Marshall M},
  year={1980},
  publisher={Lawrence Erlbaum Associates}
}

% Model-based RL
@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE},
  url= {http://arxiv.org/abs/1708.02596},
}

% Policy snapshotting
@article{groth2021curiosity,
  title={Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration},
  author={Groth, Oliver and Wulfmeier, Markus and Vezzani, Giulia and Dasagi, Vibhavari and Hertweck, Tim and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
  url = {https://arxiv.org/abs/2109.08603},
  journal={arXiv preprint arXiv:2109.08603},
  year={2021}
}

% Finetuning
@inproceedings{laskin2021urlb,
title={{URLB}: Unsupervised Reinforcement Learning Benchmark},
author={Michael Laskin and Denis Yarats and Hao Liu and Kimin Lee and Albert Zhan and Kevin Lu and Catherine Cang and Lerrel Pinto and Pieter Abbeel},
booktitle={NeurIPS 2021 Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=lwrPkQP_is}
}

% Offline RL
@inproceedings{
yarats2022don,
title={Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning},
author={Denis Yarats and David Brandfonbrener and Hao Liu and Michael Laskin and Pieter Abbeel and Alessandro Lazaric and Lerrel Pinto},
booktitle={ICLR 2022 Workshop on Generalizable Policy Learning in Physical World},
year={2022},
url={https://openreview.net/forum?id=Su-zh4a41Z5}
}

@inproceedings{NIPS2017_453fadbd,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 editor2 = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher2 = {Curran Associates, Inc.},
 title = {Hindsight Experience Replay},
 url = {https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume2 = {30},
 year = {2017}
}


@article{LittleSommer2013:PIG,
  Author = {Little, D.~Y. and Sommer, F.~T.},
  Journal = {Frontiers in Neural Circuits},
  Number = {37},
  Title = {Learning and exploration in action-perception loops},
  Volume = {7},
  Pages = {},
  Year = {2013},
  url={https://doi.org/10.3389/fncir.2013.00037}
}
  

@article{Pfaffelhuber1972:MissingInformation,
author = {E. Pfaffelhuber},
title = {Learning and Information Theory},
journal = {International Journal of Neuroscience},
volume = {3},
number = {2},
pages = {83-88},
year  = {1972},
publisher = {Taylor & Francis},
doi2 = {10.3109/00207457209147016},
URL = {https://doi.org/10.3109/00207457209147016},
eprint = {https://doi.org/10.3109/00207457209147016}
}


@book{Cover2006Elements-of-Information,
  Address = {Hoboken, New Jersey, USA},
  Author = {Cover, T.~M. and Thomas, J.~A.},
  Publisher = {Wiley},
  Title = {Elements of Information Theory},
  Volume = {2nd},
  Year = {2006},
  url2 = {http://www.elementsofinformationtheory.com/},
  }

@inproceedings{Storck1995Reinforcement-Driven-Information,
  Address = {Paris},
  Author = {J. Storck and S. Hochreiter and J. Schmidhuber},
  Booktitle = {Proceedings of the International Conference on Artificial Neural Networks},
  Editor2 = {F. Fogelman-Souli\'e and J. C. Rault and P. Gallinari and G. Dreyfus},
  Pages = {159--164},
  Publisher = {EC2 \& Cie},
  Title = {Reinforcement Driven Information Acquisition In Non-Deterministic Environments},
  Year = {1995},
  url= {https://people.idsia.ch/~juergen/icann95new.pdf}
  }

@article{schmidhuber2013powerplay,
  title={Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem},
  author={Schmidhuber, J{\"u}rgen},
  journal={Frontiers in Psychology},
  volume={4},
  pages={313},
  year={2013},
  publisher={Frontiers}
}

@article{schmidhuber2006developmental,
  title={Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts},
  author={Schmidhuber, J{\"u}rgen},
  journal={Connection Science},
  volume={18},
  number={2},
  pages={173--187},
  year={2006},
  publisher={Taylor \& Francis}
}



@article{Plappert2018MultiGoalRL,
  title={Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research},
  author={Matthias Plappert and Marcin Andrychowicz and Alex Ray and Bob McGrew and Bowen Baker and Glenn Powell and J. Schneider and Joshua Tobin and Maciek Chociej and P. Welinder and V. Kumar and W. Zaremba},
  journal={arXiv preprint arXiv:1802.09464},
  year={2018},
  url={https://arxiv.org/abs/1802.09464}
}

@InProceedings{seno2021d3rlpy,
  author = {Takuma Seno, Michita Imai},
  title = {d3rlpy: An Offline Deep Reinforcement Library},
  booktitle = {NeurIPS 2021 Offline Reinforcement Learning Workshop},
  year = {2021},
  url={https://arxiv.org/abs/2111.03788}
}

@article{kumar2020conservative,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume2={33},
  pages2={1179--1191},
  year={2020},
  url={https://arxiv.org/abs/2006.04779}
}

@inproceedings{fujimoto2021minimalist,
	title={A Minimalist Approach to Offline Reinforcement Learning},
	author={Scott Fujimoto and Shixiang Shane Gu},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2021},
	url={https://arxiv.org/abs/2106.06860}
}

@inproceedings{
kostrikov2022offline,
title={Offline Reinforcement Learning with Implicit Q-Learning},
author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
booktitle={International Conference on Learning Representations (ICLR)},
year={2022},
url={https://openreview.net/forum?id=68n2s9ZJWF8}
}

@inproceedings{JiangJanghorbaniDeMeloAhn2020SCALOR,
  title={SCALOR: Generative World Models with Scalable Object Representations},
  author={Jindong Jiang and Sepehr Janghorbani and Gerard {de Melo} and Sungjin Ahn},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  publisher2 = {OpenReview.net},
  location2 = {Addis Ababa, Ethiopia},
  url = {https://openreview.net/pdf?id=SJxrKgStDH},
}

@inproceedings{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume2={32},
  year={2019},
  url={https://papers.nips.cc/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html}
}

@article{wang2021:doin,
  author    = {Jiayu Wang and
               Chuxiong Hu and
               Yunan Wang and
               Yu Zhu},
  title     = {Dynamics Learning With Object-Centric Interaction Networks for Robot
               Manipulation},
  journal   = {{IEEE} Access},
  volume    = {9},
  pages     = {68277--68288},
  year      = {2021},
  url       = {https://doi.org/10.1109/ACCESS.2021.3077117},
  doi2       = {10.1109/ACCESS.2021.3077117},
}

@article{Driess2022:Nerf,
  doi2 = {10.48550/ARXIV.2202.11855},
  url = {https://arxiv.org/abs/2202.11855},
  author = {Driess, Danny and Huang, Zhiao and Li, Yunzhu and Tedrake, Russ and Toussaint, Marc},
  title = {Learning Multi-Object Dynamics with Compositional Neural Radiance Fields},
  eprint = {2202.11855},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2202.11855},
  year = {2022}
}

@inproceedings{Seitzer2021CID,
  title = {Causal Influence Detection for Improving Efficiency in Reinforcement Learning},
  author = {Seitzer, Maximilian and Sch{\"o}lkopf, Bernhard and Martius, Georg},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages2 = {22905--22918},
  editors2 = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  publisher2 = {Curran Associates, Inc.},
  month2 = dec,
  year = {2021},
  doi = {},
  url = {https://proceedings.neurips.cc/paper/2021/file/c1722a7941d61aad6e651a35b65a9c3e-Paper.pdf},
  month_numeric2 = {12}
}


@InProceedings{Riedmiller2018:LearningByPlaying,
  title = 	 {Learning by Playing Solving Sparse Reward Tasks from Scratch},
  author =       {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and van de Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages2 = 	 {4344--4353},
  year = 	 {2018},
  editor2 = 	 {Dy, Jennifer and Krause, Andreas},
  volume2 = 	 {80},
  series2 = 	 {Proceedings of Machine Learning Research},
  month2 = 	 {10--15 Jul},
  publisher2 =    {PMLR},
  pdf2 = 	 {http://proceedings.mlr.press/v80/riedmiller18a/riedmiller18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/riedmiller18a.html},
  abstract = 	 {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.}
}

@article{Lambert2022:iMPC,
  publtype={informal},
  author={Nathan Lambert and Markus Wulfmeier and William F. Whitney and Arunkumar Byravan and Michael Bloesch and Vibhavari Dasagi and Tim Hertweck and Martin A. Riedmiller},
  title={The Challenges of Exploration for Offline Reinforcement Learning},
  year={2022},
  journal = {arXiv:2201.11861},
  url={https://arxiv.org/abs/2201.11861}
}

@inproceedings{Zadaianchuk22:SMORL,
  title = {Self-supervised Reinforcement Learning with Independently Controllable Subgoals},
  author = {Zadaianchuk, Andrii and Martius, Georg and Yang, Fanny},
  booktitle = {Conference on Robot Learning (CoRL)},
  year = {2021},
  doi = {},
  url={https://proceedings.mlr.press/v164/zadaianchuk22a/zadaianchuk22a.pdf}
}

@inproceedings{Huber18:Emergent,
  author    = {Haber, Nick and Mrowca, Damian and Fei{-}Fei, Li and Yamins, Daniel L. K.},
  title     = {Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation},
  booktitle = {Annual Meeting of the Cognitive Science Society (CogSci)},
  year      = {2018},
  url = {https://arxiv.org/abs/1802.07461}
}

@inproceedings{Kim20:Active,
  author    = {Kim, Kuno and Sano, Megumi and De Freitas, Julian and Haber, Nick and Yamins, Daniel},
  title     = {Active World Model Learning with Progress Curiosity},
  booktitle = {International Conference on Machine Learning (ICML)},
  volume2    = {119},
  pages2     = {5306--5315},
  year      = {2020},
  url = {https://arxiv.org/abs/2007.07853}
}

@article{biza2022factored,
  title={Factored World Models for Zero-Shot Generalization in Robotic Manipulation},
  author={Biza, Ondrej and Kipf, Thomas and Klee, David and Platt, Robert and van de Meent, Jan-Willem and Wong, Lawson LS},
  journal={arXiv preprint arXiv:2202.05333},
  url={https://arxiv.org/abs/2202.05333},
  year={2022}
}

@misc{kannan2021robodesk,
  author = {Harini Kannan and Danijar Hafner and Chelsea Finn and Dumitru Erhan},
  title = {RoboDesk: A Multi-Task Reinforcement Learning Benchmark},
  year = {2021},
  howpublished = {\url{https://github.com/google-research/robodesk}},
}