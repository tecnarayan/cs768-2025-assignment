\begin{thebibliography}{86}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angelopoulos et~al.(2021)Angelopoulos, Bates, Cand{\`e}s, Jordan, and Lei]{angelopoulos2021learn}
Anastasios~N. Angelopoulos, Stephen Bates, Emmanuel~J. Cand{\`e}s, Michael~I. Jordan, and Lihua Lei.
\newblock Learn then {{Test}}: {{Calibrating Predictive Algorithms}} to {{Achieve Risk Control}}.
\newblock \emph{arXiv Preprint (arXiv:2110.01052)}, 2021.

\bibitem[Angelopoulos et~al.(2022)Angelopoulos, Kohli, Bates, Jordan, Malik, Alshaabi, Upadhyayula, and Romano]{angelopoulos2022image}
Anastasios~N. Angelopoulos, Amit~Pal Kohli, Stephen Bates, Michael Jordan, Jitendra Malik, Thayer Alshaabi, Srigokul Upadhyayula, and Yaniv Romano.
\newblock Image-to-{{Image Regression}} with {{Distribution-Free Uncertainty Quantification}} and {{Applications}} in {{Imaging}}.
\newblock \emph{{{International Conference}} on {{Machine Learning}}}, 2022.

\bibitem[Angelopoulos et~al.(2023)Angelopoulos, Bates, et~al.]{angelopoulos2023conformal}
Anastasios~N Angelopoulos, Stephen Bates, et~al.
\newblock Conformal prediction: A gentle introduction.
\newblock \emph{Foundations and Trends in Machine Learning}, 2023.

\bibitem[Angelopoulos et~al.(2024)Angelopoulos, Bates, Fisch, Lei, and Schuster]{angelopoulos2022conformal}
Anastasios~N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster.
\newblock Conformal risk control.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Bae et~al.(2023)Bae, Ko, Song, and Yun]{bae2023fast}
Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun.
\newblock Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Bai et~al.(2024)Bai, Chai, Ling, Wang, Lu, Zhang, Shi, Yu, Zhu, Zhang, et~al.]{bai2024beyond}
Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et~al.
\newblock Beyond efficiency: A systematic survey of resource-efficient large language models.
\newblock \emph{arXiv Preprint (arXiv:2401.00625)}, 2024.

\bibitem[Bao et~al.(2023)Bao, Nie, Xue, Cao, Li, Su, and Zhu]{bao2023worth}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem[Bates et~al.(2021)Bates, Angelopoulos, Lei, Malik, and Jordan]{bates2021distribution}
Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan.
\newblock Distribution-free, risk-controlling prediction sets.
\newblock \emph{Journal of the ACM}, 2021.

\bibitem[Begoli et~al.(2019)Begoli, Bhattacharya, and Kusnezov]{begoli2019need}
Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov.
\newblock The need for uncertainty quantification in machine-assisted medical decision making.
\newblock \emph{Nature Machine Intelligence}, 2019.

\bibitem[Belhasin et~al.(2023)Belhasin, Romano, Freedman, Rivlin, and Elad]{belhasin2023principal}
Omer Belhasin, Yaniv Romano, Daniel Freedman, Ehud Rivlin, and Michael Elad.
\newblock Principal uncertainty quantification with spatial correlation for image restoration problems.
\newblock \emph{Transactions on Pattern Analysis and Machine Intelligence}, 2023.

\bibitem[Bentkus(2004)]{bentkus2004hoeffding}
Vidmantas Bentkus.
\newblock On hoeffding's inequalities.
\newblock \emph{Annals of Probability}, 2004.

\bibitem[Bolukbasi et~al.(2017)Bolukbasi, Wang, Dekel, and Saligrama]{bolukbasi2017adaptive}
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama.
\newblock Adaptive neural networks for efficient inference.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Brier(1950)]{brier1950verification}
Glenn~W Brier.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly Weather Review}, 1950.

\bibitem[Cettolo et~al.(2017)Cettolo, Federico, Bentivogli, Niehues, St{\"u}ker, Sudoh, Yoshino, and Federmann]{cettolo2017overview}
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St{\"u}ker, Katsuitho Sudoh, Koichiro Yoshino, and Christian Federmann.
\newblock Overview of the iwslt 2017 evaluation campaign.
\newblock \emph{International Workshop on Spoken Language Translation}, 2017.

\bibitem[Chataoui et~al.(2023)Chataoui, Coates, et~al.]{regol2023jointly}
Joud Chataoui, Mark Coates, et~al.
\newblock Jointly-learned exit and inference for a dynamic neural network.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Chen et~al.(2020)Chen, Dai, Li, Gao, and Song]{chen2020learning}
Xinshi Chen, Hanjun Dai, Yu~Li, Xin Gao, and Le~Song.
\newblock Learning to stop while learning to predict.
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson, Franke, Roth, and Schiele]{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Dawid(2004)]{dawid2004probability}
A~Philip Dawid.
\newblock Probability forecasting.
\newblock \emph{Encyclopedia of Statistical Sciences}, 2004.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2009.

\bibitem[Deng et~al.(2024)Deng, Zollo, Snell, Pitassi, and Zemel]{deng2024distribution}
Zhun Deng, Thomas Zollo, Jake Snell, Toniann Pitassi, and Richard Zemel.
\newblock Distribution-free statistical dispersion control for societal applications.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Elbayad et~al.(2020)Elbayad, Gu, Grave, and Auli]{elbayad2019depth}
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
\newblock Depth-adaptive transformer.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Fei et~al.(2022)Fei, Yan, Wang, and Tian]{fei2022deecap}
Zhengcong Fei, Xu~Yan, Shuhui Wang, and Qi~Tian.
\newblock Deecap: Dynamic early exiting for efficient image captioning.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2022.

\bibitem[Feldman et~al.(2023)Feldman, Ringel, Bates, and Romano]{feldman2023achieving}
Shai Feldman, Liran Ringel, Stephen Bates, and Yaniv Romano.
\newblock Achieving risk control in online learning settings.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Fontana et~al.(2023)Fontana, Zeni, and Vantini]{fontana2023conformal}
Matteo Fontana, Gianluca Zeni, and Simone Vantini.
\newblock Conformal prediction: a unified review of theory and new challenges.
\newblock \emph{Bernoulli}, 2023.

\bibitem[Gholami et~al.(2022)Gholami, Kim, Dong, Yao, Mahoney, and Keutzer]{gholami2022survey}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock A survey of quantization methods for efficient neural network inference.
\newblock \emph{Low-Power Computer Vision}, 2022.

\bibitem[Gneiting and Raftery(2007)]{gneiting2007strictly}
Tilmann Gneiting and Adrian~E Raftery.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 2007.

\bibitem[Gneiting et~al.(2007)Gneiting, Balabdaoui, and Raftery]{gneiting2007probabilistic}
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian~E Raftery.
\newblock Probabilistic forecasts, calibration and sharpness.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 2007.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian Weinberger.
\newblock On calibration of modern neural networks.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Han et~al.(2021)Han, Huang, Song, Yang, Wang, and Wang]{han2021dynamic}
Yizeng Han, Gao Huang, Shiji Song, Le~Yang, Honghui Wang, and Yulin Wang.
\newblock Dynamic neural networks: A survey.
\newblock \emph{Transactions on Pattern Analysis and Machine Intelligence}, 2021.

\bibitem[Han et~al.(2022)Han, Pu, Lai, Wang, Song, Cao, Huang, Deng, and Huang]{han2022learning}
Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao, Wenhui Huang, Chao Deng, and Gao Huang.
\newblock Learning to weight samples for dynamic early-exiting networks.
\newblock \emph{European Conference on Computer Vision}, 2022.

\bibitem[Han et~al.(2023)Han, Han, Liu, Wang, Pan, Pu, Deng, Feng, Song, and Huang]{han2023dynamic}
Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, and Gao Huang.
\newblock Dynamic perceiver for efficient visual recognition.
\newblock \emph{International Conference on Computer Vision}, 2023.

\bibitem[Heal and Millner(2014)]{heal2014reflections}
Geoffrey Heal and Antony Millner.
\newblock Reflections: Uncertainty and decision making in climate change economics.
\newblock \emph{Review of Environmental Economics and Policy}, 2014.

\bibitem[Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom]{hermann2015teaching}
Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Hersbach(2000)]{hersbach2000decomposition}
Hans Hersbach.
\newblock Decomposition of the continuous ranked probability score for ensemble prediction systems.
\newblock \emph{Weather and Forecasting}, 2000.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Huang et~al.(2018)Huang, Chen, Li, Wu, van~der Maaten, and Weinberger]{huang2017multi}
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van~der Maaten, and Kilian Weinberger.
\newblock Multi-scale dense networks for resource efficient image classification.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Jazbec et~al.(2024{\natexlab{a}})Jazbec, Allingham, Zhang, and Nalisnick]{jazbec2024towards}
Metod Jazbec, James Allingham, Dan Zhang, and Eric Nalisnick.
\newblock Towards anytime classification in early-exit architectures by enforcing conditional monotonicity.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024{\natexlab{a}}.

\bibitem[Jazbec et~al.(2024{\natexlab{b}})Jazbec, Forr{\'e}, Mandt, Zhang, and Nalisnick]{jazbec2023anytime}
Metod Jazbec, Patrick Forr{\'e}, Stephan Mandt, Dan Zhang, and Eric Nalisnick.
\newblock Early-exit neural networks with nested prediction sets.
\newblock \emph{Conference on Uncertainty in Artificial Intelligence}, 2024{\natexlab{b}}.

\bibitem[Jin et~al.(2023)Jin, Ren, and Cand{\`e}s]{jin2023sensitivity}
Ying Jin, Zhimei Ren, and Emmanuel~J Cand{\`e}s.
\newblock Sensitivity analysis of individual treatment effects: A robust conformal inference approach.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2023.

\bibitem[Kaya et~al.(2019)Kaya, Hong, and Dumitras]{kaya2019shallow}
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.
\newblock Shallow-deep networks: Understanding and mitigating network overthinking.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Kim et~al.(2024)Kim, Mangalam, Moon, Malik, Mahoney, Gholami, and Keutzer]{kim2024speculative}
Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael~W Mahoney, Amir Gholami, and Kurt Keutzer.
\newblock Speculative decoding with big little decoder.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Toronto, ON, Canada}, 2009.

\bibitem[Kutiel et~al.(2022)Kutiel, Cohen, Elad, and Freedman]{kutiel2022s}
Gilad Kutiel, Regev Cohen, Michael Elad, and Daniel Freedman.
\newblock What's behind the mask: Estimating uncertainty in image-to-image problems.
\newblock \emph{arXiv Preprint (arXiv:2211.15211)}, 2022.

\bibitem[Lannelongue et~al.(2021)Lannelongue, Grealey, and Inouye]{lannelongue2021green}
Lo{\"\i}c Lannelongue, Jason Grealey, and Michael Inouye.
\newblock Green algorithms: quantifying the carbon footprint of computation.
\newblock \emph{Advanced Science}, 2021.

\bibitem[Laufer-Goldshtein et~al.(2022)Laufer-Goldshtein, Fisch, Barzilay, and Jaakkola]{laufer2022efficiently}
Bracha Laufer-Goldshtein, Adam Fisch, Regina Barzilay, and Tommi~S Jaakkola.
\newblock Efficiently controlling multiple risks with pareto testing.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Laufer-Goldshtein et~al.(2023)Laufer-Goldshtein, Fisch, Barzilay, and Jaakkola]{laufer2023risk}
Bracha Laufer-Goldshtein, Adam Fisch, Regina Barzilay, and Tommi Jaakkola.
\newblock Risk-controlling model selection via guided bayesian optimization.
\newblock \emph{arXiv Preprint (arXiv:2312.01692)}, 2023.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Ha, Shen, and Yi]{liu2021efficient}
Shiya Liu, Dong~Sam Ha, Fangyang Shen, and Yang Yi.
\newblock Efficient neural networks for edge devices.
\newblock \emph{Computers \& Electrical Engineering}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Xu, Wang, Darrell, and Shelhamer]{liu2021anytime}
Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Darrell, and Evan Shelhamer.
\newblock Anytime dense prediction with confidence adaptivity.
\newblock \emph{International Conference on Learning Representations}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock \emph{International Conference on Computer Vision}, 2015.

\bibitem[Mazumder et~al.(2021)Mazumder, Meng, Rashid, Kallakuri, Zhang, Seo, and Mohsenin]{mazumder2021survey}
Arnab~Neelim Mazumder, Jian Meng, Hasib-Al Rashid, Utteja Kallakuri, Xin Zhang, Jae-Sun Seo, and Tinoosh Mohsenin.
\newblock A survey on the optimization of neural network accelerators for micro-ai on-device inference.
\newblock \emph{IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 2021.

\bibitem[Meronen et~al.(2024)Meronen, Trapp, Pilzer, Yang, and Solin]{meronen2024fixing}
Lassi Meronen, Martin Trapp, Andrea Pilzer, Le~Yang, and Arno Solin.
\newblock Fixing overconfidence in dynamic neural networks.
\newblock \emph{Winter Conference on Applications of Computer Vision}, 2024.

\bibitem[Murphy(1973)]{murphy1973new}
Allan~H Murphy.
\newblock A new vector partition of the probability score.
\newblock \emph{Journal of Applied Meteorology and Climatology}, 1973.

\bibitem[Pan et~al.(2024)Pan, Chen, Li, Ding, and Zhou]{pan2024ee}
Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, and Jingren Zhou.
\newblock Ee-tuning: An economical yet scalable solution for tuning early-exit large language models.
\newblock \emph{arXiv Preprint (arXiv:2402.00518)}, 2024.

\bibitem[Park et~al.(2020)Park, Bastani, Matni, and Lee]{park2019pac}
Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee.
\newblock Pac confidence sets for deep neural networks via calibrated prediction.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia, Rothchild, So, Texier, and Dean]{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv Preprint (arXiv:2104.10350)}, 2021.

\bibitem[Quach et~al.(2023)Quach, Fisch, Schuster, Yala, Sohn, Jaakkola, and Barzilay]{quach2023conformal}
Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae~Ho Sohn, Tommi~S Jaakkola, and Regina Barzilay.
\newblock Conformal language modeling.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2016.

\bibitem[Richter et~al.(2016)Richter, Vineet, Roth, and Koltun]{richter2016gta}
Stephan~R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
\newblock Playing for data: Ground truth from computer games.
\newblock \emph{European Conference on Computer Vision}, 2016.

\bibitem[Ringel et~al.(2024)Ringel, Cohen, Freedman, Elad, and Romano]{ringel2024early}
Liran Ringel, Regev Cohen, Daniel Freedman, Michael Elad, and Yaniv Romano.
\newblock Early time classification with accumulated accuracy gap control.
\newblock \emph{International Conference on Machine Learning}, 2024.

\bibitem[Sankaranarayanan et~al.(2022)Sankaranarayanan, Angelopoulos, Bates, Romano, and Isola]{sankaranarayanan2022semantic}
Swami Sankaranarayanan, Anastasios~Nikolas Angelopoulos, Stephen Bates, Yaniv Romano, and Phillip Isola.
\newblock Semantic uncertainty intervals for disentangled latent spaces.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Saravanan and Kouzani(2023)]{saravanan2023advancements}
Kavya Saravanan and Abbas~Z Kouzani.
\newblock Advancements in on-device deep neural networks.
\newblock \emph{Information}, 2023.

\bibitem[Scardapane et~al.(2020)Scardapane, Scarpiniti, Baccarelli, and Uncini]{scardapane2020should}
Simone Scardapane, Michele Scarpiniti, Enzo Baccarelli, and Aurelio Uncini.
\newblock Why should we add early exits to neural networks?
\newblock \emph{Cognitive Computation}, 2020.

\bibitem[Schuster et~al.(2021)Schuster, Fisch, Jaakkola, and Barzilay]{schuster2021consistent}
Tal Schuster, Adam Fisch, Tommi Jaakkola, and Regina Barzilay.
\newblock Consistent accelerated inference via confident adaptive transformers.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2021.

\bibitem[Schuster et~al.(2022)Schuster, Fisch, Gupta, Dehghani, Bahri, Tran, Tay, and Metzler]{schuster2022confident}
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi~Tay, and Donald Metzler.
\newblock Confident adaptive language modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Shafer and Vovk(2008)]{shafer2008tutorial}
Glenn Shafer and Vladimir Vovk.
\newblock A tutorial on conformal prediction.
\newblock \emph{Journal of Machine Learning Research}, 2008.

\bibitem[Snell et~al.(2022)Snell, Zollo, Deng, Pitassi, and Zemel]{snell2022quantile}
Jake Snell, Thomas~P Zollo, Zhun Deng, Toniann Pitassi, and Richard Zemel.
\newblock Quantile risk control: A flexible framework for bounding the probability of high-loss predictions.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Sponner et~al.(2024)Sponner, Waschneck, and Kumar]{sponner2024adapting}
Max Sponner, Bernd Waschneck, and Akash Kumar.
\newblock Adapting neural networks at runtime: Current trends in at-runtime optimizations for deep learning.
\newblock \emph{ACM Computing Surveys}, 2024.

\bibitem[Tang et~al.(2023)Tang, Wang, Ding, Liang, Li, and Xu]{tang2023deediff}
Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi~Liang, Yao Li, and Dongkuan Xu.
\newblock Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation.
\newblock \emph{arXiv Preprint (arXiv:2309.17074)}, 2023.

\bibitem[Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and Kung]{teerapittayanon2016branchynet}
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural networks.
\newblock \emph{International Conference on Pattern Recognition}, 2016.

\bibitem[Teneggi et~al.(2023)Teneggi, Tivnan, Stayman, and Sulam]{teneggi2023trust}
Jacopo Teneggi, Matthew Tivnan, Web Stayman, and Jeremias Sulam.
\newblock How to trust your diffusion model: A convex optimization approach to conformal risk control.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Timans et~al.(2024)Timans, Straehle, Sakmann, and Nalisnick]{timans2024adaptive}
Alexander Timans, Christoph-Nikolas Straehle, Kaspar Sakmann, and Eric Nalisnick.
\newblock Adaptive bounding box uncertainties via two-step conformal prediction.
\newblock \emph{European Conference on Computer Vision}, 2024.

\bibitem[Wang et~al.(2020)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan, Wang, et~al.]{wang2020hrnet}
Jingdong Wang, Ke~Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et~al.
\newblock Deep high-resolution representation learning for visual recognition.
\newblock \emph{Transactions on Pattern Analysis and Machine Intelligence}, 2020.

\bibitem[Wang et~al.(2021)Wang, Huang, Song, Huang, and Huang]{wang2021not}
Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang.
\newblock Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Waudby-Smith and Ramdas(2024)]{waudby2024estimating}
Ian Waudby-Smith and Aaditya Ramdas.
\newblock Estimating means of bounded random variables by betting.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 2024.

\bibitem[Wo{\l}czyk et~al.(2021)Wo{\l}czyk, W{\'o}jcik, Ba{\l}azy, Podolak, Tabor, {\'S}mieja, and Trzcinski]{wolczyk2021zero}
Maciej Wo{\l}czyk, Bartosz W{\'o}jcik, Klaudia Ba{\l}azy, Igor~T Podolak, Jacek Tabor, Marek {\'S}mieja, and Tomasz Trzcinski.
\newblock Zero time waste: Recycling predictions in early exit neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Xin et~al.(2020)Xin, Tang, Lee, Yu, and Lin]{xin2020deebert}
Ji~Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin.
\newblock Deebert: Dynamic early exiting for accelerating bert inference.
\newblock \emph{Proceedings of the Association for Computational Linguistics}, 2020.

\bibitem[Xu and McAuley(2023)]{xu2022survey}
Canwen Xu and Julian McAuley.
\newblock A survey on dynamic neural networks for natural language processing.
\newblock \emph{Findings of the Association for Computational Linguistics}, 2023.

\bibitem[Xu et~al.(2024)Xu, Yin, Cai, Yi, Xu, Wang, Wu, Zhao, Yang, Wang, et~al.]{xu2024survey}
Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et~al.
\newblock A survey of resource-efficient llm and multimodal foundation models.
\newblock \emph{arXiv Preprint (arXiv:2401.08092)}, 2024.

\bibitem[Xu et~al.(2018)Xu, Ding, Hu, Niemier, Cong, Hu, and Shi]{xu2018scaling}
Xiaowei Xu, Yukun Ding, Sharon~Xiaobo Hu, Michael Niemier, Jason Cong, Yu~Hu, and Yiyu Shi.
\newblock Scaling for edge inference of deep neural networks.
\newblock \emph{Nature Electronics}, 2018.

\bibitem[Xu et~al.(2023)Xu, Guo, and Wei]{xu2023conformal}
Yunpeng Xu, Wenge Guo, and Zhi Wei.
\newblock Conformal risk control for ordinal classification.
\newblock \emph{Conference on Uncertainty in Artificial Intelligence}, 2023.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Xu, Ge, McAuley, Xu, and Wei]{zhou2020bert}
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke~Xu, and Furu Wei.
\newblock Bert loses patience: Fast and robust inference with early exit.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li, et~al.]{zhou2024survey}
Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al.
\newblock A survey on efficient inference for large language models.
\newblock \emph{arXiv Preprint (arXiv:2404.14294)}, 2024.

\bibitem[Zilberstein(1996)]{zilberstein1996using}
Shlomo Zilberstein.
\newblock Using anytime algorithms in intelligent systems.
\newblock \emph{AI Magazine}, 1996.

\bibitem[Zollo et~al.(2023)Zollo, Morrill, Deng, Snell, Pitassi, and Zemel]{zollo2023prompt}
Thomas~P Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann Pitassi, and Richard Zemel.
\newblock Prompt risk control: A rigorous framework for responsible deployment of large language models.
\newblock \emph{International Conference on Learning Representations}, 2023.

\end{thebibliography}
