\begin{thebibliography}{109}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agashe et~al.(2019)Agashe, Iyer, and Zettlemoyer]{agashe2019juice}
Agashe, R., Iyer, S., and Zettlemoyer, L.
\newblock Juice: A large scale distantly supervised dataset for open domain context-based code generation.
\newblock \emph{arXiv preprint arXiv:1910.02216}, 2019.

\bibitem[Ahmad et~al.(2021)Ahmad, Tushar, Chakraborty, and Chang]{ahmad-etal-2021-avatar}
Ahmad, W.~U., Tushar, M. G.~R., Chakraborty, S., and Chang, K.-W.
\newblock Avatar: A parallel corpus for java-python program translation.
\newblock \emph{arXiv preprint arXiv:2108.11590}, 2021.

\bibitem[AI(2023)]{deepseek-coder}
AI, D.
\newblock Deepseek coder: Let the code write itself.
\newblock \url{https://github.com/deepseek-ai/DeepSeek-Coder}, 2023.

\bibitem[Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey, et~al.]{allal2023santacoder}
Allal, L.~B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C.~M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., et~al.
\newblock Santacoder: don't reach for the stars!
\newblock \emph{arXiv preprint arXiv:2301.03988}, 2023.

\bibitem[Alon et~al.(2018)Alon, Brody, Levy, and Yahav]{alon2018code2seq}
Alon, U., Brody, S., Levy, O., and Yahav, E.
\newblock code2seq: Generating sequences from structured representations of code.
\newblock \emph{arXiv preprint arXiv:1808.01400}, 2018.

\bibitem[Arkoudas(2023)]{arkoudas2023gpt}
Arkoudas, K.
\newblock Gpt-4 can't reason.
\newblock \emph{arXiv preprint arXiv:2308.03762}, 2023.

\bibitem[Athiwaratkun et~al.(2022)Athiwaratkun, Gouda, Wang, Li, Tian, Tan, Ahmad, Wang, Sun, Shang, et~al.]{athiwaratkun2022multi}
Athiwaratkun, B., Gouda, S.~K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W.~U., Wang, S., Sun, Q., Shang, M., et~al.
\newblock Multi-lingual evaluation of code generation models.
\newblock \emph{arXiv preprint arXiv:2210.14868}, 2022.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Barone \& Sennrich(2017)Barone and Sennrich]{barone2017parallel}
Barone, A. V.~M. and Sennrich, R.
\newblock A parallel corpus of python functions and documentation strings for automated code documentation and code generation.
\newblock \emph{arXiv preprint arXiv:1707.02275}, 2017.

\bibitem[Berabi et~al.(2021)Berabi, He, Raychev, and Vechev]{berabi2021tfix}
Berabi, B., He, J., Raychev, V., and Vechev, M.
\newblock Tfix: Learning to fix coding errors with a text-to-text transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\  780--791. PMLR, 2021.

\bibitem[Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{berglund2023reversal}
Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A.~C., Korbak, T., and Evans, O.
\newblock The reversal curse: Llms trained on" a is b" fail to learn" b is a".
\newblock \emph{arXiv preprint arXiv:2309.12288}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman, et~al.]{cassano2022multipl}
Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C.~J., Feldman, M.~Q., et~al.
\newblock Multipl-e: A scalable and extensible approach to benchmarking neural code generation.
\newblock \emph{arXiv preprint arXiv:2208.08227}, 2022.

\bibitem[Chen et~al.(2022)Chen, Zhang, Nguyen, Zan, Lin, Lou, and Chen]{chen2022codet}
Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W.
\newblock Codet: Code generation with generated tests.
\newblock \emph{arXiv preprint arXiv:2207.10397}, 2022.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023)Chen, Lin, Sch{\"a}rli, and Zhou]{chen2023teaching}
Chen, X., Lin, M., Sch{\"a}rli, N., and Zhou, D.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}, 2023.

\bibitem[Ding et~al.(2022)Ding, Wang, Ahmad, Ramanathan, Nallapati, Bhatia, Roth, and Xiang]{ding2022cocomic}
Ding, Y., Wang, Z., Ahmad, W.~U., Ramanathan, M.~K., Nallapati, R., Bhatia, P., Roth, D., and Xiang, B.
\newblock Cocomic: Code completion by jointly modeling in-file and cross-file context.
\newblock \emph{arXiv preprint arXiv:2212.10007}, 2022.

\bibitem[Dziri et~al.(2023)Dziri, Lu, Sclar, Li, Jian, Lin, West, Bhagavatula, Bras, Hwang, et~al.]{dziri2023faith}
Dziri, N., Lu, X., Sclar, M., Li, X.~L., Jian, L., Lin, B.~Y., West, P., Bhagavatula, C., Bras, R.~L., Hwang, J.~D., et~al.
\newblock Faith and fate: Limits of transformers on compositionality.
\newblock \emph{arXiv preprint arXiv:2305.18654}, 2023.

\bibitem[Fan et~al.(2023)Fan, Gokkaya, Harman, Lyubarskiy, Sengupta, Yoo, and Zhang]{fan2023large}
Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S., and Zhang, J.~M.
\newblock Large language models for software engineering: Survey and open problems.
\newblock \emph{arXiv preprint arXiv:2310.03533}, 2023.

\bibitem[Fried et~al.(2022)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong, tau Yih, Zettlemoyer, and Lewis]{fried2022incoder}
Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., tau Yih, W., Zettlemoyer, L., and Lewis, M.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock \emph{preprint arXiv:2204.05999}, 2022.

\bibitem[Garg et~al.(2022)Garg, Moghaddam, Clement, Sundaresan, and Wu]{garg2022deepperf}
Garg, S., Moghaddam, R.~Z., Clement, C.~B., Sundaresan, N., and Wu, C.
\newblock Deepperf: A deep learning-based approach for improving software performance.
\newblock \emph{arXiv preprint arXiv:2206.13619}, 2022.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J.~D., and Papailiopoulos, D.
\newblock Looped transformers as programmable computers.
\newblock \emph{arXiv preprint arXiv:2301.13196}, 2023.

\bibitem[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel, Levine, and Song]{gudibande2023false}
Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D.
\newblock The false promise of imitating proprietary llms.
\newblock \emph{arXiv preprint arXiv:2305.15717}, 2023.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, et~al.]{gunasekar2023textbooks}
Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C.~T., Del~Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de~Rosa, G., Saarikivi, O., et~al.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}, 2023.

\bibitem[Gupta et~al.(2017)Gupta, Pal, Kanade, and Shevade]{gupta2017deepfix}
Gupta, R., Pal, S., Kanade, A., and Shevade, S.
\newblock Deepfix: Fixing common c language errors by deep learning.
\newblock In \emph{Proceedings of the aaai conference on artificial intelligence}, volume~31, 2017.

\bibitem[Haluptzok et~al.(2022)Haluptzok, Bowers, and Kalai]{haluptzok2022language}
Haluptzok, P., Bowers, M., and Kalai, A.~T.
\newblock Language models can teach themselves to program better.
\newblock \emph{arXiv preprint arXiv:2207.14502}, 2022.

\bibitem[Haque et~al.(2022)Haque, Ahmad, Lourentzou, and Brown]{haque2022fixeval}
Haque, M. M.~A., Ahmad, W.~U., Lourentzou, I., and Brown, C.
\newblock Fixeval: Execution-based evaluation of program fixes for competitive programming problems.
\newblock 2022.

\bibitem[Hasan et~al.(2021)Hasan, Muttaqueen, Ishtiaq, Mehrab, Haque, Hasan, Ahmad, Iqbal, and Shahriyar]{hasan2021codesc}
Hasan, M., Muttaqueen, T., Ishtiaq, A.~A., Mehrab, K.~S., Haque, M. M.~A., Hasan, T., Ahmad, W.~U., Iqbal, A., and Shahriyar, R.
\newblock Codesc: A large code-description parallel dataset.
\newblock \emph{arXiv preprint arXiv:2105.14220}, 2021.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, et~al.]{hendrycks2021measuring}
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et~al.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{arXiv preprint arXiv:2105.09938}, 2021.

\bibitem[Hosseini et~al.(2021)Hosseini, Reddy, Bahdanau, Hjelm, Sordoni, and Courville]{hosseini2021understanding}
Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, R.~D., Sordoni, A., and Courville, A.
\newblock Understanding by understanding not: Modeling negation in language models.
\newblock \emph{arXiv preprint arXiv:2105.03519}, 2021.

\bibitem[Husain et~al.(2019)Husain, Wu, Gazit, Allamanis, and Brockschmidt]{husain2019codesearchnet}
Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M.
\newblock Codesearchnet challenge: Evaluating the state of semantic code search.
\newblock \emph{arXiv preprint arXiv:1909.09436}, 2019.

\bibitem[Iyer et~al.(2016)Iyer, Konstas, Cheung, and Zettlemoyer]{iyer2016summarizing}
Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L.
\newblock Summarizing source code using a neural attention model.
\newblock In \emph{54th Annual Meeting of the Association for Computational Linguistics 2016}, pp.\  2073--2083. Association for Computational Linguistics, 2016.

\bibitem[Jain et~al.(2022)Jain, Vaidyanath, Iyer, Natarajan, Parthasarathy, Rajamani, and Sharma]{jain2022jigsaw}
Jain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani, S., and Sharma, R.
\newblock Jigsaw: Large language models meet program synthesis.
\newblock In \emph{Proceedings of the 44th International Conference on Software Engineering}, pp.\  1219--1231, 2022.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023{\natexlab{a}}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Liu, Lutellier, and Tan]{jiang2023impact}
Jiang, N., Liu, K., Lutellier, T., and Tan, L.
\newblock Impact of code language models on automated program repair.
\newblock \emph{arXiv preprint arXiv:2302.05020}, 2023{\natexlab{b}}.

\bibitem[Jimenez et~al.(2023)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenez2023swe}
Jimenez, C.~E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K.
\newblock Swe-bench: Can language models resolve real-world github issues?
\newblock \emph{arXiv preprint arXiv:2310.06770}, 2023.

\bibitem[Jin et~al.(2023)Jin, Shahriar, Tufano, Shi, Lu, Sundaresan, and Svyatkovskiy]{jin2023inferfix}
Jin, M., Shahriar, S., Tufano, M., Shi, X., Lu, S., Sundaresan, N., and Svyatkovskiy, A.
\newblock Inferfix: End-to-end program repair with llms.
\newblock \emph{arXiv preprint arXiv:2303.07263}, 2023.

\bibitem[Key et~al.(2022)Key, Li, and Ellis]{key2022speak}
Key, D., Li, W.-D., and Ellis, K.
\newblock I speak, you verify: Toward trustworthy neural program synthesis.
\newblock \emph{arXiv preprint arXiv:2210.00848}, 2022.

\bibitem[Lai et~al.(2023)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, Yih, Fried, Wang, and Yu]{lai2023ds}
Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  18319--18345. PMLR, 2023.

\bibitem[Le et~al.(2022)Le, Wang, Gotmare, Savarese, and Hoi]{le2022coderl}
Le, H., Wang, Y., Gotmare, A.~D., Savarese, S., and Hoi, S. C.~H.
\newblock Coderl: Mastering code generation through pretrained models and deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21314--21328, 2022.

\bibitem[LeClair et~al.(2019)LeClair, Jiang, and McMillan]{leclair2019neural}
LeClair, A., Jiang, S., and McMillan, C.
\newblock A neural model for generating natural language summaries of program subroutines.
\newblock In \emph{2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)}, pp.\  795--806. IEEE, 2019.

\bibitem[Lee et~al.(2023)Lee, Sreenivasan, Lee, Lee, and Papailiopoulos]{lee2023teaching}
Lee, N., Sreenivasan, K., Lee, J.~D., Lee, K., and Papailiopoulos, D.
\newblock Teaching arithmetic to small transformers.
\newblock \emph{arXiv preprint arXiv:2307.03381}, 2023.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal~Lago, A., et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee]{li2023textbooks}
Li, Y., Bubeck, S., Eldan, R., Del~Giorno, A., Gunasekar, S., and Lee, Y.~T.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{arXiv preprint arXiv:2309.05463}, 2023{\natexlab{b}}.

\bibitem[Liguori et~al.(2022)Liguori, Al-Hossami, Cotroneo, Natella, Cukic, and Shaikh]{liguori2022can}
Liguori, P., Al-Hossami, E., Cotroneo, D., Natella, R., Cukic, B., and Shaikh, S.
\newblock Can we generate shellcodes via natural language? an empirical study.
\newblock \emph{Automated Software Engineering}, 29\penalty0 (1):\penalty0 30, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Lu, Chen, Jiang, Svyatkovskiy, Fu, Sundaresan, and Duan]{liu2023code}
Liu, C., Lu, S., Chen, W., Jiang, D., Svyatkovskiy, A., Fu, S., Sundaresan, N., and Duan, N.
\newblock Code execution with pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2305.05383}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Ning, Teng, Liu, Zhou, and Zhang]{liu2023evaluating}
Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y.
\newblock Evaluating the logical reasoning ability of chatgpt and gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03439}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Xia, Wang, and Zhang]{liu2023your}
Liu, J., Xia, C.~S., Wang, Y., and Zhang, L.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2305.01210}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{d}})Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{liu2023pre}
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35, 2023{\natexlab{d}}.

\bibitem[Liu et~al.(2020)Liu, Gao, Chen, Nie, and Liu]{liu2020atom}
Liu, S., Gao, C., Chen, S., Nie, L.~Y., and Liu, Y.
\newblock Atom: Commit message generation based on abstract syntax tree and hybrid ranking.
\newblock \emph{IEEE Transactions on Software Engineering}, 48\penalty0 (5):\penalty0 1800--1817, 2020.

\bibitem[Liu et~al.(2023{\natexlab{e}})Liu, Xu, and McAuley]{liu2023repobench}
Liu, T., Xu, C., and McAuley, J.
\newblock Repobench: Benchmarking repository-level code auto-completion systems.
\newblock \emph{arXiv preprint arXiv:2306.03091}, 2023{\natexlab{e}}.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder}
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D.
\newblock Wizardcoder: Empowering code large language models with evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023.

\bibitem[Madaan et~al.(2023{\natexlab{a}})Madaan, Shypula, Alon, Hashemi, Ranganathan, Yang, Neubig, and Yazdanbakhsh]{madaan2023learning}
Madaan, A., Shypula, A., Alon, U., Hashemi, M., Ranganathan, P., Yang, Y., Neubig, G., and Yazdanbakhsh, A.
\newblock Learning performance-improving code edits.
\newblock \emph{arXiv preprint arXiv:2302.07867}, 2023{\natexlab{a}}.

\bibitem[Madaan et~al.(2023{\natexlab{b}})Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023{\natexlab{b}}.

\bibitem[Malik et~al.(2019)Malik, Patra, and Pradel]{malik2019nl2type}
Malik, R.~S., Patra, J., and Pradel, M.
\newblock Nl2type: inferring javascript function types from natural language information.
\newblock In \emph{2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)}, pp.\  304--315. IEEE, 2019.

\bibitem[Merrill \& Sabharwal(2023)Merrill and Sabharwal]{merrill2023expresssive}
Merrill, W. and Sabharwal, A.
\newblock The expresssive power of transformers with chain of thought.
\newblock \emph{arXiv preprint arXiv:2310.07923}, 2023.

\bibitem[Merrill et~al.(2021)Merrill, Goldberg, Schwartz, and Smith]{merrill2021provable}
Merrill, W.~C., Goldberg, Y., Schwartz, R., and Smith, N.~A.
\newblock Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 1047--1060, 2021.

\bibitem[Miceli-Barone et~al.(2023)Miceli-Barone, Barez, Konstas, and Cohen]{miceli2023larger}
Miceli-Barone, A.~V., Barez, F., Konstas, I., and Cohen, S.~B.
\newblock The larger they are, the harder they fail: Language models do not recognize identifier swaps in python.
\newblock \emph{arXiv preprint arXiv:2305.15507}, 2023.

\bibitem[Mir et~al.(2022)Mir, Lato{\v{s}}kinas, Proksch, and Gousios]{mir2022type4py}
Mir, A.~M., Lato{\v{s}}kinas, E., Proksch, S., and Gousios, G.
\newblock Type4py: Practical deep similarity learning-based type inference for python.
\newblock In \emph{Proceedings of the 44th International Conference on Software Engineering}, pp.\  2241--2252, 2022.

\bibitem[Mizrahi et~al.(2023)Mizrahi, Kaplan, Malkin, Dror, Shahaf, and Stanovsky]{mizrahi2023state}
Mizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., and Stanovsky, G.
\newblock State of what art? a call for multi-prompt llm evaluation.
\newblock \emph{arXiv preprint arXiv:2401.00595}, 2023.

\bibitem[Ni et~al.(2023)Ni, Iyer, Radev, Stoyanov, Yih, Wang, and Lin]{ni2023lever}
Ni, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W.-t., Wang, S., and Lin, X.~V.
\newblock Lever: Learning to verify language-to-code generation with execution.
\newblock In \emph{International Conference on Machine Learning}, pp.\  26106--26128. PMLR, 2023.

\bibitem[Nijkamp et~al.(2022)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong]{nijkamp2022codegen}
Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Nye, M., Andreassen, A.~J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Olausson et~al.(2023{\natexlab{a}})Olausson, Gu, Lipkin, Zhang, Solar-Lezama, Tenenbaum, and Levy]{olausson2023linc}
Olausson, T.~X., Gu, A., Lipkin, B., Zhang, C.~E., Solar-Lezama, A., Tenenbaum, J.~B., and Levy, R.
\newblock Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers.
\newblock \emph{arXiv preprint arXiv:2310.15164}, 2023{\natexlab{a}}.

\bibitem[Olausson et~al.(2023{\natexlab{b}})Olausson, Inala, Wang, Gao, and Solar-Lezama]{olausson2023demystifying}
Olausson, T.~X., Inala, J.~P., Wang, C., Gao, J., and Solar-Lezama, A.
\newblock Demystifying gpt self-repair for code generation.
\newblock \emph{arXiv preprint arXiv:2306.09896}, 2023{\natexlab{b}}.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI, R.
\newblock Gpt-4 technical report. arxiv 2303.08774.
\newblock \emph{View in Article}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez]{patil2023gorilla}
Patil, S.~G., Zhang, T., Wang, X., and Gonzalez, J.~E.
\newblock Gorilla: Large language model connected with massive apis.
\newblock \emph{arXiv preprint arXiv:2305.15334}, 2023.

\bibitem[Pearce et~al.(2022)Pearce, Ahmad, Tan, Dolan-Gavitt, and Karri]{pearce2022asleep}
Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., and Karri, R.
\newblock Asleep at the keyboard? assessing the security of github copilot’s code contributions.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pp.\  754--768. IEEE, 2022.

\bibitem[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, and Gao]{peng2023check}
Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., and Gao, J.
\newblock Check your facts and try again: Improving large language models with external knowledge and automated feedback.
\newblock \emph{arXiv preprint arXiv:2302.12813}, 2023.

\bibitem[Royzen et~al.(2023)Royzen, Wei, and Coleman]{Phind}
Royzen, M., Wei, J., and Coleman, R.
\newblock Phind, 2023.
\newblock URL \url{https://www.phind.com}.

\bibitem[Roziere et~al.(2020)Roziere, Lachaux, Chanussot, and Lample]{roziere2020unsupervised}
Roziere, B., Lachaux, M.-A., Chanussot, L., and Lample, G.
\newblock Unsupervised translation of programming languages.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 20601--20611, 2020.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Shen et~al.(2023)Shen, Zhang, Chen, Zan, Geng, Fu, Zeng, Yu, Ji, Zhao, et~al.]{shen2023pangu}
Shen, B., Zhang, J., Chen, T., Zan, D., Geng, B., Fu, A., Zeng, M., Yu, A., Ji, J., Zhao, J., et~al.
\newblock Pangu-coder2: Boosting large language models for code with ranking feedback.
\newblock \emph{arXiv preprint arXiv:2307.14936}, 2023.

\bibitem[Shi et~al.(2022)Shi, Fried, Ghazvininejad, Zettlemoyer, and Wang]{shi2022natural}
Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S.~I.
\newblock Natural language to code translation with execution.
\newblock \emph{arXiv preprint arXiv:2204.11454}, 2022.

\bibitem[Shi et~al.(2023)Shi, Chen, Misra, Scales, Dohan, Chi, Sch{\"a}rli, and Zhou]{shi2023large}
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.~H., Sch{\"a}rli, N., and Zhou, D.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock In \emph{International Conference on Machine Learning}, pp.\  31210--31227. PMLR, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Labash, and Gopinath]{shinn2023reflexion}
Shinn, N., Labash, B., and Gopinath, A.
\newblock Reflexion: an autonomous agent with dynamic memory and self-reflection.
\newblock \emph{arXiv preprint arXiv:2303.11366}, 2023.

\bibitem[Shrivastava et~al.(2023)Shrivastava, Larochelle, and Tarlow]{shrivastava2023repository}
Shrivastava, D., Larochelle, H., and Tarlow, D.
\newblock Repository-level prompt generation for large language models of code.
\newblock In \emph{International Conference on Machine Learning}, pp.\  31693--31715. PMLR, 2023.

\bibitem[Tian \& Chen(2023)Tian and Chen]{tian2023test}
Tian, Z. and Chen, J.
\newblock Test-case-driven programming understanding in large language models for better code generation.
\newblock \emph{arXiv preprint arXiv:2309.16120}, 2023.

\bibitem[Tony et~al.(2023)Tony, Mutas, Ferreyra, and Scandariato]{tony2023llmseceval}
Tony, C., Mutas, M., Ferreyra, N. E.~D., and Scandariato, R.
\newblock Llmseceval: A dataset of natural language prompts for security evaluations.
\newblock \emph{arXiv preprint arXiv:2303.09384}, 2023.

\bibitem[Tufano et~al.(2019)Tufano, Watson, Bavota, Penta, White, and Poshyvanyk]{tufano2019empirical}
Tufano, M., Watson, C., Bavota, G., Penta, M.~D., White, M., and Poshyvanyk, D.
\newblock An empirical study on learning bug-fixing patches in the wild via neural machine translation.
\newblock \emph{ACM Transactions on Software Engineering and Methodology (TOSEM)}, 28\penalty0 (4):\penalty0 1--29, 2019.

\bibitem[Tufano et~al.(2022)Tufano, Deng, Sundaresan, and Svyatkovskiy]{tufano2022methods2test}
Tufano, M., Deng, S.~K., Sundaresan, N., and Svyatkovskiy, A.
\newblock Methods2test: A dataset of focal methods mapped to test cases.
\newblock In \emph{Proceedings of the 19th International Conference on Mining Software Repositories}, pp.\  299--303, 2022.

\bibitem[Tyen et~al.(2023)Tyen, Mansoor, Chen, Mak, and C{\u{a}}rbune]{tyen2023llms}
Tyen, G., Mansoor, H., Chen, P., Mak, T., and C{\u{a}}rbune, V.
\newblock Llms cannot find reasoning errors, but can correct them!
\newblock \emph{arXiv preprint arXiv:2311.08516}, 2023.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solving}
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Li, Qian, Yang, Wang, Shang, Kumar, Tan, Ray, Bhatia, et~al.]{wang2022recode}
Wang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., et~al.
\newblock Recode: Robustness evaluation of code generation models.
\newblock \emph{arXiv preprint arXiv:2212.10264}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Zhou, Fried, and Neubig]{wang2022execution}
Wang, Z., Zhou, S., Fried, D., and Neubig, G.
\newblock Execution-based evaluation for open-domain code generation.
\newblock \emph{arXiv preprint arXiv:2212.10481}, 2022{\natexlab{b}}.

\bibitem[Watson et~al.(2020)Watson, Tufano, Moran, Bavota, and Poshyvanyk]{watson2020learning}
Watson, C., Tufano, M., Moran, K., Bavota, G., and Poshyvanyk, D.
\newblock On learning meaningful assert statements for unit test cases.
\newblock In \emph{Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering}, pp.\  1398--1409, 2020.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wei et~al.(2023)Wei, Durrett, and Dillig]{wei2023typet5}
Wei, J., Durrett, G., and Dillig, I.
\newblock Typet5: Seq2seq type inference using static analysis.
\newblock \emph{arXiv preprint arXiv:2303.09564}, 2023.

\bibitem[Wu et~al.(2023)Wu, Qiu, Ross, Aky{\"u}rek, Chen, Wang, Kim, Andreas, and Kim]{wu2023reasoning}
Wu, Z., Qiu, L., Ross, A., Aky{\"u}rek, E., Chen, B., Wang, B., Kim, N., Andreas, J., and Kim, Y.
\newblock Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.
\newblock \emph{arXiv preprint arXiv:2307.02477}, 2023.

\bibitem[Xia et~al.(2022)Xia, Wei, and Zhang]{xia2022practical}
Xia, C.~S., Wei, Y., and Zhang, L.
\newblock Practical program repair in the era of large pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2210.14179}, 2022.

\bibitem[Xu et~al.(2022)Xu, Alon, Neubig, and Hellendoorn]{xu2022systematic}
Xu, F.~F., Alon, U., Neubig, G., and Hellendoorn, V.~J.
\newblock A systematic evaluation of large language models of code.
\newblock In \emph{Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming}, pp.\  1--10, 2022.

\bibitem[Yin et~al.(2022)Yin, Li, Xiao, Rao, Wen, Shi, Howland, Bailey, Catasta, Michalewski, et~al.]{yin2022natural}
Yin, P., Li, W.-D., Xiao, K., Rao, A., Wen, Y., Shi, K., Howland, J., Bailey, P., Catasta, M., Michalewski, H., et~al.
\newblock Natural language to code generation in interactive data science notebooks.
\newblock \emph{arXiv preprint arXiv:2212.09248}, 2022.

\bibitem[Zan et~al.(2023)Zan, Chen, Zhang, Lu, Wu, Guan, Yongji, and Lou]{zan2023large}
Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Yongji, W., and Lou, J.-G.
\newblock Large language models meet nl2code: A survey.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  7443--7464, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Chen, Zhang, Liu, Zan, Mao, Lou, and Chen]{zhang2023repocoder}
Zhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.-G., and Chen, W.
\newblock Repocoder: Repository-level code completion through iterative retrieval and generation.
\newblock \emph{arXiv preprint arXiv:2303.12570}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2022)Zhang, Li, Meng, Chang, and Broeck]{zhang2022paradox}
Zhang, H., Li, L.~H., Meng, T., Chang, K.-W., and Broeck, G. V.~d.
\newblock On the paradox of learning to reason from data.
\newblock \emph{arXiv preprint arXiv:2205.11502}, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, Li, Li, and Jin]{zhang2023toolcoder}
Zhang, K., Li, G., Li, J., Li, Z., and Jin, Z.
\newblock Toolcoder: Teach code generation models to use apis with search tools.
\newblock \emph{arXiv preprint arXiv:2305.04032}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Li, Li, Li, and Jin]{zhang2023self}
Zhang, K., Li, Z., Li, J., Li, G., and Jin, Z.
\newblock Self-edit: Fault-aware code editor for code generation.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  769--787, Toronto, Canada, July 2023{\natexlab{c}}. Association for Computational Linguistics.

\bibitem[Zhang et~al.(2023{\natexlab{d}})Zhang, Wang, Xia, Wang, and Li]{zhang2023algo}
Zhang, K., Wang, D., Xia, J., Wang, W.~Y., and Li, L.
\newblock Algo: Synthesizing algorithmic programs with generated oracle verifiers.
\newblock \emph{arXiv preprint arXiv:2305.14591}, 2023{\natexlab{d}}.

\bibitem[Zhang et~al.(2023{\natexlab{e}})Zhang, Chen, Shen, Ding, Tenenbaum, and Gan]{zhang2023planning}
Zhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J.~B., and Gan, C.
\newblock Planning with large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2303.05510}, 2023{\natexlab{e}}.

\bibitem[Zhang et~al.(2023{\natexlab{f}})Zhang, Tigges, Biderman, Raginsky, and Ringer]{zhang2023can}
Zhang, S.~D., Tigges, C., Biderman, S., Raginsky, M., and Ringer, T.
\newblock Can transformers learn to solve problems recursively?
\newblock \emph{arXiv preprint arXiv:2305.14699}, 2023{\natexlab{f}}.

\bibitem[Zhang et~al.(2023{\natexlab{g}})Zhang, Yu, Hashimoto, Lewis, Yih, Fried, and Wang]{zhang2023coder}
Zhang, T., Yu, T., Hashimoto, T., Lewis, M., Yih, W.-t., Fried, D., and Wang, S.
\newblock Coder reviewer reranking for code generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  41832--41846. PMLR, 2023{\natexlab{g}}.

\bibitem[Zhang et~al.(2023{\natexlab{h}})Zhang, Chen, Liu, Liao, Gong, Yu, Li, and Wang]{zhang2023survey}
Zhang, Z., Chen, C., Liu, B., Liao, C., Gong, Z., Yu, H., Li, J., and Wang, R.
\newblock A survey on language models for code.
\newblock 2023{\natexlab{h}}.

\bibitem[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang, Li, et~al.]{zheng2023codegeex}
Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y., et~al.
\newblock Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x.
\newblock \emph{arXiv preprint arXiv:2303.17568}, 2023.

\bibitem[Zhong et~al.(2022)Zhong, Liu, Li, Kuang, Zeng, and Wang]{zhong2022codegen}
Zhong, M., Liu, G., Li, H., Kuang, J., Zeng, J., and Wang, M.
\newblock Codegen-test: An automatic code generation model integrating program test information.
\newblock \emph{arXiv preprint arXiv:2202.07612}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran]{zhou2023algorithms}
Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P.
\newblock What algorithms can transformers learn? a study in length generalization.
\newblock \emph{arXiv preprint arXiv:2310.16028}, 2023.

\bibitem[Zhu et~al.(2022)Zhu, Jain, Suresh, Ravindran, Tipirneni, and Reddy]{zhu2022xlcost}
Zhu, M., Jain, A., Suresh, K., Ravindran, R., Tipirneni, S., and Reddy, C.~K.
\newblock Xlcost: A benchmark dataset for cross-lingual code intelligence.
\newblock \emph{arXiv preprint arXiv:2206.08474}, 2022.

\end{thebibliography}
