
@InProceedings{Kim2021,
  title = 	 {Reward Identification in Inverse Reinforcement Learning},
  author =       {Kim, Kuno and Garg, Shivam and Shiragur, Kirankumar and Ermon, Stefano},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5496--5505},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21c/kim21c.pdf},
  url = 	 {http://proceedings.mlr.press/v139/kim21c.html}
}

@article{amin2017repeated,
  title={Repeated Inverse Reinforcement Learning},
  author={Amin, Kareem and Jiang, Nan and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={1815--1824},
  year={2017}
}

@article{amin2016towards,
  title={Towards resolving unidentifiability in inverse reinforcement learning},
  author={Amin, Kareem and Singh, Satinder},
  journal={arXiv preprint arXiv:1601.06569},
  year={2016}
}

@inproceedings{dvijotham2010,
author = {Dvijotham, Krishnamurthy and Todorov, Emanuel},
title = {Inverse Optimal Control with Linearly-Solvable MDPs},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {335–342},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{ratliff2006,
author = {Ratliff, Nathan D. and Bagnell, J. Andrew and Zinkevich, Martin A.},
title = {Maximum Margin Planning},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143936},
doi = {10.1145/1143844.1143936},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {729–736},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@article{balakrishnan2020efficient,
  title={Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization},
  author={Balakrishnan, Sreejith and Nguyen, Quoc Phong and Low, Bryan Kian Hsiang and Soh, Harold},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{reisinger2020regularity,
  title={Regularity and stability of feedback relaxed controls},
  author={Reisinger, Christoph and Zhang, Yufei},
  journal={SIAM Journal on Control and Optimization},
  volume={59},
  number={5},
  pages={3118--3151},
  year={2021},
  publisher={SIAM}
}
@book{seneta2006,
    title={Non-negative Matrices and {M}arkov chains},
    author={Seneta, E.},
    year={2006},
    publisher={Springer},
    edition={Revised Printing}
    }
    
@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}
@INPROCEEDINGS{Ng99policyinvariance,
    author = {Andrew Y. Ng and Daishi Harada and Stuart Russell},
    title = {Policy invariance under reward transformations: Theory and application to reward shaping},
    booktitle = {In Proceedings of the Sixteenth International Conference on Machine Learning},
    year = {1999},
    pages = {278--287},
    publisher = {Morgan Kaufmann}
}

@inproceedings{fu2017learning,
  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with gaussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={19--27},
  year={2011}
}

@article{finn2016connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@book{krylov2008controlled,
  title={Controlled diffusion processes},
  author={Krylov, Nikolaj Vladimirovi{\v{c}}},
  volume={14},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@article{Lucas1976,
title = {Econometric policy evaluation: A critique},
journal = {Carnegie-Rochester Conference Series on Public Policy},
volume = {1},
pages = {19-46},
year = {1976},
author = {Robert E. Lucas}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{bertsekas2004stochastic,
  title={Stochastic optimal control: the discrete-time case},
  author={Bertsekas, Dimitir P and Shreve, Steven},
  year={2004}
}

@book{fleming2006controlled,
  title={Controlled Markov processes and viscosity solutions},
  author={Fleming, Wendell H and Soner, Halil Mete},
  volume={25},
  year={2006},
  publisher={Springer Science \& Business Media}}

@book{keeney_raiffa_1993, 
place={New York}, 
title={Decisions with Multiple Objectives: Preferences and Value Trade-Offs}, 
publisher={Wiley}, 
author={Keeney, Ralph L. and Raiffa, Howard}, 
year={1976}}

@article{sargent1978estimation,
  title={Estimation of dynamic labor demand schedules under rational expectations},
  author={Sargent, Thomas J},
  journal={Journal of Political Economy},
  volume={86},
  number={6},
  pages={1009--1044},
  year={1978},
  publisher={The University of Chicago Press}
}


@book{dupuis2011weak,
  title={A weak convergence approach to the theory of large deviations},
  author={Dupuis, Paul and Ellis, Richard S},
  volume={902},
  year={2011},
  publisher={John Wiley \& Sons}
}


@book{boyd1994linear,
  title={Linear matrix inequalities in system and control theory},
  author={Boyd, Stephen and El Ghaoui, Laurent and Feron, Eric and Balakrishnan, Venkataramanan},
  year={1994},
  publisher={SIAM}
}

@article{Kalman64,
    author = {Kalman, R. E.},
    title = {{When Is a Linear Control System Optimal?}},
    journal = {Journal of Basic Engineering},
    volume = {86},
    number = {1},
    pages = {51-60},
    year = {1964},
    month = {03}
}
@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@inproceedings{boularias2011relative,
  title={Relative entropy inverse reinforcement learning},
  author={Boularias, Abdeslam and Kober, Jens and Peters, Jan},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={182--189},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{russell1998learning,
  title={Learning agents for uncertain environments},
  author={Russell, Stuart},
  booktitle={Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
  pages={101--103},
  year={1998}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning},
  author={Ng, Andrew Y and Russell, Stuart},
  booktitle={Proceedings of Seventeenth International Conference on Machine Learning},
  year={2000},
  organization={Citeseer}
}



@phdthesis{ziebart2010modeling,
author = {Ziebart, Brian D},
school = {Carnegie Mellon University},
title = {{Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy}},
year = {2010}
}


@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={49--58},
  year={2016}
}

@inproceedings{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in neural information processing systems},
  pages={4565--4573},
  year={2016}
}
@article{Magnac2002,
author = {Magnac, Thierry and Thesmar, David},
title = {Identifying Dynamic Discrete Decision Processes},
journal = {Econometrica},
volume = {70},
number = {2},
pages = {801-816},
year = {2002}
}

@inproceedings{
jeon2020regularized,
title={Regularized Inverse Reinforcement Learning},
author={Wonseok Jeon and Chen-Yang Su and Paul Barde and Thang Doan and Derek Nowrouzezahrai and Joelle Pineau},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=HgLO8yalfwc}
}

@inproceedings{jacq2019learning,
  title={Learning from a Learner},
  author={Jacq, Alexis and Geist, Matthieu and Paiva, Ana and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={2990--2999},
  year={2019}
}

@article{tao2020learn,
  title={Learn to Exceed: Stereo Inverse Reinforcement Learning with Concurrent Policy Optimization},
  author={Tao, Feng and Cao, Yongcan},
  journal={arXiv preprint arXiv:2009.09577},
  year={2020}
}

@article{krishnamurthy2020langevin,
  title={Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms},
  author={Krishnamurthy, Vikram and Yin, George},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={121},
  pages={1--49},
  year={2021}
}

@inproceedings{self2020online,
  title={Online inverse reinforcement learning for systems with disturbances},
  author={Self, Ryan and Abudia, Moad and Kamalapurkar, Rushikesh},
  booktitle={2020 American Control Conference (ACC)},
  pages={1118--1123},
  year={2020},
  organization={IEEE}
}

@article{wang2020reinforcement,
  title={Reinforcement learning in continuous time and space: A stochastic control approach},
  author={Wang, Haoran and Zariphopoulou, Thaleia and Zhou, Xun Yu},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={198},
  pages={1--34},
  year={2020}
}
