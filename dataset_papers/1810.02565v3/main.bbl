\begin{thebibliography}{10}

\bibitem{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock {\em The Journal of Machine Learning Research}, 18(1):8194--8244,
  2017.

\bibitem{allen2017natasha}
Zeyuan Allen-Zhu.
\newblock Natasha: Faster non-convex stochastic optimization via strongly
  non-convex parameter.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 89--97. JMLR. org, 2017.

\bibitem{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  699--707, 2016.

\bibitem{allen2016improved}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In {\em International conference on machine learning}, pages
  1080--1089, 2016.

\bibitem{arora2015simple}
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock In {\em Proceedings of Machine Learning Research}, 2015.

\bibitem{balles2016coupling}
Lukas Balles, Javier Romero, and Philipp Hennig.
\newblock Coupling adaptive batch sizes with learning rates.
\newblock {\em arXiv preprint arXiv:1612.05086}, 2016.

\bibitem{benaim1998recursive}
Michel Bena{\"\i}m.
\newblock Recursive algorithms, urn processes and chaining number of chain
  recurrent sets.
\newblock {\em Ergodic Theory and Dynamical Systems}, 18(1):53--87, 1998.

\bibitem{benaim2008class}
Michel Benaim and Jean-Yves Le~Boudec.
\newblock A class of mean field interaction models for computer and
  communication systems.
\newblock {\em Performance evaluation}, 65(11-12):823--838, 2008.

\bibitem{betancourt2018symplectic}
Michael Betancourt, Michael~I Jordan, and Ashia~C Wilson.
\newblock On symplectic optimization.
\newblock {\em arXiv preprint arXiv:1802.03653}, 2018.

\bibitem{black1973pricing}
Fischer Black and Myron Scholes.
\newblock The pricing of options and corporate liabilities.
\newblock {\em Journal of political economy}, 81(3):637--654, 1973.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem{chen2018}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, pages 6572--6583. Curran Associates, Inc., 2018.

\bibitem{chen2015solving}
Yuxin Chen and Emmanuel Candes.
\newblock Solving random quadratic systems of equations is nearly as easy as
  solving linear systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  739--747, 2015.

\bibitem{ciccone2018nais}
Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, and
  Faustino Gomez.
\newblock Nais-net: Stable deep networks from non-autonomous differential
  equations.
\newblock {\em arXiv preprint arXiv:1804.07209}, 2018.

\bibitem{daneshmand2018escaping}
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann.
\newblock Escaping saddles with stochastic gradients.
\newblock {\em arXiv preprint arXiv:1803.05999}, 2018.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in neural information processing systems}, pages
  1646--1654, 2014.

\bibitem{durrett2010probability}
Rick Durrett.
\newblock {\em Probability: theory and examples}.
\newblock Cambridge university press, 2010.

\bibitem{einstein1905motion}
Albert Einstein et~al.
\newblock On the motion of small particles suspended in liquids at rest
  required by the molecular-kinetic theory of heat.
\newblock {\em Annalen der physik}, 17:549--560, 1905.

\bibitem{feng2019uniform}
Yuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu.
\newblock Uniform-in-time weak error analysis for stochastic gradient descent
  algorithms via diffusion approximation.
\newblock {\em arXiv preprint arXiv:1902.00635}, 2019.

\bibitem{friedlander2012hybrid}
Michael~P Friedlander and Mark Schmidt.
\newblock Hybrid deterministic-stochastic methods for data fitting.
\newblock {\em SIAM Journal on Scientific Computing}, 34(3):A1380--A1405, 2012.

\bibitem{ghadimi2012optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492, 2012.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{girsanov1960transforming}
Igor~Vladimirovich Girsanov.
\newblock On transforming a certain class of stochastic processes by absolutely
  continuous substitution of measures.
\newblock {\em Theory of Probability \& Its Applications}, 5(3):285--301, 1960.

\bibitem{goel2016stochastic}
Narendra~S Goel and Nira Richter-Dyn.
\newblock {\em Stochastic models in biology}.
\newblock Elsevier, 2016.

\bibitem{hardt2016gradient}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock {\em The Journal of Machine Learning Research}, 19(1):1025--1068,
  2018.

\bibitem{harikandeh2015stopwasting}
Reza Harikandeh, Mohamed~Osama Ahmed, Alim Virani, Mark Schmidt, Jakub
  Kone{\v{c}}n{\`y}, and Scott Sallinen.
\newblock Stopwasting my gradients: Practical svrg.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2251--2259, 2015.

\bibitem{he2018differential}
Li~He, Qi~Meng, Wei Chen, Zhi-Ming Ma, and Tie-Yan Liu.
\newblock Differential equations for modeling asynchronous algorithms.
\newblock {\em arXiv preprint arXiv:1805.02991}, 2018.

\bibitem{hu2017diffusion}
Wenqing Hu, Chris~Junchi Li, Lei Li, and Jian-Guo Liu.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1705.07562}, 2017.

\bibitem{ikeda2014stochastic}
Nobuyuki Ikeda and Shinzo Watanabe.
\newblock {\em Stochastic differential equations and diffusion processes},
  volume~24.
\newblock Elsevier, 2014.

\bibitem{ito1944109}
Kiyosi It{\^o}.
\newblock Stochastic integral.
\newblock {\em Proceedings of the Imperial Academy}, 20(8):519--524, 1944.

\bibitem{jastrzkebski2017three}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{krichene2017acceleration}
Walid Krichene and Peter~L Bartlett.
\newblock Acceleration and averaging in stochastic descent dynamics.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6796--6806, 2017.

\bibitem{krichene2015accelerated}
Walid Krichene, Alexandre Bayen, and Peter~L Bartlett.
\newblock Accelerated mirror descent in continuous and discrete time.
\newblock In {\em Advances in neural information processing systems}, pages
  2845--2853, 2015.

\bibitem{kushner2003stochastic}
Harold Kushner and G~George Yin.
\newblock {\em Stochastic approximation and recursive algorithms and
  applications}, volume~35.
\newblock Springer Science \& Business Media, 2003.

\bibitem{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2348--2358, 2017.

\bibitem{li2015stochastic}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  2101--2110, 2017.

\bibitem{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem{mandt2016variational}
Stephan Mandt, Matthew Hoffman, and David Blei.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  354--363, 2016.

\bibitem{mao2007stochastic}
Xuerong Mao.
\newblock {\em Stochastic differential equations and applications}.
\newblock Elsevier, 2007.

\bibitem{mertikopoulos2018convergence}
Panayotis Mertikopoulos and Mathias Staudigl.
\newblock On the convergence of gradient-like flows with noisy gradient input.
\newblock {\em SIAM Journal on Optimization}, 28(1):163--197, 2018.

\bibitem{moulines2011non}
Eric Moulines and Francis~R Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  451--459, 2011.

\bibitem{nemirovsky1983problem}
Arkadii~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock {\em Problem complexity and method efficiency in optimization.}
\newblock John Wiley and Sons, 1983.

\bibitem{nesterov2018lectures}
Yurii Nesterov.
\newblock {\em Lectures on convex optimization}.
\newblock Springer, 2018.

\bibitem{oksendal1990stochastic}
Bernt {\O}ksendal.
\newblock When is a stochastic integral a time change of a diffusion?
\newblock {\em Journal of theoretical probability}, 3(2):207--226, 1990.

\bibitem{oksendal2003stochastic}
Bernt {\O}ksendal.
\newblock Stochastic differential equations.
\newblock In {\em Stochastic differential equations}. Springer, 2003.

\bibitem{orvietoshadowing}
Antonio Orvieto and Aurelien Lucchi.
\newblock Shadowing properties of optimization algorithms.
\newblock {\em arXiv preprint}, 2019.

\bibitem{perko2013differential}
Lawrence Perko.
\newblock {\em Differential equations and dynamical systems}, volume~7.
\newblock Springer Science \& Business Media, 2013.

\bibitem{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3(4):643--653, 1963.

\bibitem{raginsky2012continuous}
Maxim Raginsky and Jake Bouvrie.
\newblock Continuous-time stochastic mirror descent on a network: Variance
  reduction, consensus, convergence.
\newblock In {\em Decision and Control (CDC), 2012 IEEE 51st Annual Conference
  on}, pages 6793--6800. IEEE, 2012.

\bibitem{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock {\em arXiv preprint arXiv:1702.03849}, 2017.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages
  314--323, 2016.

\bibitem{reddi2016proximal}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alexander~J Smola.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1145--1153, 2016.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{roux2012stochastic}
Nicolas~L Roux, Mark Schmidt, and Francis~R Bach.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In {\em Advances in neural information processing systems}, pages
  2663--2671, 2012.

\bibitem{shi2019acceleration}
Bin Shi, Simon~S Du, Weijie~J Su, and Michael~I Jordan.
\newblock Acceleration via symplectic discretization of high-resolution
  differential equations.
\newblock {\em arXiv preprint arXiv:1902.03694}, 2019.

\bibitem{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1901.06053}, 2019.

\bibitem{stroock2007multidimensional}
Daniel~W Stroock and SR~Srinivasa Varadhan.
\newblock {\em Multidimensional diffusion processes}.
\newblock Springer, 2007.

\bibitem{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling nesterov’s accelerated
  gradient method: Theory and insights.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem{sun2016guaranteed}
Ruoyu Sun and Zhi-Quan Luo.
\newblock Guaranteed matrix completion via non-convex factorization.
\newblock {\em IEEE Transactions on Information Theory}, 62(11):6535--6579,
  2016.

\bibitem{wigner1990unreasonable}
Eugene~P Wigner.
\newblock The unreasonable effectiveness of mathematics in the natural
  sciences.
\newblock In {\em Mathematics and Science}, pages 291--306. World Scientific,
  1990.

\bibitem{wilson2016lyapunov}
Ashia~C Wilson, Benjamin Recht, and Michael~I Jordan.
\newblock A lyapunov analysis of momentum methods in optimization.
\newblock {\em arXiv preprint arXiv:1611.02635}, 2016.

\bibitem{xu2018accelerated}
Pan Xu, Tianhao Wang, and Quanquan Gu.
\newblock Accelerated stochastic mirror descent: From continuous-time dynamics
  to discrete-time algorithms.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1087--1096, 2018.

\bibitem{xu2018continuous}
Pan Xu, Tianhao Wang, and Quanquan Gu.
\newblock Continuous and discrete-time accelerated stochastic mirror descent
  for strongly convex functions.
\newblock In {\em International Conference on Machine Learning}, pages
  5488--5497, 2018.

\bibitem{zhang2016new}
Hui Zhang.
\newblock New analysis of linear convergence of gradient-type methods via
  unifying error bound conditions.
\newblock {\em Mathematical Programming}, pages 1--46, 2016.

\bibitem{zhang2013gradient}
Hui Zhang and Wotao Yin.
\newblock Gradient methods for convex minimization: better rates under weaker
  conditions.
\newblock {\em arXiv preprint arXiv:1303.4645}, 2013.

\bibitem{zhang2018direct}
Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie.
\newblock Direct runge-kutta discretization achieves acceleration.
\newblock {\em arXiv preprint arXiv:1805.00521}, 2018.

\end{thebibliography}
