% Encoding: UTF-8
@inproceedings{vaswani2017attention,
  title={{Attention Is All You Need}},
  author={Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. and Kaiser, {\L}. and Polosukhin, I.},
  booktitle={Neural Information Processing Systems},
  year={2017}
}

@inproceedings{carion2020end,
  title={{End-to-End Object Detection with Transformers}},
  author={Carion, N. and Massa, F. and Synnaeve, G. and Usunier, N. and Kirillov, A. and Zagoruyko, S.},
  booktitle={European Conference on Computer Vision},
  year={2020}
}


@techreport{brown2020language,
  title={{Language Models are Few-Shot Learners}},
  author={Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and others},
  type={Preprint},
  number={arXiv:2005.14165},
  year={2020}
}
@inproceedings{devlin2019bert,
  title={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Devlin, J. and Chang, M. and Lee, K. and Toutanova, K.},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{ott2018scaling,
  title={{Scaling Neural Machine Translation}},
  author={Ott, M. and Edunov, S. and Grangier, D. and Auli, M.},
  booktitle={Machine Translation},
  year={2018}
}

@inproceedings{wang2018glue,
  title={{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
  author={Wang, A. and Singh, A. and Michael, J. and Hill, F. and Levy, O. and Bowman, S.},
  booktitle={EMNLP Workshop BlackboxNLP},
  year={2018}
}

@techreport{mohamed2019transformers,
  title={Transformers with convolutional context for {ASR}},
  author={Mohamed, A. and Okhonko, D. and Zettlemoyer, L.},
  type={Preprint},
  number={arXiv:1904.11660},
  year={2019}
}

@techreport{child2019generating,
  title={{Generating Long Sequences with Sparse Transformers}},
  author={Child, R. and Gray, S. and Radford, A. and Sutskever, I.},
  type={Preprint},
  number={arXiv:1904.10509},
  year={2019}
}

@inproceedings{li2019enhancing,
  title={{Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting}},
  author={Li, S. and Jin, X. and Xuan, Y. and Zhou, X. and Chen, W. and Wang, Y. and Yan, X.},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@techreport{beltagy2020longformer,
  title={{Longformer: The Long-Document Transformer}},
  author={Beltagy, I. and Peters, M. and Cohan, A.},
  type={Preprint},
  number={arXiv:2004.05150},
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={{Reformer: The Efficient Transformer}},
  author={Kitaev, N. and Kaiser, L. and Levskaya, A.},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{wang2020linformer,
  title={{Linformer: Self-Attention with Linear Complexity}},
  author={Wang, S. and Li, B. and Khabsa, M. and Fang, H. and Ma, H.},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{katharopoulos2020transformers,
  title={{Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}},
  author={Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{gong2019efficient,
  title={{Efficient Training of BERT by Progressively Stacking}},
  author={Gong, L. and He, D. and Li, Z. and Qin, T. and Wang, L. and Liu, T.},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{cui2019fine,
  title={{Fine-tune BERT with Sparse Self-Attention Mechanism}},
  author={Cui, B. and Li, Y. and Chen, M. and Zhang, Z.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2019}
}

@inproceedings{kovaleva2019revealing,
  title={{Revealing the Dark Secrets of BERT}},
  author={Kovaleva, O. and Romanov, A. and Rogers, A. and Rumshisky, A.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2019}
}

@inproceedings{yun2019transformers,
  title={{Are Transformers universal approximators of sequence-to-sequence functions?}},
  author={Yun, C. and Bhojanapalli, S. and Rawat, A. and Reddi, S. and Kumar, S.},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{zaheer2020big,
  title={{Big Bird: Transformers for Longer Sequences}},
  author={Zaheer, M. and Guruganesh, G. and Dubey, K. and Ainslie, J. and Alberti, C. and Ontanon, S. and Pham, P. and Ravula, A. and Wang, Q. and Yang, L. and others},
  booktitle={Neural Information Processing Systems},
  year={2020}
}

@inproceedings{yun2020n,
  title={{${O}(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers}},
  author={Yun, C. and Chang, Y. and Bhojanapalli, S. and Rawat, A. and Reddi, S. and Kumar, S.},
  booktitle={Neural Information Processing Systems},
  year={2020}
}

@inproceedings{park2019sanvis,
  title={{SANVis: Visual Analytics for Understanding Self-Attention Networks}},
  author={Park, C. and Na, I. and Jo, Y. and Shin, S. and Yoo, J. and Kwon, B. and Zhao, J. and Noh, H. and Lee, Y. and Choo, J.},
  booktitle={IEEE Visualization Conference},
  year={2019}
}

@inproceedings{guo2019star,
  title={Star-{T}ransformer},
  author={Guo, Q. and Qiu, X. and Liu, P. and Shao, Y. and Xue, X. and Zhang, Z.},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{liu2018darts,
  title={{DARTS: Differentiable Architecture Search}},
  author={Liu, H. and Simonyan, K. and Yang, Y.},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{zhu2015aligning,
  title={{Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books}},
  author={Zhu, Y. and Kiros, R. and Zemel, R. and Salakhutdinov, R. and Urtasun, R. and Torralba, A. and Fidler, S.},
  booktitle={International Conference on Computer Vision},
  year={2015}
}

@inproceedings{clark2019does,
  title={{What Does BERT Look at? An Analysis of BERT's Attention}},
  author={Clark, K. and Khandelwal, U. and Levy, O. and Manning, C.},
  booktitle={ACL Workshop BlackboxNLP},
  year={2019}
}

@inproceedings{zellers2018swag,
  title={{SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference}},
  author={Zellers, R. and Bisk, Y. and Schwartz, R. and Choi, Y.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2018}
}

@techreport{bi2020gold,
  title={{GOLD-NAS: Gradual, One-Level, Differentiable}},
  author={Bi, K. and Xie, L. and Chen, X. and Wei, L. and Tian, Q.},
  type={Preprint},
  number={arXiv:2007.03331},
  year={2020}
}

@inproceedings{xie2018snas,
  title={{SNAS: Stochastic Neural Architecture Search}},
  author={Xie, S. and Zheng, H. and Liu, C. and Lin, L.},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@techreport{wu2016google,
  title={{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
  author={Wu, Y. and Schuster, M. and Chen, Z. and Le, Q. and Norouzi, M. and Macherey, W. and Krikun, M. and Cao, Y. and Gao, Q. and Macherey, K. and others},
  type={Preprint},
  number={arXiv:1609.08144},
  year={2016}
}

@inproceedings{michel2019sixteen,
  title={{Are Sixteen Heads Really Better than One?}},
  author={Michel, P. and Levy, O. and Neubig, G.},
  booktitle={Neural Information Processing Systems},
  year={2019}
}


@inproceedings{molchanov2017pruning,
  title={{Pruning Convolutional Neural Networks for Resource Efficient Inference}},
  author={Molchanov, P. and Tyree, S. and Karras, T. and Aila, T. and Kautz, J.},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{williams2018broad,
  title={{A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference}},
  author={Williams, A. and Nangia, N. and Bowman, S.},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018}
}

@article{chen2018quora,
  title={{Quora question pairs}},
  author={Chen, Z. and Zhang, H. and Zhang, X. and Zhao, L.},
  journal={University of Waterloo},
  year={2018}
}

@inproceedings{rajpurkar2016squad,
  title={{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
  author={Rajpurkar, P. and Zhang, J. and Lopyrev, K. and Liang, P.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2016}
}

@inproceedings{wang2018multi,
  title={{Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering}},
  author={Wang, W. and Yan, M. and Wu, C.},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{socher2013recursive,
  title={{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
  author={Socher, R. and Perelygin, A. and Wu, J. and Chuang, J. and Manning, C. and Ng, A. and Potts, C.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2013}
}

@article{warstadt2019neural,
  title={{Neural Network Acceptability Judgments}},
  author={Warstadt, A. and Singh, A. and Bowman, S.},
  journal={Transactions of the Association for Computational Linguistics},
  year={2019}
}


@inproceedings{cer2017semeval,
  title={{SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation}},
  author={Cer, D. and Diab, M. and Agirre, E. and Lopez-Gazpio, I. and Specia, L.},
  booktitle={International Workshop on Semantic Evaluation},
  year={2017}
}

@inproceedings{dolan2005automatically,
  title={{Automatically Constructing a Corpus of Sentential Paraphrases}},
  author={Dolan, W. and Brockett, C.},
  booktitle={International Workshop on Paraphrasing},
  year={2005}
}

@inproceedings{bentivogli2009fifth,
  title={{The Fifth PASCAL Recognizing Textual Entailment Challenge}},
  author={Bentivogli, L. and Dagan, I. and Hoa, D. and Giampiccolo, D. and Magnini, B.},
  booktitle={TAC 2009 Workshop},
  year={2009},
}

@inproceedings{maddison2017concrete,
  title={{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
  author={Maddison, C. and Mnih, A. and Teh, Y.},
  year={2017},
  booktitle={International Conference on Learning Representations}
}

@article{elsken2019neural,
  title={{Neural Architecture Search: A Survey}},
  author={Elsken, T. and Metzen, J. and Hutter, F.},
  journal={Journal of Machine Learning Research},
  year={2019}
}

@inproceedings{qiu2020blockwise,
  title={{Blockwise Self-Attention for Long Document Understanding}},
  author={Qiu, J. and Ma, H. and Levy, O. and Yih, W. and Wang, S. and Tang, J.},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2020}
}

@inproceedings{rajpurkar2018know,
  title={{Know What You Don't Know: Unanswerable Questions for SQuAD}},
  author={Rajpurkar, P. and Jia, R. and Liang, P.},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{dosovitskiy2020image,
  title={{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
  author={Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}