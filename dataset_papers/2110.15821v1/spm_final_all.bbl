\begin{thebibliography}{10}

\bibitem{absil2009optimization}
P.-A. Absil, R.~Mahony, and R.~Sepulchre.
\newblock {\em Optimization Algorithms on Matrix Manifolds}.
\newblock Princeton University Press, 2009.

\bibitem{absil2013extrinsic}
P.-A. Absil, R.~Mahony, and J.~Trumpf.
\newblock An extrinsic look at the {R}iemannian {H}essian.
\newblock In {\em International Conference on Geometric Science of
  Information}, pages 361--368. Springer, 2013.

\bibitem{adamczak2011restricted}
R.~Adamczak, A.~E. Litvak, A.~Pajor, and N.~Tomczak-Jaegermann.
\newblock Restricted isometry property of matrices with independent columns and
  neighborly polytopes by random sampling.
\newblock {\em Constructive Approximation}, 34(1):61--88, 2011.

\bibitem{JMLR:v15:anandkumar14b}
A.~Anandkumar, R.~Ge, D.~Hsu, S.~M. Kakade, and M.~Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em Journal of Machine Learning Research}, 15(80):2773--2832, 2014.

\bibitem{anandkumar2017analyzing}
A.~Anandkumar, R.~Ge, and M.~Janzamin.
\newblock Analyzing tensor power method dynamics in overcomplete regime.
\newblock {\em The Journal of Machine Learning Research}, 18(1):752--791, 2017.

\bibitem{anandkumar2012method}
A.~Anandkumar, D.~Hsu, and S.~M. Kakade.
\newblock A method of moments for mixture models and hidden {M}arkov models.
\newblock In {\em Conference on Learning Theory}, pages 33--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem{auddy2020perturbation}
A.~Auddy and M.~Yuan.
\newblock Perturbation bounds for orthogonally decomposable tensors and their
  applications in high dimensional data analysis.
\newblock {\em arXiv preprint arXiv:2007.09024}, 2020.

\bibitem{bandeira2018notes}
A.~S. Bandeira, A.~Perry, and A.~S. Wein.
\newblock Notes on computational-to-statistical gaps: {P}redictions using
  statistical physics.
\newblock {\em Portugaliae Mathematica}, 75(2):159--186, 2018.

\bibitem{boumal2020intromanifolds}
N.~Boumal.
\newblock An introduction to optimization on smooth manifolds.
\newblock Available online, Nov 2020.

\bibitem{bryson2019marchenko}
J.~Bryson, R.~Vershynin, and H.~Zhao.
\newblock Marchenko-pastur law with relaxed independence conditions.
\newblock {\em arXiv preprint arXiv:1912.12724}, 2019.

\bibitem{chen2016perturbation}
Y.~M. Chen, X.~S. Chen, and W.~Li.
\newblock On perturbation bounds for orthogonal projections.
\newblock {\em Numerical Algorithms}, 73(2):433--444, 2016.

\bibitem{chiantini2002weakly}
L.~Chiantini and C.~Ciliberto.
\newblock Weakly defective varieties.
\newblock {\em Transactions of the American Mathematical Society},
  354(1):151--178, 2002.

\bibitem{christensen2003introduction}
O.~Christensen et~al.
\newblock {\em An Introduction to Frames and Riesz Bases}, volume~7.
\newblock Springer, 2003.

\bibitem{chu1995rank}
M.~T. Chu, R.~E. Funderlic, and G.~H. Golub.
\newblock A rank--one reduction formula and its applications to matrix
  factorizations.
\newblock {\em \textit{SIAM Review}}, 37(4):512--530, 1995.

\bibitem{comon2008symmetric}
P.~Comon, G.~Golub, L.-H. Lim, and B.~Mourrain.
\newblock Symmetric tensors and symmetric tensor rank.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  30(3):1254--1279, 2008.

\bibitem{foobi2007}
L.~De~Lathauwer, J.~Castaing, and J.-F. Cardoso.
\newblock Fourth-order cumulant-based blind identification of underdetermined
  mixtures.
\newblock {\em \textit{IEEE Transactions on Signal Processing}},
  55(6):2965--2973, 2007.

\bibitem{fengler2019restricted}
A.~Fengler and P.~Jung.
\newblock On the restricted isometry property of centered self {K}hatri-{R}ao
  products.
\newblock {\em arXiv preprint arXiv:1905.09245}, 2019.

\bibitem{fickus2012steiner}
M.~Fickus, D.~G. Mixon, and J.~C. Tremain.
\newblock Steiner equiangular tight frames.
\newblock {\em Linear Algebra and its Applications}, 436(5):1014--1027, 2012.

\bibitem{fiedler2021stable}
C.~Fiedler, M.~Fornasier, T.~Klock, and M.~Rauchensteiner.
\newblock Stable recovery of entangled weights: Towards robust identification
  of deep neural networks from minimal samples.
\newblock {\em arXiv preprint arXiv:2101.07150}, 2021.

\bibitem{fornasier2019robust}
M.~Fornasier, T.~Klock, and M.~Rauchensteiner.
\newblock Robust and resource efficient identification of two hidden layer
  neural networks.
\newblock {\em Construction Approximation}, to appear, 2021.

\bibitem{fornasiershallow}
M.~Fornasier, J.~Vyb√≠ral, and I.~Daubechies.
\newblock {Robust and resource efficient identification of shallow neural
  networks by fewest samples}.
\newblock {\em Information and Inference: A Journal of the IMA}, 2021.

\bibitem{ge2015learning}
R.~Ge, Q.~Huang, and S.~M. Kakade.
\newblock Learning mixtures of gaussians in high dimensions.
\newblock In {\em Proceedings of the Forty-Seventh Annual ACM Symposium on
  Theory of Computing}, pages 761--770, 2015.

\bibitem{ge2020optimization}
R.~Ge and T.~Ma.
\newblock On the optimization landscape of tensor decompositions.
\newblock {\em Mathematical Programming}, pages 1--47, 2020.

\bibitem{harris2013algebraic}
J.~Harris.
\newblock {\em Algebraic Geometry: A First Course}, volume 133.
\newblock Springer Science \& Business Media, 2013.

\bibitem{hillar2013most}
C.~J. Hillar and L.-H. Lim.
\newblock Most tensor problems are {NP}-hard.
\newblock {\em \textit{Journal of the ACM (JACM)}}, 60(6):45:1--45:39, 2013.

\bibitem{janzamin2015beating}
M.~Janzamin, H.~Sedghi, and A.~Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em arXiv preprint arXiv:1506.08473}, 2015.

\bibitem{kileel2019subspace}
J.~Kileel and J.~M. Pereira.
\newblock Subspace power method for symmetric tensor decomposition and
  generalized {PCA}.
\newblock {\em arXiv preprint arXiv:1912.04007}, 2019.

\bibitem{kolda2015numerical}
T.~G. Kolda.
\newblock Numerical optimization for symmetric tensor decomposition.
\newblock {\em Mathematical Programming}, 151(1):225--248, 2015.

\bibitem{kolda2009tensor}
T.~G. Kolda and B.~W. Bader.
\newblock Tensor decompositions and applications.
\newblock {\em SIAM Review}, 51(3):455--500, 2009.

\bibitem{kolda2011shifted}
T.~G. Kolda and J.~R. Mayo.
\newblock Shifted power method for computing tensor eigenpairs.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  32(4):1095--1124, 2011.

\bibitem{kolda2014adaptive}
T.~G. Kolda and J.~R. Mayo.
\newblock An adaptive shifted power method for computing generalized tensor
  eigenpairs.
\newblock {\em \textit{SIAM Journal on Matrix Analysis and Applications}},
  35(4):1563--1581, 2014.

\bibitem{mccullagh2018tensor}
P.~McCullagh.
\newblock {\em Tensor Methods in Statistics}.
\newblock Courier Dover Publications, 2018.

\bibitem{oymak2021learning}
S.~Oymak and M.~Soltanolkotabi.
\newblock Learning a deep convolutional neural network via tensor
  decomposition.
\newblock {\em Information and Inference: A Journal of the IMA}, 2021.

\bibitem{qu2019geometric}
Q.~Qu, Y.~Zhai, X.~Li, Y.~Zhang, and Z.~Zhu.
\newblock Geometric analysis of nonconvex optimization landscapes for
  overcomplete learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{rudelson2013hanson}
M.~Rudelson and R.~Vershynin.
\newblock Hanson-{W}right inequality and sub-{G}aussian concentration.
\newblock {\em Electronic Communications in Probability}, 18, 2013.

\bibitem{sanjabi2019does}
M.~Sanjabi, S.~Baharlouei, M.~Razaviyayn, and J.~D. Lee.
\newblock When does non-orthogonal tensor decomposition have no spurious local
  minima?
\newblock {\em arXiv preprint arXiv:1911.09815}, 2019.

\bibitem{sedghi2016provable}
H.~Sedghi, M.~Janzamin, and A.~Anandkumar.
\newblock Provable tensor methods for learning mixtures of generalized linear
  models.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1223--1231.
  PMLR, 2016.

\bibitem{sherman2020estimating}
S.~Sherman and T.~G. Kolda.
\newblock Estimating higher-order moments using symmetric tensor decomposition.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  41(3):1369--1387, 2020.

\bibitem{sidiropoulos2017tensor}
N.~D. Sidiropoulos, L.~De~Lathauwer, X.~Fu, K.~Huang, E.~E. Papalexakis, and
  C.~Faloutsos.
\newblock Tensor decomposition for signal processing and machine learning.
\newblock {\em IEEE Transactions on Signal Processing}, 65(13):3551--3582,
  2017.

\bibitem{sustik2007existence}
M.~A. Sustik, J.~A. Tropp, I.~S. Dhillon, and R.~W. Heath~Jr.
\newblock On the existence of equiangular tight frames.
\newblock {\em Linear Algebra and its Applications}, 426(2-3):619--635, 2007.

\bibitem{thompson1972principal}
R.~C. Thompson.
\newblock Principal submatrices ix: Interlacing inequalities for singular
  values of submatrices.
\newblock {\em Linear Algebra and its Applications}, 5(1):1--12, 1972.

\bibitem{vershynin2010introduction}
R.~Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock {\em arXiv preprint arXiv:1011.3027}, 2010.

\bibitem{vershynin2018high}
R.~Vershynin.
\newblock {\em High-dimensional Probability: an Introduction with Applications
  in Data Science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem{vershynin2020concentration}
R.~Vershynin.
\newblock Concentration inequalities for random tensors.
\newblock {\em Bernoulli}, 26(4):3139--3162, 2020.

\bibitem{vervliet2016tensorlab}
N.~Vervliet, O.~Debals, L.~Sorber, M.~Van~Barel, and L.~De~Lathauwer.
\newblock Tensorlab 3.0. {A}vailable online.
\newblock {\em URL: http://www.tensorlab.net}, 2016.

\bibitem{weyl1912asymptotische}
H.~Weyl.
\newblock Das asymptotische verteilungsgesetz der eigenwerte linearer
  partieller differentialgleichungen (mit einer anwendung auf die theorie der
  hohlraumstrahlung).
\newblock {\em Mathematische Annalen}, 71(4):441--479, 1912.

\bibitem{white1992density}
S.~R. White.
\newblock Density matrix formulation for quantum renormalization groups.
\newblock {\em Physical Review Letters}, 69(19):2863, 1992.

\bibitem{zhong2017recovery}
K.~Zhong, Z.~Song, P.~Jain, P.~L. Bartlett, and I.~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  4140--4149. PMLR, 2017.

\end{thebibliography}
