@article{NEURIPS2022_e8fb575e,
 author = {Pidstrigach, Jakiw},
 journal = {Advances in Neural Information Processing Systems},
 pages = {35852--35865},
 title = {Score-Based Generative Models Detect Manifolds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e8fb575e3ede31f9b8c05d53514eb7c6-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{NEURIPS2018_69386f6b,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
 journal = {Advances in Neural Information Processing Systems},
 pages = {6571–6583},
 title = {Neural Ordinary Differential Equations},
 volume = {31},
 year = {2018}
}

@article{van2014probability,
  title={Probability in high dimension},
  author={Van Handel, Ramon},
  journal={Lecture Notes (Princeton University)},
  year={2014}
}

@article{hsu2021approximation,
  title={On the approximation power of two-layer networks of random {R}e{LU}s},
  author={Hsu, Daniel and Sanford, Clayton H. and Servedio, Rocco and Vlatakis-Gkaragkounis, Emmanouil Vasileios},
  journal={Conference on Learning Theory},
  volume={134},
  pages={2423--2461},
  year={2021},
  organization={PMLR}
}

@InProceedings{pmlr-v134-hsu21a,
  title = 	 {On the Approximation Power of Two-Layer Networks of Random {R}e{LU}s},
  author =       {Hsu, Daniel and Sanford, Clayton H and Servedio, Rocco and Vlatakis-Gkaragkounis, Emmanouil Vasileios},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2423--2461},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/hsu21a/hsu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/hsu21a.html},
  abstract = 	 {This paper considers the following question: how well can depth-two ReLU networks with randomly initialized bottom-level weights represent smooth functions? We give near-matching upper- and lower-bounds for L2-approximation in terms of the Lipschitz constant, the desired accuracy, and the dimension of the problem, as well as similar results in terms of Sobolev norms. Our positive results employ tools from harmonic analysis and ridgelet representation theory, while our lower-bounds are based on (robust versions of) dimensionality arguments.}
}

@article{weinan2020comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei},
  journal={Science China Mathematics},
  volume={63},
  number={7},
  pages={1235--1258},
  year={2020},
  publisher={Science in China Press}
}

@article{pmlr-v202-wu23r,
  title = 	 {The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent},
  author =       {Wu, Lei and Su, Weijie J.},
  journal = 	 {International Conference on Machine Learning},
  pages = 	 {37656--37684},
  year = 	 {2023},
  volume = 	 {202},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wu23r/wu23r.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wu23r.html},
  abstract = 	 {In this paper, we study the implicit regularization of stochastic gradient descent (SGD) through the lens of <em>dynamical stability</em> (Wu et al., 2018). We start by revising existing stability analyses of SGD, showing how the Frobenius norm and trace of Hessian relate to different notions of stability. Notably, if a global minimum is linearly stable for SGD, then the trace of Hessian must be less than or equal to $2/\eta$, where $\eta$ denotes the learning rate. By contrast, for gradient descent (GD), the stability imposes a similar constraint but only on the largest eigenvalue of Hessian. We then turn to analyze the generalization properties of these stable minima, focusing specifically on two-layer ReLU networks and diagonal linear networks. Notably, we establish the <em>equivalence</em> between these metrics of sharpness and certain parameter norms for the two models, which allows us to show that the stable minima of SGD provably generalize well. By contrast, the stability-induced regularization of GD is provably too weak to ensure satisfactory generalization. This discrepancy provides an explanation of why SGD often generalizes better than GD. Note that the learning rate (LR) plays a pivotal role in the strength of stability-induced regularization. As the LR increases, the regularization effect becomes more pronounced, elucidating why SGD with a larger LR consistently demonstrates superior generalization capabilities. Additionally, numerical experiments are provided to support our theoretical findings.}
}

@article{noe2019boltzmann,
  title={Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
  author={No{\'e}, Frank and Olsson, Simon and K{\"o}hler, Jonas and Wu, Hao},
  journal={Science},
  volume={365},
  number={6457},
  pages={eaaw1147},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{midgley2023flow,
title={Flow Annealed Importance Sampling Bootstrap},
author={Laurence Illing Midgley and Vincent Stimper and Gregor N. C. Simm and Bernhard Sch{\"o}lkopf and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=XCTVFJwS9LJ}
}


@article{aronszajn1950theory,
  title={Theory of reproducing kernels},
  author={Aronszajn, Nachman},
  journal={Transactions of the American Mathematical Society},
  volume={68},
  number={3},
  pages={337--404},
  year={1950},
  publisher={JSTOR}
}

@article{JMLR:v23:21-0368,
  author  = {Zhong Li and Jiequn Han and Weinan E and Qianxiao Li},
  title   = {Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {42},
  pages   = {1--85},
  url     = {http://jmlr.org/papers/v23/21-0368.html}
}

@inproceedings{li2021on,
title={On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis},
author={Zhong Li and Jiequn Han and Weinan E and Qianxiao Li},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8Sqhl-nF50}
}

@article{ma2019priori,
  title={A priori estimates of the population risk for two-layer neural networks},
  author={E, Weinan and Ma, Chao and Wu, Lei},
  journal={Communications in Mathematical Sciences},
  volume={17},
  number={5},
  pages={1407--1425},
  year={2019},
  publisher={International Press of Boston}
}

@article{liu2022rectified,
  title={Rectified flow: A marginal preserving approach to optimal transport},
  author={Liu, Qiang},
  journal={arXiv preprint arXiv:2209.14577},
  year={2022}
}

@article{shi2023diffusion,
title={Diffusion {S}chr\"{o}dinger Bridge Matching},
author={Yuyang Shi and Valentin De Bortoli and Andrew Campbell and Arnaud Doucet},
journal={Advances in Neural Information Processing Systems},
year={2023},
volume={36},
url={https://openreview.net/forum?id=qy07OHsJT5}
}

@article{liu20232,
  title={I$^2${SB}: Image-to-Image {S}chr\"{o}dinger Bridge},
  author={Liu, Guan-Horng and Vahdat, Arash and Huang, De-An and Theodorou, Evangelos and Nie, Weili and Anandkumar, Anima},
  journal={International Conference on Machine Learning},
  volume={202},
  pages={22042--22062},
  year={2023},
  organization={PMLR}
}

@article{wang2021deep,
  title={Deep generative learning via {S}chr{\"o}dinger bridge},
  author={Wang, Gefei and Jiao, Yuling and Xu, Qian and Wang, Yang and Yang, Can},
  journal={International Conference on Machine Learning},
  volume={139},
  pages={10794--10804},
  year={2021},
  organization={PMLR}
}

@article{vargas2021solving,
  title={Solving {S}chr{\"o}dinger bridges via maximum likelihood},
  author={Vargas, Francisco and Thodoroff, Pierre and Lamacraft, Austen and Lawrence, Neil},
  journal={Entropy},
  volume={23},
  number={9},
  pages={1134},
  year={2021},
  publisher={MDPI}
}

@article{de2021diffusion,
  title={Diffusion {S}chr{\"o}dinger bridge with applications to score-based generative modeling},
  author={De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17695--17709},
  year={2021}
}

@article{somnath2023aligned,
  title = 	 {Aligned Diffusion {S}chr\"{o}dinger Bridges},
  author =       {Somnath, Vignesh Ram and Pariset, Matteo and Hsieh, Ya-Ping and Martinez, Maria Rodriguez and Krause, Andreas and Bunne, Charlotte},
  journal = 	 {Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1985--1995},
  year = 	 {2023},
  volume = 	 {216},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/somnath23a/somnath23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/somnath23a.html},
  abstract = 	 {Diffusion Schrödinger bridges (DSBs) have recently emerged as a powerful framework for recovering stochastic dynamics via their marginal observations at different time points. Despite numerous successful applications, existing algorithms for solving DSBs have so far failed to utilize the structure of aligned data, which naturally arises in many biological phenomena. In this paper, we propose a novel algorithmic framework that, for the first time, solves DSBs while respecting the data alignment. Our approach hinges on a combination of two decades-old ideas: The classical Schrödinger bridge theory and Doob’s $h$-transform. Compared to prior methods, our approach leads to a simpler training procedure with lower variance, which we further augment with principled regularization schemes. This ultimately leads to sizeable improvements across experiments on synthetic and real data, including the tasks of predicting conformational changes in proteins and temporal evolution of cellular differentiation processes.}
}

@inproceedings{liu2023flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and Qiang Liu},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}

@article{song2023consistency,
  title = 	 {Consistency Models},
  author =       {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  journal = 	 {International Conference on Machine Learning},
  pages = 	 {32211--32252},
  year = 	 {2023},
  volume = 	 {202}, 
  pdf = 	 {https://proceedings.mlr.press/v202/song23a/song23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23a.html},
  abstract = 	 {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.}
}

@book{sarkka2019applied,
  title={Applied Stochastic Differential Equations},
  author={S{\"a}rkk{\"a}, Simo and Solin, Arno},
  volume={10},
  year={2019},
  publisher={Cambridge University Press}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{anderson1982reverse,
  title={Reverse-time diffusion equation models},
  author={Anderson, Brian D. O.},
  journal={Stochastic Processes and their Applications},
  volume={12},
  number={3},
  pages={313--326},
  year={1982},
  publisher={Elsevier}
}

@book{oksendal2003stochastic,
  title={Stochastic Differential Equations},
  author={{\O}ksendal, Bernt},
  year={2003},
  publisher={Springer}
}

@inproceedings{sehwag2022generating,
  title={Generating high fidelity data from low-density regions using diffusion models},
  author={Sehwag, Vikash and Hazirbas, Caner and Gordo, Albert and Ozgenel, Firat and Canton, Cristian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11492--11501},
  year={2022}
}

@inproceedings{koehler2023statistical,
title={Statistical Efficiency of Score Matching: The View from Isoperimetry},
author={Frederic Koehler and Alexander Heckett and Andrej Risteski},
booktitle={International Conference on Learning Representations},
year={2023}
}

@article{song2021maximum,
  title={Maximum likelihood training of score-based diffusion models},
  author={Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1415--1428},
  year={2021}
}

@article{song2020improved,
  title={Improved techniques for training score-based generative models},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12438--12448},
  year={2020}
}

@article{chen2023improved,
  title={Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions},
  author={Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  journal={International Conference on Machine Learning},
  volume={202},
  pages={4735--4763},
  year={2023},
  organization={PMLR}
}

@InProceedings{pmlr-v202-chen23q,
  title = 	 {Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions},
  author =       {Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {4735--4763},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chen23q/chen23q.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chen23q.html},
  abstract = 	 {We give an improved theoretical analysis of score-based generative modeling. Under a score estimate with small $L^2$ error (averaged across timesteps), we provide efficient convergence guarantees for any data distribution with second-order moment, by either employing early stopping or assuming smoothness condition on the score function of the data distribution. Our result does not rely on any log-concavity or functional inequality assumption and has a logarithmic dependence on the smoothness. In particular, we show that under only a finite second moment condition, approximating the following in reverse KL divergence in $\epsilon$-accuracy can be done in $\tilde O\left(\frac{d \log (1/\delta)}{\epsilon}\right)$ steps: 1) the variance-$\delta$ Gaussian perturbation of any data distribution; 2) data distributions with $1/\delta$-smooth score functions. Our analysis also provides a quantitative comparison between different discrete approximations and may guide the choice of discretization points in practice.}
}

@inproceedings{chen2023sampling,
title={Sampling is as easy as learning the score: Theory for diffusion models with minimal data assumptions},
author={Sitan Chen and Sinho Chewi and Jerry Li and Yuanzhi Li and Adil Salim and Anru Zhang},
booktitle={International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zyLVMgsZ0U_}
}

@article{vyas2023provable,
  title = 	 {On Provable Copyright Protection for Generative Models},
  author =       {Vyas, Nikhil and Kakade, Sham M. and Barak, Boaz},
  journal = 	 {International Conference on Machine Learning},
  pages = 	 {35277--35299},
  year = 	 {2023},
  volume = 	 {202},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/vyas23b/vyas23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/vyas23b.html},
  abstract = 	 {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of near access-freeness (NAF) and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $k$-NAF if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that did not access $C$ at all. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.}
}

@INPROCEEDINGS{ghalebikesabi2023differentially,
author={Ghalebikesabi, Sahra and Berrada, Leonard and Gowal, Sven and Ktena, Ira and Stanforth, Robert and Hayes, Jamie and De, Soham and Smith, Samuel L and Wiles, Olivia and Balle, Borja},
booktitle = {International Workshop on Trustworthy Federated Learning in Conjunction with IJCAI},
title = {Differentially Private Diffusion Models Generate Useful Synthetic Images},
year = {2023},
volume = {},
pages = {}
}

@article{dockhorn2023differentially,
title={Differentially Private Diffusion Models},
author={Tim Dockhorn and Tianshi Cao and Arash Vahdat and Karsten Kreis},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=ZPpQk7FJXF}
}


@article{chourasia2021differential,
  title={Differential privacy dynamics of langevin diffusion and noisy gradient descent},
  author={Chourasia, Rishav and Ye, Jiayuan and Shokri, Reza},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14771--14781},
  year={2021}
}

@article{wu2022membership,
  title={Membership Inference Attacks Against Text-to-image Generation Models},
  author={Wu, Yixin and Yu, Ning and Li, Zheng and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2210.00968},
  year={2022}
}

@article{hu2023membership,
  title={Membership inference of diffusion models},
  author={Hu, Hailong and Pang, Jun},
  journal={arXiv preprint arXiv:2301.09956},
  year={2023}
}

@INPROCEEDINGS {matsumoto2023membership,
author={Matsumoto, Tomoya and Miura, Takayuki and Yanai, Naoto},
booktitle = {IEEE Security and Privacy Workshops},
title = {Membership inference attacks against diffusion models},
year = {2023},
volume = {},
pages = {77-83},
abstract = {Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.},
keywords = {resistance;schedules;privacy;analytical models;conferences;closed box;machine learning},
url = {https://doi.ieeecomputersociety.org/10.1109/SPW59333.2023.00013}
}

@inproceedings{carlini2023quantifying,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=TatRHT_1cK}
}

@InProceedings{somepalli2023diffusion,
    author    = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
    title     = {Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year      = {2023},
    pages     = {6048-6058}
}

@inproceedings{jagielski2023measuring,
  title={Measuring Forgetting of Memorized Training Examples},
  author={Jagielski, Matthew and Thakkar, Om and Tramer, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep Guha and Papernot, Nicolas and Zhang, Chiyuan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{zhang2023counterfactual,
title={Counterfactual memorization in neural language models},
author={Chiyuan Zhang and Daphne Ippolito and Katherine Lee and Matthew Jagielski and Florian Tram{\`e}r and Nicholas Carlini},
journal={Advances in Neural Information Processing Systems},
year={2023},
volume={36},
url={https://openreview.net/forum?id=67o9UQgTD0}
}

@inproceedings{carlini2023extracting,
  title={Extracting training data from diffusion models},
  author={Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  booktitle={USENIX Security Symposium},
  pages={5253--5270},
  year={2023}
}

@article{yang2022generalization,
  title={Generalization error of {GAN} from the discriminator’s perspective},
  author={Yang, Hongkang and E, Weinan},
  journal={Research in the Mathematical Sciences},
  volume={9},
  number={1},
  pages={1-31},
  year={2022},
  publisher={Springer}
}

@article{pmlr-v145-yang22a,
  title = 	 {Generalization and Memorization: The Bias Potential Model},
  author =       {Yang, Hongkang and E, Weinan},
  journal = 	 {Mathematical and Scientific Machine Learning},
  pages = 	 {1013--1043},
  year = 	 {2022},
  volume = 	 {145},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v145/yang22a/yang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v145/yang22a.html},
  abstract = 	 {Models for learning probability distributions such as generative models and density estimators be- have quite differently from models for learning functions. One example is found in the memo- rization phenomenon, namely the ultimate convergence to the empirical distribution, that occurs in generative adversarial networks (GANs). For this reason, the issue of generalization is more subtle than that for supervised learning. For the bias potential model, we show that dimension- independent generalization accuracy is achievable if early stopping is adopted, despite that in the long term, the model either memorizes the samples or diverges. }
}

@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{vincent2011connection,
  title={A connection between score matching and denoising autoencoders},
  author={Vincent, Pascal},
  journal={Neural Computation},
  volume={23},
  number={7},
  pages={1661--1674},
  year={2011},
  publisher={MIT Press}
}

@article{karras2022elucidating,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26565--26577},
  year={2022}
}

@inproceedings{song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@article{song2020sliced,
  title={Sliced score matching: A scalable approach to density and score estimation},
  author={Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
  journal={Uncertainty in Artificial Intelligence Conference},
  pages={574--584},
  volume={115},
  year={2020},
  organization={PMLR}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Raphael Gontijo-Lopes and Burcu Karagol Ayan and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
  journal={Advances in Neural Information Processing Systems},
  year={2022},
  pages = {36479--36494},
  volume = {35},
  url={https://openreview.net/forum?id=08Yk-n5l2Al}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@inproceedings{song2022solving,
title={Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
author={Yang Song and Liyue Shen and Lei Xing and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vaRCHVj0uGI}
}

@inproceedings{xie2022crystal,
title={Crystal Diffusion Variational Autoencoder for Periodic Material Generation},
author={Tian Xie and Xiang Fu and Octavian-Eugen Ganea and Regina Barzilay and Tommi S. Jaakkola},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=03RLpj-tc_}
}

@inproceedings{xu2022geodiff,
title={GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation},
author={Minkai Xu and Lantao Yu and Yang Song and Chence Shi and Stefano Ermon and Jian Tang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=PzcvxEMzvQC}
}

@article{yoon2021adversarial,
  title={Adversarial purification with score-based generative models},
  author={Yoon, Jongmin and Hwang, Sung Ju and Lee, Juho},
  journal={International Conference on Machine Learning},
  pages={12062--12072},
  volume={139},
  year={2021},
  organization={PMLR}
}

@article{popov2021grad,
  title={Grad-tts: A diffusion probabilistic model for text-to-speech},
  author={Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail},
  journal={International Conference on Machine Learning},
  pages={8599--8608},
  year={2021},
  volume={139},
  organization={PMLR}
}

@InProceedings{wu2023tune,
    author    = {Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
    title     = {Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    year      = {2023},
    pages     = {7623-7633}
}

@inproceedings{avrahami2022blended,
  title={Blended diffusion for text-driven editing of natural images},
  author={Avrahami, Omri and Lischinski, Dani and Fried, Ohad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18208--18218},
  year={2022}
}

@article{rasul2021autoregressive,
  title={Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting},
  author={Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  journal={International Conference on Machine Learning},
  pages={8857--8868},
  year={2021},
  volume={139},
  organization={PMLR}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@inproceedings{gong2022diffuseq,
  title={DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{yang2022diffusion,
  title={Diffusion probabilistic modeling for video generation},
  author={Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  journal={Entropy},
  volume={25},
  number={10},
  pages={1469},
  year={2023},
  publisher={MDPI}
}

@article{saharia2022image,
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Super-Resolution via Iterative Refinement}, 
  year={2023},
  volume={45},
  number={4},
  pages={4713-4726},
  doi={10.1109/TPAMI.2022.3204461}
}

@article{li2022srdiff,
  title={Srdiff: Single image super-resolution with diffusion probabilistic models},
  author={Li, Haoying and Yang, Yifan and Chang, Meng and Chen, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
  journal={Neurocomputing},
  volume={479},
  pages={47--59},
  year={2022},
  publisher={Elsevier}
}

@article{NEURIPS2020_4c5bcfec,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 journal = {Advances in Neural Information Processing Systems},
 pages = {6840--6851},
 title = {Denoising Diffusion Probabilistic Models},
 volume = {33},
 year = {2020}
}

@article{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  journal = 	 {International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  volume = 	 {37},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@book{shalev2014understanding,
  title={Understanding Machine Learning: From Theory to Algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

@article{ma2022barron,
  title={The {B}arron space and the flow-induced function spaces for neural network models},
  author={E, Weinan and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  volume={55},
  number={1},
  pages={369--406},
  year={2022},
  publisher={Springer}
}

@article{yang2022mathematical,
  title={A Mathematical Framework for Learning Probability Distributions},
  author={Yang, Hongkang},
  journal={arXiv preprint arXiv:2212.11481},
  year={2022}
}


@article{JML-1-373,
author = {Yang , Hongkang},
title = {A Mathematical Framework for Learning Probability Distributions},
journal = {Journal of Machine Learning},
year = {2022},
volume = {1},
number = {4},
pages = {373--431},
abstract = {The modeling of probability distributions, specifically generative modeling and density estimation,
has become an immensely popular subject in recent years by virtue of its outstanding performance on sophisticated data such as images and texts. Nevertheless, a theoretical understanding of its success is still incomplete.
One mystery is the paradox between memorization and generalization: In theory, the model is trained to be
exactly the same as the empirical distribution of the finite samples, whereas in practice, the trained model can
generate new samples or estimate the likelihood of unseen samples. Likewise, the overwhelming diversity of
distribution learning models calls for a unified perspective on this subject. This paper provides a mathematical
framework such that all the well-known models can be derived based on simple principles. To demonstrate
its efficacy, we present a survey of our results on the approximation error, training error and generalization
error of these models, which can all be established based on this framework. In particular, the aforementioned
paradox is resolved by proving that these models enjoy implicit regularization during training, so that the
generalization error at early-stopping avoids the curse of dimensionality. Furthermore, we provide some new
results on landscape analysis and the mode collapse phenomenon.
},
issn = {2790-2048},
doi = {https://doi.org/10.4208/jml.221202},
url = {http://global-sci.org/intro/article_detail/jml/21298.html}
}

@article{shah2023learning,
title={Learning Mixtures of Gaussians Using the {DDPM} Objective},
author={Kulin Shah and Sitan Chen and Adam Klivans},
journal={Advances in Neural Information Processing Systems},
year={2023},
volume={36},
url={https://openreview.net/forum?id=aig7sgdRfI}
}