\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori \& Neu(2014)Abbasi-Yadkori and Neu]{abbasi2014online}
Abbasi-Yadkori, Y. and Neu, G.
\newblock Online learning in mdps with side information.
\newblock \emph{arXiv preprint arXiv:1406.6812}, 2014.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272, 2017.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{dann2015sample}
Dann, C. and Brunskill, E.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5713--5723, 2017.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{dann2018oracle}
Dann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and
  Schapire, R.~E.
\newblock On oracle-efficient pac reinforcement learning with rich
  observations.
\newblock \emph{arXiv preprint arXiv:1803.00606}, 2018.

\bibitem[Ghavamzadeh et~al.(2016)Ghavamzadeh, Petrik, and
  Chow]{ghavamzadeh2016safe}
Ghavamzadeh, M., Petrik, M., and Chow, Y.
\newblock Safe policy improvement by minimizing robust baseline regret.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2298--2306, 2016.

\bibitem[Hallak et~al.(2015)Hallak, Di~Castro, and
  Mannor]{hallak2015contextual}
Hallak, A., Di~Castro, D., and Mannor, S.
\newblock Contextual {M}arkov decision processes.
\newblock \emph{arXiv:1502.02259}, 2015.

\bibitem[Howard et~al.(2018)Howard, Ramdas, Mc~Auliffe, and
  Sekhon]{howard2018uniform}
Howard, S.~R., Ramdas, A., Mc~Auliffe, J., and Sekhon, J.
\newblock Uniform, nonparametric, non-asymptotic confidence sequences.
\newblock \emph{arXiv preprint arXiv:1810.08240}, 2018.

\bibitem[Jabbari et~al.(2016)Jabbari, Joseph, Kearns, Morgenstern, and
  Roth]{jabbari2016fair}
Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., and Roth, A.
\newblock Fair learning in markovian environments.
\newblock \emph{arXiv preprint arXiv:1611.03071}, 2016.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning-Volume 48}, pp.\  652--661.
  JMLR. org, 2016.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is {Q}-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Joseph et~al.(2016)Joseph, Kearns, Morgenstern, and
  Roth]{joseph2016fairness}
Joseph, M., Kearns, M., Morgenstern, J.~H., and Roth, A.
\newblock Fairness in learning: Classic and contextual bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  325--333, 2016.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University College London, 2003.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S.~M. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2002.

\bibitem[Kannan et~al.(2017)Kannan, Kearns, Morgenstern, Pai, Roth, Vohra, and
  Wu]{kannan2017fairness}
Kannan, S., Kearns, M., Morgenstern, J., Pai, M., Roth, A., Vohra, R., and Wu,
  Z.~S.
\newblock Fairness incentives for myopic agents.
\newblock In \emph{Proceedings of the 2017 ACM Conference on Economics and
  Computation}, pp.\  369--386. ACM, 2017.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 2002.

\bibitem[Lattimore \& Czepesvari(2018)Lattimore and
  Czepesvari]{lattimore2018bandit}
Lattimore, T. and Czepesvari, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2018.

\bibitem[Lattimore \& Hutter(2012)Lattimore and Hutter]{lattimore2012pac}
Lattimore, T. and Hutter, M.
\newblock Pac bounds for discounted mdps.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  320--334. Springer, 2012.

\bibitem[Li et~al.(2008)Li, Littman, and Walsh]{li2008knows}
Li, L., Littman, M.~L., and Walsh, T.~J.
\newblock Knows what it knows: a framework for self-aware learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  568--575. ACM, 2008.

\bibitem[Mahmood et~al.(2017)Mahmood, Yu, and Sutton]{mahmood2017multi}
Mahmood, A.~R., Yu, H., and Sutton, R.~S.
\newblock Multi-step off-policy learning without importance sampling ratios.
\newblock \emph{arXiv preprint arXiv:1702.03006}, 2017.

\bibitem[Modi et~al.(2018)Modi, Jiang, Singh, and Tewari]{modi2018markov}
Modi, A., Jiang, N., Singh, S., and Tewari, A.
\newblock Markov decision processes with continuous side information.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  597--618, 2018.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3003--3011, 2013.

\bibitem[Osband et~al.(2016)Osband, Van~Roy, and Wen]{osband2016generalization}
Osband, I., Van~Roy, B., and Wen, Z.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2377--2386, 2016.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, Pecorino, and
  Calandriello]{pirotta2013safe}
Pirotta, M., Restelli, M., Pecorino, A., and Calandriello, D.
\newblock Safe policy iteration.
\newblock In \emph{International Conference on Machine learning}, pp.\
  307--315, 2013.

\bibitem[Raghavan et~al.(2018)Raghavan, Slivkins, Vaughan, and
  Wu]{raghavan2018externalities}
Raghavan, M., Slivkins, A., Vaughan, J.~W., and Wu, Z.~S.
\newblock The externalities of exploration and how data diversity helps
  exploitation.
\newblock \emph{arXiv preprint arXiv:1806.00543}, 2018.

\bibitem[Sajed et~al.(2018)Sajed, Chung, and White]{sajed2018high}
Sajed, T., Chung, W., and White, M.
\newblock High-confidence error estimates for learned value functions.
\newblock \emph{arXiv preprint arXiv:1808.09127}, 2018.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman]{strehl2006pac}
Strehl, A.~L., Li, L., Wiewiora, E., Langford, J., and Littman, M.~L.
\newblock {PAC} model-free reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2006.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{strehl2009reinforcement}
Strehl, A.~L., Li, L., and Littman, M.~L.
\newblock Reinforcement learning in finite {MDP}s: {PAC} analysis.
\newblock \emph{Journal of Machine Learning Research}, 10:\penalty0 2413--2444,
  2009.

\bibitem[Szita \& Szepesv{\'a}ri(2010)Szita and Szepesv{\'a}ri]{szita2010model}
Szita, I. and Szepesv{\'a}ri, C.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)}, pp.\  1031--1038, 2010.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2139--2148, 2016.

\bibitem[Thomas et~al.(2015{\natexlab{a}})Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015highimp}
Thomas, P., Theocharous, G., and Ghavamzadeh, M.
\newblock High confidence policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2380--2388, 2015{\natexlab{a}}.

\bibitem[Thomas et~al.(2015{\natexlab{b}})Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Thomas, P.~S., Theocharous, G., and Ghavamzadeh, M.
\newblock High-confidence off-policy evaluation.
\newblock In \emph{Proceedings of the Twenty-Ninth AAAI Conference on
  Artificial Intelligence}, pp.\  3000--3006, 2015{\natexlab{b}}.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock \emph{https://arxiv.org/abs/1901.00210}, 2019.

\bibitem[Zilberstein \& Russell(1996)Zilberstein and
  Russell]{zilberstein1996optimal}
Zilberstein, S. and Russell, S.
\newblock Optimal composition of real-time systems.
\newblock \emph{Artificial Intelligence}, 82\penalty0 (1-2):\penalty0 181--213,
  1996.

\end{thebibliography}
