
@inproceedings{akhtar2018adversarial,
  title={Adversarial examples detection using no-reference image quality features},
  author={Akhtar, Zahid and Monteiro, Jo{\~a}o and Falk, Tiago H},
  booktitle={2018 international Carnahan conference on security technology (ICCST)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

@inproceedings{liu2020learning,
  title={Learning deep kernels for non-parametric two-sample tests},
  author={Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, Danica J},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{gidaris2018unsupervised,
  title={Unsupervised representation learning by predicting image rotations},
  author={Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1803.07728},
  year={2018}
}

@inproceedings{doersch2017multi,
  title={Multi-task self-supervised visual learning},
  author={Doersch, Carl and Zisserman, Andrew},
  booktitle={International Conference on Computer Vision},
  year={2017}
}

@inproceedings{doersch2015unsupervised,
  title={Unsupervised visual representation learning by context prediction},
  author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle={International Conference on Computer Vision},
  year={2015}
}

@InProceedings{pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =    {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {International Conference on Machine Learning},
  year = 	 {2020},
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2020}
}

@article{albuquerque2020improving,
  title={Improving out-of-distribution generalization via multi-task self-supervised pretraining},
  author={Albuquerque, Isabela and Naik, Nikhil and Li, Junnan and Keskar, Nitish and Socher, Richard},
  journal={arXiv preprint arXiv:2003.13525},
  year={2020}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{krueger2020out,
  title={Out-of-distribution generalization via risk extrapolation (rex)},
  author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
  journal={arXiv preprint arXiv:2003.00688},
  year={2020}
}

@inproceedings{krizhevsky2012imagenet,
  institution={ },
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {University of Toronto},
    year = {2009}
}

@inproceedings{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{monteiro2020domain,
  title={Domain Conditional Predictors for Domain Adaptation},
  author={Monteiro, Joao and Gibert, Xavier and Feng, Jianqiao and Dumoulin, Vincent and Lee, Dar-Shyang},
  booktitle={Pre-registration workshop, NeurIPS 2020},
  year={2020}
}

@inproceedings{hoffer2015deep,
  title={Deep metric learning using triplet network},
  author={Hoffer, Elad and Ailon, Nir},
  booktitle={International Workshop on Similarity-Based Pattern Recognition},
  year={2015},
}

@inproceedings{xing2003distance,
  title={Distance metric learning with application to clustering with side-information},
  author={Xing, Eric P and Jordan, Michael I and Russell, Stuart J and Ng, Andrew Y},
  booktitle={Advances in Neural Information Processing Systems},
  year={2003}
}

@inproceedings{globerson2006metric,
  title={Metric learning by collapsing classes},
  author={Globerson, Amir and Roweis, Sam T},
  booktitle={Advances in Neural Information Processing Systems},
  year={2006}
}

@article{weinberger2009distance,
  title={Distance metric learning for large margin nearest neighbor classification},
  author={Weinberger, Kilian Q and Saul, Lawrence K},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Feb},
  pages={207--244},
  year={2009}
}

@article{ying2012distance,
  title={Distance metric learning with eigenvalue optimization},
  author={Ying, Yiming and Li, Peng},
  journal={Journal of machine Learning research},
  volume={13},
  number={Jan},
  pages={1--26},
  year={2012}
}

@inproceedings{shalev2004online,
  title={Online and batch learning of pseudo-metrics},
  author={Shalev-Shwartz, Shai and Singer, Yoram and Ng, Andrew Y},
  booktitle={International Conference on Machine learning},
  year={2004},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{monteiro2020end,
  title={An end-to-end approach for the verification problem: learning the right distance},
  author={Monteiro, Joao and Albuquerque, Isabela and Alam, Jahangir and Hjelm, R Devon and Falk, Tiago},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@article{albuquerque2019adversarial,
  title={Generalizing to unseen domains via distribution matching},
  author={Albuquerque, Isabela and Monteiro, Jo{\~a}o and Darvishi, Mohammad and Falk, Tiago H and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1911.00804},
  year={2019}
}

@inproceedings{monteiro2019combining,
  title={Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning},
  author={Monteiro, Jo{\~a}o and Alam, Jahangir and Falk, Tiago H},
  booktitle={Interspeech},
  year={2019}
}

@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
}

@article{henaff2021efficient,
  title={Efficient visual pretraining with contrastive detection},
  author={H{\'e}naff, Olivier J and Koppula, Skanda and Alayrac, Jean-Baptiste and Oord, Aaron van den and Vinyals, Oriol and Carreira, Jo{\~a}o},
  journal={arXiv preprint arXiv:2103.10957},
  year={2021}
}

@InProceedings{pmlr-v97-wenliang19a,
  title = 	 {Learning deep kernels for exponential family densities},
  author = 	 {Li Wenliang and Dougal Sutherland and Heiko Strathmann and Arthur Gretton},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2019},
}

@inproceedings{monteiro2019generalizable,
  title={Generalizable adversarial examples detection based on bi-model decision mismatch},
  author={Monteiro, Jo{\~a}o and Albuquerque, Isabela and Akhtar, Zahid and Falk, Tiago H},
  booktitle={International Conference on Systems, Man and Cybernetics (SMC)},
  year={2019},
}

@inproceedings{NIPS2009_9246444d,
 title = {A Fast, Consistent Kernel Two-Sample Test},
 author = {Gretton, Arthur and Fukumizu, Kenji and Harchaoui, Za\"{\i}d and Sriperumbudur, Bharath K.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 year = {2009}
}

@article{https://doi.org/10.48550/arxiv.1611.04488,
  title = {Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy},
  author = {Sutherland, Danica J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
  journal   = {arXiv preprint arXiv:1611.04488},
  year = {2016},
}

@article{https://doi.org/10.48550/arxiv.1506.04725,
  title = {Fast Two-Sample Testing with Analytic Representations of Probability Measures}, 
  author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
  journal   = {arXiv preprint arXiv:1506.04725},
  year = {2015}
}

@article{https://doi.org/10.48550/arxiv.1605.06796,
  title = {Interpretable Distribution Features with Maximum Testing Power},
  author = {Jitkrittum, Wittawat and Szabo, Zoltan and Chwialkowski, Kacper and Gretton, Arthur},
  journal   = {arXiv preprint arXiv:1605.06796},
  year = {2016},
}

@article{https://doi.org/10.48550/arxiv.1909.11298,
  title = {Classification Logit Two-sample Testing by Neural Networks},
  author = {Cheng, Xiuyuan and Cloninger, Alexander},
  journal   = {arXiv preprint arXiv:1909.11298},
  year = {2019},
 }

@article{hendrycks2021natural,
      title={Natural Adversarial Examples}, 
      author={Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Song},
      year={2021},
      journal={arXiv preprint arXiv:1907.07174},
}

@article{srivastava2022distribution,
      title={Out of Distribution Detection on ImageNet-O}, 
      author={Anugya Srivastava and Shriya Jain and Mugdha Thigle},
      year={2022},
      jopurnal={arXiv preprint arXiv:2201.09352},
}

@article{https://doi.org/10.48550/arxiv.1512.03385,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal = {arXiv preprint arXiv:1512.03385},
  year = {2015},
}

@article{https://doi.org/10.48550/arxiv.1706.06083,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal = {arXiv preprint arXiv:1706.06083},
  year = {2017},
}

@article{assran2022masked,
  title={Masked Siamese Networks for Label-Efficient Learning},
  author={Assran, Mahmoud and Caron, Mathilde and Misra, Ishan and Bojanowski, Piotr and Bordes, Florian and Vincent, Pascal and Joulin, Armand and Rabbat, Michael and Ballas, Nicolas},
  journal={arXiv preprint arXiv:2204.07141},
  year={2022}
}

@inproceedings{zhai2016deep,
  title={Deep structured energy based models for anomaly detection},
  author={Zhai, Shuangfei and Cheng, Yu and Lu, Weining and Zhang, Zhongfei},
  booktitle={International Conference on Machine Learning},
  year={2016},
}

@article{nalisnick2018deep,
  title={Do deep generative models know what they don't know?},
  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1810.09136},
  year={2018}
}

@article{nalisnick2019detecting,
  title={Detecting Out-of-Distribution Inputs to Deep Generative Models Using a Test for Typicality},
  author={Nalisnick, Eric T and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1906.02994},
  year={2019}
}

@article{choi2018waic,
  title={Waic, but why? generative ensembles for robust anomaly detection},
  author={Choi, Hyunsun and Jang, Eric and Alemi, Alexander A},
  journal={arXiv preprint arXiv:1810.01392},
  year={2018}
}

@inproceedings{du2019implicit,
  title={Implicit generation and modeling with energy based models},
  author={Du, Yilun and Mordatch, Igor},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{ren2019likelihood,
  title={Likelihood ratios for out-of-distribution detection},
  author={Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{serra2019input,
  title={Input complexity and out-of-distribution detection with likelihood-based generative models},
  author={Serr{\`a}, Joan and {\'A}lvarez, David and G{\'o}mez, Vicen{\c{c}} and Slizovskaia, Olga and N{\'u}{\~n}ez, Jos{\'e} F and Luque, Jordi},
  journal={arXiv preprint arXiv:1909.11480},
  year={2019}
}

@article{grathwohl2019your,
  title={Your classifier is secretly an energy based model and you should treat it like one},
  author={Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\"o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  journal={arXiv preprint arXiv:1912.03263},
  year={2019}
}

@inproceedings{schlegl2017unsupervised,
  title={Unsupervised anomaly detection with generative adversarial networks to guide marker discovery},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={International conference on information processing in medical imaging},
  year={2017},
}

@inproceedings{zong2018deep,
  title={Deep autoencoding gaussian mixture model for unsupervised anomaly detection},
  author={Zong, Bo and Song, Qi and Min, Martin Renqiang and Cheng, Wei and Lumezanu, Cristian and Cho, Daeki and Chen, Haifeng},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{deecke2018image,
  title={Image anomaly detection with generative adversarial networks},
  author={Deecke, Lucas and Vandermeulen, Robert and Ruff, Lukas and Mandt, Stephan and Kloft, Marius},
  booktitle={Joint european conference on machine learning and knowledge discovery in databases},
  year={2018},
}

@inproceedings{pidhorskyi2018generative,
  title={Generative probabilistic novelty detection with adversarial autoencoders},
  author={Pidhorskyi, Stanislav and Almohsen, Ranya and Doretto, Gianfranco},
  booktitle={Advances in neural information processing systems},
  year={2018}
}

@inproceedings{perera2019ocgan,
  title={Ocgan: One-class novelty detection using gans with constrained latent representations},
  author={Perera, Pramuditha and Nallapati, Ramesh and Xiang, Bing},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@article{choi2019novelty,
  title={Novelty detection via blurring},
  author={Choi, Sungik and Chung, Sae-Young},
  journal={arXiv preprint arXiv:1911.11943},
  year={2019}
}

@inproceedings{scholkopf1999support,
  title={Support vector method for novelty detection},
  author={Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex and Shawe-Taylor, John and Platt, John},
  booktitle={Advances in neural information processing systems},
  year={1999}
}

@inproceedings{ruff2018deep,
  title={Deep one-class classification},
  author={Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and M{\"u}ller, Emmanuel and Kloft, Marius},
  booktitle={International Conference on Machine Learning},
  year={2018},
}

@inproceedings{golan2018deep,
  title={Deep anomaly detection using geometric transformations},
  author={Golan, Izhak and El-Yaniv, Ran},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{hendrycks2019using,
  title={Using self-supervised learning can improve model robustness and uncertainty},
  author={Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{bergman2020classification,
  title={Classification-based anomaly detection for general data},
  author={Bergman, Liron and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2005.02359},
  year={2020}
}

@article{winkens2020contrastive,
  title={Contrastive training for improved out-of-distribution detection},
  author={Winkens, Jim and Bunel, Rudy and Roy, Abhijit Guha and Stanforth, Robert and Natarajan, Vivek and Ledsam, Joseph R and MacWilliams, Patricia and Kohli, Pushmeet and Karthikesalingam, Alan and Kohl, Simon and others},
  journal={arXiv preprint arXiv:2007.05566},
  year={2020}
}

@article{liu2020hybrid,
  title={Hybrid discriminative-generative training via contrastive learning},
  author={Liu, Hao and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2007.09070},
  year={2020}
}

@article{liang2017enhancing,
  title={Enhancing the reliability of out-of-distribution image detection in neural networks},
  author={Liang, Shiyu and Li, Yixuan and Srikant, Rayadurgam},
  journal={arXiv preprint arXiv:1706.02690},
  year={2017}
}

@inproceedings{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{tack2020csi,
  title={Csi: Novelty detection via contrastive learning on distributionally shifted instances},
  author={Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{zuo2021exploiting,
  title={Exploiting the sensitivity of L2 adversarial examples to erase-and-restore},
  author={Zuo, Fei and Zeng, Qiang},
  booktitle={Asia Conference on Computer and Communications Security},
  year={2021}
}

@article{lust2020gran,
  title={GraN: an efficient gradient-norm based detector for adversarial and misclassified examples},
  author={Lust, Julia and Condurache, Alexandru Paul},
  journal={arXiv preprint arXiv:2004.09179},
  year={2020}
}

@article{metzen2017detecting,
  title={On detecting adversarial perturbations},
  author={Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
  journal={arXiv preprint arXiv:1702.04267},
  year={2017}
}

@article{feinman2017detecting,
  title={Detecting adversarial samples from artifacts},
  author={Feinman, Reuben and Curtin, Ryan R and Shintre, Saurabh and Gardner, Andrew B},
  journal={arXiv preprint arXiv:1703.00410},
  year={2017}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={International Conference on Computer Vision},
  year={2021}
}

@inproceedings{wu2018unsupervised,
  title={Unsupervised feature learning via non-parametric instance discrimination},
  author={Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X and Lin, Dahua},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2018}
}

@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={Computer Vision and Pattern Recognition},
  year={2006},
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2010},
}

@article{khalid2022rodd,
  title={Rodd: A self-supervised approach for robust out-of-distribution detection},
  author={Khalid, Umar and Esmaeili, Ashkan and Karim, Nazmul and Rahnavard, Nazanin},
  journal={arXiv preprint arXiv:2204.02553},
  year={2022}
}

@inproceedings{mohseni2020self,
  title={Self-supervised learning for generalizable out-of-distribution detection},
  author={Mohseni, Sina and Pitale, Mandar and Yadawa, JBS and Wang, Zhangyang},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@INPROCEEDINGS{5548123,
  author={Kitt, Bernd and Geiger, Andreas and Lategahn, Henning},
  booktitle={2010 IEEE Intelligent Vehicles Symposium}, 
  title={Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme}, 
  year={2010},
  volume={},
  number={},
  pages={486-492},
  doi={10.1109/IVS.2010.5548123}}
  
  @article{DBLP:journals/corr/SchleglSWSL17,
  title     = {Unsupervised Anomaly Detection with Generative Adversarial Networks
               to Guide Marker Discovery},
  author    = {Thomas Schlegl and
               Philipp Seeb{\"{o}}ck and
               Sebastian M. Waldstein and
               Ursula Schmidt{-}Erfurth and
               Georg Langs},
  journal       = {arXiv preprint arXiv:1703.05921},
  year      = {2017},
}

@inproceedings{haoqi2022vim,
  title = {ViM: Out-Of-Distribution with Virtual-logit Matching},
  author = {Wang, Haoqi and Li, Zhizhong and Feng, Litong and Zhang, Wayne},
  booktitle = {Conference on Computer Vision and Pattern Recognition},
  year = {2022}
}

@article{https://doi.org/10.48550/arxiv.2107.04882,
  title = {Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis},
  author = {Uwimana1, Anisie and Senanayake, Ransalu},
  journal       = {arXiv preprint arXiv:2107.04882},
  year = {2021},
}

@article{https://doi.org/10.48550/arxiv.2105.01879,
  title = {MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space},
  author = {Huang, Rui and Li, Yixuan},
  journal       = {arXiv preprint arXiv:2105.01879},
  year = {2021},
}

@InProceedings{Abusnaina_2021_ICCV,
    author    = {Abusnaina, Ahmed and Wu, Yuhang and Arora, Sunpreet and Wang, Yizhen and Wang, Fei and Yang, Hao and Mohaisen, David},
    title     = {Adversarial Example Detection Using Latent Neighborhood Graph},
    booktitle = {International Conference on Computer Vision},
    year      = {2021},
}

@article{https://doi.org/10.48550/arxiv.2006.10029,
    title = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  journal       = {arXiv preprint arXiv:2006.10029},
  year = {2020},
}

@article{https://doi.org/10.48550/arxiv.1708.03888,
  title = {Large Batch Training of Convolutional Networks},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal       = {arXiv preprint arXiv:1708.03888},
  year = {2017},
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Symposium on Security and Privacy},
  year={2017},
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@inproceedings{liu2020energy,
  title={Energy-based out-of-distribution detection},
  author={Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{dinh2016density,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}

@article{ma2018characterizing,
  title={Characterizing adversarial subspaces using local intrinsic dimensionality},
  author={Ma, Xingjun and Li, Bo and Wang, Yisen and Erfani, Sarah M and Wijewickrema, Sudanthi and Schoenebeck, Grant and Song, Dawn and Houle, Michael E and Bailey, James},
  journal={arXiv preprint arXiv:1801.02613},
  year={2018}
}

@inproceedings{hu2019new,
  title={A new defense against adversarial images: Turning a weakness into a strength},
  author={Hu, Shengyuan and Yu, Tao and Guo, Chuan and Chao, Wei-Lun and Weinberger, Kilian Q},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{papernot2018deep,
  title={Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning},
  author={Papernot, Nicolas and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1803.04765},
  year={2018}
}

@article{https://doi.org/10.48550/arxiv.2010.03759,
  title = {Energy-based Out-of-distribution Detection},
  author = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
  journal={arXiv preprint arXiv:2010.03759},
  year = {2020},}

@article{https://doi.org/10.48550/arxiv.1911.11132,
  title = {Scaling Out-of-Distribution Detection for Real-World Settings},
  author = {Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Zou, Andy and Kwon, Joe and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
  journal={arXiv preprint arXiv:1911.11132},
  year = {2019},
}

@article{https://doi.org/10.48550/arxiv.2111.12797,
  title = {ReAct: Out-of-distribution Detection With Rectified Activations},
  author = {Sun, Yiyou and Guo, Chuan and Li, Yixuan},
  journal={arXiv preprint arXiv:2111.12797},
  year = {2021},
}

@article{https://doi.org/10.48550/arxiv.1610.08401,
  title = {Universal adversarial perturbations},
  author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  journal={arXiv preprint arXiv:1610.08401},
  year = {2016},
}
@article{https://doi.org/10.48550/arxiv.1506.03365,
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
  author = {Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  journal={arXiv preprint arXiv:1506.03365},
  year = {2015}
}

@inproceedings{37648,
  title	= {Reading Digits in Natural Images with Unsupervised Feature Learning},
  author	= {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
  booktitle	= {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011},
  year	= {2011},
}

@inproceedings{sehwag2021ssd,
  title={SSD:  A Unified Framework for Self-Supervised Outlier Detection},
  author={Vikash Sehwag and Mung Chiang and Prateek Mittal},
 booktitle={International Conference on Learning Representations},
 year={2021},
 url={https://openreview.net/forum?id=v5gjXpmR8J}
}

@inproceedings{GaoLZ0L0S21,
  title = {Maximum Mean Discrepancy Test is Aware of Adversarial Attacks},
  author = {Ruize Gao and Feng Liu and Jingfeng Zhang and Bo Han 0003 and Tongliang Liu and Gang Niu 0001 and Masashi Sugiyama},
  year = {2021},
  url = {http://proceedings.mlr.press/v139/gao21b.html},
  researchr = {https://researchr.org/publication/GaoLZ0L0S21},
  cites = {0},
  citedby = {0},
  pages = {3564-3575},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event},
  editor = {Marina Meila and Tong Zhang 0001},
  volume = {139},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
}

@article{DBLP:journals/corr/DodgeK17b,
  author    = {Samuel F. Dodge and
               Lina J. Karam},
  title     = {A Study and Comparison of Human and Deep Learning Recognition Performance
               Under Visual Distortions},
  journal   = {CoRR},
  volume    = {abs/1705.02498},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.02498},
  eprinttype = {arXiv},
  eprint    = {1705.02498},
  timestamp = {Mon, 13 Aug 2018 16:47:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DodgeK17b.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-07780,
  author    = {Robert Geirhos and
               J{\"{o}}rn{-}Henrik Jacobsen and
               Claudio Michaelis and
               Richard S. Zemel and
               Wieland Brendel and
               Matthias Bethge and
               Felix A. Wichmann},
  title     = {Shortcut Learning in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2004.07780},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.07780},
  eprinttype = {arXiv},
  eprint    = {2004.07780},
  timestamp = {Thu, 14 Oct 2021 09:17:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-07780.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-11334,
  author    = {Jingkang Yang and
               Kaiyang Zhou and
               Yixuan Li and
               Ziwei Liu},
  title     = {Generalized Out-of-Distribution Detection: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2110.11334},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.11334},
  eprinttype = {arXiv},
  eprint    = {2110.11334},
  timestamp = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-11334.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{electronics11213500,
AUTHOR = {Cui, Peng and Wang, Jinjia},
TITLE = {Out-of-Distribution (OOD) Detection Based on Deep Learning: A Review},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {21},
ARTICLE-NUMBER = {3500},
URL = {https://www.mdpi.com/2079-9292/11/21/3500},
ISSN = {2079-9292},
ABSTRACT = {Out-of-Distribution (OOD) detection separates ID (In-Distribution) data and OOD data from input data through a model. This problem has attracted increasing attention in the area of machine learning. OOD detection has achieved good intrusion detection, fraud detection, system health monitoring, sensor network event detection, and ecosystem interference detection. The method based on deep learning is the most studied in OOD detection. In this paper, related basic information on OOD detection based on deep learning is described, and we categorize methods according to the training data. OOD detection is divided into supervised, semisupervised, and unsupervised. Where supervised data are used, the methods are categorized according to technical means: model-based, distance-based, and density-based. Each classification is introduced with background, examples, and applications. In addition, we present the latest applications of OOD detection based on deep learning and the problems and expectations in this field.},
DOI = {10.3390/electronics11213500}
}

@article{DBLP:journals/corr/abs-1906-06032,
  author    = {Aditi Raghunathan and
               Sang Michael Xie and
               Fanny Yang and
               John C. Duchi and
               Percy Liang},
  title     = {Adversarial Training Can Hurt Generalization},
  journal   = {CoRR},
  volume    = {abs/1906.06032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.06032},
  eprinttype = {arXiv},
  eprint    = {1906.06032},
  timestamp = {Sat, 23 Jan 2021 01:14:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-06032.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/MadryMSTV18,
  author    = {Aleksander Madry and
               Aleksandar Makelov and
               Ludwig Schmidt and
               Dimitris Tsipras and
               Adrian Vladu},
  title     = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rJzIBfZAb},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1902-10811,
  author    = {Benjamin Recht and
               Rebecca Roelofs and
               Ludwig Schmidt and
               Vaishaal Shankar},
  title     = {Do ImageNet Classifiers Generalize to ImageNet?},
  journal   = {CoRR},
  volume    = {abs/1902.10811},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10811},
  eprinttype = {arXiv},
  eprint    = {1902.10811},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10811.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1903-12261,
  author    = {Dan Hendrycks and
               Thomas G. Dietterich},
  title     = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  journal   = {CoRR},
  volume    = {abs/1903.12261},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.12261},
  eprinttype = {arXiv},
  eprint    = {1903.12261},
  timestamp = {Tue, 02 Apr 2019 12:29:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-12261.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2006-16241,
  author    = {Dan Hendrycks and
               Steven Basart and
               Norman Mu and
               Saurav Kadavath and
               Frank Wang and
               Evan Dorundo and
               Rahul Desai and
               Tyler Zhu and
               Samyak Parajuli and
               Mike Guo and
               Dawn Song and
               Jacob Steinhardt and
               Justin Gilmer},
  title     = {The Many Faces of Robustness: {A} Critical Analysis of Out-of-Distribution
               Generalization},
  journal   = {CoRR},
  volume    = {abs/2006.16241},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16241},
  eprinttype = {arXiv},
  eprint    = {2006.16241},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2005-05632,
  author    = {Nils Hulzebosch and
               Sarah Ibrahimi and
               Marcel Worring},
  title     = {Detecting CNN-Generated Facial Images in Real-World Scenarios},
  journal   = {CoRR},
  volume    = {abs/2005.05632},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.05632},
  eprinttype = {arXiv},
  eprint    = {2005.05632},
  timestamp = {Thu, 14 May 2020 16:56:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-05632.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-00133,
  author    = {Zhengzhe Liu and
               Xiaojuan Qi and
               Jiaya Jia and
               Philip H. S. Torr},
  title     = {Global Texture Enhancement for Fake Face Detection in the Wild},
  journal   = {CoRR},
  volume    = {abs/2002.00133},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.00133},
  eprinttype = {arXiv},
  eprint    = {2002.00133},
  timestamp = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-00133.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1007/978-3-031-19781-9_6,
author = {Liu, Bo and Yang, Fan and Bi, Xiuli and Xiao, Bin and Li, Weisheng and Gao, Xinbo},
title = {Detecting Generated Images by Real Images},
year = {2022},
isbn = {978-3-031-19780-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19781-9_6},
doi = {10.1007/978-3-031-19781-9_6},
abstract = {The widespread of generative models have called into question the authenticity of many things on the web. In this situation, the task of image forensics is urgent. The existing methods examine generated images and claim a forgery by detecting visual artifacts or invisible patterns, resulting in generalization issues. We observed that the noise pattern of real images exhibits similar characteristics in the frequency domain, while the generated images are far different. Therefore, we can perform image authentication by checking whether an image follows the patterns of authentic images. The experiments show that a simple classifier using noise patterns can easily detect a wide range of generative models, including GAN and flow-based models. Our method achieves state-of-the-art performance on both low- and high-resolution images from a wide range of generative models and shows superior generalization ability to unseen models. The code is available at .},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIV},
pages = {95–110},
numpages = {16},
keywords = {Image forensics, Frequency domain analysis, Image noise, Forgery detection, GAN, Generated images},
location = {Tel Aviv, Israel}
}

@inproceedings{10.5555/3327757.3327819,
author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7167–7177},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@misc{https://doi.org/10.48550/arxiv.2210.01742,
  doi = {10.48550/ARXIV.2210.01742},
  
  url = {https://arxiv.org/abs/2210.01742},
  
  author = {Guille-Escuret, Charles and Rodriguez, Pau and Vazquez, David and Mitliagkas, Ioannis and Monteiro, Joao},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{DBLP:journals/corr/LiangLS17,
  author    = {Shiyu Liang and
               Yixuan Li and
               R. Srikant},
  title     = {Principled Detection of Out-of-Distribution Examples in Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.02690},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02690},
  eprinttype = {arXiv},
  eprint    = {1706.02690},
  timestamp = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/LiangLS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2105-14399,
  author    = {David Mac{\^{e}}do and
               Teresa Bernarda Ludermir},
  title     = {Improving Entropic Out-of-Distribution Detection using Isometric Distances
               and the Minimum Distance Score},
  journal   = {CoRR},
  volume    = {abs/2105.14399},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.14399},
  eprinttype = {arXiv},
  eprint    = {2105.14399},
  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-14399.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LinMBHPRDZ14,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  eprinttype = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{5206848,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{DBLP:conf/iclr/TsiprasSETM19,
  author    = {Dimitris Tsipras and
               Shibani Santurkar and
               Logan Engstrom and
               Alexander Turner and
               Aleksander Madry},
  title     = {Robustness May Be at Odds with Accuracy},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SyxAb30cY7},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/TsiprasSETM19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1901-08573,
  author    = {Hongyang Zhang and
               Yaodong Yu and
               Jiantao Jiao and
               Eric P. Xing and
               Laurent El Ghaoui and
               Michael I. Jordan},
  title     = {Theoretically Principled Trade-off between Robustness and Accuracy},
  journal   = {CoRR},
  volume    = {abs/1901.08573},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08573},
  eprinttype = {arXiv},
  eprint    = {1901.08573},
  timestamp = {Thu, 25 Aug 2022 08:42:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-08573.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2020_61d77652,
 author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8588--8601},
 publisher = {Curran Associates, Inc.},
 title = {A Closer Look at Accuracy vs. Robustness},
 url = {https://proceedings.neurips.cc/paper/2020/file/61d77652c97ef636343742fc3dcf3ba9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@conference{nokey,
title = {High-Resolution Image Synthesis with Latent Diffusion Models},
author = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},
url = {https://github.com/CompVis/latent-diffusionhttps://arxiv.org/abs/2112.10752},
year  = {2022},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@inproceedings{DBLP:conf/icml/NicholDRSMMSC22,
  author    = {Alexander Quinn Nichol and
               Prafulla Dhariwal and
               Aditya Ramesh and
               Pranav Shyam and
               Pamela Mishkin and
               Bob McGrew and
               Ilya Sutskever and
               Mark Chen},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {{GLIDE:} Towards Photorealistic Image Generation and Editing with
               Text-Guided Diffusion Models},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {16784--16804},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/nichol22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/NicholDRSMMSC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2204-06125,
  author    = {Aditya Ramesh and
               Prafulla Dhariwal and
               Alex Nichol and
               Casey Chu and
               Mark Chen},
  title     = {Hierarchical Text-Conditional Image Generation with {CLIP} Latents},
  journal   = {CoRR},
  volume    = {abs/2204.06125},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.06125},
  doi       = {10.48550/arXiv.2204.06125},
  eprinttype = {arXiv},
  eprint    = {2204.06125},
  timestamp = {Tue, 19 Apr 2022 17:11:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-06125.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1812-08685,
  author    = {Pavel Korshunov and
               S{\'{e}}bastien Marcel},
  title     = {DeepFakes: a New Threat to Face Recognition? Assessment and Detection},
  journal   = {CoRR},
  volume    = {abs/1812.08685},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.08685},
  eprinttype = {arXiv},
  eprint    = {1812.08685},
  timestamp = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-08685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{article_learning_unknown,
author = {Boult, Terrance and Cruz, S. and Dhamija, Akshay and Günther, Manuel and Henrydoss, James and Scheirer, W.J.},
year = {2019},
month = {07},
pages = {9801-9807},
title = {Learning and the Unknown: Surveying Steps toward Open World Recognition},
volume = {33},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v33i01.33019801}
}

@article {PMID:32191881,
	Title = {Recent Advances in Open Set Recognition: A Survey},
	Author = {Geng, Chuanxing and Huang, Sheng-Jun and Chen, Songcan},
	DOI = {10.1109/tpami.2020.2981604},
	Number = {10},
	Volume = {43},
	Month = {October},
	Year = {2021},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	ISSN = {0162-8828},
	Pages = {3614—3631},
	Abstract = {In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers to not only accurately classify the seen classes, but also effectively deal with unseen ones. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, evaluation criteria, and algorithm comparisons. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also review the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field.},
	URL = {https://doi.org/10.1109/TPAMI.2020.2981604},
}

@INPROCEEDINGS {9723693,
author = {A. Mahdavi and M. Carvalho},
booktitle = {2021 IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)},
title = {A Survey on Open Set Recognition},
year = {2021},
volume = {},
issn = {},
pages = {37-44},
abstract = {Open Set Recognition (OSR) is about dealing with unknown situations that were not learned by the models during training. In this paper, we provide a survey of existing works about OSR and distinguish their respective advantages and disadvantages to help out new researchers interested in the subject. The categorization of OSR models is provided along with an extensive summary of recent progress. Additionally, the relationships between OSR and its related tasks including multi-class classification and novelty detection are analyzed. It is concluded that OSR can appropriately deal with unknown instances in the real-world where capturing all possible classes in the training data is not practical. Lastly, some new directions for future research topics are suggested.},
keywords = {training;knowledge engineering;conferences;training data;machine learning;learning (artificial intelligence);multitasking},
doi = {10.1109/AIKE52691.2021.00013},
url = {https://doi.ieeecomputersociety.org/10.1109/AIKE52691.2021.00013},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@article{10.1145/376284.375668,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {Outlier Detection for High Dimensional Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375668},
doi = {10.1145/376284.375668},
abstract = {The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.},
journal = {SIGMOD Rec.},
month = {may},
pages = {37–46},
numpages = {10}
}

@inproceedings{10.1145/375663.375668,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {Outlier Detection for High Dimensional Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375668},
doi = {10.1145/375663.375668},
abstract = {The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {37–46},
numpages = {10},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{article_survey_OD,
author = {Hodge, Victoria and Austin, Jim},
year = {2004},
month = {10},
pages = {85-126},
title = {A Survey of Outlier Detection Methodologies},
volume = {22},
journal = {Artificial Intelligence Review},
doi = {10.1023/B:AIRE.0000045502.10941.a9}
}

@ARTICLE{8786096,
  author={Wang, Hongzhi and Bah, Mohamed Jaward and Hammad, Mohamed},
  journal={IEEE Access}, 
  title={Progress in Outlier Detection Techniques: A Survey}, 
  year={2019},
  volume={7},
  number={},
  pages={107964-108000},
  doi={10.1109/ACCESS.2019.2932769}}

@article{article_review_AD,
author = {Ruff, Lukas and Kauffmann, Jacob and Vandermeulen, Robert and Montavon, Gregoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas and Müller, Klaus-Robert},
year = {2021},
month = {02},
pages = {1-40},
title = {A Unifying Review of Deep and Shallow Anomaly Detection},
volume = {PP},
journal = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2021.3052449}
}

@article{10.1145/3439950,
author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
title = {Deep Learning for Anomaly Detection: A Review},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3439950},
doi = {10.1145/3439950},
abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages, and disadvantages and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {38},
numpages = {38},
keywords = {novelty detection, deep learning, outlier detection, Anomaly detection, one-class classification}
}

@article{Bulusu2020AnomalousED,
  title={Anomalous Example Detection in Deep Learning: A Survey},
  author={Saikiran Bulusu and Bhavya Kailkhura and Bo Li and Pramod K. Varshney and Dawn Xiaodong Song},
  journal={IEEE Access},
  year={2020},
  volume={8},
  pages={132330-132347}
}

@article{PIMENTEL2014215,
title = {A review of novelty detection},
journal = {Signal Processing},
volume = {99},
pages = {215-249},
year = {2014},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2013.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S016516841300515X},
author = {Marco A.F. Pimentel and David A. Clifton and Lei Clifton and Lionel Tarassenko},
keywords = {Novelty detection, One-class classification, Machine learning},
abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as “one-class classification”, in which a model is constructed to describe “normal” training data. The novelty detection approach is typically used when the quantity of available “abnormal” data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that “normality” may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.}
}

@inproceedings{inproceedings_ND,
author = {Miljković, Dubravko},
year = {2010},
booktitle={MIPRO},
month = {05},
pages = {593-598},
title = {Review of novelty detection methods},
isbn = {978-1-4244-7763-0}
}

@INPROCEEDINGS{9578875,
  author={Li, Chun-Liang and Sohn, Kihyuk and Yoon, Jinsung and Pfister, Tomas},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CutPaste: Self-Supervised Learning for Anomaly Detection and Localization}, 
  year={2021},
  volume={},
  number={},
  pages={9659-9669},
  doi={10.1109/CVPR46437.2021.00954}}

@inproceedings{
jiang2022revisiting,
title={Revisiting flow generative models for Out-of-distribution detection},
author={Dihong Jiang and Sun Sun and Yaoliang Yu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=6y2KBh-0Fd9}
}

@article{DBLP:journals/corr/abs-1812-02765,
  author    = {Taylor Denouden and
               Rick Salay and
               Krzysztof Czarnecki and
               Vahdat Abdelzad and
               Buu Phan and
               Sachin Vernekar},
  title     = {Improving Reconstruction Autoencoder Out-of-distribution Detection
               with Mahalanobis Distance},
  journal   = {CoRR},
  volume    = {abs/1812.02765},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.02765},
  eprinttype = {arXiv},
  eprint    = {1812.02765},
  timestamp = {Fri, 30 Dec 2022 23:08:58 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-02765.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-031-20053-3_22,
author="Yang, Yijun
and Gao, Ruiyuan
and Xu, Qiang",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Out-of-Distribution Detection with Semantic Mismatch Under Masking",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="373--390",
abstract="This paper proposes a novel out-of-distribution (OOD) detection framework named MoodCat for image classifiers. MoodCat masks a random portion of the input image and uses a generative model to synthesize the masked image to a new image conditioned on the classification result. It then calculates the semantic difference between the original image and the synthesized one for OOD detection. Compared to existing solutions, MoodCat naturally learns the semantic information of the in-distribution data with the proposed mask and conditional synthesis strategy, which is critical to identify OODs. Experimental results demonstrate that MoodCat outperforms state-of-the-art OOD detection solutions by a large margin. Our code is available at https://github.com/cure-lab/MOODCat.",
isbn="978-3-031-20053-3"
}

@article{DBLP:journals/corr/abs-2109-14162,
  author    = {Haoran Wang and
               Weitang Liu and
               Alex Bocchieri and
               Yixuan Li},
  title     = {Can multi-label classification networks know what they don't know?},
  journal   = {CoRR},
  volume    = {abs/2109.14162},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.14162},
  eprinttype = {arXiv},
  eprint    = {2109.14162},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-14162.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-030-01237-3_34,
author="Vyas, Apoorv
and Jammalamadaka, Nataraj
and Zhu, Xia
and Das, Dipankar
and Kaul, Bharat
and Willke, Theodore L.",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-Out Classifiers",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="560--574",
abstract="As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin m between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al. [7] and the current state-of-the-art ODIN [13] on several OOD detection benchmarks.",
isbn="978-3-030-01237-3"
}

@InProceedings{Techapanurak_2020_ACCV,
    author    = {Techapanurak, Engkarat and Suganuma, Masanori and Okatani, Takayuki},
    title     = {Hyperparameter-Free Out-of-Distribution Detection Using Cosine Similarity},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {November},
    year      = {2020}
}

@INPROCEEDINGS{9577372,
  author={Zaeemzadeh, Alireza and Bisagno, Niccolò and Sambugaro, Zeno and Conci, Nicola and Rahnavard, Nazanin and Shah, Mubarak},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces}, 
  year={2021},
  volume={},
  number={},
  pages={9447-9456},
  doi={10.1109/CVPR46437.2021.00933}}

@misc{https://doi.org/10.48550/arxiv.2210.03150,
  doi = {10.48550/ARXIV.2210.03150},
  
  url = {https://arxiv.org/abs/2210.03150},
  
  author = {Ibrahim, Adam and Guille-Escuret, Charles and Mitliagkas, Ioannis and Rish, Irina and Krueger, David and Bashivan, Pouya},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Out-of-Distribution Adversarial Robustness},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/CarliniW17,
  author    = {Nicholas Carlini and
               David A. Wagner},
  title     = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection
               Methods},
  journal   = {CoRR},
  volume    = {abs/1705.07263},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07263},
  eprinttype = {arXiv},
  eprint    = {1705.07263},
  timestamp = {Mon, 13 Aug 2018 16:46:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CarliniW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bhagoji2017DimensionalityRA,
  title={Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers},
  author={Arjun Nitin Bhagoji and Daniel Cullina and Prateek Mittal},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.02654}
}

@article{DBLP:journals/corr/abs-2005-08087,
  author    = {Ashutosh Chaubey and
               Nikhil Agrawal and
               Kavya Barnwal and
               Keerat K. Guliani and
               Pramod Mehta},
  title     = {Universal Adversarial Perturbations: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2005.08087},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.08087},
  eprinttype = {arXiv},
  eprint    = {2005.08087},
  timestamp = {Fri, 22 May 2020 16:21:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-08087.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ on-the-detection-of-digital-face-manipulation,
  author = { Hao Dang* and Feng Liu* and Joel Stehouwer* and Xiaoming Liu and Anil Jain },
  title = { On the Detection of Digital Face Manipulation },
  booktitle = { In Proceeding of IEEE Computer Vision and Pattern Recognition },
  address = { Seattle, WA },
  month = { June },
  year = { 2020 },
}

@INPROCEEDINGS {9577592,
author = {H. Zhao and T. Wei and W. Zhou and W. Zhang and D. Chen and N. Yu},
booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Multi-attentional Deepfake Detection},
year = {2021},
volume = {},
issn = {},
pages = {2185-2194},
abstract = {Face forgery by deepfake is widely spread over the internet and has raised severe societal concerns. Recently, how to detect such forgery contents has become a hot research topic and many deepfake detection methods have been proposed. Most of them model deepfake detection as a vanilla binary classification problem, i.e, first use a backbone network to extract a global feature and then feed it into a binary classifier (real/fake). But since the difference between the real and fake images in this task is often subtle and local, we argue this vanilla solution is not optimal. In this paper, we instead formulate deepfake detection as a fine-grained classification problem and propose a new multi-attentional deepfake detection network. Specifically, it consists of three key components: 1) multiple spatial attention heads to make the network attend to different local parts; 2) textural feature enhancement block to zoom in the subtle artifacts in shallow features; 3) aggregate the low-level textural feature and high-level semantic features guided by the attention maps. Moreover, to address the learning difficulty of this network, we further introduce a new regional independence loss and an attention guided data augmentation strategy. Through extensive experiments on different datasets, we demonstrate the superiority of our method over the vanilla binary classifier counterparts, and achieve state-of-the-art performance. The models will be released recently at https://github.com/yoctta/multiple-attention.},
keywords = {measurement;semantics;feature extraction;forgery;pattern recognition;feeds;task analysis},
doi = {10.1109/CVPR46437.2021.00222},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00222},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@article{DBLP:journals/corr/abs-1912-11035,
  author    = {Sheng{-}Yu Wang and
               Oliver Wang and
               Richard Zhang and
               Andrew Owens and
               Alexei A. Efros},
  title     = {CNN-generated images are surprisingly easy to spot... for now},
  journal   = {CoRR},
  volume    = {abs/1912.11035},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11035},
  eprinttype = {arXiv},
  eprint    = {1912.11035},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-11035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7298640,
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}, 
  year={2015},
  volume={},
  number={},
  pages={427-436},
  doi={10.1109/CVPR.2015.7298640}}

@article{ABDAR2021243,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243-297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}

@ARTICLE{8371683,
  author={Kabir, H. M. Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and Nahavandi, Saeid},
  journal={IEEE Access}, 
  title={Neural Network-Based Uncertainty Quantification: A Survey of Methodologies and Applications}, 
  year={2018},
  volume={6},
  number={},
  pages={36218-36234},
  doi={10.1109/ACCESS.2018.2836917}}

  @article{NING2019434,
title = {Optimization under uncertainty in the era of big data and deep learning: When machine learning meets mathematical programming},
journal = {Computers and Chemical Engineering},
volume = {125},
pages = {434-448},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419300687},
author = {Chao Ning and Fengqi You},
keywords = {Data-driven optimization, Decision making under uncertainty, Big data, Machine learning, Deep learning},
abstract = {This paper reviews recent advances in the field of optimization under uncertainty via a modern data lens, highlights key research challenges and promise of data-driven optimization that organically integrates machine learning and mathematical programming for decision-making under uncertainty, and identifies potential research opportunities. A brief review of classical mathematical programming techniques for hedging against uncertainty is first presented, along with their wide spectrum of applications in Process Systems Engineering. A comprehensive review and classification of the relevant publications on data-driven distributionally robust optimization, data-driven chance constrained program, data-driven robust optimization, and data-driven scenario-based optimization is then presented. This paper also identifies fertile avenues for future research that focuses on a closed-loop data-driven optimization framework, which allows the feedback from mathematical programming to machine learning, as well as scenario-based optimization leveraging the power of deep learning techniques. Perspectives on online learning-based data-driven multistage optimization with a learning-while-optimizing scheme are presented.}
}

@misc{https://doi.org/10.48550/arxiv.1707.06642,
  doi = {10.48550/ARXIV.1707.06642},
  
  url = {https://arxiv.org/abs/1707.06642},
  
  author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The iNaturalist Species Classification and Detection Dataset},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{openimage-o,
author = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Veit, Andreas and Abu-El-Haija, Sami and Belongie, Serge and Cai, David and Feng, Zheyun and Ferrari, Vittorio and Gomes, Victor},
year = {2016},
month = {01},
pages = {},
title = {OpenImages: A public dataset for large-scale multi-label and multi-class image classification.}
}

@inproceedings{10.5555/3524938.3525144,
author = {Croce, Francesco and Hein, Matthias},
title = {Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-Free Attacks},
year = {2020},
publisher = {JMLR.org},
abstract = {The feld of defense strategies against adversarial attacks has signifcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insuffcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it diffcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {206},
numpages = {11},
series = {ICML'20}
}

@misc{https://doi.org/10.48550/arxiv.2010.11929,
  doi = {10.48550/ARXIV.2010.11929},
  
  url = {https://arxiv.org/abs/2010.11929},
  
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{DBLP:conf/iclr/BrockDS19,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1xsqj09Fm},
  timestamp = {Thu, 25 Jul 2019 13:03:18 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BrockDS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-00218,
  author    = {Rui Huang and
               Andrew Geng and
               Yixuan Li},
  title     = {On the Importance of Gradients for Detecting Distributional Shifts
               in the Wild},
  journal   = {CoRR},
  volume    = {abs/2110.00218},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.00218},
  eprinttype = {arXiv},
  eprint    = {2110.00218},
  timestamp = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-00218.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2111.09805,
  doi = {10.48550/ARXIV.2111.09805},
  
  url = {https://arxiv.org/abs/2111.09805},
  
  author = {Sun, Yiyou and Li, Yixuan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DICE: Leveraging Sparsification for Out-of-Distribution Detection},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
granese2021doctor,
title={{DOCTOR}: A Simple Method for Detecting Misclassification Errors},
author={Federica Granese and Marco Romanelli and Daniele Gorla and Catuscia Palamidessi and Pablo Piantanida},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=FHQBDiMwvK}
}

@article{DBLP:journals/corr/abs-2006-02003,
  author    = {Alexander Cao and
               Yuan Luo and
               Diego Klabjan},
  title     = {Open-Set Recognition with Gaussian Mixture Variational Autoencoders},
  journal   = {CoRR},
  volume    = {abs/2006.02003},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.02003},
  eprinttype = {arXiv},
  eprint    = {2006.02003},
  timestamp = {Tue, 11 Jan 2022 16:58:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-02003.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{WordNet,
author = {Christiane Fellbaum},
title = {WordNet: An Electronic Lexical Database},
year = 1998,
publisher = "MIT Press",
address = "Cambridge, MA"
}

@article{yang2022openood,
    author = {Yang, Jingkang and Wang, Pengyun and Zou, Dejian and Zhou, Zitang and Ding, Kunyuan and Peng, Wenxuan and Wang, Haoqi and Chen, Guangyao and Li, Bo and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Hendrycks, Dan and Li, Yixuan and Liu, Ziwei},
    title = {OpenOOD: Benchmarking Generalized Out-of-Distribution Detection},
    year = {2022},
    journal = {OpenReview}
}

@inproceedings{10.1145/3531146.3533231,
author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
title = {Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533231},
doi = {10.1145/3531146.3533231},
abstract = {As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset’s origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset’s lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models—such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1776–1826},
numpages = {51},
keywords = {data cards, dataset documentation, datasheets, model cards, responsible AI, transparency},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {FAccT '22}
}

@misc{djurisic2023extremely,
      title={Extremely Simple Activation Shaping for Out-of-Distribution Detection}, 
      author={Andrija Djurisic and Nebojsa Bozanic and Arjun Ashok and Rosanne Liu},
      year={2023},
      eprint={2209.09858},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zhang2023out-of-distribution,
author = {Zhang, Jinsong and Fu, Qiang and Chen, Xu and Du, Lun and Li, Zelin and Wang, Gang and Liu, Xiaoguang and Han, Shi and Zhang, Dongmei},
title = {Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy},
booktitle = {International Conference on Learning Representations (ICLR'23)},
year = {2023},
month = {February},
abstract = {Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameter-free and high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models.},
url = {https://www.microsoft.com/en-us/research/publication/out-of-distribution-detection-based-on-in-distribution-data-patterns-memorization-with-modern-hopfield-energy/},
}

@misc{kim2023neural,
      title={Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data}, 
      author={Jang-Hyun Kim and Sangdoo Yun and Hyun Oh Song},
      year={2023},
      eprint={2301.12321},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}