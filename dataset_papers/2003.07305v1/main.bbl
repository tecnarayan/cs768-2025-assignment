\begin{thebibliography}{10}

\bibitem{kalashnikov18}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, and Sergey Levine.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In {\em CoRL}, 2018.

\bibitem{shani2005recommender}
Guy Shani, David Heckerman, and Ronen~I Brafman.
\newblock An mdp-based recommender system.
\newblock {\em Journal of Machine Learning Research}, 6(Sep):1265--1295, 2005.

\bibitem{hassalt10doubleq}
Hado~van Hasselt.
\newblock Double q-learning.
\newblock In {\em Proceedings of the 23rd International Conference on Neural
  Information Processing Systems - Volume 2}, 2010.

\bibitem{Hasselt2018DeepRL}
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat,
  and Joseph Modayil.
\newblock Deep reinforcement learning and the deadly triad.
\newblock {\em ArXiv}, abs/1812.02648, 2018.

\bibitem{pmlr-v80-fujimoto18a}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1587--1596, 2018.

\bibitem{fu19diagnosing}
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}. PMLR, 2019.

\bibitem{Haarnoja18}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em CoRR}, abs/1801.01290, 2018.

\bibitem{rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{Watkins92}
Christopher~J.C.H. Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine Learning}, 8:279--292, 1992.

\bibitem{Riedmiller2005}
Martin Riedmiller.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European Conference on Machine Learning}, pages 317--328.
  Springer, 2005.

\bibitem{Mnih2015}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock {Human-level control through deep reinforcement learning}.
\newblock {\em Nature}, 518(7540):529--533, feb 2015.

\bibitem{Haarnoja2017}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{konda_ac}
Vijaymohan Konda and John~N. Tsitsiklis.
\newblock {\em Actor-Critic Algorithms}.
\newblock PhD thesis, USA, 2002.
\newblock AAI0804543.

\bibitem{yu2019meta}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In {\em Conference on Robot Learning (CoRL)}, 2019.

\bibitem{Lillicrap2015}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock {Continuous control with deep reinforcement learning}.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{rockafellar-1970a}
R.~Tyrrell Rockafellar.
\newblock {\em Convex analysis}.
\newblock Princeton Mathematical Series. Princeton University Press, Princeton,
  N. J., 1970.

\bibitem{Krantz2002TheIF}
Steven~G. Krantz and Harold~R. Parks.
\newblock The implicit function theorem: History, theory, and applications.
\newblock 2002.

\bibitem{absention}
Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and
  Jamal Mohd-Yusof.
\newblock Combating label noise in deep learning using abstention.
\newblock In {\em Proceedings of 35th International Conference on Machine
  Learning}, 05 2019.

\bibitem{munos2005error}
R{\'e}mi Munos.
\newblock Error bounds for approximate value iteration.
\newblock In {\em AAAI Conference on Artificial intelligence (AAAI)}, pages
  1006--1011. AAAI Press, 2005.

\bibitem{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2010.

\bibitem{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(May):815--857, 2008.

\bibitem{Sutton09a}
Richard~S. Sutton, Hamid~Reza Maei, and Csaba Szepesv\'{a}ri.
\newblock A convergent o(n) temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2009.

\bibitem{Sutton09b}
Richard~S. Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv\'{a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2009.

\bibitem{maei09nonlineargtd}
Hamid~R. Maei, Csaba Szepesv\'{a}ri, Shalabh Bhatnagar, Doina Precup, David
  Silver, and Richard~S. Sutton.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In {\em Proceedings of the 22nd International Conference on Neural
  Information Processing Systems}, 2009.

\bibitem{Tsitsiklis1994}
John~N. Tsitsiklis.
\newblock Asynchronous stochastic approximation and q-learning.
\newblock {\em Machine Learning}, 16(3):185--202, Sep 1994.

\bibitem{devraj2017zap}
Adithya~M. Devraj and Sean~P. Meyn.
\newblock Zap q-learning.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS’17, page 2232–2241, Red Hook, NY,
  USA, 2017. Curran Associates Inc.

\bibitem{Tsitsiklis97ananalysis}
John~N. Tsitsiklis and Benjamin~Van Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock Technical report, IEEE Transactions on Automatic Control, 1997.

\bibitem{Achiam2019TowardsCD}
Joshua Achiam, Ethan Knight, and Pieter Abbeel.
\newblock Towards characterizing divergence in deep q-learning.
\newblock {\em ArXiv}, abs/1903.08894, 2019.

\bibitem{martha2018sparse}
Vincent Liu, Raksha Kumaraswamy, Lei Le, and Martha White.
\newblock The utility of sparse representations for control in reinforcement
  learning.
\newblock {\em CoRR}, abs/1811.06626, 2018.

\bibitem{kumar19bear}
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock 2019.

\bibitem{suttonrlbook}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock Second edition, 2018.

\bibitem{Kolter2011TheFP}
J.~Zico Kolter.
\newblock The fixed points of off-policy td.
\newblock In {\em NIPS}, 2011.

\bibitem{sutton16emphatic}
Richard~S. Sutton, A.~Rupam Mahmood, and Martha White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock {\em J. Mach. Learn. Res.}, 17(1):2603–2631, January 2016.

\bibitem{schaul2019ray}
Tom Schaul, Diana Borsa, Joseph Modayil, and Razvan Pascanu.
\newblock Ray interference: a source of plateaus in deep reinforcement
  learning.
\newblock {\em CoRR}, abs/1904.11455, 2019.

\bibitem{Schaul2015}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{du2019distributioncheck}
Simon Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient $q$-learning with function approximation via
  distribution shift error checking oracle.
\newblock In {\em NeurIPS}, 06 2019.

\bibitem{bellemare2013ale}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em J. Artif. Int. Res.}, 47(1):253–279, May 2013.

\bibitem{castro18dopamine}
Pablo~Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and
  Marc~G. Bellemare.
\newblock Dopamine: {A} {R}esearch {F}ramework for {D}eep {R}einforcement
  {L}earning.
\newblock 2018.

\bibitem{machado18sticky}
Marlos~C. Machado, Marc~G. Bellemare, Erik Talvitie, Joel Veness, Matthew
  Hausknecht, and Michael Bowling.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock {\em J. Artif. Int. Res.}, 61(1):523–562, January 2018.

\bibitem{hazan2019maxent}
Elad Hazan, Sham Kakade, Karan Singh, and Abby~Van Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock 2019.

\bibitem{ahmed19understanding}
Zafarali Ahmed, Nicolas Le~Roux, Mohammad Norouzi, and Dale Schuurmans.
\newblock Understanding the impact of entropy on policy optimization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}. PMLR, 2019.

\bibitem{farias_fixed_points}
D.~Farias and B.~Roy.
\newblock On the existence of fixed points for approximate value iteration and
  temporal-difference learning.
\newblock {\em Journal of Optimization Theory and Applications}, 105:589--608,
  06 2000.

\bibitem{du2020is}
Simon~S. Du, Sham~M. Kakade, Ruosong Wang, and Lin~F. Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{perkins2002api}
Theodore~J. Perkins and Doina Precup.
\newblock A convergent form of approximate policy iteration.
\newblock NIPS’02, 2002.

\bibitem{munos2003api}
R\'{e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, ICML’03, page 560–567.
  AAAI Press, 2003.

\bibitem{scherrer15a}
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and
  Matthieu Geist.
\newblock Approximate modified policy iteration and its application to the game
  of tetris.
\newblock {\em Journal of Machine Learning Research}, 16(49):1629--1676, 2015.

\bibitem{Lesner2013TightPB}
Boris Lesner and Bruno Scherrer.
\newblock Tight performance bounds for approximate modified policy iteration
  with non-stationary policies.
\newblock {\em ArXiv}, abs/1304.5610, 2013.

\bibitem{scherrer_comparison}
Bruno Scherrer.
\newblock Approximate policy iteration schemes: A comparison.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning - Volume 32}, ICML’14, page
  II–1314–II–1322. JMLR.org, 2014.

\bibitem{fujimoto19a}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{ntk}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31}. 2018.

\bibitem{zhang2017deeper}
Shangtong Zhang and Richard~S Sutton.
\newblock A deeper look at experience replay.
\newblock {\em arXiv preprint arXiv:1712.01275}, 2017.

\bibitem{liu2018effects}
Ruishan Liu and James Zou.
\newblock The effects of memory replay in reinforcement learning.
\newblock In {\em 2018 56th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}. IEEE, 2018.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em arXiv preprint arXiv:2001.06782}, 2020.

\bibitem{haarnoja2018sacapps}
Kristian Hartikainen George Tucker Sehoon Ha Jie Tan Vikash Kumar Henry Zhu
  Abhishek Gupta Pieter~Abbeel Tuomas~Haarnoja, Aurick~Zhou and Sergey Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock Technical report, 2018.

\end{thebibliography}
