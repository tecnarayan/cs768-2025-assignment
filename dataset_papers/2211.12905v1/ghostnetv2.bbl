\begin{thebibliography}{10}

\bibitem{proxylessnas}
Han Cai, Ligeng Zhu, and Song Han.
\newblock Proxylessnas: Direct neural architecture search on target task and
  hardware.
\newblock In {\em ICLR}, 2019.

\bibitem{chen2017rethinking}
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock {\em arXiv preprint arXiv:1706.05587}, 2017.

\bibitem{sparse-shift}
Weijie Chen, Di~Xie, Yuan Zhang, and Shiliang Pu.
\newblock All you need is a few shifts: Designing efficient convolutional
  neural networks for image classification.
\newblock In {\em CVPR}, 2019.

\bibitem{david2021tensorflow}
Robert David, Jared Duke, Advait Jain, Vijay Janapa~Reddi, Nat Jeffries, Jian
  Li, Nick Kreeger, Ian Nappier, Meghna Natraj, Tiezhen Wang, et~al.
\newblock Tensorflow lite micro: Embedded machine learning for tinyml systems.
\newblock {\em Proceedings of Machine Learning and Systems}, 3:800--811, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{han2022pyramidtnt}
Kai Han, Jianyuan Guo, Yehui Tang, and Yunhe Wang.
\newblock Pyramidtnt: Improved transformer-in-transformer baselines with
  pyramid architecture.
\newblock {\em arXiv preprint arXiv:2201.00978}, 2022.

\bibitem{ghostnet}
Kai Han, Yunhe Wang, Qi~Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
\newblock Ghostnet: More features from cheap operations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1580--1589, 2020.

\bibitem{han2021transformer}
Kai Han, An~Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
\newblock Transformer in transformer.
\newblock {\em Advances in Neural Information Processing Systems},
  34:15908--15919, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hou2021coordinate}
Qibin Hou, Daquan Zhou, and Jiashi Feng.
\newblock Coordinate attention for efficient mobile network design.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13713--13722, 2021.

\bibitem{mobilev3}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1314--1324, 2019.

\bibitem{mobilenet}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hu2018squeeze}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem{mindspore}
Huawei.
\newblock Mindspore.
\newblock \url{https://www.mindspore.cn/}, 2020.

\bibitem{iandola2016squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock {\em arXiv preprint arXiv:1602.07360}, 2016.

\bibitem{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock {\em arXiv preprint arXiv:1405.3866}, 2014.

\bibitem{karpathy2014large}
Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul
  Sukthankar, and Li~Fei-Fei.
\newblock Large-scale video classification with convolutional neural networks.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 1725--1732, 2014.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{ma2018shufflenet}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{mehta2022mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: Light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{redmon2018yolov3}
Joseph Redmon and Ali Farhadi.
\newblock Yolov3: An incremental improvement.
\newblock {\em arXiv preprint arXiv:1804.02767}, 2018.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{mobilev2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem{tai2015convolutional}
Cheng Tai, Tong Xiao, Yi~Zhang, Xiaogang Wang, et~al.
\newblock Convolutional neural networks with low-rank regularization.
\newblock {\em arXiv preprint arXiv:1511.06067}, 2015.

\bibitem{mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tang2022image}
Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.
\newblock An image patch is a wave: Phase-aware vision mlp.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10935--10944, 2022.

\bibitem{tang2021augmented}
Yehui Tang, Kai Han, Chang Xu, An~Xiao, Yiping Deng, Chao Xu, and Yunhe Wang.
\newblock Augmented shortcuts for vision transformers.
\newblock {\em Advances in Neural Information Processing Systems},
  34:15316--15327, 2021.

\bibitem{tang2020scop}
Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang
  Xu.
\newblock Scop: Scientific control for reliable neural network pruning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:10936--10947, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7794--7803, 2018.

\bibitem{woo2018cbam}
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In~So Kweon.
\newblock Cbam: Convolutional block attention module.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{fbnet}
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
  Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10734--10742, 2019.

\bibitem{xu2021learning}
Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang.
\newblock Learning frequency domain approximation for binary neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:25553--25565, 2021.

\bibitem{xu2019positive}
Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng Tao, and
  Chang Xu.
\newblock Positive-unlabeled compression on the cloud.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{zhang2018shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6848--6856, 2018.

\bibitem{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 633--641, 2017.

\bibitem{mobilenext}
Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
\newblock Rethinking bottleneck structure for efficient mobile network design.
\newblock In {\em European Conference on Computer Vision}, pages 680--697.
  Springer, 2020.

\end{thebibliography}
