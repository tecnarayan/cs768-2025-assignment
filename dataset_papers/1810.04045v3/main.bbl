\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{achille2018information}
Achille, A. and Soatto, S.
\newblock {Information Dropout: Learning Optimal Representations Through Noisy
  Computation}.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2018.

\bibitem[Andrews \& Mallows(1974)Andrews and Mallows]{andrews1974scale}
Andrews, D.~F. and Mallows, C.~L.
\newblock {Scale Mixtures of Normal Distributions}.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pp.\  99--102, 1974.

\bibitem[Ba \& Frey(2013)Ba and Frey]{ba2013adaptive}
Ba, J. and Frey, B.
\newblock {Adaptive Dropout for Training Deep Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3084--3092, 2013.

\bibitem[Baldi \& Sadowski(2013)Baldi and Sadowski]{baldi2013understanding}
Baldi, P. and Sadowski, P.~J.
\newblock {Understanding Dropout}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2814--2822, 2013.

\bibitem[Barndorff-Nielsen(1977)]{barndorff1977exponentially}
Barndorff-Nielsen, O.
\newblock {Exponentially Decreasing Distributions for the Logarithm of Particle
  Size}.
\newblock \emph{Proceedings of the Royal Society of London. Series A,
  Mathematical and Physical Sciences}, 353:\penalty0 401--419, 1977.

\bibitem[Beal \& Ghahramani(2003)Beal and Ghahramani]{beal2003variationalEM}
Beal, M.~J. and Ghahramani, Z.
\newblock {The Variational Bayesian EM Algorithm for Incomplete Data: with
  application to scoring graphical model structures}.
\newblock \emph{Bayesian Statistics}, 7:\penalty0 453--464, 2003.

\bibitem[Beale \& Mallows(1959)Beale and Mallows]{beale1959scale}
Beale, E. and Mallows, C.
\newblock {Scale Mixing of Symmetric Distributions with Zero Means}.
\newblock \emph{The Annals of Mathematical Statistics}, 30\penalty0
  (4):\penalty0 1145--1151, 1959.

\bibitem[Bui et~al.(2016)Bui, Hernandez-Lobato, Hernandez-Lobato, Li, and
  Turner]{pmlr-v48-bui16}
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., and Turner, R.
\newblock {Deep Gaussian Processes for Regression using Approximate Expectation
  Propagation}.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning (ICML)}, pp.\  1472--1481, 2016.

\bibitem[Burda et~al.(2016)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock {Importance Weighted Autoencoders}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Carvalho et~al.(2009)Carvalho, Polson, and
  Scott]{carvalho2009handling}
Carvalho, C.~M., Polson, N.~G., and Scott, J.~G.
\newblock {Handling Sparsity via the Horseshoe}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AIStats)}, pp.\  73--80, 2009.

\bibitem[Dheeru \& Karra~Taniskidou(2017)Dheeru and Karra~Taniskidou]{Dua:2017}
Dheeru, D. and Karra~Taniskidou, E.
\newblock {{UCI} Machine Learning Repository}, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Fortunato et~al.(2017)Fortunato, Blundell, and
  Vinyals]{fortunato2017bayesian}
Fortunato, M., Blundell, C., and Vinyals, O.
\newblock {Bayesian Recurrent Neural Networks}.
\newblock \emph{ArXiv e-prints}, 2017.

\bibitem[Gal \& Ghahramani(2016{\natexlab{a}})Gal and Ghahramani]{gal2016conv}
Gal, Y. and Ghahramani, Z.
\newblock {Bayesian Convolutional Neural Networks with Bernoulli Approximate
  Variational Inference}.
\newblock \emph{ICLR Workshop Track}, 2016{\natexlab{a}}.

\bibitem[Gal \& Ghahramani(2016{\natexlab{b}})Gal and
  Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock {Dropout as a Bayesian Approximation: Representing Model Uncertainty
  in Deep Learning}.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML)}, pp.\  1050--1059, 2016{\natexlab{b}}.

\bibitem[Gal \& Ghahramani(2016{\natexlab{c}})Gal and
  Ghahramani]{gal2016theoretically}
Gal, Y. and Ghahramani, Z.
\newblock {A Theoretically Grounded Application of Dropout in Recurrent Neural
  Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1019--1027, 2016{\natexlab{c}}.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{gal2017concrete}
Gal, Y., Hron, J., and Kendall, A.
\newblock {Concrete Dropout}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3581--3590, 2017.

\bibitem[George \& McCulloch(1993)George and McCulloch]{george1993variable}
George, E.~I. and McCulloch, R.~E.
\newblock {Variable Selection via Gibbs Sampling}.
\newblock \emph{Journal of the American Statistical Association}, 88\penalty0
  (423):\penalty0 881--889, 1993.

\bibitem[Ghosh et~al.(2018)Ghosh, Yao, and Doshi-Velez]{ghosh2018structured}
Ghosh, S., Yao, J., and Doshi-Velez, F.
\newblock {Structured Variational Learning of Bayesian Neural Networks with
  Horseshoe Priors}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016Deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{{Deep Learning}}.
\newblock MIT press, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016Deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In \emph{Proceedings of the IEEE {C}onference on {C}omputer {V}ision
  and {P}attern {R}ecognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[Helmbold \& Long(2015)Helmbold and Long]{helmbold2015inductive}
Helmbold, D.~P. and Long, P.~M.
\newblock {On the Inductive Bias of Dropout}.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3403--3454, 2015.

\bibitem[Herlau et~al.(2015)Herlau, M{\o}rup, and Schmidt]{herlau2004bayesian}
Herlau, T., M{\o}rup, M., and Schmidt, M.~N.
\newblock {Bayesian Dropout}.
\newblock \emph{ArXiv e-prints}, 2015.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock {Probabilistic Backpropagation for Scalable Learning of Bayesian
  Neural Networks}.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML)}, pp.\  1861--1869, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R.
\newblock {Improving Neural Networks by Preventing Co-Adaptation of Feature
  Detectors}.
\newblock \emph{ArXiv e-prints}, 2012.

\bibitem[Hron et~al.(2018)Hron, Matthews, and Ghahramani]{hron2018dropout}
Hron, J., Matthews, A., and Ghahramani, Z.
\newblock {Variational Bayesian Dropout: Pitfalls and Fixes}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pp.\  1--8, 2018.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock {Deep Networks with Stochastic Depth}.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, pp.\
  646--661. Springer, 2016.

\bibitem[Ingraham \& Marks(2017)Ingraham and Marks]{ingraham2017variational}
Ingraham, J. and Marks, D.
\newblock {Variational Inference for Sparse and Undirected Models}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pp.\  1607--1616, 2017.

\bibitem[Ionides(2008)]{ionides2008truncated}
Ionides, E.~L.
\newblock {Truncated Importance Sampling}.
\newblock \emph{Journal of Computational and Graphical Statistics}, 17\penalty0
  (2):\penalty0 295--311, 2008.

\bibitem[Ji et~al.(2016)Ji, Vishwanathan, Satish, Anderson, and
  Dubey]{ji2015blackout}
Ji, S., Vishwanathan, S., Satish, N., Anderson, M.~J., and Dubey, P.
\newblock {Blackout: Speeding Up Recurrent Neural Network Language Models with
  Very Large Vocabularies}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock {Variational Dropout and the Local Reparameterization Trick}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2015.

\bibitem[Krueger et~al.(2017)Krueger, Maharaj, Kram{\'a}r, Pezeshki, Ballas,
  Ke, Goyal, Bengio, Courville, and Pal]{krueger2016zoneout}
Krueger, D., Maharaj, T., Kram{\'a}r, J., Pezeshki, M., Ballas, N., Ke, N.~R.,
  Goyal, A., Bengio, Y., Courville, A., and Pal, C.
\newblock {Zoneout: Regularizing RNNs by Randomly Preserving Hidden
  Activations}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Kuo \& Mallick(1998)Kuo and Mallick]{kuo1998variable}
Kuo, L. and Mallick, B.
\newblock {Variable Selection for Regression Models}.
\newblock \emph{Sankhy{\=a}: The Indian Journal of Statistics, Series B}, pp.\
  65--81, 1998.

\bibitem[Lang \& Witbrock(1988)Lang and Witbrock]{lang1988resnet}
Lang, K.~J. and Witbrock, M.
\newblock {Learning to Tell Two Spirals Apart}.
\newblock In \emph{1988 Connectionist Models Summer School}, 1988.

\bibitem[Liu et~al.(2019)Liu, Luo, Shen, Sun, and Li]{liu2019beta}
Liu, L., Luo, Y., Shen, X., Sun, M., and Li, B.
\newblock {$\beta$-Dropout: a Unified Dropout}.
\newblock \emph{IEEE Access}, 2019.

\bibitem[Louizos(2015)]{louizos2015smart}
Louizos, C.
\newblock {Smart Regularization of Deep Architectures}.
\newblock \emph{Masters Thesis, University of Amsterdam}, 2015.

\bibitem[Louizos et~al.(2017)Louizos, Ullrich, and
  Welling]{louizos2017bayesian}
Louizos, C., Ullrich, K., and Welling, M.
\newblock {Bayesian Compression for Deep Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3288--3298, 2017.

\bibitem[MacKay(1992)]{mackay1992bayesian}
MacKay, D.~J.
\newblock {Bayesian Interpolation}.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 415--447, 1992.

\bibitem[MacKay(1994)]{mackay1996bayesian}
MacKay, D.~J.
\newblock {Bayesian Non-Linear Modeling for the Prediction Competition}.
\newblock In \emph{Maximum Entropy and Bayesian Methods}, pp.\  221--234.
  Springer, 1994.

\bibitem[Maeda(2014)]{maeda2014bayesian}
Maeda, S.-i.
\newblock {A Bayesian Encourages Dropout}.
\newblock \emph{ArXiv e-prints}, 2014.

\bibitem[Mitchell \& Beauchamp(1988)Mitchell and
  Beauchamp]{mitchell1988bayesian}
Mitchell, T.~J. and Beauchamp, J.~J.
\newblock {Bayesian Variable Selection in Linear Regression}.
\newblock \emph{Journal of the American Statistical Association}, 83\penalty0
  (404):\penalty0 1023--1032, 1988.

\bibitem[Mohamed(2015)]{mohamed2015statistical}
Mohamed, S.
\newblock {A Statistical View of Deep Learning}.
\newblock \emph{ArXiv e-prints}, 2015.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock {Variational Dropout Sparsifies Deep Neural Networks}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pp.\  2498--2507, 2017.

\bibitem[Nakagami(1960)]{nakagami1960m}
Nakagami, M.
\newblock {The M-Distribution: A General Formula of Intensity Distribution of
  Rapid Fading}.
\newblock In \emph{Statistical Methods in Radio Wave Propagation}, pp.\  3--36.
  Elsevier, 1960.

\bibitem[Nakajima \& Sugiyama(2014)Nakajima and Sugiyama]{nakajima2014analysis}
Nakajima, S. and Sugiyama, M.
\newblock {Analysis of Empirical MAP and Empirical Partially Bayes: Can They be
  Alternatives to Variational Bayes?}
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AIStats)}, pp.\  20--28, 2014.

\bibitem[Nalisnick \& Smyth(2018)Nalisnick and Smyth]{nalisnick2018learning}
Nalisnick, E. and Smyth, P.
\newblock {Learning Priors for Invariance}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AIStats)}, pp.\  366--375, 2018.

\bibitem[Neal(1994)]{neal1995bayesian}
Neal, R.~M.
\newblock \emph{{Bayesian Learning for Neural Networks}}.
\newblock PhD thesis, University of Toronto, 1994.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{neklyudov2017structured}
Neklyudov, K., Molchanov, D., Ashukha, A., and Vetrov, D.~P.
\newblock {Structured Bayesian Pruning via Log-Normal Multiplicative Noise}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6778--6787, 2017.

\bibitem[Nelder \& Baker(2004)Nelder and Baker]{nelder2004generalized}
Nelder, J.~A. and Baker, R.~J.
\newblock {Generalized Linear Models}.
\newblock \emph{Encyclopedia of Statistical Sciences}, 4, 2004.

\bibitem[Noh et~al.(2017)Noh, You, Mun, and Han]{noh2017regularizing}
Noh, H., You, T., Mun, J., and Han, B.
\newblock {Regularizing Deep Neural Networks by Noise: Its Interpretation and
  Optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  5109--5118, 2017.

\bibitem[Osband(2016)]{osband2016risk}
Osband, I.
\newblock {Risk Versus Uncertainty in Deep Learning: Bayes, Bootstrap and the
  Dangers of Dropout}.
\newblock \emph{NIPS Workshop on Bayesian Deep Learning}, 2016.

\bibitem[{Polson} \& {Rockova}(2018){Polson} and
  {Rockova}]{2018arXiv180309138P}
{Polson}, N. and {Rockova}, V.
\newblock {Posterior Concentration for Sparse Deep Learning}.
\newblock \emph{ArXiv e-prints}, 2018.

\bibitem[Shen et~al.(2018)Shen, Tian, Liu, Xu, and Tao]{contdropout}
Shen, X., Tian, X., Liu, T., Xu, F., and Tao, D.
\newblock {Continuous Dropout}.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  pp.\  1--12, 2018.

\bibitem[Singh et~al.(2016)Singh, Hoiem, and Forsyth]{singh2016swapout}
Singh, S., Hoiem, D., and Forsyth, D.
\newblock {Swapout: Learning an Ensemble of Deep Architectures}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  28--36, 2016.

\bibitem[{Srinivas} \& {Venkatesh Babu}(2016){Srinivas} and {Venkatesh
  Babu}]{2016arXiv161106791S}
{Srinivas}, S. and {Venkatesh Babu}, R.
\newblock {Generalized Dropout}.
\newblock \emph{ArXiv e-prints}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock {Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015training}
Srivastava, R.~K., Greff, K., and Schmidhuber, J.
\newblock {Training Very Deep Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2377--2385, 2015.

\bibitem[Steel(2000)]{steel2000bayesian}
Steel, M.~F.
\newblock {Bayesian Regression Analysis With Scale Mixtures of Normals}.
\newblock \emph{Econometric Theory}, 16\penalty0 (01):\penalty0 80--101, 2000.

\bibitem[Tipping(2001)]{tipping2001sparse}
Tipping, M.~E.
\newblock {Sparse Bayesian Learning and the Relevance Vector Machine}.
\newblock \emph{The Journal of Machine Learning Research}, 1:\penalty0
  211--244, 2001.

\bibitem[Tomczak(2013)]{tomczak2013prediction}
Tomczak, J.~M.
\newblock {Prediction of Breast Cancer Recurrence Using Classification
  Restricted Boltzmann Machine with Dropping}.
\newblock \emph{ArXiv e-prints}, 2013.

\bibitem[Tompson et~al.(2015)Tompson, Goroshin, Jain, LeCun, and
  Bregler]{tompson2015efficient}
Tompson, J., Goroshin, R., Jain, A., LeCun, Y., and Bregler, C.
\newblock {Efficient Object Localization Using Convolutional Networks}.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  648--656, 2015.

\bibitem[{Vehtari} et~al.(2014){Vehtari}, {Gelman}, {Sivula}, {Jyl{\"a}nki},
  {Tran}, {Sahai}, {Blomstedt}, {Cunningham}, {Schiminovich}, and
  {Robert}]{epWayofLife}
{Vehtari}, A., {Gelman}, A., {Sivula}, T., {Jyl{\"a}nki}, P., {Tran}, D.,
  {Sahai}, S., {Blomstedt}, P., {Cunningham}, J.~P., {Schiminovich}, D., and
  {Robert}, C.
\newblock {Expectation Propagation as a Way of Life: A framework for Bayesian
  inference on partitioned data}.
\newblock \emph{ArXiv e-prints}, 2014.

\bibitem[Vehtari et~al.(2015)Vehtari, Gelman, and Gabry]{vehtari2015pareto}
Vehtari, A., Gelman, A., and Gabry, J.
\newblock {Pareto Smoothed Importance Sampling}.
\newblock \emph{ArXiv Preprint}, 2015.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
Wager, S., Wang, S., and Liang, P.~S.
\newblock {Dropout Training as Adaptive Regularization}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  351--359, 2013.

\bibitem[Wager et~al.(2014)Wager, Fithian, Wang, and Liang]{wager2014altitude}
Wager, S., Fithian, W., Wang, S., and Liang, P.~S.
\newblock {Altitude Training: Strong Bounds for Single-Layer Dropout}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  100--108, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{wan2013regularization}
Wan, L., Zeiler, M., Zhang, S., Le~Cun, Y., and Fergus, R.
\newblock {Regularization of Neural Networks Using DropConnect}.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML)}, pp.\  1058--1066, 2013.

\bibitem[{Wang} et~al.(2018){Wang}, {Liu}, and {Liu}]{wang2018tailadaptive}
{Wang}, D., {Liu}, H., and {Liu}, Q.
\newblock {Variational Inference with Tail-adaptive f-Divergence}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Wang \& Manning(2013)Wang and Manning]{wang2013fast}
Wang, S. and Manning, C.
\newblock {Fast Dropout Training}.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML)}, pp.\  118--126, 2013.

\bibitem[{Wu} et~al.(2019){Wu}, {Nowozin}, {Meeds}, {Turner},
  {Hern{\'a}ndez-Lobato}, and {Gaunt}]{wu2018fixingVB}
{Wu}, A., {Nowozin}, S., {Meeds}, E., {Turner}, R.~E., {Hern{\'a}ndez-Lobato},
  J.~M., and {Gaunt}, A.~L.
\newblock {Deterministic Variational Inference for Robust Bayesian Neural
  Networks}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Zhai \& Wang(2018)Zhai and Wang]{zhai2018adaptive}
Zhai, K. and Wang, H.
\newblock {Adaptive Dropout with Rademacher Complexity Regularization}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Zolna et~al.(2018)Zolna, Arpit, Suhubdy, and
  Bengio]{zolna2018fraternal}
Zolna, K., Arpit, D., Suhubdy, D., and Bengio, Y.
\newblock {Fraternal Dropout}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\end{thebibliography}
