\begin{thebibliography}{10}

\bibitem{ahn2019variational}
S.~Ahn, S.~X. Hu, A.~Damianou, N.~D. Lawrence, and Z.~Dai.
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9163--9171, 2019.

\bibitem{cai2018cascade}
Z.~Cai and N.~Vasconcelos.
\newblock Cascade r-cnn: Delving into high quality object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6154--6162, 2018.

\bibitem{chen2017learning}
G.~Chen, W.~Choi, X.~Yu, T.~Han, and M.~Chandraker.
\newblock Learning efficient object detection models with knowledge
  distillation.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{chen2019mmdetection}
K.~Chen, J.~Wang, J.~Pang, Y.~Cao, Y.~Xiong, X.~Li, S.~Sun, W.~Feng, Z.~Liu,
  J.~Xu, et~al.
\newblock Mmdetection: Open mmlab detection toolbox and benchmark.
\newblock {\em arXiv preprint arXiv:1906.07155}, 2019.

\bibitem{chen2018encoder}
L.-C. Chen, Y.~Zhu, G.~Papandreou, F.~Schroff, and H.~Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 801--818, 2018.

\bibitem{chen2021distilling}
P.~Chen, S.~Liu, H.~Zhao, and J.~Jia.
\newblock Distilling knowledge via knowledge review.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5008--5017, 2021.

\bibitem{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{du2023stable}
C.~Du, Y.~Li, Z.~Qiu, and C.~Xu.
\newblock Stable diffusion is untable.
\newblock {\em arXiv preprint arXiv:2306.02583}, 2023.

\bibitem{2021Distilling}
Z.~Du, R.~Zhang, M.~Chang, X.~Zhang, S.~Liu, T.~Chen, and Y.~Chen.
\newblock Distilling object detectors with feature richness.
\newblock In {\em 35th Conference on Neural Information Processing Systems},
  2021.

\bibitem{guo2021distilling}
J.~Guo, K.~Han, Y.~Wang, H.~Wu, X.~Chen, C.~Xu, and C.~Xu.
\newblock Distilling object detectors via decoupled features.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2154--2164, 2021.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2019knowledge}
T.~He, C.~Shen, Z.~Tian, D.~Gong, C.~Sun, and Y.~Yan.
\newblock Knowledge adaptation for efficient semantic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 578--587, 2019.

\bibitem{heo2019comprehensive}
B.~Heo, J.~Kim, S.~Yun, H.~Park, N.~Kwak, and J.~Y. Choi.
\newblock A comprehensive overhaul of feature distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1921--1930, 2019.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{huang2022knowledge}
T.~Huang, S.~You, F.~Wang, C.~Qian, and C.~Xu.
\newblock Knowledge distillation from a stronger teacher.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors, {\em
  Advances in Neural Information Processing Systems}, 2022.

\bibitem{huang2023masked}
T.~Huang, Y.~Zhang, S.~You, F.~Wang, C.~Qian, J.~Cao, and C.~Xu.
\newblock Masked distillation with receptive tokens.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{kundu2021analyzing}
S.~Kundu, Q.~Sun, Y.~Fu, M.~Pedram, and P.~Beerel.
\newblock Analyzing the confidentiality of undistillable teachers in knowledge
  distillation.
\newblock {\em Advances in Neural Information Processing Systems},
  34:9181--9192, 2021.

\bibitem{li2017mimicking}
Q.~Li, S.~Jin, and J.~Yan.
\newblock Mimicking very efficient network for object detection.
\newblock In {\em Proceedings of the ieee conference on computer vision and
  pattern recognition}, pages 6356--6364, 2017.

\bibitem{li2022asymmetric}
X.-C. Li, W.-S. Fan, S.~Song, Y.~Li, S.~Yunfeng, D.-C. Zhan, et~al.
\newblock Asymmetric temperature scaling makes larger networks teach well
  again.
\newblock {\em Advances in Neural Information Processing Systems},
  35:3830--3842, 2022.

\bibitem{lin2017feature}
T.-Y. Lin, P.~Doll{\'a}r, R.~Girshick, K.~He, B.~Hariharan, and S.~Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017.

\bibitem{lin2017focal}
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2019structured}
Y.~Liu, K.~Chen, C.~Liu, Z.~Qin, Z.~Luo, and J.~Wang.
\newblock Structured knowledge distillation for semantic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2604--2613, 2019.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{lu2022dpmsolver}
C.~Lu, Y.~Zhou, F.~Bao, J.~Chen, C.~Li, and J.~Zhu.
\newblock {DPM}-solver: A fast {ODE} solver for diffusion probabilistic model
  sampling in around 10 steps.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors, {\em
  Advances in Neural Information Processing Systems}, 2022.

\bibitem{marcel2010torchvision}
S.~Marcel and Y.~Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In {\em Proceedings of the 18th ACM international conference on
  Multimedia}, pages 1485--1488, 2010.

\bibitem{mirzadeh2020improved}
S.~I. Mirzadeh, M.~Farajtabar, A.~Li, N.~Levine, A.~Matsukawa, and
  H.~Ghasemzadeh.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 5191--5198, 2020.

\bibitem{park2021learning}
D.~Y. Park, M.-H. Cha, D.~Kim, B.~Han, et~al.
\newblock Learning student-friendly teacher networks for knowledge
  distillation.
\newblock {\em Advances in Neural Information Processing Systems},
  34:13292--13303, 2021.

\bibitem{park2019relational}
W.~Park, D.~Kim, Y.~Lu, and M.~Cho.
\newblock Relational knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3967--3976, 2019.

\bibitem{passalis2020probabilistic}
N.~Passalis, M.~Tzelepi, and A.~Tefas.
\newblock Probabilistic knowledge transfer for lightweight deep representation
  learning.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(5):2030--2039, 2020.

\bibitem{peebles2022scalable}
W.~Peebles and S.~Xie.
\newblock Scalable diffusion models with transformers.
\newblock {\em arXiv preprint arXiv:2212.09748}, 2022.

\bibitem{ren2015faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{sandler2018mobilenetv2}
M.~Sandler, A.~Howard, M.~Zhu, A.~Zhmoginov, and L.-C. Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{shu2021channel}
C.~Shu, Y.~Liu, J.~Gao, Z.~Yan, and C.~Shen.
\newblock Channel-wise knowledge distillation for dense prediction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5311--5320, 2021.

\bibitem{son2021densely}
W.~Son, J.~Na, J.~Choi, and W.~Hwang.
\newblock Densely guided knowledge distillation using multiple teacher
  assistants.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9395--9404, 2021.

\bibitem{song2021denoising}
J.~Song, C.~Meng, and S.~Ermon.
\newblock Denoising diffusion implicit models.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{tian2019contrastive}
Y.~Tian, D.~Krishnan, and P.~Isola.
\newblock Contrastive representation distillation.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{tian2019fcos}
Z.~Tian, C.~Shen, H.~Chen, and T.~He.
\newblock Fcos: Fully convolutional one-stage object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9627--9636, 2019.

\bibitem{wang2020intra}
Y.~Wang, W.~Zhou, T.~Jiang, X.~Bai, and Y.~Xu.
\newblock Intra-class feature variation distillation for semantic segmentation.
\newblock In {\em European Conference on Computer Vision}, pages 346--362.
  Springer, 2020.

\bibitem{wightman2021resnet}
R.~Wightman, H.~Touvron, and H.~J{\'e}gou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock {\em arXiv preprint arXiv:2110.00476}, 2021.

\bibitem{xie2017aggregated}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{yang2022cross}
C.~Yang, H.~Zhou, Z.~An, X.~Jiang, Y.~Xu, and Q.~Zhang.
\newblock Cross-image relational knowledge distillation for semantic
  segmentation.
\newblock {\em arXiv preprint arXiv:2204.06986}, 2022.

\bibitem{yang2020knowledge}
J.~Yang, B.~Martinez, A.~Bulat, and G.~Tzimiropoulos.
\newblock Knowledge distillation via softmax regression representation
  learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yang2021focal}
Z.~Yang, Z.~Li, X.~Jiang, Y.~Gong, Z.~Yuan, D.~Zhao, and C.~Yuan.
\newblock Focal and global knowledge distillation for detectors.
\newblock {\em arXiv preprint arXiv:2111.11837}, 2021.

\bibitem{yang2019reppoints}
Z.~Yang, S.~Liu, H.~Hu, L.~Wang, and S.~Lin.
\newblock Reppoints: Point set representation for object detection.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9657--9666, 2019.

\bibitem{zhang2021improve}
L.~Zhang and K.~Ma.
\newblock Improve object detection with feature-based knowledge distillation:
  Towards accurate and efficient detectors.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2023avatar}
Y.~Zhang, W.~Chen, Y.~Lu, T.~Huang, X.~Sun, and J.~Cao.
\newblock Avatar knowledge distillation: Self-ensemble teacher paradigm with
  uncertainty.
\newblock {\em arXiv preprint arXiv:2305.02722}, 2023.

\bibitem{zhang2023freekd}
Y.~Zhang, T.~Huang, J.~Liu, T.~Jiang, K.~Cheng, and S.~Zhang.
\newblock Freekd: Knowledge distillation via semantic frequency prompt, 2023.

\bibitem{zhao2022decoupled}
B.~Zhao, Q.~Cui, R.~Song, Y.~Qiu, and J.~Liang.
\newblock Decoupled knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on computer vision and
  pattern recognition}, pages 11953--11962, 2022.

\bibitem{zhao2017pyramid}
H.~Zhao, J.~Shi, X.~Qi, X.~Wang, and J.~Jia.
\newblock Pyramid scene parsing network.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2881--2890, 2017.

\end{thebibliography}
