\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ali et~al.(2019)Ali, Kolter, and Tibshirani]{ali2019continuous}
Alnur Ali, J~Zico Kolter, and Ryan~J Tibshirani.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1370--1378. PMLR, 2019.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In \emph{International Conference on Machine Learning}, pages
  233--244. PMLR, 2020.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{arXiv preprint arXiv:1905.13655}, 2019.

\bibitem[Bach and Moulines(2013)]{bach2013non}
Francis Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate $o(1/n)$.
\newblock \emph{Advances in neural information processing systems},
  26:\penalty0 773--781, 2013.

\bibitem[Bahadur(1967)]{Bahadur1967RatesOC}
R.~R. Bahadur.
\newblock Rates of convergence of estimates and test statistics.
\newblock \emph{Annals of Mathematical Statistics}, 38:\penalty0 303--324,
  1967.

\bibitem[Bahadur(1971)]{doi:10.1137/1.9781611970630}
R.~R. Bahadur.
\newblock \emph{Some Limit Theorems in Statistics}.
\newblock Society for Industrial and Applied Mathematics, 1971.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Dauber et~al.(2020)Dauber, Feder, Koren, and Livni]{dauber2020can}
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni.
\newblock Can implicit bias explain generalization? stochastic convex
  optimization as a case study.
\newblock \emph{arXiv preprint arXiv:2003.06152}, 2020.

\bibitem[D{\'e}fossez and Bach(2015)]{defossez2015averaged}
Alexandre D{\'e}fossez and Francis Bach.
\newblock Averaged least-mean-squares: Bias-variance trade-offs and optimal
  sampling distributions.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 205--213,
  2015.

\bibitem[Dhillon et~al.(2013)Dhillon, Foster, Kakade, and
  Ungar]{dhillon2013risk}
Paramveer~S Dhillon, Dean~P Foster, Sham~M Kakade, and Lyle~H Ungar.
\newblock A risk comparison of ordinary least squares vs ridge regression.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1505--1511, 2013.

\bibitem[Dieuleveut et~al.(2017)Dieuleveut, Flammarion, and
  Bach]{dieuleveut2017harder}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3520--3570, 2017.

\bibitem[Dobriban et~al.(2018)Dobriban, Wager, et~al.]{dobriban2018high}
Edgar Dobriban, Stefan Wager, et~al.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247--279,
  2018.

\bibitem[Friedman et~al.(2001)Friedman, Hastie, Tibshirani,
  et~al.]{friedman2001elements}
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et~al.
\newblock \emph{The elements of statistical learning}, volume~1.
\newblock Springer series in statistics New York, 2001.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{hsu2012random}
Daniel Hsu, Sham~M Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock In \emph{Conference on learning theory}, pages 9--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Jain et~al.(2017{\natexlab{a}})Jain, Kakade, Kidambi, Netrapalli,
  Pillutla, and Sidford]{jain2017markov}
Prateek Jain, Sham~M Kakade, Rahul Kidambi, Praneeth Netrapalli,
  Venkata~Krishna Pillutla, and Aaron Sidford.
\newblock A markov chain theory approach to characterizing the minimax
  optimality of stochastic gradient descent (for least squares).
\newblock \emph{arXiv preprint arXiv:1710.09430}, 2017{\natexlab{a}}.

\bibitem[Jain et~al.(2017{\natexlab{b}})Jain, Netrapalli, Kakade, Kidambi, and
  Sidford]{jain2017parallelizing}
Prateek Jain, Praneeth Netrapalli, Sham~M Kakade, Rahul Kidambi, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8258--8299, 2017{\natexlab{b}}.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kobak et~al.(2020)Kobak, Lomond, and Sanchez]{kobak2020optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez.
\newblock The optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (169):\penalty0 1--16, 2020.

\bibitem[Liu et~al.(2019)Liu, Papailiopoulos, and Achlioptas]{liu2019bad}
Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{arXiv preprint arXiv:1906.02613}, 2019.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{arXiv preprint arXiv:2005.06398}, 2020.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 10608--10619, 2018.

\bibitem[Tihonov(1963)]{tihonov1963solution}
Andrei~Nikolajevits Tihonov.
\newblock Solution of incorrectly formulated problems and the regularization
  method.
\newblock \emph{Soviet Math.}, 4:\penalty0 1035--1038, 1963.

\bibitem[Tsigler and Bartlett(2020)]{tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Wu and Xu(2020)]{wu2020optimal}
Denny Wu and Ji~Xu.
\newblock On the optimal weighted \textbackslash ell\_2 regularization in
  overparameterized linear regression.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 10112--10123. Curran Associates, Inc., 2020.

\bibitem[Xu and Hsu(2019)]{xu2019number}
Ji~Xu and Daniel Hsu.
\newblock On the number of variables to use in principal component regression.
\newblock \emph{arXiv preprint arXiv:1906.01139}, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zou et~al.(2021)Zou, Wu, Braverman, Gu, and Kakade]{zou2021benign}
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham~M Kakade.
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock \emph{arXiv preprint arXiv:2103.12692}, 2021.

\end{thebibliography}
