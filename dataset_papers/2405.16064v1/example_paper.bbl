\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  41--48, 2009.

\bibitem[Benoit et~al.(2013)Benoit, Lehalle, Molina, Tijus, and Jouen]{benoit2013young}
Benoit, L., Lehalle, H., Molina, M., Tijus, C., and Jouen, F.
\newblock Young childrenâ€™s mapping between arrays, number words, and digits.
\newblock \emph{Cognition}, 129\penalty0 (1):\penalty0 95--101, 2013.

\bibitem[Bradley et~al.(2000)Bradley, Bennett, and Demiriz]{bradley2000constrained}
Bradley, P.~S., Bennett, K.~P., and Demiriz, A.
\newblock Constrained k-means clustering.
\newblock \emph{Microsoft Research, Redmond}, 20, 2000.

\bibitem[Bridle(1989)]{bridle1989training}
Bridle, J.
\newblock Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 2, 1989.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2023)Chen, Wu, Quan, Wang, Yan, and Zhang]{chenetal2023mcc}
Chen, H., Wu, S., Quan, X., Wang, R., Yan, M., and Zhang, J.
\newblock {MCC}-{KD}: Multi-{C}o{T} consistent knowledge distillation.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  6805--6820. Association for Computational Linguistics, December 2023.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Diao et~al.(2023)Diao, Wang, Lin, and Zhang]{diao2023active}
Diao, S., Wang, P., Lin, Y., and Zhang, T.
\newblock Active prompting with chain-of-thought for large language models.
\newblock \emph{arXiv preprint arXiv:2302.12246}, 2023.

\bibitem[Elman(1993)]{elman1993learning}
Elman, J.~L.
\newblock Learning and development in neural networks: The importance of starting small.
\newblock \emph{Cognition}, 48\penalty0 (1):\penalty0 71--99, 1993.

\bibitem[Feng et~al.(2022)Feng, Li, Yuan, and Wang]{feng2022freekd}
Feng, K., Li, C., Yuan, Y., and Wang, G.
\newblock Freekd: Free-direction knowledge distillation for graph neural networks.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  357--366, 2022.

\bibitem[Feng et~al.(2024)Feng, Li, Ren, Yuan, and Wang]{feng2024road}
Feng, K., Li, C., Ren, D., Yuan, Y., and Wang, G.
\newblock On the road to portability: Compressing end-to-end motion planner for autonomous driving.
\newblock \emph{arXiv preprint arXiv:2403.01238}, 2024.

\bibitem[Fu et~al.(2023)Fu, Peng, Ou, Sabharwal, and Khot]{fu2023specializing}
Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T.
\newblock Specializing smaller language models towards multi-step reasoning.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Ho et~al.(2023)Ho, Schmid, and Yun]{hoetal2023large}
Ho, N., Schmid, L., and Yun, S.-Y.
\newblock Large language models are reasoning teachers.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  14852--14882. Association for Computational Linguistics, July 2023.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh2023distilling}
Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  8003--8017. Association for Computational Linguistics, July 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jia et~al.(2023)Jia, Liu, Tang, and Zhu]{jiaetal2023sample}
Jia, Q., Liu, Y., Tang, H., and Zhu, K.
\newblock In-sample curriculum learning by sequence completion for natural language generation.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  11937--11950. Association for Computational Linguistics, July 2023.

\bibitem[Jiang et~al.(2014)Jiang, Meng, Yu, Lan, Shan, and Hauptmann]{jiang2014self}
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann, A.
\newblock Self-paced learning with diversity.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 27, 2014.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Kong et~al.(2021)Kong, Liu, Wang, and Tao]{kong2021adaptive}
Kong, Y., Liu, L., Wang, J., and Tao, D.
\newblock Adaptive curriculum learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  5067--5076, 2021.

\bibitem[Krueger \& Dayan(2009)Krueger and Dayan]{krueger2009flexible}
Krueger, K.~A. and Dayan, P.
\newblock Flexible shaping: How learning in small steps helps.
\newblock \emph{Cognition}, 110\penalty0 (3):\penalty0 380--394, 2009.

\bibitem[Li et~al.(2023)Li, Hessel, Yu, Ren, Chang, and Choi]{lietal2023symbolic}
Li, L.~H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and Choi, Y.
\newblock Symbolic chain-of-thought distillation: Small models can also {``}think{''} step-by-step.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  2665--2679. Association for Computational Linguistics, July 2023.

\bibitem[Li et~al.(2022)Li, Feldman, Kazemi, and Karbasi]{li2022submodular}
Li, W., Feldman, M., Kazemi, E., and Karbasi, A.
\newblock Submodular maximization in clean linear time.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 35:\penalty0 17473--17487, 2022.

\bibitem[Liang et~al.(2021)Liang, Jiang, Liu, He, Chen, Gao, and Zhao]{liang2021token}
Liang, C., Jiang, H., Liu, X., He, P., Chen, W., Gao, J., and Zhao, T.
\newblock Token-wise curriculum learning for neural machine translation.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  3658--3670, 2021.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom]{ling2017program}
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P.
\newblock Program induction by rationale generation: Learning to solve and explain algebraic word problems.
\newblock \emph{arXiv preprint arXiv:1705.04146}, 2017.

\bibitem[Magister et~al.(2023)Magister, Mallinson, Adamek, Malmi, and Severyn]{magisteretal2023teaching}
Magister, L.~C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A.
\newblock Teaching small language models to reason.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1773--1781. Association for Computational Linguistics, July 2023.

\bibitem[Miao et~al.(2021)Miao, Liang, and Su]{miao2021diverse}
Miao, S.-Y., Liang, C.-C., and Su, K.-Y.
\newblock A diverse corpus for evaluating and developing english math word problem solvers.
\newblock \emph{arXiv preprint arXiv:2106.15772}, 2021.

\bibitem[Molina \& Jouen(1998)Molina and Jouen]{molina1998modulation}
Molina, M. and Jouen, F.
\newblock Modulation of the palmar grasp behavior in neonates according to texture property.
\newblock \emph{Infant Behavior and Development}, 21\penalty0 (4):\penalty0 659--666, 1998.

\bibitem[Narayan(1997)]{narayan1997generalized}
Narayan, S.
\newblock The generalized sigmoid activation function: Competitive supervised learning.
\newblock \emph{Information Sciences}, 99\penalty0 (1-2):\penalty0 69--82, 1997.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel2021nlp}
Patel, A., Bhattamishra, S., and Goyal, N.
\newblock Are nlp models really able to solve simple math word problems?
\newblock \emph{arXiv preprint arXiv:2103.07191}, 2021.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and Manning]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.~D.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, pp.\  1532--1543, 2014.

\bibitem[Peterson(2004)]{peterson2004day}
Peterson, G.~B.
\newblock A day of great illumination: Bf skinner's discovery of shaping.
\newblock \emph{Journal of the experimental analysis of behavior}, 82\penalty0 (3):\penalty0 317--328, 2004.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor2019commonsenseqa}
Talmor, A., Herzig, J., Lourie, N., and Berant, J.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4149--4158, 2019.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Wan et~al.(2020)Wan, Yang, Wong, Zhou, Chao, Zhang, and Chen]{wanetal2020self}
Wan, Y., Yang, B., Wong, D.~F., Zhou, Y., Chao, L.~S., Zhang, H., and Chen, B.
\newblock Self-paced learning for neural machine translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  1074--1080. Association for Computational Linguistics, November 2020.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Wang, Mi, Deng, Wang, Liang, Xu, and Wong]{wang2023cue}
Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang, B., Xu, R., and Wong, K.-F.
\newblock Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  12047--12064, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wang, Li, Gao, Yin, and Ren]{wangetal2023scott}
Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., and Ren, X.
\newblock {SCOTT}: Self-consistent chain-of-thought distillation.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  5546--5558. Association for Computational Linguistics, July 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wang et~al.(2021)Wang, Wang, Liang, Cai, and Hooi]{wang2021curgraph}
Wang, Y., Wang, W., Liang, Y., Cai, Y., and Hooi, B.
\newblock Curgraph: Curriculum learning for graph classification.
\newblock In \emph{Proceedings of the Web Conference 2021}, pp.\  1238--1248, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Annual Conference on Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{yang2023large}
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.~V., Zhou, D., and Chen, X.
\newblock Large language models as optimizers.
\newblock \emph{arXiv preprint arXiv:2309.03409}, 2023.

\bibitem[Ye et~al.(2023)Ye, Chen, Xu, Zu, Shao, Liu, Cui, Zhou, Gong, Shen, et~al.]{ye2023comprehensive}
Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y., et~al.
\newblock A comprehensive capability analysis of gpt-3 and gpt-3.5 series models.
\newblock \emph{arXiv preprint arXiv:2303.10420}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{zhang2022automatic}
Zhang, Z., Zhang, A., Li, M., and Smola, A.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{arXiv preprint arXiv:2210.03493}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Li, Zhao, Karypis, and Smola]{zhang2023multimodal}
Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2302.00923}, 2023.

\end{thebibliography}
