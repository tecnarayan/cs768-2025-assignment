@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@incollection{SutskeverNIPS2014,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3104--3112},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}


@article{Volansky-translationese,
    author = {Volansky, Vered and Ordan, Noam and Wintner, Shuly},
    title = "{On the features of translationese}",
    journal = {Digital Scholarship in the Humanities},
    volume = {30},
    number = {1},
    pages = {98-118},
    year = {2013},
    month = {07},
    abstract = "{Much research in translation studies indicates that translated texts are ontologically different from original non-translated ones. Translated texts, in any language, can be considered a dialect of that language, known as ‘translationese’. Several characteristics of translationese have been proposed as universal in a series of hypotheses. In this work, we test these hypotheses using a computational methodology that is based on supervised machine learning. We define several classifiers that implement various linguistically informed features, and assess the degree to which different sets of features can distinguish between translated and original texts. We demonstrate that some feature sets are indeed good indicators of translationese, thereby corroborating some hypotheses, whereas others perform much worse (sometimes at chance level), indicating that some ‘universal’ assumptions have to be reconsidered.In memoriam: Miriam Shlesinger, 1947–2012}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqt031},
    url = {https://doi.org/10.1093/llc/fqt031},
    eprint = {https://academic.oup.com/dsh/article-pdf/30/1/98/21521905/fqt031.pdf},
}





@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{
multiagent,
title={Multi-Agent Dual Learning},
author={Yiren Wang and Yingce Xia and Tianyu He and Fei Tian and Tao Qin and ChengXiang Zhai and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyGhN2A5tm},
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}


@inproceedings{knowledge_distill_kim_rush_2016,
    title = "Sequence-Level Knowledge Distillation",
    author = "Kim, Yoon  and
      Rush, Alexander M.",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1139",
    doi = "10.18653/v1/D16-1139",
    pages = "1317--1327",
}

@article{ensemble_distill_freitag2017,
  title={Ensemble distillation for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser and Sankaran, Baskaran},
  journal={arXiv preprint arXiv:1702.01802},
  year={2017}
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{chelba2013one,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
  journal={arXiv preprint arXiv:1312.3005},
  year={2013}
}

@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512"
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@inproceedings{borsch2011,
	Address = {Canberra, Australia},
	Author = {Benjamin Borschinger and Mark Johnson},
	Booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	Month = {December},
	Pages = {10--18},
	Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
	Year = {2011}}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@InProceedings{P16-1001,
  author =	"Goodman, James
  	 and Vlachos, Andreas
	     and Naradowsky, Jason",
  title =    "Noise reduction and targeted exploration in imitation learning for      Abstract Meaning Representation parsing    ",
  booktitle = 	    "Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year =    "2016",
  publisher =	"Association for Computational Linguistics",
  pages =   "1--11",
  location =	"Berlin, Germany",
  doi =    "10.18653/v1/P16-1001",
  url =    "http://aclweb.org/anthology/P16-1001"
}

@InProceedings{C14-1001,
  author =	"Harper, Mary",
  title = 	"Learning from 26 Languages: Program Management and Science in the Babel Program",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year =    "2014",
  publisher =	"Dublin City University and Association for Computational Linguistics",
  pages =   "1",
  location =	"Dublin, Ireland",
  url =    "http://aclweb.org/anthology/C14-1001"
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@techreport{Chelba-LM,
title	= {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},
author	= {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
year	= {2013},
URL	= {http://arxiv.org/abs/1312.3005},
institution	= {Google}
}

@article{yu2018qanet,
  title={Qanet: Combining local convolution with global self-attention for reading comprehension},
  author={Yu, Adams Wei and Dohan, David and Luong, Minh-Thang and Zhao, Rui and Chen, Kai and Norouzi, Mohammad and Le, Quoc V},
  journal={arXiv preprint arXiv:1804.09541},
  year={2018}
}


@inproceedings{devlin2018bert,
    title = "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}


@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@inproceedings{Koehn:2003:SPT,
 author = {Koehn, Philipp and Och, Franz Josef and Marcu, Daniel},
 title = {Statistical Phrase-based Translation},
 booktitle = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1},
 series = {NAACL '03},
 year = {2003},
 location = {Edmonton, Canada},
 pages = {48--54},
 numpages = {7},
 url = {https://doi.org/10.3115/1073445.1073462},
 doi = {10.3115/1073445.1073462},
 acmid = {1073462},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 


@InProceedings{luong2015effective,
  author = 	"Luong, Thang
		and Pham, Hieu
		and Manning, Christopher D.",
  title = 	"Effective Approaches to Attention-based Neural Machine Translation",
  series = EMNLP,
  booktitle = 	"Proceedings of the 2015 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2015",
  publisher = 	"ACL",
  pages = 	"1412--1421",
  location = 	"Lisbon, Portugal",
}

@book{Koehn:2010:SMT,
 author = {Koehn, Philipp},
 title = {Statistical Machine Translation},
 year = {2010},
 isbn = {0521874157, 9780521874151},
 edition = {1st},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 


@inproceedings{wmt16encs_data_uedin-nmt-wmt16,
    address = "Berlin, Germany",
    author = "Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
    booktitle = "{Proc. of the First Conference on Machine Translation (WMT16)}",
    title = "{Edinburgh Neural Machine Translation Systems for WMT 16}",
    year = "2016"
} 




@inproceedings{gehring2017convolutional,
  author    = {Gehring, Jonas, and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  title     = "{Convolutional Sequence to Sequence Learning}",
  booktitle = {Proc. of ICML},
  year      = 2017,
}



@InProceedings{sennrich2015neural,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Neural Machine Translation of Rare Words with Subword Units",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1715--1725",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1162",
  url = 	"http://www.aclweb.org/anthology/P16-1162"
}


@inproceedings{towardsphrasenmthuang2017towards,
  title={Towards Neural Phrase-based Machine Translation},
  author={Huang, Po-Sen and Wang, Chong and Huang, Sitao and Zhou, Dengyong and Deng, Li},
  booktitle={ICLR},
  year={2018}
}




@article{externalphrasememtang2016neural,
  title={Neural machine translation with external phrase memory},
  author={Tang, Yaohua and Meng, Fandong and Lu, Zhengdong and Li, Hang and Yu, Philip LH},
  journal={arXiv preprint arXiv:1606.01792},
  year={2016}
}

@article{seqsegmentwang2017sequence,
  title={Sequence modeling via segmentations},
  author={Wang, Chong and Wang, Yining and Huang, Po-Sen and Mohamed, Abdelrahman and Zhou, Dengyong and Deng, Li},
  journal={arXiv preprint arXiv:1702.07463},
  year={2017}
}

@inproceedings{wang2017SWAN,
  author    = {Chong Wang and
               Yining Wang and
               Po{-}Sen Huang and
               Abdelrahman Mohamed and
               Dengyong Zhou and
               Li Deng},
  title     = {Sequence Modeling via Segmentations},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  pages     = {3674--3683},
  year      = {2017},
}



@InProceedings{translatephrasewang2017translating,
  author = 	"Wang, Xing
		and Tu, Zhaopeng
		and Xiong, Deyi
		and Zhang, Min",
  title = 	"Translating Phrases in Neural Machine Translation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1421--1431",
  location = 	"Copenhagen, Denmark",
  url = 	"http://aclweb.org/anthology/D17-1149"
}


@article{nmtphrasehybriddahlmann2017neural,
  title={Neural machine translation leveraging phrase-based models in a hybrid search},
  author={Dahlmann, Leonard and Matusov, Evgeny and Petrushkov, Pavel and Khadivi, Shahram},
  journal={arXiv preprint arXiv:1708.03271},
  year={2017}
}



@InProceedings{Dahlmann-17-Phrase,
  author = 	"Dahlmann, Leonard
		and Matusov, Evgeny
		and Petrushkov, Pavel
		and Khadivi, Shahram",
  title = 	"Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1411--1420",
  location = 	"Copenhagen, Denmark",
  
  url = 	"http://aclweb.org/anthology/D17-1148"
}


@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{tensor2tensor,
  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and
    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and
    \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and
    Noam Shazeer and Jakob Uszkoreit},
  title     = {Tensor2Tensor for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1803.07416},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07416},
}


@inproceedings{kingma2014adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
}

@InProceedings{average_attention_net,
  author = 	"Zhang, Biao
		and Xiong, Deyi
		and jinsong",
  title = 	"Accelerating Neural Transformer via an Average Attention Network",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1789--1798",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1166"
}

@incollection{quocle_seq2seq_NIPS2014_5346,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3104--3112},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}

@inproceedings{
slicenet_kaiser2018depthwise,
title={Depthwise Separable Convolutions for Neural Machine Translation},
author={Lukasz Kaiser and Aidan N. Gomez and Francois Chollet},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1jBcueAb},
}

@incollection{neural_gpu_NIPS2016_6295,
title = {Can Active Memory Replace Attention?},
author = {Kaiser, \L ukasz and Bengio, Samy},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3781--3789},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6295-can-active-memory-replace-attention.pdf}
}

@article{byte_net_kalchbrenner2016neural,
  title={Neural machine translation in linear time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1610.10099},
  year={2016}
}

@inproceedings{bleu_score_papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{lstm_hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@InProceedings{self_relative,
  author = 	"Shaw, Peter
		and Uszkoreit, Jakob
		and Vaswani, Ashish",
  title = 	"Self-Attention with Relative Position Representations",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 2 (Short Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"464--468",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2074",
  url = 	"http://aclweb.org/anthology/N18-2074"
}

@inproceedings{eval_back_translation_translationese,
    title = "On The Evaluation of Machine Translation Systems Trained With Back-Translation",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Ranzato, Marc{'}Aurelio  and
      Auli, Michael",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.253",
    doi = "10.18653/v1/2020.acl-main.253",
    pages = "2836--2846",
    abstract = "Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.",
}

@article{weighted_transformer,
  author    = {Karim Ahmed and
               Nitish Shirish Keskar and
               Richard Socher},
  title     = {Weighted Transformer Network for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02132},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02132},
  archivePrefix = {arXiv},
  eprint    = {1711.02132},
  timestamp = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-02132},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tip_train_transformer,
  title={Training Tips for the Transformer Model},
  author={Popel, Martin and Bojar, Ond{\v{r}}ej},
  journal={The Prague Bulletin of Mathematical Linguistics},
  volume={110},
  number={1},
  pages={43--70},
  year={2018},
  publisher={De Gruyter Open}
}

@inproceedings{scaling_nmt_ott2018scaling,
  title = {Scaling Neural Machine Translation},
  author = {Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  booktitle = {Proceedings of the Third Conference on Machine Translation (WMT)},
  year = 2018,
}

@inproceedings{payless_wu2018,
title={Pay Less Attention with Lightweight and Dynamic Convolutions},
author={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkVhlh09tX},
}



@inproceedings{iterative-hoang-etal-2018,
    title = "Iterative Back-Translation for Neural Machine Translation",
    author = "Hoang, Vu Cong Duy  and
      Koehn, Philipp  and
      Haffari, Gholamreza  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2703",
    doi = "10.18653/v1/W18-2703",
    pages = "18--24",
    abstract = "We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 Germanâ†”English tasks.",
}

@inproceedings{copied-currey-etal-2017,
    title = "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
    author = "Currey, Anna  and
      Miceli Barone, Antonio Valerio  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4715",
    doi = "10.18653/v1/W17-4715",
    pages = "148--156",
}

@inproceedings{effectove-artetxe-etal-2019,
    title = "An Effective Approach to Unsupervised Machine Translation",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1019",
    doi = "10.18653/v1/P19-1019",
    pages = "194--203",
    abstract = "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.",
}

@inproceedings{
bert_nmt,
title={Incorporating BERT into Neural Machine Translation},
author={Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wengang Zhou and Houqiang Li and Tieyan Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hyl7ygStwB}
}


@inproceedings{
hier_accum,
title={Tree-Structured Attention with Hierarchical Accumulation},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJxK5pEYvr},
note={under review}
}


@article{drucker1994boosting,
  title={Boosting and other ensemble methods},
  author={Drucker, Harris and Cortes, Corinna and Jackel, Lawrence D and LeCun, Yann and Vapnik, Vladimir},
  journal={Neural Computation},
  volume={6},
  number={6},
  pages={1289--1301},
  year={1994},
  publisher={MIT Press}
}

@article{hinton2015knowledge_distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{understanding_backtranslation_scale,
    title = "Understanding Back-Translation at Scale",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Auli, Michael  and
      Grangier, David",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1045",
    doi = "10.18653/v1/D18-1045",
    pages = "489--500",
    abstract = "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT{'}14 English-German test set.",
}



@inproceedings{flores,
    title = "The {FLORES} Evaluation Datasets for Low-Resource Machine Translation: {N}epali{--}{E}nglish and {S}inhala{--}{E}nglish",
    author = "Guzm{\'a}n, Francisco  and
      Chen, Peng-Jen  and
      Ott, Myle  and
      Pino, Juan  and
      Lample, Guillaume  and
      Koehn, Philipp  and
      Chaudhary, Vishrav  and
      Ranzato, Marc{'}Aurelio",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1632",
    doi = "10.18653/v1/D19-1632",
    pages = "6098--6111",
    abstract = "For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for Nepali{--}English and Sinhala{--} English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.",
}

@article{diverse_beamli2016simple,
  title={A simple, fast diverse decoding algorithm for neural generation},
  author={Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1611.08562},
  year={2016}
}

@article{biased_sampling_graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@inproceedings{backtranslate_sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}


@InProceedings{so2019evolved, 
    title = {The Evolved Transformer}, author = {So, David and Le, Quoc and Liang, Chen}, 
    pages = {5877--5886}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, 
    volume = {97}, series = {Proceedings of Machine Learning Research}, address = {Long Beach, California, USA}, 
    month = {09--15 Jun}, publisher = {PMLR}, 
    pdf = {http://proceedings.mlr.press/v97/so19a/so19a.pdf}, 
    url = {http://proceedings.mlr.press/v97/so19a.html}, 
    abstract = {Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments – the Evolved Transformer – demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT’14 English-German; at smaller sizes, it achieves the same quality as the original "big" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.
    } 
}


@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{sacredbleu_post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@inproceedings{data_aug_low_resource_fadaee-etal-2017-data,
    title = "Data Augmentation for Low-Resource Neural Machine Translation",
    author = "Fadaee, Marzieh  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-2090",
    doi = "10.18653/v1/P17-2090",
    pages = "567--573",
    abstract = "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.",
}

@inproceedings{switchout,
    title = "{S}witch{O}ut: an Efficient Data Augmentation Algorithm for Neural Machine Translation",
    author = "Wang, Xinyi  and
      Pham, Hieu  and
      Dai, Zihang  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1100",
    doi = "10.18653/v1/D18-1100",
    pages = "856--861",
    abstract = "In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",
}


@inproceedings{soft_data_aug_gao-etal-2019-soft,
    title = "Soft Contextual Data Augmentation for Neural Machine Translation",
    author = "Gao, Fei  and
      Zhu, Jinhua  and
      Wu, Lijun  and
      Xia, Yingce  and
      Qin, Tao  and
      Cheng, Xueqi  and
      Zhou, Wengang  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1555",
    doi = "10.18653/v1/P19-1555",
    pages = "5539--5544",
    abstract = "While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.",
}

@inproceedings{exploit_mono_at_scale_wu-etal-2019-exploiting,
    title = "Exploiting Monolingual Data at Scale for Neural Machine Translation",
    author = "Wu, Lijun  and
      Wang, Yiren  and
      Xia, Yingce  and
      QIN, Tao  and
      Lai, Jianhuang  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1430",
    doi = "10.18653/v1/D19-1430",
    pages = "4205--4215",
    abstract = "While target-side monolingual data has been proven to be very useful to improve neural machine translation (briefly, NMT) through back translation, source-side monolingual data is not well investigated. In this work, we study how to use both the source-side and target-side monolingual data for NMT, and propose an effective strategy leveraging both of them. First, we generate synthetic bitext by translating monolingual data from the two domains into the other domain using the models pretrained on genuine bitext. Next, a model is trained on a noised version of the concatenated synthetic bitext where each source sequence is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext and a clean version of a subset of the synthetic bitext without adding any noise. Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 English$\leftrightarrow$German translations and WMT19 German$\to$French translations, which demonstrate the effectiveness of our method. We also conduct a comprehensive study on how each part in the pipeline works.",
}

@article{data_aug_robust_li2019improving,
  title={Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation},
  author={Li, Zhenhao and Specia, Lucia},
  journal={arXiv preprint arXiv:1910.03009},
  year={2019}
}


@article{unsup_data_aug_xie2019unsupervised,
  title={Unsupervised data augmentation},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}


@inproceedings{
qanet,
title={Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution},
author={Adams Wei Yu and David Dohan and Quoc Le and Thang Luong and Rui Zhao and Kai Chen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B14TlG-RW},
}


@article{mixture_model_nmt_shen2019,
  title = {Mixture Models for Diverse Machine Translation: Tricks of the Trade},
  author = {Tianxiao Shen and Myle Ott and Michael Auli and Marc'Aurelio Ranzato},
  journal = {International Conference on Machine Learning},
  year = 2019,
}



@inproceedings{decoding_nmt,
    title = "Decoding with Large-Scale Neural Language Models Improves Translation",
    author = "Vaswani, Ashish  and
      Zhao, Yinggong  and
      Fossum, Victoria  and
      Chiang, David",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1140",
    pages = "1387--1392",
}


@techreport{when_net_disaggree_perrone1992networks,
  title={When networks disagree: Ensemble methods for hybrid neural networks},
  author={Perrone, Michael P and Cooper, Leon N},
  year={1992},
  institution={BROWN UNIV PROVIDENCE RI INST FOR BRAIN AND NEURAL SYSTEMS}
}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@article{xie2013horizontal,
  title={Horizontal and vertical ensemble with deep representation for classification},
  author={Xie, Jingjing and Xu, Bing and Chuang, Zhang},
  journal={arXiv preprint arXiv:1306.2759},
  year={2013}
}

@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get m for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}

@inproceedings{exploit_src_monolingual,
    title = "Exploiting Source-side Monolingual Data in Neural Machine Translation",
    author = "Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1160",
    doi = "10.18653/v1/D16-1160",
    pages = "1535--1545",
}


@article{jensen1906fonctions,
  title={Sur les fonctions convexes et les in{\'e}galit{\'e}s entre les valeurs moyennes},
  author={Jensen, Johan Ludwig William Valdemar and others},
  journal={Acta mathematica},
  volume={30},
  pages={175--193},
  year={1906},
  publisher={Institut Mittag-Leffler}
}




@article{kim2016sequence_knowledge_distillation,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
}







@InProceedings{bornagain_pmlr-v80-furlanello18a,
  title = 	 {Born Again Neural Networks},
  author = 	 {Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1607--1616},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/furlanello18a.html},
  abstract = 	 {Knowledge Distillation (KD) consists of transferring “knowledge” from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student’s compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.}
}
