\begin{thebibliography}{10}

\bibitem{ALLW18}
Jacob Abernethy, Kfir Levy, Kevin Lai, and Jun-Kun Wang.
\newblock Faster rates for convex-concave games.
\newblock {\em COLT}, 2018.

\bibitem{AW17}
Jacob Abernethy and Jun-Kun Wang.
\newblock Frank-wolfe and equilibrium computation.
\newblock {\em NIPS}, 2017.

\bibitem{abernethy2008optimal}
Jacob Abernethy, Manfred~K Warmuth, and Joel Yellin.
\newblock Optimal strategies from random walks.
\newblock In {\em Proceedings of The 21st Annual Conference on Learning
  Theory}, pages 437--446. Citeseer, 2008.

\bibitem{AO17}
Zeyuan Allen-Zhu and Lorenzo Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock {\em ITCS}, 2017.

\bibitem{balduzzi2018mechanics}
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls,
  and Thore Graepel.
\newblock The mechanics of n-player differentiable games.
\newblock {\em arXiv preprint arXiv:1802.05642}, 2018.

\bibitem{BT09}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM J. on Imaging Sciences}, 2009.

\bibitem{BLS15}
Sabastien Bubeck, Yin~Tat Lee, and Mohit Singh.
\newblock A geometric alternative to nesterov's accelerated gradient descent.
\newblock 2015.

\bibitem{CT93}
Gong Chen and Marc Teboulle.
\newblock Convergence analysis of a proximal-like minimization algorithm using
  bregman functions.
\newblock {\em SIAM Journal on Optimization}, 1993.

\bibitem{CJ12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, , and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock 2012.

\bibitem{daskalakis2017training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock {\em arXiv preprint arXiv:1711.00141}, 2017.

\bibitem{FB15}
Nicolas Flammarion and Francis Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock {\em COLT}, 2015.

\bibitem{gidel2018negative}
Gauthier Gidel, Reyhane~Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi
  Lepriol, Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock Negative momentum for improved game dynamics.
\newblock {\em arXiv preprint arXiv:1807.04740}, 2018.

\bibitem{KS09}
Sham Kakade and Shai Shalev-Shwartz.
\newblock Mind the duality gap: Logarithmic regret algorithms for online
  optimization.
\newblock {\em NIPS}, 2009.

\bibitem{LLM11}
Guanghui Lan, Zhaosong Lu, and Renato D.~C. Monteiro.
\newblock Primal-dual first-order methods with $o(1/\epsilon)$
  iteration-complexity for cone programming.
\newblock {\em Mathematical Programming}, 2011.

\bibitem{LZ17}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock {\em Mathematical Programming}, 2017.

\bibitem{LRP16}
Laurent Lessard, Benjamin Recht, and Andrew Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock {\em SIAM Journal on Optimization}, 2016.

\bibitem{NIPS2013_5148}
Brendan McMahan and Jacob Abernethy.
\newblock Minimax optimal algorithms for unconstrained linear optimization.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  26}, pages 2724--2732. Curran Associates, Inc., 2013.

\bibitem{N83b}
Yuri Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $o(1/k^2)$.
\newblock {\em Doklady AN USSR}, 1983.

\bibitem{N83a}
Yuri Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock {\em Soviet Mathematics Doklady}, 27:372--376, 1983.

\bibitem{N88}
Yuri Nesterov.
\newblock On an approach to the construction of optimal methods of minimization
  of smooth convex functions.
\newblock {\em Ekonom. i. Mat. Metody}, 24:509--517, 1988.

\bibitem{N04}
Yuri Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock {\em Springer}, 2004.

\bibitem{N05}
Yuri Nesterov.
\newblock Smooth minimization of nonsmooth functions.
\newblock {\em Mathematical programming}, 2005.

\bibitem{PB14}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends in Optimization}, 2014.

\bibitem{RK13}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online learning with predictable sequences.
\newblock {\em COLT}, 2013.

\bibitem{RS13}
Alexander Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock {\em NIPS}, 2013.

\bibitem{R96}
Tyrrell Rockafellar.
\newblock Convex analysis.
\newblock {\em Princeton University Press}, 1996.

\bibitem{SBC14}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling nesterov's accelerated gradient
  method: Theory and insights.
\newblock {\em NIPS}, 2014.

\bibitem{SALS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock {\em NIPS}, 2015.

\bibitem{T08}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock 2008.

\bibitem{wibisono2016variational}
Andre Wibisono, Ashia~C Wilson, and Michael~I Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(47):E7351--E7358, 2016.

\bibitem{Z03}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock {\em ICML}, 2003.

\end{thebibliography}
