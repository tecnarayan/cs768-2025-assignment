\begin{thebibliography}{10}

\bibitem{ansell2021composable}
Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli{\'c}.
\newblock Composable sparse fine-tuning for cross-lingual transfer.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1778--1796, Dublin, Ireland, 2022. Association for Computational Linguistics.

\bibitem{bai2023sequential}
Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei~A Efros.
\newblock Sequential modeling enables scalable learning for large vision models.
\newblock {\em ArXiv preprint}, abs/2312.00785, 2023.

\bibitem{bapna2019simple}
Ankur Bapna and Orhan Firat.
\newblock Simple, scalable adaptation for neural machine translation.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 1538--1548, Hong Kong, China, 2019. Association for Computational Linguistics.

\bibitem{bellec2017deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert~A. Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In {\em 6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.

\bibitem{zaken2021bitfit}
Elad Ben~Zaken, Yoav Goldberg, and Shauli Ravfogel.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 1--9, Dublin, Ireland, 2022. Association for Computational Linguistics.

\bibitem{berlinet2011reproducing}
Alain Berlinet and Christine Thomas-Agnan.
\newblock {\em Reproducing kernel Hilbert spaces in probability and statistics}.
\newblock Springer Science \& Business Media, 2011.

\bibitem{caelles2017one}
Sergi Caelles, Kevis{-}Kokitsi Maninis, Jordi Pont{-}Tuset, Laura Leal{-}Taix{\'{e}}, Daniel Cremers, and Luc~Van Gool.
\newblock One-shot video object segmentation.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages 5320--5329. {IEEE} Computer Society, 2017.

\bibitem{chen2022adaptformer}
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
\newblock Adaptformer: Adapting vision transformers for scalable visual recognition.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16664--16678, 2022.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey~E. Hinton.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In {\em Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of {\em Proceedings of Machine Learning Research}, pages 1597--1607. {PMLR}, 2020.

\bibitem{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In {\em 2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2021, Montreal, QC, Canada, October 10-17, 2021}, pages 9620--9629. {IEEE}, 2021.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V. Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pages 3965--3977, 2021.

\bibitem{dataset2011novel}
E~Dataset.
\newblock Novel datasets for fine-grained image categorization.
\newblock In {\em First Workshop on Fine Grained Visual Categorization, CVPR. Citeseer. Citeseer}. Citeseer, 2011.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

\bibitem{ding2021openprompt}
Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun.
\newblock {O}pen{P}rompt: An open-source framework for prompt-learning.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, pages 105--113, Dublin, Ireland, 2022. Association for Computational Linguistics.

\bibitem{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno~Jialin Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain surgeon.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages 4857--4867, 2017.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em 9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In {\em 7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{fu2023effectiveness}
Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier.
\newblock On the effectiveness of parameter-efficient fine-tuning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 12799--12807, 2023.

\bibitem{gebru2017fine}
Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li~Fei{-}Fei.
\newblock Fine-grained car detection for visual census estimation.
\newblock In Satinder~P. Singh and Shaul Markovitch, editors, {\em Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA}}, pages 4502--4508. {AAAI} Press, 2017.

\bibitem{guo2020parameter}
Demi Guo, Alexander Rush, and Yoon Kim.
\newblock Parameter-efficient transfer learning with diff pruning.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4884--4896, Online, 2021. Association for Computational Linguistics.

\bibitem{han2016eie}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark~A Horowitz, and William~J Dally.
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock {\em ACM SIGARCH Computer Architecture News}, 44(3):243--254, 2016.

\bibitem{he2023sensitivity}
Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang.
\newblock Sensitivity-aware visual parameter-efficient fine-tuning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11825--11835, 2023.

\bibitem{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'{a}}r, and Ross~B. Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages 15979--15988. {IEEE}, 2022.

\bibitem{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross~B. Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020}, pages 9726--9735. {IEEE}, 2020.

\bibitem{van2015building}
Grant~Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge~J. Belongie.
\newblock Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2015, Boston, MA, USA, June 7-12, 2015}, pages 595--604. {IEEE} Computer Society, 2015.

\bibitem{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of {\em Proceedings of Machine Learning Research}, pages 2790--2799. {PMLR}, 2019.

\bibitem{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.

\bibitem{hu2016network}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards efficient deep architectures.
\newblock {\em ArXiv preprint}, abs/1607.03250, 2016.

\bibitem{NEURIPS2022_e7599c4b}
Piotr Indyk and Sandeep Silwal.
\newblock Faster linear algebra for distance matrices.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems}, volume~35, pages 35576--35589. Curran Associates, Inc., 2022.

\bibitem{jia2022visual}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In {\em European Conference on Computer Vision}, pages 709--727. Springer, 2022.

\bibitem{ju2022prompting}
Chen Ju, Tengda Han, Kunhao Zheng, Ya~Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In {\em European Conference on Computer Vision}, pages 105--124. Springer, 2022.

\bibitem{koutroumbas2008pattern}
Konstantinos Koutroumbas and Sergios Theodoridis.
\newblock {\em Pattern recognition}.
\newblock Academic Press, 2008.

\bibitem{li2022cross}
Wei{-}Hong Li, Xialei Liu, and Hakan Bilen.
\newblock Cross-domain few-shot learning with task-specific adapters.
\newblock In {\em {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages 7151--7160. {IEEE}, 2022.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, Online, 2021. Association for Computational Linguistics.

\bibitem{lian2022scaling}
Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
\newblock Scaling \& shifting your features: A new baseline for efficient model tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:109--123, 2022.

\bibitem{liu2024dora}
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang~Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.
\newblock Dora: Weight-decomposed low-rank adaptation.
\newblock {\em ArXiv preprint}, abs/2402.09353, 2024.

\bibitem{liu2021p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\newblock {\em ArXiv preprint}, abs/2110.07602, 2021.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em 2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2021, Montreal, QC, Canada, October 10-17, 2021}, pages 9992--10002. {IEEE}, 2021.

\bibitem{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao{-}Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In {\em {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages 11966--11976. {IEEE}, 2022.

\bibitem{loshchilov2018fixing}
Ilya Loshchilov and Frank Hutter.
\newblock Fixing weight decay regularization in adam.
\newblock 2018.

\bibitem{louizos2017learning}
Christos Louizos, Max Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through l{\_}0 regularization.
\newblock In {\em 6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.

\bibitem{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry~P. Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the 34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, volume~70 of {\em Proceedings of Machine Learning Research}, pages 2498--2507. {PMLR}, 2017.

\bibitem{molchanov2019importance}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pages 11264--11272. Computer Vision Foundation / {IEEE}, 2019.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian conference on computer vision, graphics \& image processing}, pages 722--729. IEEE, 2008.

\bibitem{pei2023dynamics}
Zhengqi Pei and Shuhui Wang.
\newblock Dynamics-inspired neuromorphic visual representation learning.
\newblock In {\em International Conference on Machine Learning}, pages 27521--27541. PMLR, 2023.

\bibitem{pfeiffer2020adapterfusion}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"u}ckl{\'e}, Kyunghyun Cho, and Iryna Gurevych.
\newblock {A}dapter{F}usion: Non-destructive task composition for transfer learning.
\newblock In {\em Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 487--503, Online, 2021. Association for Computational Linguistics.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115:211--252, 2015.

\bibitem{srinivas2015data}
Suraj Srinivas and R.~Venkatesh Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In Xianghua Xie, Mark~W. Jones, and Gary K.~L. Tam, editors, {\em Proceedings of the British Machine Vision Conference 2015, {BMVC} 2015, Swansea, UK, September 7-10, 2015}, pages 31.1--31.12. {BMVA} Press, 2015.

\bibitem{sun2023stem}
Xue-Lian Sun, Zhen-Hua Chen, Xize Guo, Jingjing Wang, Mengmeng Ge, Samuel Zheng~Hao Wong, Ting Wang, Si~Li, Mingze Yao, Laura~A Johnston, et~al.
\newblock Stem cell competition driven by the axin2-p53 axis controls brain size during murine development.
\newblock {\em Developmental Cell}, 58(9):744--759, 2023.

\bibitem{sung2022vl}
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.
\newblock Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5227--5237, 2022.

\bibitem{suykens1999chaos}
Johan~AK Suykens and Joos Vandewalle.
\newblock Chaos control using least-squares support vector machines.
\newblock {\em International journal of circuit theory and applications}, 27(6):605--615, 1999.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em ArXiv preprint}, abs/2307.09288, 2023.

\bibitem{tu2023visual}
Cheng-Hao Tu, Zheda Mai, and Wei-Lun Chao.
\newblock Visual query tuning: Towards effective usage of intermediate representations for parameter and memory efficient transfer learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7725--7735, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, {\em Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages 5998--6008, 2017.

\bibitem{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem{xu2021raise}
Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang.
\newblock Raise a child in large language model: Towards effective and generalizable fine-tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 9514--9528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.

\bibitem{yang2017designing}
Tien{-}Ju Yang, Yu{-}Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using energy-aware pruning.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages 6071--6079. {IEEE} Computer Society, 2017.

\bibitem{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12104--12113, 2022.

\bibitem{zhai2019large}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task adaptation benchmark.
\newblock {\em ArXiv preprint}, abs/1910.04867, 2019.

\bibitem{zhang2023mosa}
Qizhe Zhang, Bocheng Zou, Ruichuan An, Jiaming Liu, and Shanghang Zhang.
\newblock Mosa: Mixture of sparse adapters for visual efficient tuning.
\newblock {\em ArXiv preprint}, abs/2312.02923, 2023.

\bibitem{zhang2021tip}
Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu~Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free clip-adapter for better vision-language modeling.
\newblock {\em ArXiv preprint}, abs/2111.03930, 2021.

\bibitem{zhang2022neural}
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.
\newblock Neural prompt search.
\newblock {\em ArXiv preprint}, abs/2206.04673, 2022.

\bibitem{DBLP:conf/cvpr/ZhangZGZSZZ24}
Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, and Shanghang Zhang.
\newblock Gradient-based parameter selection for efficient fine-tuning.
\newblock In {\em {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2024, Seattle, WA, USA, June 16-22, 2024}, pages 28566--28577. {IEEE}, 2024.

\bibitem{zhao2020masking}
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch{\"u}tze.
\newblock Masking as an efficient alternative to finetuning for pretrained language models.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 2226--2241, Online, 2020. Association for Computational Linguistics.

\end{thebibliography}
