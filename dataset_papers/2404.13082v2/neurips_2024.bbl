\begin{thebibliography}{10}

\bibitem{cai2023humanintheloop}
Zefan Cai, Baobao Chang, and Wenjuan Han.
\newblock Human-in-the-loop through chain-of-thought, 2023.

\bibitem{chakrabarty2008online}
Deeparnab Chakrabarty, Yunhong Zhou, and Rajan Lukose.
\newblock Online knapsack problems.
\newblock In {\em Workshop on internet and network economics (WINE)}, 2008.

\bibitem{chen2023frugalgpt}
Lingjiao Chen, Matei Zaharia, and James Zou.
\newblock Frugalgpt: How to use large language models while reducing cost and
  improving performance.
\newblock {\em arXiv preprint arXiv:2305.05176}, 2023.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{openai_embedding_model}
Ryan Greene, Ted Sanders, Lilian Weng, and Aarving Neelakantan.
\newblock New and improved embedding model, 2022.
\newblock Accessed: 2023-10-27.

\bibitem{lin2022teaching}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Teaching models to express their uncertainty in words, 2022.

\bibitem{madaan2023automix}
Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya~Pranavi Potharaju, Swaroop
  Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu,
  Yiming Yang, Shyam Upadhyay, Mausam, and Manaal Faruqui.
\newblock Automix: Automatically mixing language models, 2023.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{naik2023diversity}
Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and
  Besmira Nushi.
\newblock Diversity of thought improves reasoning abilities of large language
  models.
\newblock {\em arXiv preprint arXiv:2310.07088}, 2023.

\bibitem{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{saha2018complex}
Amrita Saha, Vardaan Pahuja, Mitesh Khapra, Karthik Sankaranarayanan, and
  Sarath Chandar.
\newblock Complex sequential question answering: Towards learning to converse
  over linked question answer pairs with a knowledge graph.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{si2023prompting}
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan
  Boyd-Graber, and Lijuan Wang.
\newblock Prompting gpt-3 to be reliable, 2023.

\bibitem{togetherpricing}
together.ai.
\newblock together pricing.
\newblock \url{https://www.together.ai/pricing}.

\bibitem{touvron2023Llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock {\em arXiv preprint arXiv:2203.11171}, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24824--24837, 2022.

\bibitem{xiong2023llms}
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan
  Hooi.
\newblock Can llms express their uncertainty? an empirical evaluation of
  confidence elicitation in llms, 2023.

\bibitem{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang,
  James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large
  language models.
\newblock {\em arXiv preprint arXiv:2309.12284}, 2023.

\bibitem{yue2023large}
Murong Yue, Jie Zhao, Min Zhang, Du~Liang, and Ziyu Yao.
\newblock Large language model cascades with mix-ture of thought
  representations for cost-efficient reasoning.
\newblock {\em arXiv preprint arXiv:2310.03094}, 2023.

\end{thebibliography}
