\begin{thebibliography}{10}

\bibitem{trabucco2022design}
Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine.
\newblock Design-bench: Benchmarks for data-driven offline model-based
  optimization.
\newblock {\em arXiv preprint arXiv:2202.08450}, 2022.

\bibitem{hamidieh2018data}
Kam Hamidieh.
\newblock A data-driven statistical model for predicting the critical
  temperature of a superconductor.
\newblock {\em Computational Materials Science}, 2018.

\bibitem{sarkisyan2016local}
Karen~S Sarkisyan et~al.
\newblock Local fitness landscape of the green fluorescent protein.
\newblock {\em Nature}, 2016.

\bibitem{angermueller2019model}
Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin
  Murphy, and Lucy Colwell.
\newblock Model-based reinforcement learning for biological sequence design.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2019.

\bibitem{barrera2016survey}
Luis~A Barrera et~al.
\newblock Survey of variation in human transcription factors reveals prevalent
  dna binding changes.
\newblock {\em Science}, 2016.

\bibitem{sample2019human}
Paul~J Sample, Ban Wang, David~W Reid, Vlad Presnyak, Iain~J McFadyen, David~R
  Morris, and Georg Seelig.
\newblock Human 5 {UTR} design and variant effect prediction from a massively
  parallel translation assay.
\newblock {\em Nature Biotechnology}, 2019.

\bibitem{yu2021roma}
Sihyun Yu, Sungsoo Ahn, Le~Song, and Jinwoo Shin.
\newblock Roma: Robust model adaptation for offline model-based optimization.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{trabucco2021conservative}
Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine.
\newblock Conservative objective models for effective offline model-based
  optimization.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2021.

\bibitem{fu2021offline}
Justin Fu and Sergey Levine.
\newblock Offline model-based optimization via normalized maximum likelihood
  estimation.
\newblock {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2021.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as {G}aussian processes.
\newblock {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2017.

\bibitem{yang2021tensor}
Greg Yang and Etai Littwin.
\newblock Tensor programs iib: Architectural universality of neural tangent
  kernel training dynamics.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2021.

\bibitem{lee2020finite}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2020.

\bibitem{nguyen2020dataset}
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2020.

\bibitem{yuan2021neural}
Chia-Hung Yuan and Shan-Hung Wu.
\newblock Neural tangent generalization attacks.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2021.

\bibitem{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimiliano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2018.

\bibitem{kumar2020model}
Aviral Kumar and Sergey Levine.
\newblock Model inversion networks for model-based optimization.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2015.

\bibitem{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{ahn2020robel}
Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta,
  Sergey Levine, and Vikash Kumar.
\newblock Robel: Robotics benchmarks for learning with low-cost robots.
\newblock In {\em Conf. on Robot Lea. (CoRL)}, 2020.

\bibitem{brookes2019conditioning}
David Brookes, Hahnbeom Park, and Jennifer Listgarten.
\newblock Conditioning by adaptive sampling for robust design.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2019.

\bibitem{fannjiang2020autofocused}
Clara Fannjiang and Jennifer Listgarten.
\newblock Autofocused oracles for model-based design.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2020.

\bibitem{wilson2017reparameterization}
James~T Wilson, Riccardo Moriconi, Frank Hutter, and Marc~Peter Deisenroth.
\newblock The reparameterization trick for acquisition functions.
\newblock {\em arXiv preprint arXiv:1712.00424}, 2017.

\bibitem{hansen2006cma}
Nikolaus Hansen.
\newblock The {CMA} evolution strategy: A comparing review.
\newblock In Jose~A. Lozano, Pedro Larra{\~{n}}aga, I{\~{n}}aki Inza, and
  Endika Bengoetxea, editors, {\em Towards a New Evolutionary Computation:
  Advances in the Estimation of Distribution Algorithms}, pages 75--102.
  Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 1992.

\bibitem{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2020.

\bibitem{bradbury2020jax}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock Jax: composable transformations of python+ numpy programs.
\newblock {\em URL http://github.com/google/jax}, 2020.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{Arora2020Harnessing}
Sanjeev Arora, Simon~S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2020.

\bibitem{zhang2019you}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Accelerating adversarial training via
  maximal principle.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2019.

\bibitem{zhou2022dataset}
Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.
\newblock Dataset distillation using neural feature regression.
\newblock {\em arXiv preprint arXiv:2206.00719}, 2022.

\bibitem{cui2022dc}
Justin Cui, Ruochen Wang, Si~Si, and Cho-Jui Hsieh.
\newblock {DC-BENCH}: Dataset condensation benchmark.
\newblock {\em arXiv preprint arXiv:2207.09639}, 2022.

\bibitem{sachdeva2022infinite}
Noveen Sachdeva, Mehak~Preet Dhaliwal, Carole-Jean Wu, and Julian McAuley.
\newblock Infinite recommendation networks: A data-centric approach.
\newblock {\em arXiv preprint arXiv:2206.02626}, 2022.

\bibitem{liu2022datasetdistill}
Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang.
\newblock Dataset distillation via factorization.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2022.

\bibitem{bohdal2020flexible}
Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock {\em arXiv preprint arXiv:2006.08572}, 2020.

\bibitem{sucholutsky2021soft}
Ilia Sucholutsky and Matthias Schonlau.
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock In {\em Proc. Int. Joint Conf. on Neur. Net. (IJCNN)}, 2021.

\bibitem{nguyen2021dataset}
Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{chen2022gradient}
Can Chen, Xi~Chen, Chen Ma, Zixuan Liu, and Xue Liu.
\newblock Gradient-based bi-level optimization for deep learning: A survey.
\newblock {\em arXiv preprint arXiv:2207.11719}, 2022.

\bibitem{giovannelli2023inexactBilevel}
Tommaso {Giovannelli}, Griffin {Kent}, and Luis {Nunes Vicente}.
\newblock Inexact bilevel stochastic gradient methods for constrained and
  unconstrained lower-level problems.
\newblock {\em arXiv preprint arXiv:2110.00604}, 2022.

\bibitem{giovannelli2023bilevelMultiobjective}
Tommaso {Giovannelli}, Griffin {Kent}, and Luis {Nunes Vicente}.
\newblock Bilevel optimization with a multi-objective lower-level problem:
  Risk-neutral and risk-averse formulations.
\newblock {\em arXiv preprint arXiv:2302.05540}, 2023.

\bibitem{hu2019learning}
Zhiting Hu, Bowen Tan, Russ~R Salakhutdinov, Tom~M Mitchell, and Eric~P Xing.
\newblock Learning data manipulation for augmentation and weighting.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2018.

\bibitem{chen2022unbiased}
Can Chen, Chen Ma, Xi~Chen, Sirui Song, Hao Liu, and Xue Liu.
\newblock Unbiased implicit feedback via bi-level optimization.
\newblock {\em arXiv preprint arXiv:2206.00147}, 2022.

\bibitem{chen2021generalized}
Can Chen, Shuhao Zheng, Xi~Chen, Erqun Dong, Xue~Steve Liu, Hao Liu, and Dejing
  Dou.
\newblock Generalized data weighting via class-level gradient manipulation.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{chen2022structure}
Can Chen, Jingbo Zhou, Fan Wang, Xue Liu, and Dejing Dou.
\newblock Structure-aware protein self-supervised learning.
\newblock {\em arXiv preprint arXiv:2204.04213}, 2022.

\bibitem{chi2022metafscil}
Zhixiang Chi, Li~Gu, Huan Liu, Yang Wang, Yuanhao Yu, and Jin Tang.
\newblock Metafscil: A meta-learning approach for few-shot class incremental
  learning.
\newblock In {\em CVPR}, 2022.

\bibitem{huang2022artificial}
Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec,
  Connor~W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik.
\newblock Artificial intelligence foundation for therapeutic science.
\newblock {\em Nature Chemical Biology}, 2022.

\bibitem{wan2022machine}
Changlin Wan.
\newblock {\em Machine Learning Approaches to Reveal Discrete Signals in Gene
  Expression}.
\newblock PhD thesis, Purdue University Graduate School, 2022.

\bibitem{luo2020expectation}
Xiao Luo, Xinming Tu, Yang Ding, Ge~Gao, and Minghua Deng.
\newblock Expectation pooling: an effective and interpretable pooling method
  for predicting dna--protein binding.
\newblock {\em Bioinformatics}, 2020.

\bibitem{ji2021dnabert}
Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana~V Davuluri.
\newblock Dnabert: pre-trained bidirectional encoder representations from
  transformers model for dna-language in genome.
\newblock {\em Bioinformatics}, 2021.

\bibitem{elnaggar2020prottrans}
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu~Wang,
  Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,
  et~al.
\newblock {ProtTrans}: towards cracking the language of life's code through
  self-supervised deep learning and high performance computing.
\newblock {\em arXiv preprint arXiv:2007.06225}, 2020.

\bibitem{chen2023bidirectional}
Can Chen, Yingxue Zhang, Xue Liu, and Mark Coates.
\newblock Bidirectional learning for offline model-based biological sequence
  design.
\newblock {\em arXiv preprint arXiv:2301.02931}, 2023.

\bibitem{gonzalez2019pervasive}
Courtney~E Gonzalez and Marc Ostermeier.
\newblock Pervasive pairwise intragenic epistasis among sequential mutations in
  tem-1 $\beta$-lactamase.
\newblock {\em Journal of molecular biology}, 2019.

\bibitem{firnberg2014comprehensive}
Elad Firnberg, Jason~W Labonte, Jeffrey~J Gray, and Marc Ostermeier.
\newblock A comprehensive, high-resolution map of a geneâ€™s fitness landscape.
\newblock {\em Molecular biology and evolution}, 2014.

\bibitem{zhang2021unifying}
Dinghuai Zhang, Jie Fu, Yoshua Bengio, and Aaron Courville.
\newblock Unifying likelihood-free inference with black-box optimization and
  beyond.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2021.

\bibitem{wrenbeck2017single}
Emily~E Wrenbeck, Laura~R Azouz, and Timothy~A Whitehead.
\newblock Single-mutation fitness landscapes for an enzyme on multiple
  substrates reveal specificity is globally encoded.
\newblock {\em Nature Communications}, 2017.

\bibitem{klesmith2015comprehensive}
Justin~R Klesmith, John-Paul Bacik, Ryszard Michalczyk, and Timothy~A
  Whitehead.
\newblock Comprehensive sequence-flux mapping of a levoglucosan utilization
  pathway in e. coli.
\newblock {\em ACS Synthetic Biology}, 2015.

\bibitem{weile2017framework}
Jochen Weile, Song Sun, Atina~G Cote, Jennifer Knapp, Marta Verby, Joseph~C
  Mellor, Yingzhou Wu, Carles Pons, Cassandra Wong, Natascha van Lieshout,
  et~al.
\newblock A framework for exhaustively mapping functional missense variants.
\newblock {\em Molecular Systems Biology}, 2017.

\bibitem{zhang2022protein}
Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano,
  Payel Das, and Jian Tang.
\newblock Protein representation learning by geometric structure pretraining.
\newblock {\em arXiv preprint arXiv:2203.06125}, 2022.

\end{thebibliography}
