\begin{thebibliography}{10}

\bibitem{Argenson2021ModelBasedOP}
A.~Argenson and G.~Dulac-Arnold.
\newblock Model-{B}ased {O}ffline {P}lanning.
\newblock {\em ArXiv}, abs/2008.05556, 2021.

\bibitem{Asadi2018TowardsAS}
K.~Asadi, E.~Cater, D.~K. Misra, and M.~L. Littman.
\newblock Towards a {S}imple {A}pproach to {M}ulti-step {M}odel-based
  {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/1811.00128, 2018.

\bibitem{Brockman2016OpenAIG}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Open{AI} {G}ym.
\newblock {\em ArXiv}, abs/1606.01540, 2016.

\bibitem{Buckman2018SampleEfficientRL}
J.~Buckman, D.~Hafner, G.~Tucker, E.~Brevdo, and H.~Lee.
\newblock Sample-{E}fficient {R}einforcement {L}earning with {S}tochastic
  {E}nsemble {V}alue {E}xpansion.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem{Camacho2013model}
E.~F. Camacho and C.~B. Alba.
\newblock {\em Model {P}redictive {C}ontrol}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{Chelba2014OneBW}
C.~Chelba, T.~Mikolov, M.~Schuster, Q.~Ge, T.~Brants, P.~T. Koehn, and
  T.~Robinson.
\newblock One {B}illion {W}ord {B}enchmark for {M}easuring {P}rogress in
  {S}tatistical {L}anguage {M}odeling.
\newblock {\em ArXiv}, abs/1312.3005, 2014.

\bibitem{Chen2021DecisionTR}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch.
\newblock Decision {T}ransformer: {R}einforcement {L}earning via {S}equence
  {M}odeling.
\newblock {\em ArXiv}, abs/2106.01345, 2021.

\bibitem{Chua2018DeepRL}
K.~Chua, R.~Calandra, R.~McAllister, and S.~Levine.
\newblock Deep {R}einforcement {L}earning in a {H}andful of {T}rials using
  {P}robabilistic {D}ynamics {M}odels.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem{Deng2009ImageNetAL}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Image{N}et: {A} {L}arge-{S}cale {H}ierarchical {I}mage {D}atabase.
\newblock In {\em {IEEE} {C}onference on {C}omputer {V}ision and {P}attern
  {R}ecognition (CVPR)}, 2009.

\bibitem{Deng2021SCORESC}
Z.~Deng, Z.~Fu, L.~Wang, Z.~Yang, C.~Bai, Z.~Wang, and J.~Jiang.
\newblock S{CORE}: {S}purious {CO}rrelation {RE}duction for {O}ffline
  {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/2110.12468, 2021.

\bibitem{Diehl2021UMBRELLAUM}
C.~P. Diehl, T.~Sievernich, M.~Kr{\"u}ger, F.~Hoffmann, and T.~Bertram.
\newblock U{MBRELLA}: {U}ncertainty-{A}ware {M}odel-{B}ased {O}ffline
  {R}einforcement {L}earning {L}everaging {P}lanning.
\newblock {\em ArXiv}, abs/2111.11097, 2021.

\bibitem{Edwards2018ForwardBackwardRL}
A.~D. Edwards, L.~Downs, and J.~C. Davidson.
\newblock Forward-{B}ackward {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/1803.10227, 2018.

\bibitem{Finn2017DeepVF}
C.~Finn and S.~Levine.
\newblock Deep {V}isual {F}oresight for {P}lanning {R}obot {M}otion.
\newblock In {\em International Conference on Robotics and Automation (ICRA)},
  2017.

\bibitem{Fu2020D4RLDF}
J.~Fu, A.~Kumar, O.~Nachum, G.~Tucker, and S.~Levine.
\newblock D4{RL}: {D}atasets for {D}eep {D}ata-{D}riven {R}einforcement
  {L}earning.
\newblock {\em ArXiv}, abs/2004.07219, 2020.

\bibitem{Fujimoto2021AMA}
S.~Fujimoto and S.~S. Gu.
\newblock A {M}inimalist {A}pproach to {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Fujimoto2019OffPolicyDR}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning without {E}xploration.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{Fujimoto2018AddressingFA}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing {F}unction {A}pproximation {E}rror in {A}ctor-{C}ritic
  {M}ethods.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{Gelada2019OffPolicyDR}
C.~Gelada and M.~G. Bellemare.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning by {B}ootstrapping the
  {C}ovariate {S}hift.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~33, 2019.

\bibitem{Gottesman2018EvaluatingRL}
O.~Gottesman, F.~D. Johansson, J.~Meier, J.~Dent, D.~Lee, S.~Srinivasan,
  L.~Zhang, Y.~Ding, D.~Wihl, X.~Peng, J.~Yao, I.~Lage, C.~Mosch, L.~wei
  H.~Lehman, M.~Komorowski, A.~A. Faisal, L.~A. Celi, D.~A. Sontag, and
  F.~Doshi-Velez.
\newblock Evaluating {R}einforcement {L}earning {A}lgorithms in {O}bservational
  {H}ealth {S}ettings.
\newblock {\em ArXiv}, abs/1805.12298, 2018.

\bibitem{goyal2018recall}
A.~Goyal, P.~Brakel, W.~Fedus, S.~Singhal, T.~Lillicrap, S.~Levine,
  H.~Larochelle, and Y.~Bengio.
\newblock Recall {T}races: {B}acktracking {M}odels for {E}fficient
  {R}einforcement {L}earning.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{Haarnoja2018SoftAO}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft {A}ctor-{C}ritic: {O}ff-{P}olicy {M}aximum {E}ntropy {D}eep
  {R}einforcement {L}earning with a {S}tochastic {A}ctor.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{Holyoak1999BidirectionalRI}
K.~J. Holyoak and D.~Simon.
\newblock Bidirectional {R}easoning in {D}ecision {M}aking by {C}onstraint
  {S}atisfaction.
\newblock {\em Journal of Experimental Psychology: General}, 128:3--31, 1999.

\bibitem{Janner2019WhenTT}
M.~Janner, J.~Fu, M.~Zhang, and S.~Levine.
\newblock When to {T}rust {Y}our {M}odel: {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Janner2021ReinforcementLA}
M.~Janner, Q.~Li, and S.~Levine.
\newblock Offline {R}einforcement {L}earning as {O}ne {B}ig {S}equence
  {M}odeling {P}roblem.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Kaelbling1996ReinforcementLA}
L.~P. Kaelbling, M.~L. Littman, and A.~W. Moore.
\newblock Reinforcement {L}earning: {A} {S}urvey.
\newblock {\em Journal of Artificial Intelligence Research}, 4:237--285, 1996.

\bibitem{Kidambi2020MOReLM}
R.~Kidambi, A.~Rajeswaran, P.~Netrapalli, and T.~Joachims.
\newblock {MOReL}: {M}odel-{B}ased {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{Kingma2014AutoEncodingVB}
D.~P. Kingma and M.~Welling.
\newblock Auto-{E}ncoding {V}ariational {B}ayes.
\newblock {\em ArXiv}, abs/1312.6114, 2013.

\bibitem{kostrikov2022offline}
I.~Kostrikov, A.~Nair, and S.~Levine.
\newblock {Offline Reinforcement Learning with Implicit Q-Learning}.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Kostrikov2021OfflineRL}
I.~Kostrikov, J.~Tompson, R.~Fergus, and O.~Nachum.
\newblock Offline {R}einforcement {L}earning with {F}isher {D}ivergence
  {C}ritic {R}egularization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{Kumar2021DR3VD}
A.~Kumar, R.~Agarwal, T.~Ma, A.~C. Courville, G.~Tucker, and S.~Levine.
\newblock {DR}3: {V}alue-{B}ased {D}eep {R}einforcement {L}earning {R}equires
  {E}xplicit {R}egularization.
\newblock {\em ArXiv}, abs/2112.04716, 2021.

\bibitem{Kumar2019StabilizingOQ}
A.~Kumar, J.~Fu, G.~Tucker, and S.~Levine.
\newblock Stabilizing {O}ff-{P}olicy {Q}-{L}earning via {B}ootstrapping {E}rror
  {R}eduction.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Kumar2020ConservativeQF}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine.
\newblock Conservative {Q}-{L}earning for {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{Lai2020BidirectionalMP}
H.~Lai, J.~Shen, W.~Zhang, and Y.~Yu.
\newblock Bidirectional {M}odel-based {P}olicy {O}ptimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem{Lange2012BatchRL}
S.~Lange, T.~Gabel, and M.~A. Riedmiller.
\newblock Batch {R}einforcement {L}earning.
\newblock In {\em Reinforcement Learning}, 2012.

\bibitem{Laroche2019SafePI}
R.~Laroche and P.~Trichelair.
\newblock Safe {P}olicy {I}mprovement with {B}aseline {B}ootstrapping.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{lee2021representation}
B.-J. Lee, J.~Lee, and K.-E. Kim.
\newblock Representation {B}alancing {O}ffline {M}odel-based {R}einforcement
  {L}earning.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{pmlrv119lee20g}
K.~Lee, Y.~Seo, S.~Lee, H.~Lee, and J.~Shin.
\newblock Context-aware {D}ynamics {M}odel for {G}eneralization in
  {M}odel-{B}ased {R}einforcement {L}earning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem{Levine2020OfflineRL}
S.~Levine, A.~Kumar, G.~Tucker, and J.~Fu.
\newblock Offline {R}einforcement {L}earning: {T}utorial, {R}eview, and
  {P}erspectives on {O}pen {P}roblems.
\newblock {\em ArXiv}, abs/2005.01643, 2020.

\bibitem{Liu2019OffPolicyPG}
Y.~Liu, A.~Swaminathan, A.~Agarwal, and E.~Brunskill.
\newblock Off-{P}olicy {P}olicy {G}radient with {S}tate {D}istribution
  {C}orrection.
\newblock {\em ArXiv}, abs/1904.08473, 2019.

\bibitem{Liu2020ProvablyGB}
Y.~Liu, A.~Swaminathan, A.~Agarwal, and E.~Brunskill.
\newblock Provably {G}ood {B}atch {R}einforcement {L}earning {W}ithout {G}reat
  {E}xploration.
\newblock {\em ArXiv}, abs/2007.08202, 2020.

\bibitem{Lu2022RevisitingDC}
C.~Lu, P.~J. Ball, J.~Parker-Holder, M.~A. Osborne, and S.~J. Roberts.
\newblock {Revisiting Design Choices in Offline Model Based Reinforcement
  Learning}.
\newblock In {\em International Conference on Learning Representation}, 2022.

\bibitem{lyu2022mildly}
J.~Lyu, X.~Ma, X.~Li, and Z.~Lu.
\newblock {Mildly Conservative Q-learning for Offline Reinforcement Learning}.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems}, 2022.

\bibitem{Ma2021OfflineRL}
X.~Ma, Y.~Yang, H.~Hu, Q.~Liu, J.~Yang, C.~Zhang, Q.~Zhao, and B.~Liang.
\newblock Offline {R}einforcement {L}earning with {V}alue-based {E}pisodic
  {M}emory.
\newblock {\em ArXiv}, abs/2110.09796, 2021.

\bibitem{Ma2021ConservativeOD}
Y.~J. Ma, D.~Jayaraman, and O.~Bastani.
\newblock Conservative {O}ffline {D}istributional {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  volume~34, 2021.

\bibitem{Mandlekar2020IRISIR}
A.~Mandlekar, F.~Ramos, B.~Boots, L.~Fei-Fei, A.~Garg, and D.~Fox.
\newblock {IRIS}: {I}mplicit {R}einforcement without {I}nteraction at {S}cale
  for {L}earning {C}ontrol from {O}ffline {R}obot {M}anipulation {D}ata.
\newblock In {\em International Conference on Robotics and Automation (ICRA)},
  2020.

\bibitem{matsushima2021deploymentefficient}
T.~Matsushima, H.~Furuta, Y.~Matsuo, O.~Nachum, and S.~Gu.
\newblock Deployment-{E}fficient {R}einforcement {L}earning via {M}odel-{B}ased
  {O}ffline {O}ptimization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{Meng2021OfflinePM}
L.~Meng, M.~Wen, Y.~Yang, C.~Le, X.~Li, W.~Zhang, Y.~Wen, H.~Zhang, J.~Wang,
  and B.~Xu.
\newblock Offline {P}re-trained {M}ulti-{A}gent {D}ecision {T}ransformer: {O}ne
  {B}ig {S}equence {M}odel {T}ackles {A}ll {SMAC} {T}asks.
\newblock {\em ArXiv}, abs/2112.02845, 2021.

\bibitem{Munos2016SafeAE}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~G. Bellemare.
\newblock Safe and {E}fficient {O}ff-{P}olicy {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2016.

\bibitem{Nachum2019DualDICEBE}
O.~Nachum, Y.~Chow, B.~Dai, and L.~Li.
\newblock Dual{DICE}: {B}ehavior-{A}gnostic {E}stimation of {D}iscounted
  {S}tationary {D}istribution {C}orrections.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Nachum2019AlgaeDICEPG}
O.~Nachum, B.~Dai, I.~Kostrikov, Y.~Chow, L.~Li, and D.~Schuurmans.
\newblock Algae{DICE}: {P}olicy {G}radient from {A}rbitrary {E}xperience.
\newblock {\em ArXiv}, abs/1912.02074, 2019.

\bibitem{Nair2020AcceleratingOR}
A.~Nair, M.~Dalal, A.~Gupta, and S.~Levine.
\newblock Accelerating {O}nline {R}einforcement {L}earning with {O}ffline
  {D}atasets.
\newblock {\em ArXiv}, abs/2006.09359, 2020.

\bibitem{Ovadia2019CanYT}
Y.~Ovadia, E.~Fertig, J.~Ren, Z.~Nado, D.~Sculley, S.~Nowozin, J.~V. Dillon,
  B.~Lakshminarayanan, and J.~Snoek.
\newblock Can {Y}ou {T}rust {Y}our {M}odel's {U}ncertainty? {E}valuating
  {P}redictive {U}ncertainty {U}nder {D}ataset {S}hift.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Precup2001OffPolicyTD}
D.~Precup, R.~S. Sutton, and S.~Dasgupta.
\newblock Off-{P}olicy {T}emporal {D}ifference {L}earning with {F}unction
  {A}pproximation.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2001.

\bibitem{Rafailov2021OfflineRL}
R.~Rafailov, T.~Yu, A.~Rajeswaran, and C.~Finn.
\newblock Offline {R}einforcement {L}earning from {I}mages with {L}atent
  {S}pace {M}odels.
\newblock In {\em Learning for Dynamics and Control (L4DC)}, 2021.

\bibitem{Rajeswaran2018LearningCD}
A.~Rajeswaran, V.~Kumar, A.~Gupta, J.~Schulman, E.~Todorov, and S.~Levine.
\newblock Learning {C}omplex {D}exterous {M}anipulation with {D}eep
  {R}einforcement {L}earning and {D}emonstrations.
\newblock {\em ArXiv}, abs/1709.10087, 2018.

\bibitem{Ross2012AgnosticSI}
S.~Ross and J.~A. Bagnell.
\newblock Agnostic {S}ystem {I}dentification for {M}odel-{B}ased
  {R}einforcement {L}earning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2012.

\bibitem{Singh2020COGCN}
A.~Singh, A.~Yu, J.~Yang, J.~Zhang, A.~Kumar, and S.~Levine.
\newblock {COG}: {C}onnecting {N}ew {S}kills to {P}ast {E}xperience with
  {O}ffline {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/2010.14500, 2020.

\bibitem{Sohn2015LearningSO}
K.~Sohn, H.~Lee, and X.~Yan.
\newblock Learning {S}tructured {O}utput {R}epresentation using {D}eep
  {C}onditional {G}enerative {M}odels.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2015.

\bibitem{Strehl2010LearningFL}
A.~L. Strehl, J.~Langford, L.~Li, and S.~M. Kakade.
\newblock Learning from {L}ogged {I}mplicit {E}xploration {D}ata.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2010.

\bibitem{Subramanyam2016InfusionME}
R.~Subramanyam, M.~A. Mahmoud, D.~Buck, and A.~M. Varughese.
\newblock Infusion {M}edication {E}rror {R}eduction by {T}wo-{P}erson
  {V}erification: {A} {Q}uality {I}mprovement {I}nitiative.
\newblock {\em Pediatrics}, 138, 2016.

\bibitem{Sutton1990IntegratedAF}
R.~S. Sutton.
\newblock Integrated {A}rchitectures for {L}earning, {P}lanning, and {R}eacting
  {B}ased on {A}pproximating {D}ynamic {P}rogramming.
\newblock In {\em International Conference on Machine Learning (ICML)}, 1990.

\bibitem{Sutton2005ReinforcementLA}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement {L}earning: {A}n {I}ntroduction}.
\newblock MIT Press, 2018.

\bibitem{Sutton2016AnEA}
R.~S. Sutton, A.~R. Mahmood, and M.~White.
\newblock An {E}mphatic {A}pproach to the {P}roblem of {O}ff-policy
  {T}emporal-{D}ifference {L}earning.
\newblock {\em Journal of Machine Learning Research}, 17:2603--2631, 2016.

\bibitem{Swaminathan2015BatchLF}
A.~Swaminathan and T.~Joachims.
\newblock Batch {L}earning from {L}ogged {B}andit {F}eedback through
  {C}ounterfactual {R}isk {M}inimization.
\newblock {\em J. Mach. Learn. Res.}, 16:1731--1755, 2015.

\bibitem{Swazinna2021OvercomingMB}
P.~Swazinna, S.~Udluft, and T.~A. Runkler.
\newblock Overcoming {M}odel {B}ias for {R}obust {O}ffline {D}eep
  {R}einforcement {L}earning.
\newblock {\em Engineering Applications of Artificial Intelligence},
  104:104366, 2021.

\bibitem{Todorov2012MuJoCoAP}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mu{J}o{C}o: {A} {P}hysics {E}ngine for {M}odel-based {C}ontrol.
\newblock {\em IEEE/RSJ International Conference on Intelligent Robots and
  Systems}, 2012.

\bibitem{Van2012ANA}
D.~N. Van, L.~Kuhnert, S.~Thamke, J.~Schlemper, and K.-D. Kuhnert.
\newblock A {N}ovel {A}pproach for {A} {D}ouble-{C}heck of {P}assable
  {V}egetation {D}etection in {A}utonomous {G}round {V}ehicles.
\newblock In {\em IEEE Conference on Intelligent Transportation Systems}, 2012.

\bibitem{Walsh2010IntegratingSP}
T.~J. Walsh, S.~Goschin, and M.~L. Littman.
\newblock Integrating {S}ample-{B}ased {P}lanning and {M}odel-{B}ased
  {R}einforcement {L}earning.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2010.

\bibitem{Wang2021OfflineRL}
J.~Wang, W.~Li, H.~Jiang, G.~Zhu, S.~Li, and C.~Zhang.
\newblock Offline {R}einforcement {L}earning with {R}everse {M}odel-based
  {I}magination.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Wang2018SupervisedRL}
L.~Wang, W.~Zhang, X.~He, and H.~Zha.
\newblock Supervised {R}einforcement {L}earning with {R}ecurrent {N}eural
  {N}etwork for {D}ynamic {T}reatment {R}ecommendation.
\newblock In {\em ACM SIGKDD International Conference on Knowledge Discovery \&
  Data Mining}, 2018.

\bibitem{Wang2020Exploring}
T.~Wang and J.~Ba.
\newblock Exploring {M}odel-based {P}lanning with {P}olicy {N}etworks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{Wu2019BehaviorRO}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior {R}egularized {O}ffline {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/1911.11361, 2019.

\bibitem{Wu2021UncertaintyWA}
Y.~Wu, S.~Zhai, N.~Srivastava, J.~M. Susskind, J.~Zhang, R.~Salakhutdinov, and
  H.~Goh.
\newblock Uncertainty {W}eighted {A}ctor-{C}ritic for {O}ffline {R}einforcement
  {L}earning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{Yu2021COMBOCO}
T.~Yu, A.~Kumar, R.~Rafailov, A.~Rajeswaran, S.~Levine, and C.~Finn.
\newblock {COMBO}: {C}onservative {O}ffline {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Yu2020MOPOMO}
T.~Yu, G.~Thomas, L.~Yu, S.~Ermon, J.~Y. Zou, S.~Levine, C.~Finn, and T.~Ma.
\newblock {MOPO}: {M}odel-based {O}ffline {P}olicy {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{Zanette2021ProvableBO}
A.~Zanette, M.~J. Wainwright, and E.~Brunskill.
\newblock Provable {B}enefits of {A}ctor-{C}ritic {M}ethods for {O}ffline
  {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Zhan2021ModelBasedOP}
X.~Zhan, X.~Zhu, and H.~Xu.
\newblock Model-{B}ased {O}ffline {P}lanning with {T}rajectory {P}runing.
\newblock {\em ArXiv}, abs/2105.07351, 2021.

\bibitem{Zhou2020PLASLA}
W.~Zhou, S.~Bajracharya, and D.~Held.
\newblock P{LAS}: {L}atent {A}ction {S}pace for {O}ffline {R}einforcement
  {L}earning.
\newblock In {\em Conference on Robot Learning (CoRL)}, 2020.

\end{thebibliography}
