\begin{thebibliography}{10}

\bibitem{aitchison2019optimal}
M.~Aitchison.
\newblock Optimal use of experience in first person shooter environments.
\newblock In {\em 2019 IEEE Conference on Games (CoG)}, pages 1--4. IEEE, 2019.

\bibitem{aitchison2022atari}
M.~Aitchison, P.~Sweetser, and M.~Hutter.
\newblock Atari-5: Distilling the arcade learning environment down to five
  games.
\newblock {\em arXiv preprint arXiv:2210.02019}, 2022.

\bibitem{andrychowicz2020matters}
M.~Andrychowicz, A.~Raichuk, P.~Sta{\'n}czyk, M.~Orsini, S.~Girgin,
  R.~Marinier, L.~Hussenot, M.~Geist, O.~Pietquin, M.~Michalski, et~al.
\newblock What matters for on-policy deep actor-critic methods? a large-scale
  study.
\newblock In {\em International conference on learning representations}, 2020.

\bibitem{badia2020agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, Z.~D. Guo,
  and C.~Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In {\em International Conference on Machine Learning}, pages
  507--517. PMLR, 2020.

\bibitem{bellemare2017distributional}
M.~G. Bellemare, W.~Dabney, and R.~Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  449--458. PMLR, 2017.

\bibitem{bellemare2013arcade}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{burda2018exploration}
Y.~Burda, H.~Edwards, A.~Storkey, and O.~Klimov.
\newblock Exploration by random network distillation.
\newblock {\em arXiv preprint arXiv:1810.12894}, 2018.

\bibitem{chen2021randomized}
X.~Chen, C.~Wang, Z.~Zhou, and K.~Ross.
\newblock Randomized ensembled double q-learning: Learning fast without a
  model.
\newblock {\em arXiv preprint arXiv:2101.05982}, 2021.

\bibitem{cichosz1994truncating}
P.~Cichosz.
\newblock Truncating temporal differences: On the efficient implementation of
  td (lambda) for reinforcement learning.
\newblock {\em Journal of Artificial Intelligence Research}, 2:287--318, 1994.

\bibitem{cobbe2020leveraging}
K.~Cobbe, C.~Hesse, J.~Hilton, and J.~Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  2048--2056. PMLR, 2020.

\bibitem{cobbe2021phasic}
K.~W. Cobbe, J.~Hilton, O.~Klimov, and J.~Schulman.
\newblock Phasic policy gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  2020--2027. PMLR, 2021.

\bibitem{engstrom2019implementation}
L.~Engstrom, A.~Ilyas, S.~Santurkar, D.~Tsipras, F.~Janoos, L.~Rudolph, and
  A.~Madry.
\newblock Implementation matters in deep rl: A case study on ppo and trpo.
\newblock In {\em International conference on learning representations}, 2019.

\bibitem{espeholt2018impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em International Conference on Machine Learning}, pages
  1407--1416. PMLR, 2018.

\bibitem{hausknecht2015deep}
M.~Hausknecht and P.~Stone.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In {\em 2015 AAAI fall symposium series}, 2015.

\bibitem{hessel2021muesli}
M.~Hessel, I.~Danihelka, F.~Viola, A.~Guez, S.~Schmitt, L.~Sifre, T.~Weber,
  D.~Silver, and H.~Van~Hasselt.
\newblock Muesli: Combining improvements in policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  4214--4226. PMLR, 2021.

\bibitem{hessel2018rainbow}
M.~Hessel, J.~Modayil, H.~Van~Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, J.~Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{horgan2018distributed}
D.~Horgan, J.~Quan, D.~Budden, G.~Barth-Maron, M.~Hessel, H.~Van~Hasselt, and
  D.~Silver.
\newblock Distributed prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1803.00933}, 2018.

\bibitem{shengyi2022the37implementation}
S.~Huang, R.~F.~J. Dossa, A.~Raffin, A.~Kanervisto, and W.~Wang.
\newblock The 37 implementation details of proximal policy optimization.
\newblock In {\em ICLR Blog Track}, 2022.
\newblock
  https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.

\bibitem{kearns2000bias}
M.~J. Kearns and S.~P. Singh.
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In {\em COLT}, pages 142--147. Citeseer, 2000.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{machado2018revisiting}
M.~C. Machado, M.~G. Bellemare, E.~Talvitie, J.~Veness, M.~Hausknecht, and
  M.~Bowling.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock {\em Journal of Artificial Intelligence Research}, 61:523--562, 2018.

\bibitem{mccandlish2018empirical}
S.~McCandlish, J.~Kaplan, D.~Amodei, and O.~D. Team.
\newblock An empirical model of large-batch training.
\newblock {\em arXiv preprint arXiv:1812.06162}, 2018.

\bibitem{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{pardo2018time}
F.~Pardo, A.~Tavakoli, V.~Levdik, and P.~Kormushev.
\newblock Time limits in reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4045--4054. PMLR, 2018.

\bibitem{raileanu2021decoupling}
R.~Raileanu and R.~Fergus.
\newblock Decoupling value and policy for generalization in reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  8787--8798. PMLR, 2021.

\bibitem{schrittwieser2020mastering}
J.~Schrittwieser, I.~Antonoglou, T.~Hubert, K.~Simonyan, L.~Sifre, S.~Schmitt,
  A.~Guez, E.~Lockhart, D.~Hassabis, T.~Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock {\em Nature}, 588(7839):604--609, 2020.

\bibitem{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem{schulman2015high}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{silver2018general}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144, 2018.

\bibitem{sutton1988learning}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44, 1988.

\bibitem{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem{wang2016dueling}
Z.~Wang, T.~Schaul, M.~Hessel, H.~Hasselt, M.~Lanctot, and N.~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1995--2003. PMLR, 2016.

\bibitem{williams1992simple}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3):229--256, 1992.

\bibitem{yu2021surprising}
C.~Yu, A.~Velu, E.~Vinitsky, Y.~Wang, A.~Bayen, and Y.~Wu.
\newblock The surprising effectiveness of ppo in cooperative, multi-agent
  games.
\newblock {\em arXiv preprint arXiv:2103.01955}, 2021.

\end{thebibliography}
