\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man{\'e}.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.06565}.

\bibitem[Antverg and Belinkov(2022)]{antverg2022on}
Omer Antverg and Yonatan Belinkov.
\newblock On the pitfalls of analyzing individual neurons in language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=8uz0EWPQIMu}.

\bibitem[Azaria and Mitchell(2023)]{azaria-mitchell-2023-internal}
Amos Azaria and Tom Mitchell.
\newblock The internal state of an {LLM} knows when it{'}s lying.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 967--976, Singapore, dec 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.68}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.68}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.
\newblock URL \url{https://arxiv.org/abs/1607.06450}.

\bibitem[Bengio et~al.(2024)Bengio, Hinton, Yao, Song, Abbeel, Darrell, Harari, Zhang, Xue, Shalev-Shwartz, Hadfield, Clune, Maharaj, Hutter, Baydin, McIlraith, Gao, Acharya, Krueger, Dragan, Torr, Russell, Kahneman, Brauner, and Mindermann]{bengio2023managing}
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval~Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım~Güneş Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, and Sören Mindermann.
\newblock Managing extreme {AI} risks amid rapid progress.
\newblock \emph{Science}, 384\penalty0 (6698):\penalty0 842--845, 2024.
\newblock \doi{10.1126/science.adn0117}.
\newblock URL \url{https://www.science.org/doi/abs/10.1126/science.adn0117}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: {A} suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/biderman23a.html}.

\bibitem[Bills et~al.(2023)Bills, Cammarata, Mossing, Tillman, Gao, Goh, Sutskever, Leike, Wu, and Saunders]{bills2023language}
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
\newblock Language models can explain neurons in language models.
\newblock \emph{OpenAI Blog}, 2023.
\newblock URL \url{https://openai.com/index/language-models-can-explain-neurons-in-language-models/}.

\bibitem[Bricken et~al.(2023)Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Anil, Denison, Askell, Lasenby, Wu, Kravec, Schiefer, Maxwell, Joseph, Hatfield-Dodds, Tamkin, Nguyen, McLean, Burke, Hume, Carter, Henighan, and Olah]{bricken2023monosemanticity}
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah~E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.
\newblock Towards monosemanticity: {Decomposing} language models with dictionary learning.
\newblock \emph{Transformer Circuits Thread}, 2023.
\newblock URL \url{https://transformer-circuits.pub/2023/monosemantic-features/index.html}.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and Steinhardt]{burns2023discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ETKGuby0hcs}.

\bibitem[Cancedda(2024)]{cancedda2024spectral}
Nicola Cancedda.
\newblock Spectral filters, dark signals, and attention sinks.
\newblock \emph{arXiv preprint arXiv:2402.09221}, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.09221}.

\bibitem[Chang and Bergen(2022)]{10.1162/tacl_a_00444}
Tyler~A. Chang and Benjamin~K. Bergen.
\newblock Word acquisition in neural language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 1--16, 01 2022.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00444}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00444}.

\bibitem[Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and Garriga-Alonso]{conmy2023towards}
Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri{\`a} Garriga-Alonso.
\newblock Towards automated circuit discovery for mechanistic interpretability.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 16318--16352, 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html}.

\bibitem[Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and Wei]{dai-etal-2022-knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8493--8502, Dublin, Ireland, may 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.581}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.581}.

\bibitem[Dalvi et~al.(2019)Dalvi, Durrani, Sajjad, Belinkov, Bau, and Glass]{dalvi2019one}
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass.
\newblock What is one grain of sand in the desert? analyzing individual neurons in deep nlp models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 6309--6317, 2019.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/4592}.

\bibitem[Dar et~al.(2023)Dar, Geva, Gupta, and Berant]{dar2022analyzing}
Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.
\newblock Analyzing transformers in embedding space.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 16124--16170, Toronto, Canada, jul 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.893}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.893}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock URL \url{https://transformer-circuits.pub/2021/framework/index.html}.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Nanda, Henighan, Johnston, ElShowk, Joseph, DasSarma, Mann, Hernandez, Askell, Ndousse, Jones, Drain, Chen, Bai, Ganguli, Lovitt, Hatfield-Dodds, Kernion, Conerly, Kravec, Fort, Kadavath, Jacobson, Tran-Johnson, Kaplan, Clark, Brown, McCandlish, Amodei, and Olah]{elhage2022solu}
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah.
\newblock Softmax linear units.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock URL \url{https://transformer-circuits.pub/2022/solu/index.html}.

\bibitem[Ferrando et~al.(2024)Ferrando, Sarti, Bisazza, and Costa-juss{\`a}]{ferrando2024primer}
Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta~R Costa-juss{\`a}.
\newblock A primer on the inner workings of transformer-based language models.
\newblock \emph{arXiv preprint arXiv:2405.00208}, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.00208}.

\bibitem[Gal et~al.(2016)]{gal2016uncertainty}
Yarin Gal et~al.
\newblock Uncertainty in deep learning, 2016.
\newblock URL \url{http://106.54.215.74/2019/20190729-liuzy.pdf}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: {An} 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.
\newblock URL \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Gawlikowski et~al.(2023)Gawlikowski, Tassi, Ali, Lee, Humt, Feng, Kruspe, Triebel, Jung, Roscher, et~al.]{gawlikowski2023survey}
Jakob Gawlikowski, Cedrique Rovile~Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et~al.
\newblock A survey of uncertainty in deep neural networks.
\newblock \emph{Artificial Intelligence Review}, 56\penalty0 (Suppl 1):\penalty0 1513--1589, 2023.
\newblock URL \url{https://link.springer.com/article/10.1007/s10462-023-10562-9}.

\bibitem[Geng et~al.(2023)Geng, Cai, Wang, Koeppl, Nakov, and Gurevych]{geng2023survey}
Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych.
\newblock A survey of language model confidence estimation and calibration.
\newblock \emph{arXiv preprint arXiv:2311.08298}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.08298}.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and Levy]{geva-etal-2021-transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495, Online and Punta Cana, Dominican Republic, nov 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.446}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.446}.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer}
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 30--45, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.3}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.3}.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus, 2019.
\newblock URL \url{http://Skylion007.github.io/OpenWebTextCorpus}.

\bibitem[Google(2024)]{geminiteam2024gemini}
Gemini~Team Google.
\newblock Gemini: {A} family of highly capable multimodal models, 2024.
\newblock URL \url{https://arxiv.org/abs/2312.11805}.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and Bertsimas]{gurnee2023finding}
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.
\newblock Finding neurons in a haystack: {Case} studies with sparse probing.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=JYs1R9IMJr}.

\bibitem[Gurnee et~al.(2024)Gurnee, Horsley, Guo, Kheirkhah, Sun, Hathaway, Nanda, and Bertsimas]{gurnee2024universal}
Wes Gurnee, Theo Horsley, Zifan~Carl Guo, Tara~Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas.
\newblock Universal neurons in {GPT}2 language models.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=ZeI104QZ8I}.

\bibitem[Hanna et~al.(2023)Hanna, Liu, and Variengien]{hanna2023how}
Michael Hanna, Ollie Liu, and Alexandre Variengien.
\newblock How does {GPT}-2 compute greater-than?: {Interpreting} mathematical abilities in a pre-trained language model.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=p4PckNQR8k}.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen, Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk, Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant, Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, sep 2020.
\newblock \doi{10.1038/s41586-020-2649-2}.
\newblock URL \url{https://doi.org/10.1038/s41586-020-2649-2}.

\bibitem[Heimersheim and Janiak(2023)]{docstring_circuit}
Stefan Heimersheim and Jett Janiak.
\newblock A circuit for python docstrings in a 4-layer attention-only transformer.
\newblock Alignment Forum, 2023.
\newblock URL \url{https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only}.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.08415}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Carlini, Schulman, and Steinhardt]{hendrycks2021unsolved}
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.
\newblock Unsolved problems in ml safety.
\newblock \emph{arXiv preprint arXiv:2109.13916}, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.13916}.

\bibitem[Hou et~al.(2023)Hou, Liu, Qian, Andreas, Chang, and Zhang]{hou2023decomposing}
Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang.
\newblock Decomposing uncertainty for large language models through input clarification ensembling.
\newblock \emph{arXiv preprint arXiv:2311.08718}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.08718}.

\bibitem[Huang and Kwon(2023)]{huang2023does}
Brian~RY Huang and Joe Kwon.
\newblock Does it know?: {Probing} for uncertainty in language model latent beliefs.
\newblock In \emph{NeurIPS Workshop on Attributing Model Behavior at Scale}, 2023.
\newblock URL \url{https://openreview.net/forum?id=uSvN2oozRK}.

\bibitem[Huben et~al.(2024)Huben, Cunningham, Smith, Ewart, and Sharkey]{huben2024sparse}
Robert Huben, Hoagy Cunningham, Logan~Riggs Smith, Aidan Ewart, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=F76bwRSLeK}.

\bibitem[Inc.(2015)]{plotly}
Plotly~Technologies Inc.
\newblock Collaborative data science, 2015.
\newblock URL \url{https://plot.ly}.

\bibitem[Javaheripi et~al.(2023)Javaheripi, Bubeck, Abdin, Aneja, Bubeck, Mendes, Chen, Del~Giorno, Eldan, Gopi, et~al.]{javaheripi2023phi}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C{\'e}sar~Teodoro Mendes, Weizhu Chen, Allie Del~Giorno, Ronen Eldan, Sivakanth Gopi, et~al.
\newblock Phi-2: {The} surprising power of small language models.
\newblock \emph{Microsoft Research Blog}, 2023.
\newblock URL \url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}.

\bibitem[Jiang et~al.(2021)Jiang, Araki, Ding, and Neubig]{jiang-etal-2021-know}
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig.
\newblock How can we know when language models know? on the calibration of language models for question answering.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 962--977, 2021.
\newblock \doi{10.1162/tacl_a_00407}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.57}.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al.
\newblock Language models (mostly) know what they know.
\newblock \emph{arXiv preprint arXiv:2207.05221}, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.05221}.

\bibitem[Karpathy et~al.(2015)Karpathy, Johnson, and Fei-Fei]{karpathy2015visualizing}
Andrej Karpathy, Justin Johnson, and Li~Fei-Fei.
\newblock Visualizing and understanding recurrent networks.
\newblock \emph{arXiv preprint arXiv:1506.02078}, 2015.
\newblock URL \url{https://arxiv.org/abs/1506.02078}.

\bibitem[Katz and Belinkov(2023)]{katz-belinkov-2023-visit}
Shahar Katz and Yonatan Belinkov.
\newblock {VISIT}: {Visualizing} and interpreting the semantic information flow of transformers.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 14094--14113, Singapore, dec 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.939}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.939}.

\bibitem[Kissane et~al.(2024)Kissane, Krzyzanowski, Conmy, and Nanda]{attention_saes}
Connor Kissane, Robert Krzyzanowski, Arthur Conmy, and Neel Nanda.
\newblock Sparse autoencoders work on attention layer outputs.
\newblock Alignment Forum, 2024.
\newblock URL \url{https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ}.

\bibitem[Krzyzanowski et~al.(2024)Krzyzanowski, Kissane, Conmy, and Nanda]{gpt2_attention_saes_3}
Robert Krzyzanowski, Connor Kissane, Arthur Conmy, and Neel Nanda.
\newblock We inspected every head in {GPT-2} small using {SAEs} so you don’t have to.
\newblock Alignment Forum, 2024.
\newblock URL \url{https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don}.

\bibitem[Kuhn et~al.(2023)Kuhn, Gal, and Farquhar]{kuhn2023semantic}
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar.
\newblock Semantic uncertainty: {Linguistic} invariances for uncertainty estimation in natural language generation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=VD-AYtP0dve}.

\bibitem[Lieberum et~al.(2023)Lieberum, Rahtz, Kram{\'a}r, Irving, Shah, and Mikulik]{lieberum2023does}
Tom Lieberum, Matthew Rahtz, J{\'a}nos Kram{\'a}r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik.
\newblock Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla.
\newblock \emph{arXiv preprint arXiv:2307.09458}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09458}.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022teaching}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Teaching models to express their uncertainty in words.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=8s8K2UZGTZ}.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2018fixing}
Ilya Loshchilov and Frank Hutter.
\newblock Fixing weight decay regularization in adam, 2018.
\newblock URL \url{https://openreview.net/forum?id=rk6qdGgCZ}.

\bibitem[Malinin and Gales(2021)]{malinin2021uncertainty}
Andrey Malinin and Mark Gales.
\newblock Uncertainty estimation in autoregressive structured prediction.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=jN5y-zb5Q7m}.

\bibitem[Marks and Tegmark(2024)]{marks2024the}
Samuel Marks and Max Tegmark.
\newblock The geometry of truth: {Emergent} linear structure in large language model representations of true/false datasets, 2024.
\newblock URL \url{https://openreview.net/forum?id=CeJEfNKstt}.

\bibitem[Meister et~al.(2023)Meister, Stokowiec, Pimentel, Yu, Rimell, and Kuncoro]{meister-etal-2023-natural}
Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, and Adhiguna Kuncoro.
\newblock A natural bias for language generation models.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 243--255, Toronto, Canada, jul 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-short.22}.
\newblock URL \url{https://aclanthology.org/2023.acl-short.22}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex~J Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=-h6WAS6eE4}.

\bibitem[Nanda and Bloom(2022)]{nanda2022transformerlens}
Neel Nanda and Joseph Bloom.
\newblock Transformerlens, 2022.
\newblock URL \url{https://github.com/TransformerLensOrg/TransformerLens}.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=9XFSbDPmdW}.

\bibitem[Nostalgebraist(2020)]{nostalgebraist}
Nostalgebraist.
\newblock Interpreting gpt: {The} logit lens, 2020.
\newblock URL \url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.
\newblock Feature visualization.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00007}.
\newblock URL \url{https://distill.pub/2017/feature-visualization}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez, Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock URL \url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}.

\bibitem[OpenAI(2024)]{openai2024gpt4}
OpenAI.
\newblock {GPT-4} technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_bdbca288}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: {An} imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf}.

\bibitem[Pearl(2001)]{pearl2022direct}
Judea Pearl.
\newblock Direct and indirect effects.
\newblock In \emph{Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence}, UAI'01, page 411–420, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558608001.
\newblock URL \url{https://dl.acm.org/doi/10.5555/2074022.2074073}.

\bibitem[Radford et~al.(2018)Radford, Jozefowicz, and Sutskever]{radford2018learning}
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
\newblock Learning to generate reviews and discovering sentiment, 2018.
\newblock URL \url{https://openreview.net/forum?id=SJ71VXZAZ}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.
\newblock URL \url{https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock URL \url{https://www.jmlr.org/papers/v21/20-074.html}.

\bibitem[Robins and Greenland(1992)]{robins1992identifiability}
James~M Robins and Sander Greenland.
\newblock Identifiability and exchangeability for direct and indirect effects.
\newblock \emph{Epidemiology}, 3\penalty0 (2):\penalty0 143--155, 1992.
\newblock URL \url{https://journals.lww.com/epidem/abstract/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx}.

\bibitem[Si et~al.(2023)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and Wang]{si2023prompting}
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan~Lee Boyd-Graber, and Lijuan Wang.
\newblock Prompting {GPT}-3 to be reliable.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=98p5x51L5af}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: {A} simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0 (1):\penalty0 1929--1958, 2014.
\newblock URL \url{https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,}.

\bibitem[Stolfo et~al.(2023)Stolfo, Belinkov, and Sachan]{stolfo-etal-2023-mechanistic}
Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan.
\newblock A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7035--7052, Singapore, dec 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.435}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.435}.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: {Open} models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.08295}.

\bibitem[Tian et~al.(2023)Tian, Mitchell, Zhou, Sharma, Rafailov, Yao, Finn, and Manning]{tian-etal-2023-just}
Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning.
\newblock Just ask for calibration: {Strategies} for eliciting calibrated confidence scores from language models fine-tuned with human feedback.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 5433--5442, Singapore, dec 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.330}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.330}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: {Open} foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09288}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/7181-attention-is-all}.

\bibitem[Vig and Belinkov(2019)]{vig-belinkov-2019-analyzing}
Jesse Vig and Yonatan Belinkov.
\newblock Analyzing the structure of attention in a transformer language model.
\newblock In Tal Linzen, Grzegorz Chrupa{\l}a, Yonatan Belinkov, and Dieuwke Hupkes, editors, \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 63--76, Florence, Italy, aug 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-4808}.
\newblock URL \url{https://aclanthology.org/W19-4808}.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Singer, and Shieber]{NEURIPS2020_92650b2e}
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber.
\newblock Investigating gender bias in language models using causal mediation analysis.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 12388--12401. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf}.

\bibitem[Voita et~al.(2023)Voita, Ferrando, and Nalmpantis]{voita2023neurons}
Elena Voita, Javier Ferrando, and Christoforos Nalmpantis.
\newblock Neurons in large language models: {Dead,} n-gram, positional.
\newblock \emph{arXiv preprint arXiv:2309.04827}, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.04827}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2023interpretability}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the wild: {A} circuit for indirect object identification in {GPT}-2 small.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=NpsVSN6o4ul}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=1PL1NIMMrw}.

\bibitem[{W}es {M}c{K}inney(2010)]{mckinney-proc-scipy-2010}
{W}es {M}c{K}inney.
\newblock {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython.
\newblock In {S}t\'efan van~der {W}alt and {J}arrod {M}illman, editors, \emph{{P}roceedings of the 9th {P}ython in {S}cience {C}onference}, pages 56 -- 61, 2010.
\newblock \doi{10.25080/Majora-92bf1922-00a}.
\newblock URL \url{https://scipy.org}.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=NG7sS51zVF}.

\bibitem[Yun et~al.(2021)Yun, Chen, Olshausen, and LeCun]{yun-etal-2021-transformer}
Zeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun.
\newblock Transformer visualization via dictionary learning: {Contextualized} embedding as a linear superposition of transformer factors.
\newblock In Eneko Agirre, Marianna Apidianaki, and Ivan Vuli{\'c}, editors, \emph{Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures}, pages 1--10, Online, jun 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.deelio-1.1}.
\newblock URL \url{https://aclanthology.org/2021.deelio-1.1}.

\bibitem[Zhang and Sennrich(2019)]{NEURIPS2019_1e8a1942}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf}.

\bibitem[Zouhar(2023)]{ryanizebib}
Vilém Zouhar.
\newblock Ryanize bib, 2023.
\newblock URL \url{https://github.com/zouharvi/ryanize-bib}.

\end{thebibliography}
