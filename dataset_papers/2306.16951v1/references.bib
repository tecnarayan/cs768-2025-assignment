@article{adams1958spectralsequences,
  author   = {J. F. Adams},
  title    = {On the structure and applications of the steenrod algebra},
  journal  = {Commentarii Mathematici Helvetici},
  year     = {1958},
  volume   = {32},
  pages    = {180--214},
}

@article{brown1957finitelycomputability,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1969664},
 author = {Edgar H. Brown},
 journal = {Annals of Mathematics},
 number = {1},
 pages = {1--20},
 publisher = {Annals of Mathematics},
 title = {Finite Computability of Postnikov Complexes},
 urldate = {2023-03-04},
 volume = {65},
 year = {1957}
}

@article{rubio-kenzo-2002,
title = {Constructive algebraic topology},
journal = {Bulletin des Sciences Mathématiques},
volume = {126},
number = {5},
pages = {389-412},
year = {2002},
issn = {0007-4497},
doi = {https://doi.org/10.1016/S0007-4497(02)01119-3},
url = {https://www.sciencedirect.com/science/article/pii/S0007449702011193},
author = {Julio Rubio and Francis Sergeraert},
keywords = {Algebraic topology, Effective homology, Homotopy groups, Functional programming, Symbolic computation},
abstract = {The classical “computation” methods in Algebraic Topology most often work by means of highly infinite objects and in fact are not constructive. Typical examples are shown to describe the nature of the problem. The Rubio–Sergeraert solution for Constructive Algebraic Topology is recalled. This is not only a theoretical solution: the concrete computer program Kenzo has been written down which precisely follows this method. This program has been used in various cases, opening new research subjects and producing in several cases significant results unreachable by hand. In particular the Kenzo program can compute the first homotopy groups of a simply connected arbitrary simplicial set.}
}

@article{davies2021guiding,
    title   = {Advancing mathematics by guiding human intuition with AI},
    author  = {Davies, A. and Veličković, P. and Buesing, L. and et al.},
    journal = {Nature},
    year    = {2021},
    volume  = {600},
    pages   = {70-74},
}

@article{douglas2022machine,
    title   = {Machine learning as a tool in theoretical science},
    author  = {Douglas, M.R.},
    journal = {Nat Rev Phys},
    volume  = {4},
    pages   = {145-146},
    year    = {2022},
}

@article{wu2001combinatorial,
    title   = {Combinatorial descriptions of homotopy groups of certain spaces},
    volume  = {130},
    DOI     = {10.1017/S030500410100487X},
    number  = {3},
    journal = {Mathematical Proceedings of the Cambridge Philosophical Society}, 
    author  = {Wu, J.},
    year    = {2001},
    pages   = {489–513}
}

@article{atkinson1992generating,
title = {Generating binary trees at random},
journal = {Information Processing Letters},
volume = {41},
number = {1},
pages = {21-23},
year = {1992},
issn = {0020-0190},
doi = {https://doi.org/10.1016/0020-0190(92)90075-7},
url = {https://www.sciencedirect.com/science/article/pii/0020019092900757},
author = {M.D. Atkinson and J.-R. Sack},
keywords = {Analysis of algorithms, binary trees, bracket sequences, data structure},
abstract = {We give a new constructive proof of the Chung-Feller theorem. Our proof provides a new and simple linear-time algorithm for generating random binary trees on n nodes; the algorithm uses integers no larger than 2n.}
}

@inproceedings{zehui2019intersectgan,
author = {Yao, Zehui and Zhang, Boyan and Wang, Zhiyong and Ouyang, Wanli and Xu, Dong and Feng, Dagan},
year = {2019},
month = {10},
pages = {1842-1850},
title = {IntersectGAN: Learning Domain Intersection for Generating Images with Multiple Attributes},
isbn = {978-1-4503-6889-6},
doi = {10.1145/3343031.3350908}
}

@article{bengio2003neural,
    author  = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
    title   = {A Neural Probabilistic Language Model},
    year    = {2003},
    journal = {Journal of Machine Learning Research},
    volume  = {3},
    pages   = {1137 - 1155},
}

@article{hochreiter1997long,
    author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year    = {1997},
    month   = {12},
    pages   = {1735-80},
    title   = {Long Short-term Memory},
    volume  = {9},
    journal = {Neural computation},
    doi     = {10.1162/neco.1997.9.8.1735}
}

@article{hopfield-nn-82,
    author  = {Hopfield, JJ.},
    title   = {Neural networks and physical systems with emergent collective computational abilities},
    journal = {Proc Natl Acad Sci U S A},
    year    = {1982},
    volume  = {79},
    number  = {8},
    pages   = {2554-8},
}

@misc{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bousfield1966mod,
  title={The mod-p lower central series and the Adams spectral sequence},
  author={Bousfield, Aldridge Knight and Curtis, Edward Baldwin and Kan, Daniel Marinus and Quillen, Daniel Gray and Rector, David Lee and Schlesinger, James William},
  journal={Topology},
  volume={5},
  number={4},
  pages={331--342},
  year={1966},
  publisher={Pergamon}
}

@article{arone1999goodwillie,
  title={The Goodwillie tower of the identity functor and the unstable periodic homotopy of spheres},
  author={Arone, Greg and Mahowald, Mark},
  journal={Inventiones mathematicae},
  volume={135},
  pages={743--788},
  year={1999},
  publisher={Springer}
}

@book{behrens2012goodwillie,
  title={The Goodwillie tower and the EHP sequence},
  author={Behrens, Mark},
  volume={218},
  number={1026},
  year={2012},
  publisher={American Mathematical Society}
}

@misc{mikhailov2021homotopy,
  doi = {10.48550/ARXIV.2111.00737},
  
  url = {https://arxiv.org/abs/2111.00737},
  
  author = {Mikhailov, Roman},
  
  keywords = {Group Theory (math.GR), Algebraic Topology (math.AT), K-Theory and Homology (math.KT), FOS: Mathematics, FOS: Mathematics},
  
  title = {Homotopy patterns in group theory},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}

@article{appel19774color,
    author = {Appel, K. and Haken, W.},
    title = {The solution of the four-color-map problem.},
    year = {1977},
    journal = {Scientific American},
    volume = {237},
    number = {4},
    pages = {108--121},
}

@article{hales2005kepler,
    author = {Hales, T.C.},
    title = {A proof of the Kepler conjecture},
    journal = {Annals of the mathematics},
    pages = {1065 -- 1185},
    year = {2005},
}

@misc{raghu2020survey,
      title={A Survey of Deep Learning for Scientific Discovery}, 
      author={Maithra Raghu and Eric Schmidt},
      year={2020},
      eprint={2003.11755},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wagner2021constructions,
      title={Constructions in combinatorics via neural networks}, 
      author={Adam Zsolt Wagner},
      year={2021},
      eprint={2104.14516},
      archivePrefix={arXiv},
      primaryClass={math.CO}
}

@misc{peifer2020learning,
      title={Learning selection strategies in Buchberger's algorithm}, 
      author={Dylan Peifer and Michael Stillman and Daniel Halpern-Leistner},
      year={2020},
      eprint={2005.01917},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lample2019deep,
      title={Deep Learning for Symbolic Mathematics}, 
      author={Guillaume Lample and François Charton},
      year={2019},
      eprint={1912.01412},
      archivePrefix={arXiv},
      primaryClass={cs.SC}
}

@misc{he2021machinelearning,
      title={Machine-Learning Mathematical Structures}, 
      author={Yang-Hui He},
      year={2021},
      eprint={2101.06317},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{hatcher2002algebraictopology,
  added-at = {2011-11-11T03:25:59.000+0100},
  address = {Cambridge},
  author = {Hatcher, Allen},
  biburl = {https://www.bibsonomy.org/bibtex/2a001f8991d1935a94831c18d9e85756d/mwpb479},
  groups = {public},
  mrclass = {55-01 (55-00)},
  mrnumber = {1867354 (2002k:55001)},
  mrreviewer = {Donald W. Kahn},
  publisher = {Cambridge University Press},
  title = {Algebraic topology},
  year = 2002
}

@article{ismet2014computing,
author = {Karaca, Ismet and Meric, Tugce and Vergili, Tane},
year = {2014},
month = {09},
pages = {},
title = {Computing Higher Dimensional Digital Homotopy Groups},
volume = {8},
journal = {Applied Mathematics \& Information Sciences},
doi = {10.12785/amis/080537}
}

@article{freedman2010algebraic,
author = {Freedman, Daniel and Chen, Chao},
year = {2010},
month = {08},
pages = {},
title = {Algebraic topology for computer vision},
journal = {Computer Vision}
}
@article{bhattacharya2018path,
author={Bhattacharya, Subhrajit
and Ghrist, Robert},
title={Path homotopy invariants and their application to optimal trajectory planning},
journal={Annals of Mathematics and Artificial Intelligence},
year={2018},
month={12},
day={01},
volume={84},
number={3},
pages={139-160},
issn={1573-7470},
doi={10.1007/s10472-018-9596-8},
url={https://doi.org/10.1007/s10472-018-9596-8}
}

@article{watson1989modern,
  title     = "Modern homotopy methods in optimization",
  author    = "Watson, Layne T and Haftka, Raphael T",
  abstract  = "Probability-one homotopy methods are a class of algorithms for
               solving nonlinear systems of equations that are accurate,
               robust, and converge from an arbitrary starting point almost
               surely. These new techniques have been successfully applied to
               solve Brouwer fixed point problems, polynomial systems of
               equations, and discretizations of nonlinear two-point boundary
               value problems based on shooting, finite differences,
               collocation and finite elements. This paper summarizes the
               theory of globally convergent homotopy algorithms for
               unconstrained and constrained optimization, and gives some
               examples of actual application of homotopy techniques to
               engineering optimization problems.",
  journal   = "Comput. Methods Appl. Mech. Eng.",
  publisher = "Elsevier BV",
  volume    =  74,
  number    =  3,
  pages     = "289--305",
  month     =  sep,
  year      =  1989,
  language  = "en"
}

@article{zhang2019homotopy,
  title     = "Homotopy method for solving mathematical programs with bounded
               box-constrained variational inequalities",
  author    = "Zhang, Chunyang and Zhu, Zhichuan and Yao, Yonghong and Liu,
               Qinghuai",
  journal   = "Optimization",
  publisher = "Informa UK Limited",
  volume    =  68,
  number    =  12,
  pages     = "2297--2316",
  month     =  dec,
  year      =  2019,
  language  = "en"
}

@misc{cadek2014polynomialtime,
      title={Polynomial-time computation of homotopy groups and Postnikov systems in fixed dimension}, 
      author={Martin Cadek and Marek Krcal and Jiri Matousek and Lukas Vokrinek and Uli Wagner},
      year={2014},
      eprint={1211.3093},
      archivePrefix={arXiv},
      primaryClass={cs.CG}
}

@article{rubio2002constructive,
title = {Constructive algebraic topology},
journal = {Bulletin des Sciences Mathématiques},
volume = {126},
number = {5},
pages = {389-412},
year = {2002},
issn = {0007-4497},
doi = {https://doi.org/10.1016/S0007-4497(02)01119-3},
url = {https://www.sciencedirect.com/science/article/pii/S0007449702011193},
author = {Julio Rubio and Francis Sergeraert},
keywords = {Algebraic topology, Effective homology, Homotopy groups, Functional programming, Symbolic computation},
abstract = {The classical “computation” methods in Algebraic Topology most often work by means of highly infinite objects and in fact are not constructive. Typical examples are shown to describe the nature of the problem. The Rubio–Sergeraert solution for Constructive Algebraic Topology is recalled. This is not only a theoretical solution: the concrete computer program Kenzo has been written down which precisely follows this method. This program has been used in various cases, opening new research subjects and producing in several cases significant results unreachable by hand. In particular the Kenzo program can compute the first homotopy groups of a simply connected arbitrary simplicial set.}
}

@book{goodfellow2016deep,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{khurana2023natural,
  title     = "Natural language processing: state of the art, current trends
               and challenges",
  author    = "Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh,
               Sukhdev",
  abstract  = "Natural language processing (NLP) has recently gained much
               attention for representing and analyzing human language
               computationally. It has spread its applications in various
               fields such as machine translation, email spam detection,
               information extraction, summarization, medical, and question
               answering etc. In this paper, we first distinguish four phases
               by discussing different levels of NLP and components of Natural
               Language Generation followed by presenting the history and
               evolution of NLP. We then discuss in detail the state of the art
               presenting the various applications of NLP, current trends, and
               challenges. Finally, we present a discussion on some available
               datasets, models, and evaluation metrics in NLP.",
  journal   = "Multimed. Tools Appl.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  82,
  number    =  3,
  pages     = "3713--3744",
  year      =  2023,
  keywords  = "NLP applications; NLP evaluation metrics; Natural language
               generation; Natural language processing; Natural language
               understanding",
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@article{meurer2017sympy,
     title = {SymPy: symbolic computing in Python},
     author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\'{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
     year = 2017,
     month = jan,
     keywords = {Python, Computer algebra system, Symbolics},
     abstract = {
                SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.
             },
     volume = 3,
     pages = {e103},
     journal = {PeerJ Computer Science},
     issn = {2376-5992},
     url = {https://doi.org/10.7717/peerj-cs.103},
     doi = {10.7717/peerj-cs.103}
    }

@inproceedings{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@book{jurafsky2000speech,
author = {Jurafsky, Daniel and Martin, James H.},
title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
year = {2000},
isbn = {0130950696},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {1st},
abstract = {From the Publisher:This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora. Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.}
}

@misc{li2022pretrained,
      title={Pretrained Language Models for Text Generation: A Survey}, 
      author={Junyi Li and Tianyi Tang and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
      year={2022},
      eprint={2201.05273},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{christopher2006pattern,
  title     = "Pattern Recognition and Machine Learning",
  author    = "M., Christopher",
  publisher = "Springer",
  series    = "Information Science and Statistics",
  edition   =  1,
  month     =  aug,
  year      =  2006,
  address   = "New York, NY",
  language  = "en"
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{bengio2008neural,
  title     = "Neural net language models",
  author    = "Bengio, Yoshua",
  abstract  = "A dental flossing aid for dispensing and aiding in the use of
               dental floss for cleaning teeth, comprises an elongated
               bodymember on which is mounted a supply of dental floss and
               which comprises a plurality of holes bored through the
               bodymember. The bodymember is elongated for projecting into the
               mouth cavity. Dental floss is threaded through the bored holes
               of the bodymember and extended to the outermost end of the
               bodymember; the bores serving to develop tension in the dental
               floss to resist the removal of floss from the floss supply. A
               pushkey assembly is provided for overcoming the tension and
               assisting in advancing the dental floss.",
  journal   = "Scholarpedia J.",
  publisher = "Scholarpedia",
  volume    =  3,
  number    =  1,
  pages     = "3881",
  year      =  2008
}

@ARTICLE{rumelhart1986learning,
  title     = "Learning representations by back-propagating errors",
  author    = "Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J",
  abstract  = "We describe a new learning procedure, back-propagation, for
               networks of neurone-like units. The procedure repeatedly adjusts
               the weights of the connections in the network so as to minimize
               a measure of the difference between the actual output vector of
               the net and the desired output vector. As a result of the weight
               adjustments, internal `hidden' units which are not part of the
               input or output come to represent important features of the task
               domain, and the regularities in the task are captured by the
               interactions of these units. The ability to create useful new
               features distinguishes back-propagation from earlier, simpler
               methods such as the perceptron-convergence procedure1.",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  323,
  number    =  6088,
  pages     = "533--536",
  month     =  oct,
  year      =  1986,
  language  = "en"
}

@misc{liu2021pretrain,
      title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}, 
      author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
      year={2021},
      eprint={2107.13586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nguyen2019understanding,
      title={Understanding Neural Networks via Feature Visualization: A survey}, 
      author={Anh Nguyen and Jason Yosinski and Jeff Clune},
      year={2019},
      eprint={1904.08939},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{linder2021fast,
  title     = "Fast activation maximization for molecular sequence design",
  author    = "Linder, Johannes and Seelig, Georg",
  abstract  = "BACKGROUND: Optimization of DNA and protein sequences based on
               Machine Learning models is becoming a powerful tool for
               molecular design. Activation maximization offers a simple design
               strategy for differentiable models: one-hot coded sequences are
               first approximated by a continuous representation, which is then
               iteratively optimized with respect to the predictor oracle by
               gradient ascent. While elegant, the current version of the
               method suffers from vanishing gradients and may cause predictor
               pathologies leading to poor convergence. RESULTS: Here, we
               introduce Fast SeqProp, an improved activation maximization
               method that combines straight-through approximation with
               normalization across the parameters of the input sequence
               distribution. Fast SeqProp overcomes bottlenecks in earlier
               methods arising from input parameters becoming skewed during
               optimization. Compared to prior methods, Fast SeqProp results in
               up to 100-fold faster convergence while also finding improved
               fitness optima for many applications. We demonstrate Fast
               SeqProp's capabilities by designing DNA and protein sequences
               for six deep learning predictors, including a protein structure
               predictor. CONCLUSIONS: Fast SeqProp offers a reliable and
               efficient method for general-purpose sequence optimization
               through a differentiable fitness predictor. As demonstrated on a
               variety of deep learning models, the method is widely
               applicable, and can incorporate various regularization
               techniques to maintain confidence in the sequence designs. As a
               design tool, Fast SeqProp may aid in the development of novel
               molecules, drug therapies and vaccines.",
  journal   = "BMC Bioinformatics",
  publisher = "Springer Science and Business Media LLC",
  volume    =  22,
  number    =  1,
  pages     = "510",
  month     =  oct,
  year      =  2021,
  keywords  = "Activation maximization; DNA; Deep learning; Design; Gradient
               ascent; Neural network; Optimization; Protein; RNA; Sequence
               design",
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@INCOLLECTION{freitas2003survey,
  title     = "A survey of evolutionary algorithms for data mining and
               knowledge discovery",
  booktitle = "Natural Computing Series",
  author    = "Freitas, Alex A",
  publisher = "Springer Berlin Heidelberg",
  pages     = "819--845",
  series    = "Natural computing series",
  year      =  2003,
  address   = "Berlin, Heidelberg"
}

@misc{jabbar2020survey,
      title={A Survey on Generative Adversarial Networks: Variants, Applications, and Training}, 
      author={Abdul Jabbar and Xi Li and Bourahla Omar},
      year={2020},
      eprint={2006.05132},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yu2017seqgan,
      title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}, 
      author={Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},
      year={2017},
      eprint={1609.05473},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{tillmann2003word,
  title     = "Word reordering and a dynamic programming beam search algorithm
               for statistical machine translation",
  author    = "Tillmann, Christoph and Ney, Hermann",
  abstract  = "In this article, we describe an efficient beam search algorithm
               for statistical machine translation based on dynamic programming
               (DP). The search algorithm uses the translation model presented
               in Brown et al. (1993). Starting from a DP-based solution to the
               traveling-salesman problem, we present a novel technique to
               restrict the possible word reorderings between source and target
               language in order to achieve an efficient search algorithm. Word
               reordering restrictions especially useful for the translation
               direction German to English are presented. The restrictions are
               generalized, and a set of four parameters to control the word
               reordering is introduced, which then can easily be adopted to
               new translation directions. The beam search procedure has been
               successfully tested on the Verbmobil task (German to English,
               8,000-word vocabulary) and on the Canadian Hansards task (French
               to English, 100,000-word vocabulary). For the medium-sized
               Verbmobil task, a sentence can be translated in a few seconds,
               only a small number of search errors occur, and there is no
               performance degradation as measured by the word error criterion
               used in this article.",
  journal   = "Comput. Linguist. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  29,
  number    =  1,
  pages     = "97--133",
  month     =  mar,
  year      =  2003,
  language  = "en"
}

@ARTICLE{keskar2019ctrl,
  title         = "{CTRL}: A conditional transformer language model for
                   controllable generation",
  author        = "Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R
                   and Xiong, Caiming and Socher, Richard",
  abstract      = "Large-scale language models show promising text generation
                   capabilities, but users cannot easily control particular
                   aspects of the generated text. We release CTRL, a 1.63
                   billion-parameter conditional transformer language model,
                   trained to condition on control codes that govern style,
                   content, and task-specific behavior. Control codes were
                   derived from structure that naturally co-occurs with raw
                   text, preserving the advantages of unsupervised learning
                   while providing more explicit control over text generation.
                   These codes also allow CTRL to predict which parts of the
                   training data are most likely given a sequence. This
                   provides a potential method for analyzing large amounts of
                   data via model-based source attribution. We have released
                   multiple full-sized, pretrained versions of CTRL at
                   https://github.com/salesforce/ctrl.",
  month         =  sep,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.05858"
}

@misc{holtzman2020curious,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{celikyilmaz2020evaluation,
  title         = "Evaluation of Text Generation: A Survey",
  author        = "Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng",
  abstract      = "The paper surveys evaluation methods of natural language
                   generation (NLG) systems that have been developed in the
                   last few years. We group NLG evaluation methods into three
                   categories: (1) human-centric evaluation metrics, (2)
                   automatic metrics that require no training, and (3)
                   machine-learned metrics. For each category, we discuss the
                   progress that has been made and the challenges still being
                   faced, with a focus on the evaluation of recently proposed
                   NLG tasks and neural NLG models. We then present two
                   examples for task-specific NLG evaluations for automatic
                   text summarization and long text generation, and conclude
                   the paper by proposing future research directions.",
  month         =  jun,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.14799"
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@BOOK{toda1963compositional,
  title     = "Compositional methods in homotopy groups of spheres. ({AM-49})",
  author    = "Toda, Hiroshi",
  publisher = "Princeton University Press",
  series    = "Annals of Mathematics Studies",
  month     =  jan,
  year      =  1963,
  address   = "Princeton, NJ"
}

@BOOK{burnham2002model,
  title     = "Model selection and multimodel inference",
  author    = "Burnham, Kenneth P and Anderson, David R",
  editor    = "Burnham, Kenneth P and Anderson, David R",
  publisher = "Springer",
  edition   =  2,
  month     =  sep,
  year      =  2002,
  address   = "New York, NY",
  language  = "en"
}

@BOOK{venables2010modern,
  title     = "Modern applied statistics with {S}",
  author    = "Venables, W N and Ripley, Brian D",
  publisher = "Springer",
  series    = "Statistics and Computing",
  month     =  dec,
  year      =  2010,
  address   = "New York, NY",
  language  = "en"
}

@inproceedings{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@misc{li2022diversifying,
      title={Diversifying Neural Dialogue Generation via Negative Distillation}, 
      author={Yiwei Li and Shaoxiong Feng and Bin Sun and Kan Li},
      year={2022},
      eprint={2205.02795},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{gou2021knowledge,
  title     = "Knowledge distillation: A survey",
  author    = "Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao,
               Dacheng",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  129,
  number    =  6,
  pages     = "1789--1819",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@misc{gukov2020learning,
      title={Learning to Unknot}, 
      author={Sergei Gukov and James Halverson and Fabian Ruehle and Piotr Sułkowski},
      year={2020},
      eprint={2010.16263},
      archivePrefix={arXiv},
      primaryClass={math.GT}
}

@misc{hughes2016neural,
      title={A neural network approach to predicting and computing knot invariants}, 
      author={Mark C. Hughes},
      year={2016},
      eprint={1610.05744},
      archivePrefix={arXiv},
      primaryClass={math.GT}
}

@misc{levitt2019big,
      title={Big Data Approaches to Knot Theory: Understanding the Structure of the Jones Polynomial}, 
      author={Jesse S F Levitt and Mustafa Hajij and Radmila Sazdanovic},
      year={2019},
      eprint={1912.10086},
      archivePrefix={arXiv},
      primaryClass={math.GT}
}

@article{jejjala2019deep,
title = {Deep learning the hyperbolic volume of a knot},
journal = {Physics Letters B},
volume = {799},
pages = {135033},
year = {2019},
issn = {0370-2693},
doi = {https://doi.org/10.1016/j.physletb.2019.135033},
url = {https://www.sciencedirect.com/science/article/pii/S0370269319307555},
author = {Vishnu Jejjala and Arjun Kar and Onkar Parrikar},
keywords = {Machine learning, Neural network, Topological field theory, Knot theory},
abstract = {An important conjecture in knot theory relates the large-N, double scaling limit of the colored Jones polynomial JK,N(q) of a knot K to the hyperbolic volume of the knot complement, Vol(K). A less studied question is whether Vol(K) can be recovered directly from the original Jones polynomial (N=2). In this report we use a deep neural network to approximate Vol(K) from the Jones polynomial. Our network is robust and correctly predicts the volume with 97.6\% accuracy when training on 10\% of the data. This points to the existence of a more direct connection between the hyperbolic volume and the Jones polynomial.}
}

@INCOLLECTION{rabe2021towards,
  title     = "Towards the automatic mathematician",
  booktitle = "Automated Deduction -- {CADE} 28",
  author    = "Rabe, Markus N and Szegedy, Christian",
  abstract  = "AbstractOver the recent years deep learning has found successful
               applications in mathematical reasoning. Today, we can predict
               fine-grained proof steps, relevant premises, and even useful
               conjectures using neural networks. This extended abstract
               summarizes recent developments of machine learning in
               mathematical reasoning and the vision of the N2Formal group at
               Google Research to create an automatic mathematician. The second
               part discusses the key challenges on the road ahead.",
  publisher = "Springer International Publishing",
  pages     = "25--37",
  series    = "Lecture notes in computer science",
  year      =  2021,
  address   = "Cham",
  copyright = "https://creativecommons.org/licenses/by/4.0"
}

@misc{ramesh2021zeroshot,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@InProceedings{pu2018joint,
  title = 	 {{J}oint{GAN}: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets},
  author =       {Pu, Yunchen and Dai, Shuyang and Gan, Zhe and Wang, Weiyao and Wang, Guoyin and Zhang, Yizhe and Henao, Ricardo and Duke, Lawrence Carin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4151--4160},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pu18a/pu18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/pu18a.html},
  abstract = 	 {A new generative adversarial network is developed for joint distribution matching.Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain.The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning.From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.}
}

@article{polu2022formal,
  title={Formal mathematics statement curriculum learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}
@article{han2021proof,
  title={Proof artifact co-training for theorem proving with language models},
  author={Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas},
  journal={arXiv preprint arXiv:2102.06203},
  year={2021}
}

@article{romero2006computing,
  title={Computing spectral sequences},
  author={Romero, Ana and Rubio, Julio and Sergeraert, Francis},
  journal={Journal of symbolic computation},
  volume={41},
  number={10},
  pages={1059--1079},
  year={2006},
  publisher={Elsevier}
}
@article{sergeraert1994computability,
  title={The computability problem in algebraic topology},
  author={Sergeraert, Francis},
  journal={Advances in Mathematics},
  volume={104},
  number={1},
  pages={1--29},
  year={1994},
  publisher={Academic Press}
}
@article{taubin1991estimation,
  title={Estimation of planar curves, surfaces, and nonplanar space curves defined by implicit equations with applications to edge and range image segmentation},
  author={Taubin, Gabriel},
  journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  volume={13},
  number={11},
  pages={1115--1138},
  year={1991},
  publisher={Citeseer}
}
@article{droste2002analysis,
  title={On the analysis of the (1+ 1) evolutionary algorithm},
  author={Droste, Stefan and Jansen, Thomas and Wegener, Ingo},
  journal={Theoretical Computer Science},
  volume={276},
  number={1-2},
  pages={51--81},
  year={2002},
  publisher={Elsevier}
}
@inproceedings{cao2020research,
  title={Research progress of zero-shot learning beyond computer vision},
  author={Cao, Weipeng and Zhou, Cong and Wu, Yuhao and Ming, Zhong and Xu, Zhiwu and Zhang, Jiyong},
  booktitle={Algorithms and Architectures for Parallel Processing: 20th International Conference, ICA3PP 2020, New York City, NY, USA, October 2--4, 2020, Proceedings, Part II 20},
  pages={538--551},
  year={2020},
  organization={Springer}
}
@article{wang2019survey,
  title={A survey of zero-shot learning: Settings, methods, and applications},
  author={Wang, Wei and Zheng, Vincent W and Yu, Han and Miao, Chunyan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--37},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}
@inproceedings{filakovsky2018computing,
  title={Computing simplicial representatives of homotopy group elements},
  author={Filakovsky, Marek and Franek, Peter and Wanger, Uli and Zhechev, Stephan},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1135--1151},
  year={2018},
  organization={SIAM}
}
@inproceedings{anick1987computational,
  title={On the computational complexity of rational homotopy},
  author={Anick, DJ},
  booktitle={Homotopy Theory: Proceedings of the Durham Symposium 1985},
  volume={117},
  pages={1},
  year={1987},
  organization={Cambridge University Press}
}

@ARTICLE{hahn2020theoretical,
  title     = "Theoretical limitations of self-attention in neural sequence
               models",
  author    = "Hahn, Michael",
  abstract  = "Transformers are emerging as the new workhorse of NLP, showing
               great success across tasks. Unlike LSTMs, transformers process
               input sequences entirely through self-attention. Previous work
               has suggested that the computational capabilities of
               self-attention to process hierarchical structures are limited.
               In this work, we mathematically investigate the computational
               power of self-attention to model formal languages. Across both
               soft and hard attention, we show strong theoretical limitations
               of the computational abilities of self-attention, finding that
               it cannot model periodic finite-state languages, nor
               hierarchical structure, unless the number of layers or heads
               increases with input length. These limitations seem surprising
               given the practical success of self-attention and the prominent
               role assigned to hierarchical structure in linguistics,
               suggesting that natural language can be approximated well with
               models that are too weak for the formal languages typically
               assumed in theoretical linguistics.",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  8,
  pages     = "156--171",
  month     =  dec,
  year      =  2020,
  language  = "en"
}

@misc{ebrahimi2020selfattention,
      title={How Can Self-Attention Networks Recognize Dyck-n Languages?}, 
      author={Javid Ebrahimi and Dhruv Gelda and Wei Zhang},
      year={2020},
      eprint={2010.04303},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{weiss2021thinking,
  title = 	 {Thinking Like Transformers},
  author =       {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11080--11090},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/weiss21a.html},
  abstract = 	 {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.}
}

@misc{weiss2018practical,
      title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition}, 
      author={Gail Weiss and Yoav Goldberg and Eran Yahav},
      year={2018},
      eprint={1805.04908},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{suzgun2019memoryaugmented,
      title={Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages}, 
      author={Mirac Suzgun and Sebastian Gehrmann and Yonatan Belinkov and Stuart M. Shieber},
      year={2019},
      eprint={1911.03329},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@book{goulden2004combinatorial,
  title={Combinatorial enumeration},
  author={Goulden, Ian P and Jackson, David M},
  year={2004},
  publisher={Courier Corporation}
}
@article{curtis1971simplicial,
  title={Simplicial homotopy theory},
  author={Curtis, Edward B},
  journal={Advances in Mathematics},
  volume={6},
  number={2},
  pages={107--209},
  year={1971},
  publisher={Elsevier}
}
@article{tangora1990computing,
  title={Computing with the lambda algebra},
  author={Tangora, Martin C},
  journal={language},
  year={1990}
}
@article{eilenberg1950semi,
  title={Semi-simplicial complexes and singular homology},
  author={Eilenberg, Samuel and Zilber, Joseph A},
  journal={Annals of Mathematics},
  pages={499--513},
  year={1950},
  publisher={JSTOR}
}
@book{quillen2006homotopical,
  title={Homotopical algebra},
  author={Quillen, Daniel G},
  volume={43},
  year={2006},
  publisher={Springer}
}
@book{gabriel2012calculus,
  title={Calculus of fractions and homotopy theory},
  author={Gabriel, Peter and Zisman, Michel},
  volume={35},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{moore1954homotopie,
  title={Homotopie des complexes monoidaux, I},
  author={Moore, John C},
  journal={Seminaire Henri Cartan},
  volume={7},
  number={2},
  pages={1--8},
  year={1954}
}
@article{awodey2018cubical,
  title={A cubical model of homotopy type theory},
  author={Awodey, Steve},
  journal={Annals of Pure and Applied Logic},
  volume={169},
  number={12},
  pages={1270--1294},
  year={2018},
  publisher={Elsevier}
}
@article{tonks1992cubical,
  title={Cubical groups which are Kan},
  author={Tonks, Andrew P},
  journal={Journal of pure and applied algebra},
  volume={81},
  number={1},
  pages={83--87},
  year={1992},
  publisher={Elsevier}
}
@article{quillen1994group,
  title={On the group completion of a simplicial monoid},
  author={Quillen, Daniel},
  journal={preprint},
  year={1994}
}
@book{kargapolov1979fundamentals,
  title={Fundamentals of the Theory of Groups},
  author={Kargapolov, Mikhail Ivanovich and Merzljakov, Jurij Ivanovi{\v{c}}},
  volume={62},
  year={1979},
  publisher={Springer}
}
@article{hilton1988brief,
  title={A brief, subjective history of homology and homotopy theory in this century},
  author={Hilton, Peter},
  journal={Mathematics magazine},
  volume={61},
  number={5},
  pages={282--291},
  year={1988},
  publisher={Taylor \& Francis}
}
@book{mac2013categories,
  title={Categories for the working mathematician},
  author={Mac Lane, Saunders},
  volume={5},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{zubkov1998matrix,
  title={On a matrix representation of a free group},
  author={Zubkov, Alexandr Nikolaevich},
  journal={Mathematical Notes},
  volume={64},
  pages={745--752},
  year={1998},
  publisher={Springer}
}
@inproceedings{sarkar2012low,
  title={Low distortion delaunay embedding of trees in hyperbolic plane},
  author={Sarkar, Rik},
  booktitle={Graph Drawing: 19th International Symposium, GD 2011, Eindhoven, The Netherlands, September 21-23, 2011, Revised Selected Papers 19},
  pages={355--366},
  year={2012},
  organization={Springer}
}
@article{killoran2017generating,
  title={Generating and designing DNA with deep generative models},
  author={Killoran, Nathan and Lee, Leo J and Delong, Andrew and Duvenaud, David and Frey, Brendan J},
  journal={arXiv preprint arXiv:1712.06148},
  year={2017}
}
@article{bartholdi2015commutator,
  title={On commutator length in free groups},
  author={Bartholdi, Laurent and Fialkovski, Danil and Ivanov, Sergei O},
  journal={arXiv preprint arXiv:1504.04261},
  year={2015}
}
@article{hall1950basis,
  title={A basis for free Lie rings and higher commutators in free groups},
  author={Hall, Marshall},
  journal={Proceedings of the American Mathematical Society},
  volume={1},
  number={5},
  pages={575--581},
  year={1950}
}
