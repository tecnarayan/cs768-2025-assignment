\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2011)Acar, Dunlavy, Kolda, and M{\o}rup]{acar2011scalable}
Evrim Acar, Daniel~M Dunlavy, Tamara~G Kolda, and Morten M{\o}rup.
\newblock Scalable tensor factorizations for incomplete data.
\newblock \emph{Chemometrics and Intelligent Laboratory Systems}, 106\penalty0
  (1):\penalty0 41--56, 2011.

\bibitem[Ahmadi-Asl et~al.(2021)Ahmadi-Asl, Abukhovich, Asante-Mensah,
  Cichocki, Phan, Tanaka, and Oseledets]{ahmadi2021randomized}
Salman Ahmadi-Asl, Stanislav Abukhovich, Maame~G Asante-Mensah, Andrzej
  Cichocki, Anh~Huy Phan, Tohishisa Tanaka, and Ivan Oseledets.
\newblock Randomized algorithms for computation of {T}ucker decomposition and
  higher order {SVD} ({HOSVD}).
\newblock \emph{IEEE Access}, 9:\penalty0 28684--28706, 2021.

\bibitem[Alaoui and Mahoney(2015)]{ahmed2015fast}
Ahmed Alaoui and Michael~W. Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~28, 2015.

\bibitem[Alman and Williams(2021)]{alman2021refined}
Josh Alman and Virginia~Vassilevska Williams.
\newblock A refined laser method and faster matrix multiplication.
\newblock In \emph{Proceedings of the 2021 ACM-SIAM Symposium on Discrete
  Algorithms}, pages 522--539. SIAM, 2021.

\bibitem[Arriaga and Vempala(2006)]{arriaga2006algorithmic}
Rosa~I Arriaga and Santosh Vempala.
\newblock An algorithmic theory of learning: Robust concepts and random
  projection.
\newblock \emph{Machine learning}, 63\penalty0 (2):\penalty0 161--182, 2006.

\bibitem[Avron et~al.(2017)Avron, Clarkson, and Woodruff]{avron2017sharper}
Haim Avron, Kenneth~L. Clarkson, and David~P. Woodruff.
\newblock Sharper bounds for regularized data fitting.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques (APPROX/RANDOM 2017)}, volume~81,
  pages 27:1--27:22, 2017.

\bibitem[Bader and Kolda(2021)]{matlab}
Brett~W. Bader and Tamara~G. Kolda.
\newblock Tensor toolbox for {MATLAB}, version 3.2.1.
\newblock \url{https://www.tensortoolbox.org/}, 2021.

\bibitem[Balu and Furon(2016)]{balu2016differentially}
Raghavendran Balu and Teddy Furon.
\newblock Differentially private matrix factorization using sketching
  techniques.
\newblock In \emph{Proceedings of the 4th ACM Workshop on Information Hiding
  and Multimedia Security}, pages 57--62, 2016.

\bibitem[Battaglino et~al.(2018)Battaglino, Ballard, and
  Kolda]{battaglino2018practical}
Casey Battaglino, Grey Ballard, and Tamara~G Kolda.
\newblock A practical randomized cp tensor decomposition.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 39\penalty0
  (2):\penalty0 876--901, 2018.

\bibitem[Chaudhuri et~al.(2011)Chaudhuri, Monteleoni, and
  Sarwate]{chaudhuri2011differentially}
Kamalika Chaudhuri, Claire Monteleoni, and Anand~D Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (3), 2011.

\bibitem[Che and Wei(2019)]{che2019randomized}
Maolin Che and Yimin Wei.
\newblock Randomized algorithms for the approximations of tucker and the tensor
  train decompositions.
\newblock \emph{Advances in Computational Mathematics}, 45\penalty0
  (1):\penalty0 395--428, 2019.

\bibitem[Chen and Price(2019)]{chen2019active}
Xue Chen and Eric Price.
\newblock Active regression via linear-sample sparsification.
\newblock In \emph{Conference on Learning Theory}, pages 663--695. PMLR, 2019.

\bibitem[Cheng et~al.(2016)Cheng, Peng, Liu, and Perros]{cheng2016spals}
Dehua Cheng, Richard Peng, Yan Liu, and Ioakeim Perros.
\newblock {SPALS}: {F}ast alternating least squares via implicit leverage
  scores sampling.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 721--729, 2016.

\bibitem[Chowdhury et~al.(2018)Chowdhury, Yang, and
  Drineas]{chowdhury2018iterative}
Agniva Chowdhury, Jiasen Yang, and Petros Drineas.
\newblock An iterative, sketching-based framework for ridge regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  989--998. PMLR, 2018.

\bibitem[Cohen et~al.(2015)Cohen, Lee, Musco, Musco, Peng, and
  Sidford]{cohen2015uniform}
Michael~B Cohen, Yin~Tat Lee, Cameron Musco, Christopher Musco, Richard Peng,
  and Aaron Sidford.
\newblock Uniform sampling for matrix approximation.
\newblock In \emph{Proceedings of the 2015 Conference on Innovations in
  Theoretical Computer Science}, pages 181--190, 2015.

\bibitem[Cohen et~al.(2017)Cohen, Musco, and Musco]{cohen2017input}
Michael~B Cohen, Cameron Musco, and Christopher Musco.
\newblock Input sparsity time low-rank approximation via ridge leverage score
  sampling.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pages 1758--1777. SIAM, 2017.

\bibitem[Diao et~al.(2018)Diao, Song, Sun, and Woodruff]{diao2018sketching}
Huaian Diao, Zhao Song, Wen Sun, and David Woodruff.
\newblock Sketching for kronecker product regression and p-splines.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1299--1308. PMLR, 2018.

\bibitem[Diao et~al.(2019)Diao, Jayaram, Song, Sun, and
  Woodruff]{diao2019optimal}
Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff.
\newblock Optimal sketching for {K}ronecker product regression and low rank
  approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Drineas et~al.(2006)Drineas, Kannan, and Mahoney]{drineas2006fast}
Petros Drineas, Ravi Kannan, and Michael~W Mahoney.
\newblock Fast {M}onte {C}arlo algorithms for matrices {I}: {A}pproximating
  matrix multiplication.
\newblock \emph{SIAM Journal on Computing}, 36\penalty0 (1):\penalty0 132--157,
  2006.

\bibitem[Drineas et~al.(2011)Drineas, Mahoney, Muthukrishnan, and
  Sarl{\'o}s]{drineas2011faster}
Petros Drineas, Michael~W. Mahoney, Shan Muthukrishnan, and Tam{\'a}s
  Sarl{\'o}s.
\newblock Faster least squares approximation.
\newblock \emph{Numerische Mathematik}, 117\penalty0 (2):\penalty0 219--249,
  2011.

\bibitem[Ehrlacher et~al.(2021)Ehrlacher, Grigori, Lombardi, and
  Song]{ehrlacher2021adaptive}
Virginie Ehrlacher, Laura Grigori, Damiano Lombardi, and Hao Song.
\newblock Adaptive hierarchical subtensor partitioning for tensor compression.
\newblock \emph{SIAM Journal on Scientific Computing}, 43\penalty0
  (1):\penalty0 A139--A163, 2021.

\bibitem[Fahrbach et~al.(2021)Fahrbach, Ghadiri, and Fu]{fahrbach2021fast}
Matthew Fahrbach, Mehrdad Ghadiri, and Thomas Fu.
\newblock Fast low-rank tensor decomposition by ridge leverage score sampling.
\newblock \emph{arXiv preprint arXiv:2107.10654}, 2021.

\bibitem[Filipovi{\'c} and Juki{\'c}(2015)]{filipovic2015tucker}
Marko Filipovi{\'c} and Ante Juki{\'c}.
\newblock Tucker factorization with missing data with application to
  low-$n$-rank tensor completion.
\newblock \emph{Multidimensional systems and signal processing}, 26\penalty0
  (3):\penalty0 677--692, 2015.

\bibitem[Frandsen and Ge(2020)]{frandsen2020optimization}
Abraham Frandsen and Rong Ge.
\newblock Optimization landscape of {T}ucker decomposition.
\newblock \emph{Mathematical Programming}, pages 1--26, 2020.

\bibitem[Gall and Urrutia(2018)]{gall2018improved}
Fran{\c{c}}ois~Le Gall and Florent Urrutia.
\newblock Improved rectangular matrix multiplication using powers of the
  coppersmith-winograd tensor.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1029--1046. SIAM, 2018.

\bibitem[Ghadiri et~al.(2023{\natexlab{a}})Ghadiri, Fahrbach, Fu, and
  Mirrokni]{ghadiri2023approximately}
Mehrdad Ghadiri, Matthew Fahrbach, Gang Fu, and Vahab Mirrokni.
\newblock Approximately optimal core shapes for tensor decompositions.
\newblock \emph{arXiv preprint arXiv:2302.03886}, 2023{\natexlab{a}}.

\bibitem[Ghadiri et~al.(2023{\natexlab{b}})Ghadiri, Peng, and
  Vempala]{ghadiri2023bit}
Mehrdad Ghadiri, Richard Peng, and Santosh~S Vempala.
\newblock The bit complexity of efficient continuous optimization.
\newblock \emph{arXiv preprint arXiv:2304.02124}, 2023{\natexlab{b}}.

\bibitem[Grasedyck et~al.(2015)Grasedyck, Kluge, and
  Kramer]{grasedyck2015variants}
Lars Grasedyck, Melanie Kluge, and Sebastian Kramer.
\newblock Variants of alternating least squares tensor completion in the tensor
  train format.
\newblock \emph{SIAM Journal on Scientific Computing}, 37\penalty0
  (5):\penalty0 A2424--A2450, 2015.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf
  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,
  Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
  Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez
  del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant,
  Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
  Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, September
  2020.
\newblock \doi{10.1038/s41586-020-2649-2}.
\newblock URL \url{https://doi.org/10.1038/s41586-020-2649-2}.

\bibitem[Hashemizadeh et~al.(2020)Hashemizadeh, Liu, Miller, and
  Rabusseau]{hashemizadeh2020adaptive}
Meraj Hashemizadeh, Michelle Liu, Jacob Miller, and Guillaume Rabusseau.
\newblock Adaptive tensor learning with tensor networks.
\newblock \emph{Workshop on Quantum Tensor Networks in Machine Learning, 34th
  Conference on Neural Information Processing Systems}, 2020.

\bibitem[Hillar and Lim(2013)]{hillar2013most}
Christopher~J Hillar and Lek-Heng Lim.
\newblock Most tensor problems are {NP}-hard.
\newblock \emph{Journal of the ACM (JACM)}, 60\penalty0 (6):\penalty0 1--39,
  2013.

\bibitem[Jain and Oh(2014)]{jain2014provable}
Prateek Jain and Sewoong Oh.
\newblock Provable tensor factorization with missing data.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Jain et~al.(2013)Jain, Netrapalli, and Sanghavi]{jain2013low}
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In \emph{Proceedings of the Forty-Fifth Annual ACM Symposium on
  Theory of Computing}, pages 665--674, 2013.

\bibitem[James(1978)]{james1978generalised}
M~James.
\newblock The generalised inverse.
\newblock \emph{The Mathematical Gazette}, 62\penalty0 (420):\penalty0
  109--114, 1978.

\bibitem[Jang and Kang(2021)]{jang2021fast}
Jun-Gi Jang and U~Kang.
\newblock Fast and memory-efficient tucker decomposition for answering diverse
  time range queries.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 725--735, 2021.

\bibitem[Johnson and Lindenstrauss(1984)]{johnson1984extensions}
William~B Johnson and Joram Lindenstrauss.
\newblock Extensions of lipschitz mappings into a {H}ilbert space.
\newblock \emph{Contemporary mathematics}, 26, 1984.

\bibitem[Kacham and Woodruff(2022)]{kacham2022sketching}
Praneeth Kacham and David Woodruff.
\newblock Sketching algorithms and lower bounds for ridge regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  10539--10556. PMLR, 2022.

\bibitem[Kasai and Mishra(2016)]{kasai2016low}
Hiroyuki Kasai and Bamdev Mishra.
\newblock Low-rank tensor completion: a {R}iemannian manifold preconditioning
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  1012--1021. PMLR, 2016.

\bibitem[Kolda and Bader(2009)]{kolda2009tensor}
Tamara~G. Kolda and Brett~W. Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM Review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Kossaifi et~al.(2019)Kossaifi, Panagakis, Anandkumar, and
  Pantic]{kossaifi2019tensorly}
Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic.
\newblock Tensorly: {T}ensor learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 20\penalty0 (26),
  2019.

\bibitem[Kressner et~al.(2014)Kressner, Steinlechner, and
  Vandereycken]{kressner2014low}
Daniel Kressner, Michael Steinlechner, and Bart Vandereycken.
\newblock Low-rank tensor completion by {R}iemannian optimization.
\newblock \emph{BIT Numerical Mathematics}, 54\penalty0 (2):\penalty0 447--468,
  2014.

\bibitem[Larsen and Kolda(2022{\natexlab{a}})]{larsen2020practical}
Brett~W. Larsen and Tamara~G Kolda.
\newblock Practical leverage-based sampling for low-rank tensor decomposition.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 43\penalty0
  (3):\penalty0 1488--1517, 2022{\natexlab{a}}.

\bibitem[Larsen and Kolda(2022{\natexlab{b}})]{larsen2022sketching}
Brett~W. Larsen and Tamara~G. Kolda.
\newblock Sketching matrix least squares via leverage scores estimates.
\newblock \emph{arXiv preprint arXiv:2201.10638}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2013)Li, Miller, and Peng]{li2013iterative}
Mu~Li, Gary~L Miller, and Richard Peng.
\newblock Iterative row sampling.
\newblock In \emph{2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science}, pages 127--136. IEEE, 2013.

\bibitem[Li et~al.(2019)Li, Ton, Oglic, and Sejdinovic]{li2019towards}
Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic.
\newblock Towards a unified analysis of random {F}ourier features.
\newblock In \emph{International Conference on Machine Learning}, pages
  3905--3914. PMLR, 2019.

\bibitem[Liu and Moitra(2020)]{liu2020tensor}
Allen Liu and Ankur Moitra.
\newblock Tensor completion made practical.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18905--18916, 2020.

\bibitem[Liu et~al.(2012)Liu, Musialski, Wonka, and Ye]{liu2012tensor}
Ji~Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye.
\newblock Tensor completion for estimating missing values in visual data.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (1):\penalty0 208--220, 2012.

\bibitem[Ma and Solomonik(2021)]{ma2021fast}
Linjian Ma and Edgar Solomonik.
\newblock Fast and accurate randomized algorithms for low-rank tensor
  decompositions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Malik and Becker(2018)]{malik2018low}
Osman~Asif Malik and Stephen Becker.
\newblock Low-rank {T}ucker decomposition of large tensors using
  {T}ensor{S}ketch.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 10096--10106, 2018.

\bibitem[Malik and Becker(2021)]{malik2021sampling}
Osman~Asif Malik and Stephen Becker.
\newblock A sampling-based method for tensor ring decomposition.
\newblock In \emph{International Conference on Machine Learning}, pages
  7400--7411. PMLR, 2021.

\bibitem[Marco et~al.(2019)Marco, Mart{\'\i}nez, and Via{\~n}a]{marco2019least}
Ana Marco, Jos{\'e}-Javier Mart{\'\i}nez, and Raquel Via{\~n}a.
\newblock Least squares problems involving generalized kronecker products and
  application to bivariate polynomial regression.
\newblock \emph{Numerical Algorithms}, 82\penalty0 (1):\penalty0 21--39, 2019.

\bibitem[McCurdy(2018)]{mccurdy2018ridge}
Shannon McCurdy.
\newblock Ridge regression and provable deterministic ridge leverage score
  sampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Musco and Musco(2017)]{musco2017recursive}
Cameron Musco and Christopher Musco.
\newblock Recursive sampling for the {N}ystr\"om method.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Nascimento et~al.(2016)Nascimento, Amano, and
  Foster]{nascimento2016spatial}
S{\'e}rgio~MC Nascimento, Kinjiro Amano, and David~H Foster.
\newblock Spatial distributions of local illumination color in natural scenes.
\newblock \emph{Vision research}, 120:\penalty0 39--44, 2016.

\bibitem[Nene et~al.(1996)Nene, Nayar, Murase, et~al.]{nene1996columbia}
Sameer~A Nene, Shree~K Nayar, Hiroshi Murase, et~al.
\newblock Columbia object image library (coil-20).
\newblock 1996.

\bibitem[Nimishakavi et~al.(2018)Nimishakavi, Jawanpuria, and
  Mishra]{madhav2018dual}
Madhav Nimishakavi, Pratik~Kumar Jawanpuria, and Bamdev Mishra.
\newblock A dual framework for low-rank tensor completion.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Pagh(2013)]{pagh2013compressed}
Rasmus Pagh.
\newblock Compressed matrix multiplication.
\newblock \emph{ACM Transactions on Computation Theory (TOCT)}, 5\penalty0
  (3):\penalty0 1--17, 2013.

\bibitem[Planitz(1979)]{planitz19793}
M.~Planitz.
\newblock Inconsistent systems of linear equations.
\newblock \emph{The Mathematical Gazette}, 63\penalty0 (425):\penalty0
  181--185, 1979.

\bibitem[Rabanser et~al.(2017)Rabanser, Shchur, and
  G{\"u}nnemann]{rabanser2017introduction}
Stephan Rabanser, Oleksandr Shchur, and Stephan G{\"u}nnemann.
\newblock Introduction to tensor decompositions and their applications in
  machine learning.
\newblock \emph{arXiv preprint arXiv:1711.10781}, 2017.

\bibitem[Reddy et~al.(2022)Reddy, Song, and Zhang]{reddy2022dynamic}
Aravind Reddy, Zhao Song, and Lichen Zhang.
\newblock Dynamic tensor product regression.
\newblock \emph{arXiv preprint arXiv:2210.03961}, 2022.

\bibitem[Saad(2003)]{saad2003iterative}
Yousef Saad.
\newblock \emph{Iterative Methods for Sparse Linear Systems}.
\newblock SIAM, 2003.

\bibitem[Sidiropoulos et~al.(2017)Sidiropoulos, De~Lathauwer, Fu, Huang,
  Papalexakis, and Faloutsos]{sidiropoulos2017tensor}
Nicholas~D Sidiropoulos, Lieven De~Lathauwer, Xiao Fu, Kejun Huang, Evangelos~E
  Papalexakis, and Christos Faloutsos.
\newblock Tensor decomposition for signal processing and machine learning.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (13):\penalty0 3551--3582, 2017.

\bibitem[Song et~al.(2019)Song, Woodruff, and Zhong]{song2019relative}
Zhao Song, David~P Woodruff, and Peilin Zhong.
\newblock Relative error tensor low rank approximation.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2772--2789. SIAM, 2019.

\bibitem[Sun et~al.(2020)Sun, Guo, Luo, Tropp, and Udell]{sun2020low}
Yiming Sun, Yang Guo, Charlene Luo, Joel Tropp, and Madeleine Udell.
\newblock Low-rank {T}ucker approximation of a tensor from streaming data.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1123--1150, 2020.

\bibitem[Traore et~al.(2019)Traore, Berar, and
  Rakotomamonjy]{traore2019singleshot}
Abraham Traore, Maxime Berar, and Alain Rakotomamonjy.
\newblock Singleshot: {A} scalable {T}ucker tensor decomposition.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[van~den Brand and Nanongkai(2019)]{van2019dynamic}
Jan van~den Brand and Danupon Nanongkai.
\newblock Dynamic approximate shortest paths and beyond: Subquadratic and
  worst-case update time.
\newblock In \emph{2019 IEEE 60th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 436--455. IEEE, 2019.

\bibitem[Wang et~al.(2017)Wang, Gittens, and Mahoney]{wang2017sketched}
Shusen Wang, Alex Gittens, and Michael~W Mahoney.
\newblock Sketched ridge regression: {O}ptimization perspective, statistical
  perspective, and model averaging.
\newblock In \emph{International Conference on Machine Learning}, pages
  3608--3616. PMLR, 2017.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
David~P. Woodruff.
\newblock \emph{Sketching as a Tool for Numerical Linear Algebra}.
\newblock 2014.

\bibitem[Xiao and Yang(2021)]{xiao2021rank}
Chuanfu Xiao and Chao Yang.
\newblock A rank-adaptive higher-order orthogonal iteration algorithm for
  truncated {T}ucker decomposition.
\newblock \emph{arXiv preprint arXiv:2110.12564}, 2021.

\bibitem[Yu and Liu(2016)]{yu2016learning}
Rose Yu and Yan Liu.
\newblock Learning from multiway data: {S}imple and efficient tensor
  regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  373--381. PMLR, 2016.

\bibitem[Zhang and Ding(2013)]{zhang2013kronecker}
Huamin Zhang and Feng Ding.
\newblock On the kronecker products and their applications.
\newblock \emph{Journal of Applied Mathematics}, 2013, 2013.

\bibitem[Zhou et~al.(2014)Zhou, Cichocki, and Xie]{zhou2014decomposition}
Guoxu Zhou, Andrzej Cichocki, and Shengli Xie.
\newblock Decomposition of big tensors with low multilinear rank.
\newblock \emph{arXiv preprint arXiv:1412.1885}, 2014.

\bibitem[Zhou et~al.(2013)Zhou, Li, and Zhu]{zhou2013tensor}
Hua Zhou, Lexin Li, and Hongtu Zhu.
\newblock Tensor regression with applications in neuroimaging data analysis.
\newblock \emph{Journal of the American Statistical Association}, 108\penalty0
  (502):\penalty0 540--552, 2013.

\end{thebibliography}
