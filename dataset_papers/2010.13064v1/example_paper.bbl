\begin{thebibliography}{10}

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational {B}ayes,'' {\em arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{dinh2014nice}
L.~Dinh, D.~Krueger, and Y.~Bengio, ``Nice: Non-linear independent components
  estimation,'' {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{nalisnick2018do}
E.~Nalisnick, A.~Matsukawa, Y.~W. Teh, D.~Gorur, and B.~Lakshminarayanan, ``Do
  deep generative models know what they don't know?,'' in {\em International
  Conference on Learning Representations}, 2019.

\bibitem{hendrycks2019deep}
D.~Hendrycks, M.~Mazeika, and T.~Dietterich, ``Deep anomaly detection with
  outlier exposure,'' {\em arXiv preprint arXiv:1812.04606}, 2018.

\bibitem{du2019implicit}
Y.~Du and I.~Mordatch, ``Implicit generation and modeling with energy based
  models,'' in {\em Advances in Neural Information Processing Systems 32},
  pp.~3608--3618, 2019.

\bibitem{grathwohl2019your}
W.~Grathwohl, K.-C. Wang, J.-H. Jacobsen, D.~Duvenaud, M.~Norouzi, and
  K.~Swersky, ``Your classifier is secretly an energy based model and you
  should treat it like one,'' {\em arXiv preprint arXiv:1912.03263}, 2019.

\bibitem{maaloe2019biva}
L.~Maal{\o}e, M.~Fraccaro, V.~Li{\'e}vin, and O.~Winther, ``{BIVA}: A very deep
  hierarchy of latent variables for generative modeling,'' in {\em Advances in
  neural information processing systems}, pp.~6548--6558, 2019.

\bibitem{louizos2019the}
C.~Louizos, X.~Shi, K.~Schutte, and M.~Welling, ``The functional neural
  process,'' in {\em Advances in Neural Information Processing Systems 32}
  (H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\'Alch\'{e} Buc, E.~Fox, and
  R.~Garnett, eds.), pp.~8746--8757, Curran Associates, Inc., 2019.

\bibitem{nalisnick2019hybrid}
E.~Nalisnick, A.~Matsukawa, Y.~W. Teh, D.~Gorur, and B.~Lakshminarayanan,
  ``Hybrid models with deep and invertible features,'' in {\em International
  Conference on Machine Learning}, pp.~4723--4732, 2019.

\bibitem{butepage2019modeling}
J.~B{\"u}tepage, P.~Poklukar, and D.~Kragic, ``Modeling assumptions and
  evaluation schemes: On the assessment of deep latent variable models,'' in
  {\em The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  Workshops}, 2019.

\bibitem{choi2018waic}
H.~Choi, E.~Jang, and A.~A. Alemi, ``{WAIC}, but why? generative ensembles for
  robust anomaly detection,'' 2018.

\bibitem{cover2012elements}
T.~M. Cover and J.~A. Thomas, {\em Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem{bobkov2011concentration}
S.~Bobkov, M.~Madiman, {\em et~al.}, ``Concentration of the information in data
  with log-concave distributions,'' {\em The Annals of Probability}, vol.~39,
  no.~4, pp.~1528--1543, 2011.

\bibitem{box1970distribution}
G.~E. Box and D.~A. Pierce, ``Distribution of residual autocorrelations in
  autoregressive-integrated moving average time series models,'' {\em Journal
  of the American statistical Association}, vol.~65, no.~332, pp.~1509--1526,
  1970.

\bibitem{chen2018pixelsnail}
X.~Chen, N.~Mishra, M.~Rohaninejad, and P.~Abbeel, ``{P}ixel{SNAIL}: An
  improved autoregressive generative model,'' in {\em International Conference
  on Machine Learning}, pp.~864--872, 2018.

\bibitem{salimans2017pixelcnnpp}
T.~Salimans, A.~Karpathy, X.~Chen, and D.~P. Kingma, ``Pixel{CNN}++: Improving
  the {P}ixel{CNN} with discretized logistic mixture likelihood and other
  modifications,'' {\em arXiv preprint arXiv:1701.05517}, 2017.

\bibitem{dai2018diagnosing}
B.~Dai and D.~Wipf, ``Diagnosing and enhancing {VAE} models,'' in {\em
  International Conference on Learning Representations}, 2018.

\bibitem{serra2020input}
J.~Serr{\`a}, D.~{\'A}lvarez, V.~G{\'o}mez, O.~Slizovskaia, J.~F.
  N{\'u}{\~n}ez, and J.~Luque, ``Input complexity and out-of-distribution
  detection with likelihood-based generative models,'' in {\em International
  Conference on Learning Representations}, 2020.

\bibitem{nalisnick2019detecting}
E.~Nalisnick, A.~Matsukawa, Y.~W. Teh, and B.~Lakshminarayanan, ``Detecting
  out-of-distribution inputs to deep generative models using typicality,'' {\em
  arXiv preprint arXiv:1906.02994}, 2019.

\bibitem{ly2016harold}
A.~Ly, J.~Verhagen, and E.-J. Wagenmakers, ``Harold {J}effreysâ€™s default
  {B}ayes factor hypothesis tests: Explanation, extension, and application in
  psychology,'' {\em Journal of Mathematical Psychology}, vol.~72, pp.~19--32,
  2016.

\bibitem{torralba200880}
A.~Torralba, R.~Fergus, and W.~T. Freeman, ``80 million tiny images: A large
  data set for nonparametric object and scene recognition,'' {\em IEEE
  transactions on pattern analysis and machine intelligence}, vol.~30, no.~11,
  pp.~1958--1970, 2008.

\bibitem{brock2018large}
A.~Brock, J.~Donahue, and K.~Simonyan, ``Large scale {GAN} training for high
  fidelity natural image synthesis,'' in {\em International Conference on
  Learning Representations}, 2018.

\bibitem{zech2018variable}
J.~R. Zech, M.~A. Badgeley, M.~Liu, A.~B. Costa, J.~J. Titano, and E.~K.
  Oermann, ``Variable generalization performance of a deep learning model to
  detect pneumonia in chest radiographs: a cross-sectional study,'' {\em PLoS
  medicine}, vol.~15, no.~11, 2018.

\bibitem{nijkamp2019anatomy}
E.~Nijkamp, M.~Hill, T.~Han, S.-C. Zhu, and Y.~N. Wu, ``On the anatomy of
  mcmc-based maximum likelihood learning of energy-based models,'' {\em arXiv
  preprint arXiv:1903.12370}, 2019.

\bibitem{ren2019likelihood}
J.~Ren, P.~J. Liu, E.~Fertig, J.~Snoek, R.~Poplin, M.~A. DePristo, J.~V.
  Dillon, and B.~Lakshminarayanan, ``Likelihood ratios for out-of-distribution
  detection,'' {\em arXiv preprint arXiv:1906.02845}, 2019.

\bibitem{alemi2018uncertainty}
A.~A. Alemi, I.~Fischer, and J.~V. Dillon, ``Uncertainty in the variational
  information bottleneck,'' {\em arXiv preprint arXiv:1807.00906}, 2018.

\bibitem{liang2017enhancing}
S.~Liang, Y.~Li, and R.~Srikant, ``Enhancing the reliability of
  out-of-distribution image detection in neural networks,'' {\em arXiv preprint
  arXiv:1706.02690}, 2017.

\bibitem{lee2018simple}
K.~Lee, K.~Lee, H.~Lee, and J.~Shin, ``A simple unified framework for detecting
  out-of-distribution samples and adversarial attacks,'' in {\em Advances in
  Neural Information Processing Systems}, pp.~7167--7177, 2018.

\bibitem{bergman2020classification}
L.~Bergman and Y.~Hoshen, ``Classification-based anomaly detection for general
  data,'' {\em arXiv preprint arXiv:2005.02359}, 2020.

\bibitem{golan2018deep}
I.~Golan and R.~El-Yaniv, ``Deep anomaly detection using geometric
  transformations,'' in {\em Advances in Neural Information Processing
  Systems}, pp.~9758--9769, 2018.

\bibitem{hendrycks2019using}
D.~Hendrycks, M.~Mazeika, S.~Kadavath, and D.~Song, ``Using self-supervised
  learning can improve model robustness and uncertainty,'' in {\em Advances in
  Neural Information Processing Systems}, pp.~15663--15674, 2019.

\bibitem{ruff2018deep}
L.~Ruff, R.~Vandermeulen, N.~Goernitz, L.~Deecke, S.~A. Siddiqui, A.~Binder,
  E.~M{\"u}ller, and M.~Kloft, ``Deep one-class classification,'' in {\em
  International conference on machine learning}, pp.~4393--4402, 2018.

\bibitem{sabeti2019data}
E.~Sabeti and A.~Host-Madsen, ``Data discovery and anomaly detection using
  atypicality for real-valued data,'' {\em Entropy}, vol.~21, no.~3, p.~219,
  2019.

\bibitem{grunwald2007minimum}
P.~D. Gr{\"u}nwald and A.~Grunwald, {\em The minimum description length
  principle}.
\newblock MIT press, 2007.

\bibitem{cortes2005confidence}
C.~Cortes and M.~Mohri, ``Confidence intervals for the area under the roc
  curve,'' in {\em Advances in neural information processing systems},
  pp.~305--312, 2005.

\end{thebibliography}
