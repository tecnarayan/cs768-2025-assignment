\begin{thebibliography}{10}

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with applications to imaging.
\newblock {\em Journal of Mathematical Imaging and Vision}, 40:120--145, 2011.

\bibitem{facchinei2007finite}
F.~Facchinei and J.S. Pang.
\newblock {\em Finite-Dimensional Variational Inequalities and Complementarity Problems}.
\newblock Springer Series in Operations Research and Financial Engineering. Springer New York, 2007.

\bibitem{basar1999dynamic}
T.~Basar and G.J. Olsder.
\newblock {\em Dynamic Noncooperative Game Theory}.
\newblock Classics in Applied Mathematics. Society for Industrial and Applied Mathematics, 1999.

\bibitem{pinto2017robust}
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine Learning}. PMLR, 2017.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2014.

\bibitem{gidel2019variational}
Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{stampacchia1964bilinear}
G.~Stampacchia.
\newblock Formes bilineaires coercitives sur les ensembles convexes.
\newblock In {\em Académie des Sciences de Paris}, volume 258, pages 4413--4416, 1964.

\bibitem{korpelevich1976extragradient}
G.~M. Korpelevich.
\newblock The extragradient method for finding saddle points and other problems.
\newblock {\em Matecon}, 12:747--756, 1976.

\bibitem{popov1980modification}
Leonid~D. Popov.
\newblock A modification of the {Arrow-Hurwicz} method for search of saddle points.
\newblock {\em Mathematical Notes of the Academy of Sciences of the USSR}, 28:845--848, 1980.

\bibitem{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence o(1/t) for variational inequalities with {Lipschitz} continuous monotone operators and smooth convex-concave saddle point problems.
\newblock {\em {SIAM} Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nesterov2007dual}
Yurii Nesterov.
\newblock Dual extrapolation and its applications to solving variational inequalities and related problems.
\newblock {\em Math. Program.}, 109(2–3):319–344, 2007.

\bibitem{rakhlin2013online}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online learning with predictable sequences.
\newblock In Shai Shalev-Shwartz and Ingo Steinwart, editors, {\em Proceedings of the 26th Annual Conference on Learning Theory}, volume~30 of {\em Proceedings of Machine Learning Research}, pages 993--1019, Princeton, NJ, USA, 2013. PMLR.

\bibitem{malitsky2018golden}
Yura Malitsky.
\newblock Golden ratio algorithms for variational inequalities.
\newblock {\em Mathematical Programming}, 184:383 -- 410, 2018.

\bibitem{daskalakis2018training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training {GAN}s with optimism.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{malitsky2020forward}
Yura Malitsky and Matthew~K. Tam.
\newblock A forward-backward splitting method for monotone inclusions without cocoercivity.
\newblock {\em SIAM Journal on Optimization}, 30(2):1451--1472, 2020.

\bibitem{nemirovski1992information}
Arkadi Nemirovski.
\newblock Information-based complexity of linear operator equations.
\newblock {\em Journal of Complexity}, 8(2):153--175, 1992.

\bibitem{ouyang2021lower}
Yuyuan Ouyang and Yangyang Xu.
\newblock Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems.
\newblock {\em Math. Program.}, 185(1–2):1–35, 2021.

\bibitem{Monteiro2010}
Renato D.~C. Monteiro and B.~F. Svaiter.
\newblock On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean.
\newblock {\em SIAM Journal on Optimization}, 20(6):2755--2787, 2010.

\bibitem{Monteiro2012}
Renato D.~C. Monteiro and Benar~F. Svaiter.
\newblock Iteration-complexity of a {Newton} proximal extragradient method for monotone variational inequalities and inclusion problems.
\newblock {\em SIAM Journal on Optimization}, 22(3):914--935, 2012.

\bibitem{bullins2022higher}
Brian Bullins and Kevin~A. Lai.
\newblock Higher-order methods for convex-concave min-max optimization and monotone variational inequalities.
\newblock {\em SIAM Journal on Optimization}, 32(3):2208--2229, 2022.

\bibitem{jiang2022generalized}
Ruichen Jiang and Aryan Mokhtari.
\newblock Generalized optimistic methods for convex-concave saddle point problems.
\newblock {\em arXiv preprint arXiv:2202.09674}, 2022.

\bibitem{adil2022optimal}
Deeksha Adil, Brian Bullins, Arun Jambulapati, and Sushant Sachdeva.
\newblock Optimal methods for higher-order smooth monotone variational inequalities.
\newblock {\em arXiv preprint arXiv:2205.06167}, 2022.

\bibitem{lin2024perseus}
Tianyi Lin and Michael~I Jordan.
\newblock Perseus: A simple and optimal high-order method for variational inequalities.
\newblock {\em Mathematical Programming}, pages 1--42, 2024.

\bibitem{nesterov2021implementable}
Yurii Nesterov.
\newblock Implementable tensor methods in unconstrained convex optimization.
\newblock {\em Math. Program.}, 186(1–2):157–183, 2021.

\bibitem{nesterov2021superfast}
Yurii~E. Nesterov.
\newblock Superfast second-order methods for unconstrained convex optimization.
\newblock {\em J. Optim. Theory Appl.}, 191(1):1--30, 2021.

\bibitem{nesterov2021inexacthigh}
Yurii Nesterov.
\newblock Inexact high-order proximal-point methods with auxiliary search procedure.
\newblock {\em SIAM Journal on Optimization}, 31(4):2807--2828, 2021.

\bibitem{nesterov2022inexactbasic}
Yurii Nesterov.
\newblock Inexact basic tensor methods for some classes of convex optimization problems.
\newblock {\em Optimization Methods and Software}, 37(3):878--906, 2022.

\bibitem{kovalev2024first}
Dmitry Kovalev and Alexander Gasnikov.
\newblock The first optimal acceleration of high-order methods in smooth convex optimization.
\newblock In {\em Proceedings of the 36th International Conference on Neural Information Processing Systems}, Red Hook, NY, USA, 2024. Curran Associates Inc.

\bibitem{nesterov2006cubic}
Yurii Nesterov and Boris Polyak.
\newblock Cubic regularization of {Newton} method and its global performance.
\newblock {\em Mathematical Programming}, 108:177--205, 2006.

\bibitem{HIPNES}
M.~Marques Alves, J.~M. Pereira, and B.~F. Svaiter.
\newblock A search-free $\mathcal{O}(1/k^{3/2})$ homotopy inexact {proximal-Newton} extragradient algorithm for monotone variational inequalities.
\newblock {\em arXiv preprint arXiv:2308.05887}, 2023.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock {\em Journal of Machine Learning Research}, 12(61):2121--2159, 2011.

\bibitem{levy2018online}
Kfir~Y. Levy, Alp Yurtsever, and Volkan Cevher.
\newblock Online adaptive methods, universality and acceleration.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{kavis2019unixgrad}
Ali Kavis, Kfir~Y. Levy, Francis Bach, and Volkan Cevher.
\newblock Unixgrad: A universal, adaptive algorithm with optimal guarantees for constrained optimization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{bach2019universal}
Francis Bach and Kfir~Y Levy.
\newblock A universal algorithm for variational inequalities adaptive to smoothness and noise.
\newblock {\em arXiv preprint arXiv:1902.01637}, 2019.

\bibitem{ene2021adaptive}
Alina Ene, Huy~L Nguyen, and Adrian Vladu.
\newblock Adaptive gradient methods for constrained convex optimization and variational inequalities.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence}, 35(8), 2021.

\bibitem{mokhtari2020convergence}
Aryan Mokhtari, Asuman~E. Ozdaglar, and Sarath Pattathil.
\newblock Convergence rate of ${O}(1/k)$ for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 30(4):3230--3251, 2020.

\bibitem{mokhtari2020unified}
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach.
\newblock In {\em Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}. PMLR, 2020.

\bibitem{martinet1970regularization}
B.~Martinet.
\newblock {R\'egularisation} d'in\'equations variationnelles par approximations successives.
\newblock {\em Revue fran\c{c}aise d'informatique et de recherche op\'erationnelle. S\'erie rouge}, 4(R3):154--158, 1970.

\bibitem{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock {\em SIAM Journal on Control and Optimization}, 14(5):877--898, 1976.

\bibitem{orabona2019modern}
Francesco Orabona.
\newblock A modern introduction to online learning.
\newblock {\em arXiv preprint arXiv:1912.13213}, 2019.

\bibitem{lin2023monotone}
Tianyi Lin and Michael~I Jordan.
\newblock Monotone inclusions, acceleration, and closed-loop control.
\newblock {\em Mathematics of Operations Research}, 48(4):2353--2382, 2023.

\bibitem{Arjevani2019}
Yossi Arjevani, Ohad Shamir, and Ron Shiff.
\newblock Oracle complexity of second-order methods for smooth convex optimization.
\newblock {\em Math. Program.}, 178(1-2):327--360, 2019.

\bibitem{lin2024explicit}
Tianyi Lin, Panayotis Mertikopoulos, and Michael~I. Jordan.
\newblock Explicit second-order min-max optimization methods with optimal convergence guarantee.
\newblock {\em arXiv preprint arXiv:2210.12860}, 2024.

\bibitem{ying2016stochastic}
Yiming Ying, Longyin Wen, and Siwei Lyu.
\newblock Stochastic online {AUC} maximization.
\newblock In {\em Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2016.

\bibitem{shen2018towards}
Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication.
\newblock In {\em Proceedings of the 35th International Conference on Machine Learning}. PMLR, 2018.

\bibitem{ryu2022large}
Ernest~K Ryu and Wotao Yin.
\newblock {\em Large-scale convex optimization: algorithms \& analyses via monotone operators}.
\newblock Cambridge University Press, 2022.

\end{thebibliography}
