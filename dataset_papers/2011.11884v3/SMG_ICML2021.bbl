\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{ahn2020sgd}
Ahn, K., Yun, C., and Sra, S.
\newblock Sgd with shuffling: optimal rates without component convexity and
  large epoch requirements.
\newblock \emph{arXiv preprint arXiv:2006.06946}, 2020.

\bibitem[Bertsekas(2011)]{Bertsekas2011}
Bertsekas, D.
\newblock Incremental proximal methods for large scale convex optimization.
\newblock \emph{Math. Program.}, 129\penalty0 (2):\penalty0 163--195, 2011.

\bibitem[Bottou(2009)]{bottou2009curiously}
Bottou, L.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock In \emph{Proceedings of the symposium on learning and data science,
  Paris}, volume~8, pp.\  2624--2633, 2009.

\bibitem[Bottou(2012)]{bottou2012stochastic}
Bottou, L.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  421--436.
  Springer, 2012.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock {O}ptimization {M}ethods for {L}arge-{S}cale {M}achine {L}earning.
\newblock \emph{SIAM Rev.}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chang \& Lin(2011)Chang and Lin]{LIBSVM}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
Chen, X., Liu, S., Sun, R., and Hong, M.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{ICLR}, 2018.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
Chollet, F. et~al.
\newblock Keras, 2015.
\newblock URL \url{https://github.com/fchollet/keras}.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1646--1654, 2014.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Dozat, T.
\newblock Incorporating nesterov momentum into {ADAM}.
\newblock \emph{ICLR Workshop}, 1:\penalty0 2013–--2016, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Gürbüzbalaban et~al.(2019)Gürbüzbalaban, Ozdaglar, and
  Parrilo]{Gurbuzbalaban2019}
Gürbüzbalaban, M., Ozdaglar, A., and Parrilo, P.~A.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, Oct 2019.
\newblock ISSN 1436-4646.
\newblock \doi{10.1007/s10107-019-01440-w}.
\newblock URL \url{http://dx.doi.org/10.1007/s10107-019-01440-w}.

\bibitem[Haochen \& Sra(2019)Haochen and Sra]{haochen2019random}
Haochen, J. and Sra, S.
\newblock Random shuffling beats sgd after finite epochs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2624--2633. PMLR, 2019.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pp.\  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014}
Kingma, D.~P. and Ba, J.
\newblock {ADAM}: {A} {M}ethod for {S}tochastic {O}ptimization.
\newblock \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, abs/1412.6980, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{CIFAR10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{MNIST}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2020)Li, Zhuang, and Orabona]{li2020exponential}
Li, L., Zhuang, Z., and Orabona, F.
\newblock Exponential step sizes for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2002.05273}, 2020.

\bibitem[Li et~al.(2019)Li, Zhu, So, and Lee]{li2019incremental}
Li, X., Zhu, Z., So, A., and Lee, J.~D.
\newblock Incremental methods for weakly convex optimization.
\newblock \emph{arXiv preprint arXiv:1907.11687}, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov10sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2017.

\bibitem[Meng et~al.(2019)Meng, Chen, Wang, Ma, and Liu]{meng2019convergence}
Meng, Q., Chen, W., Wang, Y., Ma, Z.-M., and Liu, T.-Y.
\newblock Convergence analysis of distributed stochastic gradient descent with
  shuffling.
\newblock \emph{Neurocomputing}, 337:\penalty0 46--57, 2019.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled Ragab~Bayoumi, and
  Richt{\'a}rik]{mishchenko2020random}
Mishchenko, K., Khaled Ragab~Bayoumi, A., and Richt{\'a}rik, P.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{nagaraj2019sgd}
Nagaraj, D., Jain, P., and Netrapalli, P.
\newblock Sgd without replacement: Sharper rates for general smooth convex
  functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4703--4711, 2019.

\bibitem[Nedi{\'c} \& Bertsekas(2001)Nedi{\'c} and
  Bertsekas]{nedic2001convergence}
Nedi{\'c}, A. and Bertsekas, D.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock In \emph{Stochastic optimization: algorithms and applications}, pp.\
  223--264. Springer, 2001.

\bibitem[Nedic \& Bertsekas(2001)Nedic and Bertsekas]{nedic2001incremental}
Nedic, A. and Bertsekas, D.~P.
\newblock Incremental subgradient methods for nondifferentiable optimization.
\newblock \emph{SIAM J. on Optim.}, 12\penalty0 (1):\penalty0 109--138, 2001.

\bibitem[Nesterov(1983)]{Nesterov1983}
Nesterov, Y.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $\mathcal{O}(1/k^2)$.
\newblock \emph{Doklady AN SSSR}, 269:\penalty0 543--547, 1983.
\newblock Translated as Soviet Math. Dokl.

\bibitem[Nesterov(2004)]{Nesterov2004}
Nesterov, Y.
\newblock \emph{{I}ntroductory lectures on convex optimization: {A} basic
  course}, volume~87 of \emph{Applied Optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg,
  and Takac]{Nguyen2018_sgdhogwild}
Nguyen, L., Nguyen, P.~H., van Dijk, M., Richtarik, P., Scheinberg, K., and
  Takac, M.
\newblock {SGD} and {H}ogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning-Volume 80}, pp.\  3747--3755, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{Nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2613--2621. JMLR. org, 2017.

\bibitem[Nguyen et~al.(2019)Nguyen, Nguyen, Richt{{\'a}}rik, Scheinberg,
  Tak{{\'a}}{\v{c}}, and van Dijk]{Nguyen2019_sgd_new_aspects}
Nguyen, L.~M., Nguyen, P.~H., Richt{{\'a}}rik, P., Scheinberg, K.,
  Tak{{\'a}}{\v{c}}, M., and van Dijk, M.
\newblock New convergence aspects of stochastic gradient algorithms.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (176):\penalty0 1--49, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-759.html}.

\bibitem[Nguyen et~al.(2020)Nguyen, Tran-Dinh, Phan, Nguyen, and van
  Dijk]{nguyen2020unified}
Nguyen, L.~M., Tran-Dinh, Q., Phan, D.~T., Nguyen, P.~H., and van Dijk, M.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{arXiv preprint arXiv:2002.08246}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rajput et~al.(2020)Rajput, Gupta, and
  Papailiopoulos]{rajput2020closing}
Rajput, S., Gupta, A., and Papailiopoulos, D.
\newblock Closing the convergence gap of sgd without replacement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7964--7973. PMLR, 2020.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{RM1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.

\bibitem[Ruder(2017)]{ruder2017overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem[Safran \& Shamir(2020)Safran and Shamir]{safran2020good}
Safran, I. and Shamir, O.
\newblock How good is sgd with random shuffling?
\newblock In \emph{Conference on Learning Theory}, pp.\  3250--3284. PMLR,
  2020.

\bibitem[Shamir(2016)]{shamir2016without}
Shamir, O.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  46--54, 2016.

\bibitem[Smith(2017)]{smith2017cyclical}
Smith, L.~N.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  464--472. IEEE, 2017.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Pham, Phan, and
  Nguyen]{tran2019hybrid}
Tran-Dinh, Q., Pham, N.~H., Phan, D.~T., and Nguyen, L.~M.
\newblock Hybrid stochastic gradient descent algorithms for stochastic
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1905.05920}, 2019.

\bibitem[Wang et~al.(2020)Wang, Nguyen, Bertozzi, Baraniuk, and
  Osher]{wang2020scheduled}
Wang, B., Nguyen, T.~M., Bertozzi, A.~L., Baraniuk, R.~G., and Osher, S.~J.
\newblock Scheduled restart momentum for accelerated stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:2002.10583}, 2020.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2019spiderboost}
Wang, Z., Ji, K., Zhou, Y., Liang, Y., and Tarokh, V.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 2406--2416, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Ying et~al.(2017)Ying, Yuan, and Sayed]{ying2017convergence}
Ying, B., Yuan, K., and Sayed, A.~H.
\newblock Convergence of variance-reduced stochastic learning under random
  reshuffling.
\newblock \emph{arXiv preprint arXiv:1708.01383}, 2\penalty0 (3):\penalty0 6,
  2017.

\end{thebibliography}
