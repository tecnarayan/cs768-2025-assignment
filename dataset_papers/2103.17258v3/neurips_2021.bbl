\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018{\natexlab{a}})Abdolmaleki, Springenberg,
  Degrave, Bohez, Tassa, Belov, Heess, and Riedmiller]{abdolmaleki2018relative}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval
  Tassa, Dan Belov, Nicolas Heess, and Martin Riedmiller.
\newblock Relative entropy regularized policy iteration.
\newblock \emph{arXiv preprint arXiv:1812.02256}, 2018{\natexlab{a}}.

\bibitem[Abdolmaleki et~al.(2018{\natexlab{b}})Abdolmaleki, Springenberg,
  Tassa, Munos, Heess, and Riedmiller]{Abdolmaleki2018mpo}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
  Heess, and Martin Riedmiller.
\newblock {Maximum a posteriori policy optimisation}.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Andrychowicz et~al.(2021)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski, Gelly, and
  Bachem]{andrychowicz2021what}
Marcin Andrychowicz, Anton Raichuk, Piotr Sta{\'n}czyk, Manu Orsini, Sertan
  Girgin, Rapha{\"e}l Marinier, Leonard Hussenot, Matthieu Geist, Olivier
  Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem.
\newblock What matters for on-policy deep actor-critic methods? a large-scale
  study.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baird~III(1993)]{baird1993advantage}
Leemon~C Baird~III.
\newblock Advantage updating.
\newblock Technical report, WRIGHT LAB WRIGHT-PATTERSON AFB OH, 1993.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Ciosek and Whiteson(2020)]{epg-journal}
Kamil Ciosek and Shimon Whiteson.
\newblock Expected policy gradients for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and
  Hochreiter]{clevert2016fast}
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{engstrom2019implementation}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus
  Janoos, Larry Rudolph, and Aleksander Madry.
\newblock Implementation matters in deep rl: A case study on ppo and trpo.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fellows et~al.(2019)Fellows, Mahajan, Rudner, and
  Whiteson]{fellows2019virel}
Matthew Fellows, Anuj Mahajan, Tim~GJ Rudner, and Shimon Whiteson.
\newblock Virel: A variational inference framework for reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2015taming}
Roy Fox, Ari Pakman, and Naftali Tishby.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{fujimoto2018td3}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Fujita et~al.(2021)Fujita, Nagarajan, Kataoka, and
  Ishikawa]{fujita2019chainerrl}
Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa.
\newblock Chainerrl: A deep reinforcement learning library.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019rmdp}
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.
\newblock A theory of regularized {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour2020divergence}
Seyed Kamyar~Seyed Ghasemipour, Richard Zemel, and Shixiang Gu.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Grathwohl et~al.(2017)Grathwohl, Choi, Wu, Roeder, and
  Duvenaud]{grathwohl2017backpropagation}
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock \emph{arXiv preprint arXiv:1711.00123}, 2017.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{gu2016continuous}
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Gu et~al.(2017{\natexlab{a}})Gu, Lillicrap, Ghahramani, Turner, and
  Levine]{gu2016qprop}
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard~E. Turner, and
  Sergey Levine.
\newblock {Q-Prop}: Sample-efficient policy gradient with an off-policy critic.
\newblock In \emph{International Conference on Learning Representations},
  2017{\natexlab{a}}.

\bibitem[Gu et~al.(2017{\natexlab{b}})Gu, Lillicrap, Ghahramani, Turner,
  Schölkopf, and Levine]{gu2017interpolated}
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard~E. Turner, Bernhard
  Schölkopf, and Sergey Levine.
\newblock Interpolated policy gradient: Merging on-policy and off-policy
  gradient estimation for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2017{\natexlab{b}}.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{Haarnoja:2018uya}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and Levine]{haarnoja2018sacapps}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey
  Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hafner et~al.(2020)Hafner, Ortega, Ba, Parr, Friston, and
  Heess]{hafner2020action}
Danijar Hafner, Pedro~A Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and
  Nicolas Heess.
\newblock Action and perception as divergence minimization.
\newblock \emph{arXiv preprint arXiv:2009.01791}, 2020.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2017deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, Henderson, Novikov,
  Colmenarejo, Cabi, Gulcehre, Paine, Cowie, Wang, Piot, and
  de~Freitas]{hoffman2020acme}
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal
  Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate
  Baumli, Sarah Henderson, Alex Novikov, Sergio~Gómez Colmenarejo, Serkan
  Cabi, Caglar Gulcehre, Tom~Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and
  Nando de~Freitas.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.00979}, 2020.

\bibitem[Islam et~al.(2017)Islam, Henderson, Gomrokchi, and
  Precup]{islam2017reproducibility}
Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup.
\newblock Reproducibility of benchmarked deep reinforcement learning tasks for
  continuous control.
\newblock \emph{arXiv preprint arXiv:1708.04133}, 2017.

\bibitem[Jaques et~al.(2017)Jaques, Gu, Bahdanau, Hern{\'a}ndez-Lobato, Turner,
  and Eck]{jaques2017sequence}
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, Richard~E Turner, and Douglas Eck.
\newblock Sequence tutor: Conservative fine-tuning of sequence generation
  models with kl-control.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Kober and Peters(2008)]{kober2008power}
Jens Kober and Jan Peters.
\newblock Policy search for motor primitives in robotics.
\newblock In \emph{Advances in neural information processing systems}, 2008.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kuznetsov et~al.(2020)Kuznetsov, Shvechikov, Grishin, and
  Vetrov]{kuznetsov2020tqc}
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov.
\newblock Controlling overestimation bias with truncated mixture of continuous
  distributional quantile critics.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Lee et~al.(2020)Lee, Nagabandi, Abbeel, and Levine]{lee2020slac}
Alex~X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Levine(2018)]{levine2018reinforcement}
Sergey Levine.
\newblock {Reinforcement Learning and Control as Probabilistic Inference}:
  Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2016}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Liu et~al.(2017)Liu, Feng, Mao, Zhou, Peng, and Liu]{liu2017action}
Hao Liu, Yihao Feng, Yi~Mao, Dengyong Zhou, Jian Peng, and Qiang Liu.
\newblock Action-depedent control variates for policy optimization via stein's
  identity.
\newblock \emph{arXiv preprint arXiv:1710.11198}, 2017.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Qiang Liu and Dilin Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock \emph{arXiv preprint arXiv:1608.04471}, 2016.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{Munos2016SafeAE}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc~G. Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, and Schuurmans]{Nachum2017urex}
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans.
\newblock {Improving policy gradient by exploring under-appreciated rewards}.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Neumann et~al.(2011)]{neumann2011variational}
Gerhard Neumann et~al.
\newblock Variational inference for policy search in changing situations.
\newblock In \emph{International Conference on Machine Learning}, 2011.

\bibitem[Norouzi et~al.(2016)Norouzi, Bengio, Jaitly, Schuster, Wu, Schuurmans,
  et~al.]{norouzi2016reward}
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale
  Schuurmans, et~al.
\newblock Reward augmented maximum likelihood for neural structured prediction.
\newblock In \emph{Advances In Neural Information Processing Systems}, 2016.

\bibitem[O'Donoghue et~al.(2020)O'Donoghue, Osband, and
  Ionescu]{odonoghue2020making}
Brendan O'Donoghue, Ian Osband, and Catalin Ionescu.
\newblock Making sense of reinforcement learning and probabilistic inference.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Oh et~al.(2018)Oh, Guo, Singh, and Lee]{Oh2018SIL}
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.
\newblock Self-imitation learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Okada and Taniguchi(2019)]{MasashiOkada2019}
Masashi Okada and Tadahiro Taniguchi.
\newblock Variational inference mpc for bayesian model-based reinforcement
  learning.
\newblock In \emph{Conference on Robot Learning}, 2019.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019awr}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock {Advantage-Weighted Regression}: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Peters and Schaal(2007)]{peters2007reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, 2007.

\bibitem[Peters et~al.(2010)Peters, M\"{u}lling, and Alt\"{u}n]{peters2010reps}
Jan Peters, Katharina M\"{u}lling, and Yasemin Alt\"{u}n.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2010.

\bibitem[Rawlik et~al.(2012)Rawlik, Toussaint, and Vijayakumar]{rawlik2012psi}
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2012.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{Schulman:2015uk}
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter
  Abbeel.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2016gae}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman2017PPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{Siegel2020KeepDW}
Noah~Y. Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, and Martin~A. Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Song et~al.(2020)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, Heess, Belov, Riedmiller, and
  Botvinick]{song2020vmpo}
H.~Francis Song, Abbas Abdolmaleki, Jost~Tobias Springenberg, Aidan Clark,
  Hubert Soyer, Jack~W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala,
  Nicolas Heess, Dan Belov, Martin Riedmiller, and Matthew~M. Botvinick.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tang and Kucukelbir(2020)]{tang2020hindsight}
Yunhao Tang and Alp Kucukelbir.
\newblock Hindsight expectation maximization for goal-conditioned reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.07549}, 2020.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, de~Las~Casas,
  Budden, Abdolmaleki, Merel, Lefrancq, Lillicrap, and
  Riedmiller]{tassa2018deepmind}
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego
  de~Las~Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,
  Timothy Lillicrap, and Martin Riedmiller.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Todorov(2006)]{todorov2006linearly}
Emanuel Todorov.
\newblock Linearly-solvable markov decision problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2006.

\bibitem[Todorov(2008)]{todorov2008general}
Emanuel Todorov.
\newblock General duality between optimal control and estimation.
\newblock In \emph{IEEE Conference on Decision and Control}, 2008.

\bibitem[Toussaint(2009)]{toussaint2009robot}
Marc Toussaint.
\newblock Robot trajectory optimization using approximate inference.
\newblock In \emph{International Conference on Machine Learning}, 2009.

\bibitem[Toussaint and Storkey(2006)]{toussaint2006probabilistic}
Marc Toussaint and Amos Storkey.
\newblock Probabilistic inference for solving discrete and continuous state
  markov decision processes.
\newblock In \emph{International conference on Machine learning}, 2006.

\bibitem[Tucker et~al.(2018)Tucker, Bhupatiraju, Gu, Turner, Ghahramani, and
  Levine]{tucker18amirage}
George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin
  Ghahramani, and Sergey Levine.
\newblock The mirage of action-dependent baselines in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Vieillard et~al.(2021)Vieillard, Kozuno, Scherrer, Pietquin, Munos,
  and Geist]{vieillard2021leverage}
Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Rémi Munos,
  and Matthieu Geist.
\newblock Leverage the average: an analysis of kl regularization in rl.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{wang2019benchmarking}
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
  Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba.
\newblock Benchmarking model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.02057}, 2019.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Springenberg, Reed, Shahriari,
  Siegel, Merel, Gulcehre, Heess, and de~Freitas]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost~Tobias Springenberg, Scott
  Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas
  Heess, and Nando de~Freitas.
\newblock Critic regularized regression.
\newblock \emph{arXiv preprint arXiv:2006.15134}, 2020.

\bibitem[Wu et~al.(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade, Mordatch,
  and Abbeel]{wu2018variance}
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre~M Bayen, Sham
  Kakade, Igor Mordatch, and Pieter Abbeel.
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock \emph{arXiv preprint arXiv:1803.07246}, 2018.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{Wu:2019wl}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock {Behavior Regularized Offline Reinforcement Learning}.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\end{thebibliography}
