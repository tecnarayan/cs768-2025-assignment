\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adolph et~al.(2012)Adolph, Cole, Komati, Garciaguirre, Badaly,
  Lingeman, Chan, and Sotsky]{adolph2012you}
Adolph, K.~E., Cole, W.~G., Komati, M., Garciaguirre, J.~S., Badaly, D.,
  Lingeman, J.~M., Chan, G.~L., and Sotsky, R.~B.
\newblock How do you learn to walk? thousands of steps and dozens of falls per
  day.
\newblock \emph{Psychological science}, 23\penalty0 (11):\penalty0 1387--1394,
  2012.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, O.~P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5048--5058, 2017.

\bibitem[Baird(1994)]{baird1994reinforcement}
Baird, L.~C.
\newblock Reinforcement learning in continuous time: Advantage updating.
\newblock In \emph{Neural Networks, 1994. IEEE World Congress on Computational
  Intelligence., 1994 IEEE International Conference on}, volume~4, pp.\
  2448--2453. IEEE, 1994.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto2017successor}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., van Hasselt,
  H.~P., and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4055--4065, 2017.

\bibitem[Barreto et~al.(2018)Barreto, Borsa, Quan, Schaul, Silver, Hessel,
  Mankowitz, Zidek, and Munos]{barreto2018transfer}
Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M.,
  Mankowitz, D., Zidek, A., and Munos, R.
\newblock Transfer in deep reinforcement learning using successor features and
  generalised policy improvement.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  501--510, 2018.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Dayan(1993)]{dayan1993improving}
Dayan, P.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural Computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2015taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Uncertainty
  in Artificial Intelligence}, UAI'16, pp.\  202--211, Arlington, Virginia,
  United States, 2016. AUAI Press.
\newblock ISBN 978-0-9966431-1-5.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3020948.3020970}.

\bibitem[Gil et~al.(2013)Gil, Alajaji, and Linder]{gil2013renyi}
Gil, M., Alajaji, F., and Linder, T.
\newblock R{\'e}nyi divergence measures for commonly used univariate continuous
  distributions.
\newblock \emph{Information Sciences}, 249:\penalty0 124--131, 2013.

\bibitem[Gu et~al.(2015)Gu, Ghahramani, and Turner]{gu2015neural}
Gu, S., Ghahramani, Z., and Turner, R.~E.
\newblock Neural adaptive sequential monte carlo.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2629--2637, 2015.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Pong, Zhou, Dalal,
  Abbeel, and Levine]{haarnoja2018composable}
Haarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S.
\newblock Composable deep reinforcement learning for robotic manipulation.
\newblock In \emph{International Conference on Robotics and Automation},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{b}}.

\bibitem[Haith \& Krakauer(2013)Haith and Krakauer]{haith2013model}
Haith, A.~M. and Krakauer, J.~W.
\newblock Model-based and model-free mechanisms of human motor learning.
\newblock In \emph{Progress in motor control}, pp.\  1--21. Springer, 2013.

\bibitem[Harmon et~al.(1995)Harmon, Baird~III, and Klopf]{harmon1995advantage}
Harmon, M.~E., Baird~III, L.~C., and Klopf, A.~H.
\newblock Advantage updating applied to a differential game.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  353--360, 1995.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kappen(2005)]{kappen2005path}
Kappen, H.~J.
\newblock Path integrals and symmetry breaking for optimal control theory.
\newblock \emph{Journal of statistical mechanics: theory and experiment},
  2005\penalty0 (11):\penalty0 P11011, 2005.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Murphy(2012)]{Murphy2012}
Murphy, K.~P.
\newblock \emph{Machine Learning: A Probabilistic Perspective}.
\newblock The MIT Press, 2012.
\newblock ISBN 0262018020, 9780262018029.

\bibitem[Pan et~al.(2015)Pan, Theodorou, and Kontitsis]{pan2015sample}
Pan, Y., Theodorou, E., and Kontitsis, M.
\newblock Sample efficient path integral control under uncertainty.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2314--2322, 2015.

\bibitem[Precup et~al.(1998)Precup, Sutton, and Singh]{precup1998theoretical}
Precup, D., Sutton, R.~S., and Singh, S.
\newblock Theoretical results on reinforcement learning with temporally
  abstract options.
\newblock In \emph{European conference on machine learning}, pp.\  382--393.
  Springer, 1998.

\bibitem[Saxe et~al.(2017)Saxe, Earle, and Rosman]{saxe2017hierarchy}
Saxe, A.~M., Earle, A.~C., and Rosman, B.
\newblock Hierarchy through composition with multitask {LMDP}s.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3017--3026, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/saxe17a.html}.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1312--1320, 2015.

\bibitem[Schrempf et~al.(2005)Schrempf, Feiermann, and
  Hanebeck]{schrempf2005optimal}
Schrempf, O.~C., Feiermann, O., and Hanebeck, U.~D.
\newblock Optimal mixture approximation of the product of mixtures.
\newblock In \emph{International Conference on Information Fusion}, volume~1,
  pp.\  8--pp. IEEE, 2005.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{tassa2018deepmind}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d.~L., Budden,
  D., Abdolmaleki, A., Merel, J., Lefrancq, A., et~al.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Todorov(2009)]{todorov2009compositionality}
Todorov, E.
\newblock Compositionality of optimal control laws.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1856--1864, 2009.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS)}, pp.\  5026--5033.
  IEEE, 2012.

\bibitem[van Niekerk et~al.(2018)van Niekerk, James, Earle, and
  Rosman]{van2018will}
van Niekerk, B., James, S., Earle, A., and Rosman, B.
\newblock Will it blend? composing value functions in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1807.04439}, 2018.

\bibitem[Wang et~al.(2015)Wang, Schaul, Hessel, Van~Hasselt, Lanctot, and
  De~Freitas]{wang2015dueling}
Wang, Z., Schaul, T., Hessel, M., Van~Hasselt, H., Lanctot, M., and De~Freitas,
  N.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1511.06581}, 2015.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
