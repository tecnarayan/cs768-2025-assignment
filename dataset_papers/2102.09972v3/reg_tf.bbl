\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2011)Acar, Dunlavy, Kolda, and M{\o}rup]{acar2011scalable}
Acar, E., Dunlavy, D.~M., Kolda, T.~G., and M{\o}rup, M.
\newblock Scalable tensor factorizations for incomplete data.
\newblock \emph{Chemometrics and Intelligent Laboratory Systems}, 106\penalty0
  (1):\penalty0 41--56, 2011.

\bibitem[Advani \& Saxe(2017)Advani and Saxe]{advani2017high}
Advani, M.~S. and Saxe, A.~M.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Ali, A., Dobriban, E., and Tibshirani, R.~J.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Anandkumar et~al.(2014)Anandkumar, Ge, Hsu, Kakade, and
  Telgarsky]{anandkumar2014tensor}
Anandkumar, A., Ge, R., Hsu, D., Kakade, S.~M., and Telgarsky, M.
\newblock Tensor decompositions for learning latent variable models.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 2773--2832,
  2014.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  244--253, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  7413--7424, 2019.

\bibitem[Balda et~al.(2018)Balda, Behboodi, and Mathar]{balda2018tensor}
Balda, E.~R., Behboodi, A., and Mathar, R.
\newblock A tensor analysis on dense connectivity via convolutional arithmetic
  circuits.
\newblock \emph{Preprint}, 2018.

\bibitem[Brutzkus \& Globerson(2020)Brutzkus and
  Globerson]{brutzkus2020inductive}
Brutzkus, A. and Globerson, A.
\newblock On the inductive bias of a cnn for orthogonal patterns distributions.
\newblock \emph{arXiv preprint arXiv:2002.09781}, 2020.

\bibitem[Cai et~al.(2019)Cai, Li, Poor, and Chen]{cai2019nonconvex}
Cai, C., Li, G., Poor, H.~V., and Chen, Y.
\newblock Nonconvex low-rank tensor completion from noisy data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1863--1874, 2019.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi2019nonconvex}
Chi, Y., Lu, Y.~M., and Chen, Y.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (20):\penalty0 5239--5269, 2019.

\bibitem[Chizat \& Bach(2020)Chizat and Bach]{chizat2020implicit}
Chizat, L. and Bach, F.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  1305--1338,
  2020.

\bibitem[Chou et~al.(2020)Chou, Gieshoff, Maly, and Rauhut]{chou2020gradient}
Chou, H.-H., Gieshoff, C., Maly, J., and Rauhut, H.
\newblock Gradient descent for deep matrix factorization: Dynamics and implicit
  bias towards low rank.
\newblock \emph{arXiv preprint arXiv:2011.13772}, 2020.

\bibitem[Cohen \& Shashua(2014)Cohen and Shashua]{cohen2014simnets}
Cohen, N. and Shashua, A.
\newblock Simnets: A generalization of convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS),
  Deep Learning Workshop}, 2014.

\bibitem[Cohen \& Shashua(2016)Cohen and Shashua]{cohen2016convolutional}
Cohen, N. and Shashua, A.
\newblock Convolutional rectifier networks as generalized tensor
  decompositions.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Cohen \& Shashua(2017)Cohen and Shashua]{cohen2017inductive}
Cohen, N. and Shashua, A.
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Cohen et~al.(2016{\natexlab{a}})Cohen, Sharir, and
  Shashua]{cohen2016deep}
Cohen, N., Sharir, O., and Shashua, A.
\newblock Deep simnets.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016{\natexlab{a}}.

\bibitem[Cohen et~al.(2016{\natexlab{b}})Cohen, Sharir, and
  Shashua]{cohen2016expressive}
Cohen, N., Sharir, O., and Shashua, A.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock \emph{Conference On Learning Theory (COLT)}, 2016{\natexlab{b}}.

\bibitem[Cohen et~al.(2017)Cohen, Sharir, Levine, Tamari, Yakira, and
  Shashua]{cohen2017analysis}
Cohen, N., Sharir, O., Levine, Y., Tamari, R., Yakira, D., and Shashua, A.
\newblock Analysis and design of convolutional networks via hierarchical tensor
  decompositions.
\newblock \emph{Intel Collaborative Research Institute for Computational
  Intelligence (ICRI-CI) Special Issue on Deep Learning Theory}, 2017.

\bibitem[Cohen et~al.(2018)Cohen, Tamari, and Shashua]{cohen2018boosting}
Cohen, N., Tamari, R., and Shashua, A.
\newblock Boosting dilated convolutional networks with mixed tensor
  decompositions.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Davenport \& Romberg(2016)Davenport and
  Romberg]{davenport2016overview}
Davenport, M.~A. and Romberg, J.
\newblock An overview of low-rank matrix recovery from incomplete observations.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (4):\penalty0 608--622, 2016.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  384--395, 2018.

\bibitem[Eftekhari \& Zygalakis(2020)Eftekhari and
  Zygalakis]{eftekhari2020implicit}
Eftekhari, A. and Zygalakis, K.
\newblock Implicit regularization in matrix sensing: A geometric view leads to
  stronger results.
\newblock \emph{arXiv preprint arXiv:2008.12091}, 2020.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gidel, G., Bach, F., and Lacoste-Julien, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3196--3206, 2019.

\bibitem[Gissin et~al.(2020)Gissin, Shalev-Shwartz, and
  Daniely]{gissin2020implicit}
Gissin, D., Shalev-Shwartz, S., and Daniely, A.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov{\'a}]{goldt2019dynamics}
Goldt, S., Advani, M., Saxe, A.~M., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6979--6989, 2019.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, volume~80, pp.\  1832--1841, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  9461--9471, 2018{\natexlab{b}}.

\bibitem[Hackbusch(2012)]{hackbusch2012tensor}
Hackbusch, W.
\newblock \emph{Tensor spaces and numerical tensor calculus}, volume~42.
\newblock Springer, 2012.

\bibitem[Hillar \& Lim(2013)Hillar and Lim]{hillar2013most}
Hillar, C.~J. and Lim, L.-H.
\newblock Most tensor problems are np-hard.
\newblock \emph{Journal of the ACM (JACM)}, 60\penalty0 (6):\penalty0 1--39,
  2013.

\bibitem[Huber(1964)]{huber1964robust}
Huber, P.~J.
\newblock Robust estimation of a location parameter.
\newblock \emph{Annals of Mathematical Statistics}, 35:\penalty0 73--101, 1964.

\bibitem[Ibrahim et~al.(2020)Ibrahim, Fu, and Li]{ibrahim2020recoverability}
Ibrahim, S., Fu, X., and Li, X.
\newblock On recoverability of randomly compressed tensors with low cp rank.
\newblock \emph{IEEE Signal Processing Letters}, 27:\penalty0 1125--1129, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pp.\  8571--8580, 2018.

\bibitem[Jain \& Oh(2014)Jain and Oh]{jain2014provable}
Jain, P. and Oh, S.
\newblock Provable tensor factorization with missing data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1431--1439, 2014.

\bibitem[Ji \& Telgarsky(2019{\natexlab{a}})Ji and Telgarsky]{ji2019gradient}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019{\natexlab{a}}.

\bibitem[Ji \& Telgarsky(2019{\natexlab{b}})Ji and Telgarsky]{ji2019implicit}
Ji, Z. and Telgarsky, M.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  1772--1798,
  2019{\natexlab{b}}.

\bibitem[Karlsson et~al.(2016)Karlsson, Kressner, and
  Uschmajew]{karlsson2016parallel}
Karlsson, L., Kressner, D., and Uschmajew, A.
\newblock Parallel algorithms for tensor completion in the cp format.
\newblock \emph{Parallel Computing}, 57:\penalty0 222--234, 2016.

\bibitem[Khrulkov et~al.(2018)Khrulkov, Novikov, and
  Oseledets]{khrulkov2018expressive}
Khrulkov, V., Novikov, A., and Oseledets, I.
\newblock Expressive power of recurrent neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Khrulkov et~al.(2019)Khrulkov, Hrinchuk, and
  Oseledets]{khrulkov2019generalized}
Khrulkov, V., Hrinchuk, O., and Oseledets, I.
\newblock Generalized tensor models for recurrent neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kolda \& Bader(2009)Kolda and Bader]{kolda2009tensor}
Kolda, T.~G. and Bader, B.~W.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Lampinen \& Ganguli(2019)Lampinen and Ganguli]{lampinen2019analytic}
Lampinen, A.~K. and Ganguli, S.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Levine et~al.(2018{\natexlab{a}})Levine, Sharir, and
  Shashua]{levine2018benefits}
Levine, Y., Sharir, O., and Shashua, A.
\newblock Benefits of depth for long-term memory of recurrent networks.
\newblock \emph{International Conference on Learning Representations (ICLR)
  Workshop}, 2018{\natexlab{a}}.

\bibitem[Levine et~al.(2018{\natexlab{b}})Levine, Yakira, Cohen, and
  Shashua]{levine2018deep}
Levine, Y., Yakira, D., Cohen, N., and Shashua, A.
\newblock Deep learning and quantum entanglement: Fundamental connections with
  implications to network design.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018{\natexlab{b}}.

\bibitem[Levine et~al.(2019)Levine, Sharir, Cohen, and
  Shashua]{levine2019quantum}
Levine, Y., Sharir, O., Cohen, N., and Shashua, A.
\newblock Quantum entanglement in deep learning architectures.
\newblock \emph{To appear in Physical Review Letters}, 2019.

\bibitem[Levine et~al.(2020)Levine, Wies, Sharir, Bata, and
  Shashua]{levine2020limits}
Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A.
\newblock Limits to depth efficiencies of self-attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Li et~al.(2020)Li, Sun, Su, Suzuki, and Huang]{li2020understanding}
Li, J., Sun, Y., Su, J., Suzuki, T., and Huang, F.
\newblock Understanding generalization in deep learning via tensor methods.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, pp.\  504--515. PMLR, 2020.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory
  (COLT)}, pp.\  2--47, 2018.

\bibitem[Li et~al.(2021)Li, Luo, and Lyu]{li2021towards}
Li, Z., Luo, Y., and Lyu, K.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Ma et~al.(2018)Ma, Wang, Chi, and Chen]{ma2018implicit}
Ma, C., Wang, K., Chi, Y., and Chen, Y.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  3351--3360, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  2388--2464,
  2019.

\bibitem[Milanesi et~al.(2021)Milanesi, Kadri, Ayache, and
  Arti{\`e}res]{milanesi2021implicit}
Milanesi, P., Kadri, H., Ayache, S., and Arti{\`e}res, T.
\newblock Implicit regularization in deep tensor factorization.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  2021.

\bibitem[Mulayoff \& Michaeli(2020)Mulayoff and Michaeli]{mulayoff2020unique}
Mulayoff, R. and Michaeli, T.
\newblock Unique properties of wide minima in deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Nacson, M.~S., Lee, J., Gunasekar, S., Savarese, P. H.~P., Srebro, N., and
  Soudry, D.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{Proceedings of Machine Learning Research}, volume~89, pp.\
  3420--3428, 2019.

\bibitem[Narita et~al.(2012)Narita, Hayashi, Tomioka, and
  Kashima]{narita2012tensor}
Narita, A., Hayashi, K., Tomioka, R., and Kashima, H.
\newblock Tensor factorization using auxiliary information.
\newblock \emph{Data Mining and Knowledge Discovery}, 25\penalty0 (2):\penalty0
  298--324, 2012.

\bibitem[Oymak \& Soltanolkotabi(2019)Oymak and
  Soltanolkotabi]{oymak2019overparameterized}
Oymak, S. and Soltanolkotabi, M.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4951--4960, 2019.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Rahaman et~al.(2019)Rahaman, Arpit, Baratin, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2018spectral}
Rahaman, N., Arpit, D., Baratin, A., Draxler, F., Lin, M., Hamprecht, F.~A.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  5301--5310, 2019.

\bibitem[Rauhut et~al.(2017)Rauhut, Schneider, and Stojanac]{rauhut2017low}
Rauhut, H., Schneider, R., and Stojanac, {\v{Z}}.
\newblock Low rank tensor recovery via iterative hard thresholding.
\newblock \emph{Linear Algebra and its Applications}, 523:\penalty0 220--262,
  2017.

\bibitem[Razin \& Cohen(2020)Razin and Cohen]{razin2020implicit}
Razin, N. and Cohen, N.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Recht, B., Fazel, M., and Parrilo, P.~A.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Sharir \& Shashua(2018)Sharir and Shashua]{sharir2018expressive}
Sharir, O. and Shashua, A.
\newblock On the expressive power of overlapping architectures of deep
  learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Sharir et~al.(2016)Sharir, Tamari, Cohen, and
  Shashua]{sharir2016tensorial}
Sharir, O., Tamari, R., Cohen, N., and Shashua, A.
\newblock Tensorial mixture models.
\newblock \emph{arXiv preprint}, 2016.

\bibitem[Song et~al.(2019)Song, Ge, Caverlee, and Hu]{song2019tensor}
Song, Q., Ge, H., Caverlee, J., and Hu, X.
\newblock Tensor completion algorithms in big data analytics.
\newblock \emph{ACM Transactions on Knowledge Discovery from Data (TKDD)},
  13\penalty0 (1):\penalty0 1--48, 2019.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Teschl(2012)]{teschl2012ordinary}
Teschl, G.
\newblock \emph{Ordinary differential equations and dynamical systems}, volume
  140.
\newblock American Mathematical Soc., 2012.

\bibitem[Tu et~al.(2016)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2016low}
Tu, S., Boczar, R., Simchowitz, M., Soltanolkotabi, M., and Recht, B.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  964--973, 2016.

\bibitem[Vardi \& Shamir(2021)Vardi and Shamir]{vardi2020implicit}
Vardi, G. and Shamir, O.
\newblock Implicit regularization in relu networks with the square loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2021.

\bibitem[Wang et~al.(2020)Wang, Wu, Lee, Ma, and Ge]{wang2020lazy}
Wang, X., Wu, C., Lee, J.~D., Ma, T., and Ge, R.
\newblock Beyond lazy training for over-parameterized tensor decomposition.
\newblock 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  3635--3673,
  2020.

\bibitem[Wu et~al.(2020)Wu, Dobriban, Ren, Wu, Li, Gunasekar, Ward, and
  Liu]{wu2019implicit}
Wu, X., Dobriban, E., Ren, T., Wu, S., Li, Z., Gunasekar, S., Ward, R., and
  Liu, Q.
\newblock Implicit regularization and convergence for weight normalization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Xia \& Yuan(2017)Xia and Yuan]{xia2017polynomial}
Xia, D. and Yuan, M.
\newblock On polynomial time methods for exact low rank tensor completion.
\newblock \emph{arXiv preprint arXiv:1702.06980}, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xu(2018)]{xu2018understanding}
Xu, Z.~J.
\newblock Understanding training and generalization in deep learning by fourier
  analysis.
\newblock \emph{arXiv preprint arXiv:1808.04295}, 2018.

\bibitem[Yokota et~al.(2016)Yokota, Zhao, and Cichocki]{yokota2016smooth}
Yokota, T., Zhao, Q., and Cichocki, A.
\newblock Smooth parafac decomposition for tensor completion.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (20):\penalty0 5423--5436, 2016.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{yun2021unifying}
Yun, C., Krishnan, S., and Mobahi, H.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Zhou et~al.(2017)Zhou, Lu, Lin, and Zhang]{zhou2017tensor}
Zhou, P., Lu, C., Lin, Z., and Zhang, C.
\newblock Tensor factorization for low-rank tensor completion.
\newblock \emph{IEEE Transactions on Image Processing}, 27\penalty0
  (3):\penalty0 1152--1163, 2017.

\end{thebibliography}
