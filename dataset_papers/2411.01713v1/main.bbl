\begin{thebibliography}{10}

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock {\em arXiv preprint arXiv:1708.03888}, 2017.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76 minutes.
\newblock {\em arXiv preprint arXiv:1904.00962}, 2019.

\bibitem{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages 1139--1147. PMLR, 2013.

\bibitem{ruder2016overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em arXiv preprint arXiv:1609.04747}, 2016.

\bibitem{lee2022surgical}
Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn.
\newblock Surgical fine-tuning improves adaptation to distribution shifts.
\newblock {\em arXiv preprint arXiv:2210.11466}, 2022.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{he2022sparseadapter}
Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao.
\newblock Sparseadapter: An easy approach for improving the parameter-efficiency of adapters.
\newblock {\em arXiv preprint arXiv:2210.04284}, 2022.

\bibitem{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In {\em International Conference on Machine Learning}, pages 2790--2799. PMLR, 2019.

\bibitem{tian2023trainable}
Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, and Zsolt Kira.
\newblock Trainable projected gradient method for robust fine-tuning.
\newblock {\em arXiv preprint arXiv:2303.10720}, 2023.

\bibitem{tian2024fast}
Junjiao Tian, Yen-Cheng Liu, James~S Smith, and Zsolt Kira.
\newblock Fast trainable projection for robust fine-tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{gouk2020distance}
Henry Gouk, Timothy~M Hospedales, and Massimiliano Pontil.
\newblock Distance-based regularisation of deep networks for fine-tuning.
\newblock {\em ICLR}, 2021.

\bibitem{xuhong2018explicit}
LI~Xuhong, Yves Grandvalet, and Franck Davoine.
\newblock Explicit inductive bias for transfer learning with convolutional networks.
\newblock In {\em International Conference on Machine Learning}, pages 2825--2834. PMLR, 2018.

\bibitem{wortsman2022robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et~al.
\newblock Robust fine-tuning of zero-shot models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7959--7971, 2022.

\bibitem{chandra2022gradient}
Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer.
\newblock Gradient descent: The ultimate optimizer.
\newblock {\em Advances in Neural Information Processing Systems}, 35:8214--8225, 2022.

\bibitem{wang2021guarantees}
Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge.
\newblock Guarantees for tuning the step size using a learning-to-learn approach.
\newblock In {\em International Conference on Machine Learning}, pages 10981--10990. PMLR, 2021.

\bibitem{feurer2019hyperparameter}
Matthias Feurer and Frank Hutter.
\newblock Hyperparameter optimization.
\newblock {\em Automated machine learning: Methods, systems, challenges}, pages 3--33, 2019.

\bibitem{gouk2021regularisation}
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael~J Cree.
\newblock Regularisation of neural networks by enforcing lipschitz continuity.
\newblock {\em Machine Learning}, 110:393--416, 2021.

\bibitem{kumar2022fine}
Ananya Kumar et~al.
\newblock Fine-tuning can distort pretrained features and underperform out-of-distribution.
\newblock {\em ICLR}, 2022.

\bibitem{goyal2022finetune}
Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan.
\newblock Finetune like you pretrain: Improved finetuning of zero-shot vision models.
\newblock {\em arXiv preprint arXiv:2212.00638}, 2022.

\bibitem{zhuang2020adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar~C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed gradients.
\newblock {\em Advances in neural information processing systems}, 33:18795--18806, 2020.

\bibitem{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 1406--1415, 2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International Conference on Machine Learning}, pages 8748--8763. PMLR, 2021.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em International Conference on Machine Learning}, pages 5389--5400. PMLR, 2019.

\bibitem{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15262--15271, 2021.

\bibitem{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8340--8349, 2021.

\bibitem{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em IJCV}, 2010.

\bibitem{liu2022polyhistor}
Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, and Zsolt Kira.
\newblock Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks.
\newblock {\em arXiv preprint arXiv:2210.03265}, 2022.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{xie2021segformer}
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose~M Alvarez, and Ping Luo.
\newblock Segformer: Simple and efficient design for semantic segmentation with transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 34:12077--12090, 2021.

\bibitem{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}, 2019.

\bibitem{hu2023llm}
Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee.
\newblock Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.
\newblock {\em arXiv preprint arXiv:2304.01933}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{beyer_paligemma_2024}
Lucas Beyer, Andreas Steiner, André~Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi~Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi~Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.
\newblock {PaliGemma}: {A} versatile {3B} {VLM} for transfer, July 2024.
\newblock arXiv:2407.07726 [cs] version: 1.

\bibitem{goyal_making_2017}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the {V} in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}, May 2017.
\newblock arXiv:1612.00837 [cs].

\bibitem{agarwal_towards_2020}
Vedika Agarwal, Rakshith Shetty, and Mario Fritz.
\newblock Towards {Causal} {VQA}: {Revealing} and {Reducing} {Spurious} {Correlations} by {Invariant} and {Covariant} {Semantic} {Editing}.
\newblock In {\em 2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})}, pages 9687--9695, Seattle, WA, USA, June 2020. IEEE.

\bibitem{shah_cycle-consistency_2019}
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
\newblock Cycle-{Consistency} for {Robust} {Visual} {Question} {Answering}, February 2019.
\newblock arXiv:1902.05660 [cs].

\bibitem{agrawal_dont_2018}
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
\newblock Don't {Just} {Assume}; {Look} and {Answer}: {Overcoming} {Priors} for {Visual} {Question} {Answering}, June 2018.
\newblock arXiv:1712.00377 [cs].

\bibitem{dancette_beyond_2021}
Corentin Dancette, Remi Cadene, Damien Teney, and Matthieu Cord.
\newblock Beyond {Question}-{Based} {Biases}: {Assessing} {Multimodal} {Shortcut} {Learning} in {Visual} {Question} {Answering}, September 2021.
\newblock arXiv:2104.03149 [cs].

\bibitem{sheng_human-adversarial_2021}
Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Alberto~Lopez Magana, Wojciech Galuba, Devi Parikh, and Douwe Kiela.
\newblock Human-{Adversarial} {Visual} {Question} {Answering}, June 2021.
\newblock arXiv:2106.02280 [cs].

\bibitem{singh_towards_2019}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards {VQA} {Models} {That} {Can} {Read}, May 2019.
\newblock arXiv:1904.08920 [cs].

\bibitem{bigham_vizwiz_nodate}
Jeffrey~P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert~C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh.
\newblock {VizWiz}: nearly real-time answers to visual questions.

\bibitem{reichman_outside_2023}
Benjamin~Z. Reichman, Anirudh Sundar, Christopher Richardson, Tamara Zubatiy, Prithwijit Chowdhury, Aaryan Shah, Jack Truxal, Micah Grimes, Dristi Shah, Woo~Ju Chee, Saif Punjwani, Atishay Jain, and Larry Heck.
\newblock Outside {Knowledge} {Visual} {Question} {Answering} {Version} 2.0.
\newblock In {\em {ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})}, pages 1--5, June 2023.
\newblock ISSN: 2379-190X.

\bibitem{pmlr-v139-touvron21a}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou.
\newblock Training data-efficient image transformers and distillation through attention.
\newblock In {\em International Conference on Machine Learning}, volume 139, pages 10347--10357, July 2021.

\bibitem{zaken2021bitfit}
Elad~Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock {\em arXiv preprint arXiv:2106.10199}, 2021.

\bibitem{zhang2020revisiting}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample bert fine-tuning.
\newblock {\em arXiv preprint arXiv:2006.05987}, 2020.

\bibitem{balles2018dissecting}
Lukas Balles and Philipp Hennig.
\newblock Dissecting adam: The sign, magnitude and variance of stochastic gradients.
\newblock In {\em International Conference on Machine Learning}, pages 404--413. PMLR, 2018.

\bibitem{garrigos2023handbook}
Guillaume Garrigos and Robert~M Gower.
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock {\em arXiv preprint arXiv:2301.11235}, 2023.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM review}, 60(2):223--311, 2018.

\bibitem{larsson2016fractalnet}
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock {\em arXiv preprint arXiv:1605.07648}, 2016.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2818--2826, 2016.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with localizable features.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 6023--6032, 2019.

\bibitem{li2022lavislibrarylanguagevisionintelligence}
Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C.~H. Hoi.
\newblock Lavis: A library for language-vision intelligence, 2022.

\end{thebibliography}
