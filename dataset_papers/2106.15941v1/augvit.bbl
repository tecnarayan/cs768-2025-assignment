\begin{thebibliography}{10}

\bibitem{balduzzi2017shattered}
David Balduzzi, Marcus Frean, Lennox Leary, JP~Lewis, Kurt Wan-Duo Ma, and
  Brian McWilliams.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In {\em International Conference on Machine Learning}, pages
  342--350. PMLR, 2017.

\bibitem{brigham1988fast}
E~Oran Brigham.
\newblock {\em The fast Fourier transform and its applications}.
\newblock Prentice-Hall, Inc., 1988.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, pages 213--229.
  Springer, 2020.

\bibitem{chen2020pre}
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei
  Ma, Chunjing Xu, Chao Xu, and Wen Gao.
\newblock Pre-trained image processing transformer.
\newblock {\em arXiv preprint arXiv:2012.00364}, 2020.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dietrich1997fast}
Claude~R Dietrich and Garry~N Newsam.
\newblock Fast and exact simulation of stationary gaussian processes through
  circulant embedding of the covariance matrix.
\newblock {\em SIAM Journal on Scientific Computing}, 18(4):1088--1107, 1997.

\bibitem{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock {\em arXiv preprint arXiv:2103.03404}, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{gray2006toeplitz}
Robert~M Gray.
\newblock Toeplitz and circulant matrices: A review.
\newblock 2006.

\bibitem{greff2016lstm}
Klaus Greff, Rupesh~K Srivastava, Jan Koutn{\'\i}k, Bas~R Steunebrink, and
  J{\"u}rgen Schmidhuber.
\newblock Lstm: A search space odyssey.
\newblock {\em IEEE transactions on neural networks and learning systems},
  28(10):2222--2232, 2016.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hinrichs2011johnson}
Aicke Hinrichs and Jan Vyb{\'\i}ral.
\newblock Johnson-lindenstrauss lemma for circulant matrices.
\newblock {\em Random Structures \& Algorithms}, 39(3):391--398, 2011.

\bibitem{hoffer2020augment}
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel
  Soudry.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8129--8138, 2020.

\bibitem{kra2012circulant}
Irwin Kra and Santiago~R Simanca.
\newblock On circulant matrices.
\newblock {\em Notices of the AMS}, 59(3):368--377, 2012.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{larsson2016fractalnet}
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock {\em arXiv preprint arXiv:1605.07648}, 2016.

\bibitem{NEURIPS2019_7716d0fc}
Tianyi Liu, Minshuo Chen, Mo~Zhou, Simon~S Du, Enlu Zhou, and Tuo Zhao.
\newblock Towards understanding the importance of shortcut connections in
  residual networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE, 2008.

\bibitem{nussbaumer1981fast}
Henri~J Nussbaumer.
\newblock The fast fourier transform.
\newblock In {\em Fast Fourier Transform and Convolution Algorithms}, pages
  80--111. Springer, 1981.

\bibitem{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In {\em 2012 IEEE conference on computer vision and pattern
  recognition}, pages 3498--3505. IEEE, 2012.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}, 2017.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{veit2016residual}
Andreas Veit, Michael Wilber, and Serge Belongie.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock {\em arXiv preprint arXiv:1605.06431}, 2016.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis~EH Tay, Jiashi
  Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6023--6032, 2019.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}
