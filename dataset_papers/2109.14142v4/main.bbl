\begin{thebibliography}{}

\bibitem[Alemohammad et~al., 2021]{alemohammad2021the}
Alemohammad, S., Wang, Z., Balestriero, R., and Baraniuk, R. (2021).
\newblock The recurrent neural tangent kernel.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Allen-Zhu and Li, 2019a]{allenzhu2019sgd}
Allen-Zhu, Z. and Li, Y. (2019a).
\newblock Can sgd learn recurrent neural networks with provable generalization?
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Allen-Zhu and Li, 2019b]{NEURIPS2019_5857d68c}
Allen-Zhu, Z. and Li, Y. (2019b).
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Allen-Zhu et~al., 2019a]{NEURIPS201962dad6e2}
Allen-Zhu, Z., Li, Y., and Liang, Y. (2019a).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Allen-Zhu et~al., 2019b]{allenzhu2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2019b).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Chaudhuri, K. and Salakhutdinov, R., editors, {\em Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of {\em Proceedings of
  Machine Learning Research}, pages 242--252. {PMLR}.

\bibitem[Allen-Zhu et~al., 2019c]{rate}
Allen-Zhu, Z., Li, Y., and Song, Z. (2019c).
\newblock On the convergence rate of training recurrent neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Arora et~al., 2019]{sa}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R. (2019).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In Chaudhuri, K. and Salakhutdinov, R., editors, {\em Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of {\em Proceedings of
  Machine Learning Research}, pages 322--332. {PMLR}.

\bibitem[Bai and Lee, 2020]{Bai2020Beyond}
Bai, Y. and Lee, J.~D. (2020).
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Bartlett et~al., 2019]{BartlettHLM19}
Bartlett, P.~L., Harvey, N., Liaw, C., and Mehrabian, A. (2019).
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock {\em J. Mach. Learn. Res.}, 20:63:1--63:17.

\bibitem[BergJens et~al., 1984]{Christian}
BergJens, C., Reus, P., and Ressel, C. (1984).
\newblock {\em Harmonic Analysis on Semigroups Theory of Positive Definite and
  Related Functions}.
\newblock Springer Netherlands.

\bibitem[Bietti and Bach, 2021]{bietti2021deep}
Bietti, A. and Bach, F. (2021).
\newblock Deep equals shallow for relu networks in kernel regimes.

\bibitem[Boucheron et~al., 2013]{2013Concentration}
Boucheron, S., Lugosi, G., and Massart, P. (2013).
\newblock {\em Concentration inequalities : a non asymptotic theory of
  independence}.
\newblock Oxford University Press.

\bibitem[Cao and Gu, 2019]{cao}
Cao, Y. and Gu, Q. (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Chen et~al., 2020a]{NEURIPS2020_fb647ca6}
Chen, M., Bai, Y., Lee, J.~D., Zhao, T., Wang, H., Xiong, C., and Socher, R.
  (2020a).
\newblock Towards understanding hierarchical learning: Benefits of neural
  representations.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H., editors, {\em Advances in Neural Information Processing Systems},
  volume~33, pages 22134--22145. Curran Associates, Inc.

\bibitem[Chen et~al., 2020b]{chen2020overparameterization}
Chen, Z., Cao, Y., Zou, D., and Gu, Q. (2020b).
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?

\bibitem[Daniely et~al., 2016]{NIPS2016_abea47ba}
Daniely, A., Frostig, R., and Singer, Y. (2016).
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.,
  editors, {\em Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc.

\bibitem[Du and Lee, 2018]{du2018on}
Du, S.~S. and Lee, J.~D. (2018).
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock {\em International conference on machine learning}, pages 1328--1337.

\bibitem[Du et~al., 2018]{du2018when}
Du, S.~S., Lee, J.~D., and Tian, Y. (2018).
\newblock When is a convolutional filter easy to learn.
\newblock {\em International conference on machine learning}.

\bibitem[Du et~al., 2019]{DBLP}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A. (2019).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[Freeman and Bruna, 2016]{freeman2016topology}
Freeman, C.~D. and Bruna, J. (2016).
\newblock Topology and geometry of half-rectified network optimization.
\newblock {\em International conference on machine learning}.

\bibitem[Ge et~al., 2017]{171100501}
Ge, R., Lee, J.~D., and Ma, T. (2017).
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock {\em CoRR}, abs/1711.00501.

\bibitem[Hardt et~al., 2016]{2016Gradient}
Hardt, M., Ma, T., and Recht, B. (2016).
\newblock Gradient descent learns linear dynamical systems.
\newblock {\em Journal of Machine Learning Research}, 19.

\bibitem[Huang et~al., 2020]{huang2020deep}
Huang, K., Wang, Y., Tao, M., and Zhao, T. (2020).
\newblock Why do deep residual networks generalize better than deep feedforward
  networks? - {A} neural tangent kernel perspective.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[Jacot et~al., 2018]{NEURIPS20185a4be1fa}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc.

\bibitem[Ji and Telgarsky, 2020]{ji2020polylogarithmic}
Ji, Z. and Telgarsky, M. (2020).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.

\bibitem[Li and Liang, 2018]{NEURIPS201854fe976b}
Li, Y. and Liang, Y. (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc.

\bibitem[Li et~al., 2020]{li2020learning}
Li, Y., Ma, T., and Zhang, H.~R. (2020).
\newblock Learning over-parametrized two-layer relu neural networks beyond ntk.

\bibitem[Mahdi et~al., 2018]{MahdiTheoretical}
Mahdi, S., Adel, J., and D., L.~J. (2018).
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}.

\bibitem[Safran and Shamir, 2018]{safran2018spurious}
Safran, I. and Shamir, O. (2018).
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock {\em International conference on machine learning}, pages 4430--4438.

\bibitem[Tian, 2017]{tian2017symmetry-breaking}
Tian, Y. (2017).
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with relu nonlinearity.
\newblock {\em International conference on learning representations}.

\bibitem[Wang et~al., 2020]{ijcai2020-387}
Wang, L., Shen, B., Zhao, N., and Zhang, Z. (2020).
\newblock Is the skip connection provable to reform the neural network loss
  landscape?
\newblock In Bessiere, C., editor, {\em Proceedings of the Twenty-Ninth
  International Joint Conference on Artificial Intelligence, {IJCAI-20}}, pages
  2792--2798. International Joint Conferences on Artificial Intelligence
  Organization.
\newblock Main track.

\bibitem[Zhang et~al., 2017]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net.

\end{thebibliography}
