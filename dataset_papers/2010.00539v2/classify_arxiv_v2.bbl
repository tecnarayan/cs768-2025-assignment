\begin{thebibliography}{41}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Angluin and Laird(1988)}]{angluin1988rcn}
\textsc{Angluin, D.} and \textsc{Laird, P.} (1988).
\newblock Learning from noisy examples.
\newblock \textit{Machine Learning} \textbf{2} 343--370.

\bibitem[{Awasthi et~al.(2017)Awasthi, Balcan and Long}]{awasthi}
\textsc{Awasthi, P.}, \textsc{Balcan, M.} and \textsc{Long, P.~M.} (2017).
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock \textit{J. {ACM}} \textbf{63} 50:1--50:27.

\bibitem[{Awasthi et~al.(2015)Awasthi, Balcan, Haghtalab and
  Urner}]{awasthi2015massart}
\textsc{Awasthi, P.}, \textsc{Balcan, M.-F.}, \textsc{Haghtalab, N.} and
  \textsc{Urner, R.} (2015).
\newblock Efficient learning of linear separators under bounded noise.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Awasthi et~al.(2016)Awasthi, Balcan, Haghtalab and
  Zhang}]{awasthi20161bitcompressednoise}
\textsc{Awasthi, P.}, \textsc{Balcan, M.-F.}, \textsc{Haghtalab, N.} and
  \textsc{Zhang, H.} (2016).
\newblock Learning and 1-bit compressed sensing under asymmetric noise.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Balcan and Haghtalab(2021)}]{balcan2020noise}
\textsc{Balcan, M.-F.} and \textsc{Haghtalab, N.} (2021).
\newblock Noise in classification.
\newblock In \textit{Beyond Worst Case Analysis of Algorithms} (T.~Roughgarden,
  ed.), chap.~16. Cambridge University Press.

\bibitem[{Balcan and Zhang(2017)}]{balcan2017logconcave}
\textsc{Balcan, M.-F.~F.} and \textsc{Zhang, H.} (2017).
\newblock Sample and computationally efficient learning algorithms under
  s-concave distributions.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Bartlett et~al.(2006)Bartlett, Jordan and
  McAuliffe}]{bartlett2006convexity}
\textsc{Bartlett, P.~L.}, \textsc{Jordan, M.~I.} and \textsc{McAuliffe, J.~D.}
  (2006).
\newblock Convexity, classification, and risk bounds.
\newblock \textit{Journal of the American Statistical Association} \textbf{101}
  138--156.
\newblock (Was Department of Statistics, U.C.\ Berkeley Technical Report number
  638, 2003).

\bibitem[{Ben-David et~al.(2012)Ben-David, Loker, Srebro and
  Sridharan}]{bendavid2012surrogate}
\textsc{Ben-David, S.}, \textsc{Loker, D.}, \textsc{Srebro, N.} and
  \textsc{Sridharan, K.} (2012).
\newblock Minimizing the misclassification error rate using a surrogate convex
  loss.
\newblock In \textit{International Conference on Machine Learning (ICML)}.

\bibitem[{Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin and
  Schapire}]{beygelzimer}
\textsc{Beygelzimer, A.}, \textsc{Langford, J.}, \textsc{Li, L.},
  \textsc{Reyzin, L.} and \textsc{Schapire, R.~E.} (2011).
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \textit{Conference on Artificial Intelligence and Statistics
  (AISTATS)}.

\bibitem[{Blum et~al.(1998)Blum, Frieze, Kannan and Vempala}]{blum1998rcn}
\textsc{Blum, A.}, \textsc{Frieze, A.}, \textsc{Kannan, R.} and
  \textsc{Vempala, S.} (1998).
\newblock A polynomial-time algorithm for learning noisy linear threshold
  functions.
\newblock \textit{Algorithmica} \textbf{22} 35--52.

\bibitem[{Boser et~al.(1992)Boser, Guyon and Vapnik}]{boser1992svm}
\textsc{Boser, B.~E.}, \textsc{Guyon, I.~M.} and \textsc{Vapnik, V.~N.} (1992).
\newblock A training algorithm for optimal margin classifiers.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Cao and Gu(2020)}]{cao2020generalization}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2020).
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \textit{Association for the Advancement of Artificial Intelligence
  (AAAI)}.

\bibitem[{Daniely(2016)}]{daniely2016complexity}
\textsc{Daniely, A.} (2016).
\newblock Complexity theoretic limitations on learning halfspaces.
\newblock In \textit{ACM Symposium on Theory of Computing (STOC)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{a}})Diakonikolas, Goel, Karmalkar,
  Klivans and Soltanolkotabi}]{diakonikolas2020relu}
\textsc{Diakonikolas, I.}, \textsc{Goel, S.}, \textsc{Karmalkar, S.},
  \textsc{Klivans, A.~R.} and \textsc{Soltanolkotabi, M.} (2020{\natexlab{a}}).
\newblock Approximation schemes for relu regression.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Diakonikolas et~al.(2019)Diakonikolas, Gouleakis and
  Tzamos}]{diakonikolas2019massart}
\textsc{Diakonikolas, I.}, \textsc{Gouleakis, T.} and \textsc{Tzamos, C.}
  (2019).
\newblock Distribution-independent pac learning of halfspaces with massart
  noise.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{b}})Diakonikolas, Kane and
  Zarifis}]{diakonikolas2020sqlb}
\textsc{Diakonikolas, I.}, \textsc{Kane, D.~M.} and \textsc{Zarifis, N.}
  (2020{\natexlab{b}}).
\newblock Near-optimal sq lower bounds for agnostically learning halfspaces and
  relus under gaussian marginals.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{c}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020massart}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{c}}).
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{d}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020tsybakov}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{d}}).
\newblock Learning halfspaces with tsybakov noise.
\newblock \textit{arXiv preprint arXiv:2006.06467} .

\bibitem[{Diakonikolas et~al.(2020{\natexlab{e}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020nonconvex}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{e}}).
\newblock Non-convex sgd learns halfspaces with adversarial label noise.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Foster et~al.(2018)Foster, Sekhari and Sridharan}]{foster2018}
\textsc{Foster, D.~J.}, \textsc{Sekhari, A.} and \textsc{Sridharan, K.} (2018).
\newblock Uniform convergence of gradients for non-convex learning and
  optimization.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Frei et~al.(2019)Frei, Cao and Gu}]{frei2019resnet}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Algorithm-dependent generalization bounds for overparameterized deep
  residual networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Frei et~al.(2020)Frei, Cao and Gu}]{frei2020singleneuron}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2020).
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Goel et~al.(2020)Goel, Gollakota and Klivans}]{goel2020sqlb}
\textsc{Goel, S.}, \textsc{Gollakota, A.} and \textsc{Klivans, A.} (2020).
\newblock Statistical-query lower bounds via functional gradients.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Goel et~al.(2019)Goel, Karmalkar and Klivans}]{goel2019relugaussian}
\textsc{Goel, S.}, \textsc{Karmalkar, S.} and \textsc{Klivans, A.~R.} (2019).
\newblock Time/accuracy tradeoffs for learning a relu with respect to gaussian
  marginals.
\newblock In \textit{Advances in Neural Information Processing Systems 32}.

\bibitem[{Guruswami and Raghavendra(2009)}]{guruswami2009hardness}
\textsc{Guruswami, V.} and \textsc{Raghavendra, P.} (2009).
\newblock Hardness of learning halfspaces with noise.
\newblock \textit{SIAM Journal on Computing} \textbf{39} 742--765.

\bibitem[{Ji et~al.(2020)Ji, Dud{\'i}k, Schapire and
  Telgarsky}]{ji2020regularization}
\textsc{Ji, Z.}, \textsc{Dud{\'i}k, M.}, \textsc{Schapire, R.~E.} and
  \textsc{Telgarsky, M.} (2020).
\newblock Gradient descent follows the regularization path for general losses.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Ji and Telgarsky(2020)}]{jitelgarsky2019.polylog}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2020).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Kalai et~al.(2008)Kalai, Klivans, Mansour and
  Servedio}]{kalai08agnostichalfspace}
\textsc{Kalai, A.~T.}, \textsc{Klivans, A.~R.}, \textsc{Mansour, Y.} and
  \textsc{Servedio, R.~A.} (2008).
\newblock Agnostically learning halfspaces.
\newblock \textit{SIAM J. Comput.} \textbf{37} 1777--1805.

\bibitem[{Kearns et~al.(1994)Kearns, Schapire and Sellie}]{kearns.agnostic}
\textsc{Kearns, M.~J.}, \textsc{Schapire, R.~E.} and \textsc{Sellie, L.~M.}
  (1994).
\newblock Toward efficient agnostic learning.
\newblock \textit{Machine Learning} \textbf{17} 115--141.

\bibitem[{Lov\'{a}sz and Vempala(2007)}]{lovasz}
\textsc{Lov\'{a}sz, L.} and \textsc{Vempala, S.} (2007).
\newblock The geometry of logconcave functions and sampling algorithms.
\newblock \textit{Random Struct. Algorithms} \textbf{30} 307â€“358.

\bibitem[{Massart et~al.(2006)Massart, N{\'e}d{\'e}lec
  et~al.}]{massart2006noise}
\textsc{Massart, P.}, \textsc{N{\'e}d{\'e}lec, {\'E}.} \textsc{et~al.} (2006).
\newblock Risk bounds for statistical learning.
\newblock \textit{The Annals of Statistics} \textbf{34} 2326--2366.

\bibitem[{Rosenblatt(1958)}]{rosenblatt1958perceptron}
\textsc{Rosenblatt, F.} (1958).
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock \textit{Psychological review} \textbf{65} 386.

\bibitem[{Servedio(1999)}]{servidio99average}
\textsc{Servedio, R.~A.} (1999).
\newblock On pac learning using winnow, perceptron, and a perceptron-like
  algorithm.
\newblock In \textit{Conference on Computational Learning Theory}.

\bibitem[{Shalev-Shwartz and Ben-David(2014)}]{shalevshwartz}
\textsc{Shalev-Shwartz, S.} and \textsc{Ben-David, S.} (2014).
\newblock \textit{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, New York, NY, USA.

\bibitem[{Shamir(2020)}]{shamir2020}
\textsc{Shamir, O.} (2020).
\newblock Gradient methods never overfit on separable data.
\newblock \textit{arXiv preprint} \textbf{arXiv:2007.00028}.

\bibitem[{Sloan(1988)}]{sloan1988}
\textsc{Sloan, R.} (1988).
\newblock Types of noise in data for concept learning.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Srebro et~al.(2010)Srebro, Sridharan and
  Tewari}]{srebro2010smoothness}
\textsc{Srebro, N.}, \textsc{Sridharan, K.} and \textsc{Tewari, A.} (2010).
\newblock Smoothness, low noise and fast rates.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Sridharan et~al.(2009)Sridharan, Shalev-Shwartz and
  Srebro}]{sridharan2009fast}
\textsc{Sridharan, K.}, \textsc{Shalev-Shwartz, S.} and \textsc{Srebro, N.}
  (2009).
\newblock Fast rates for regularized objectives.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Tsybakov et~al.(2004)}]{tsybakov2004noise}
\textsc{Tsybakov, A.~B.} \textsc{et~al.} (2004).
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \textit{The Annals of Statistics} \textbf{32} 135--166.

\bibitem[{Vershynin(2010)}]{vershynin}
\textsc{Vershynin, R.} (2010).
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \textit{arXiv preprint} \textbf{arXiv:1011.3027}.

\bibitem[{Zhang et~al.(2019)Zhang, Yu, Wang and Gu}]{zhang2019relu}
\textsc{Zhang, X.}, \textsc{Yu, Y.}, \textsc{Wang, L.} and \textsc{Gu, Q.}
  (2019).
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}.

\end{thebibliography}
