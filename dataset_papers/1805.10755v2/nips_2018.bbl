\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel et~al.(2005)Abbeel, Ganapathi, and Ng]{Abbeel2005}
Pieter Abbeel, Varun Ganapathi, and Andrew~Y Ng.
\newblock Learning vehicular dynamics, with application to modeling
  helicopters.
\newblock In \emph{NIPS}, pages 1--8, 2005.

\bibitem[Anthony et~al.(2017)Anthony, Tian, and Barber]{anthony2017thinking}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock \emph{arXiv preprint arXiv:1705.08439}, 2017.

\bibitem[Atkeson(1994)]{atkeson1994using}
Christopher~G Atkeson.
\newblock Using local trajectory optimizers to speed up global optimization in
  dynamic programming.
\newblock In \emph{Advances in neural information processing systems}, pages
  663--670, 1994.

\bibitem[Atkeson(2012)]{atkeson2012efficient}
Christopher~G Atkeson.
\newblock Efficient robust policy optimization.
\newblock In \emph{American Control Conference (ACC), 2012}, pages 5220--5227.
  IEEE, 2012.

\bibitem[Atkeson and Morimoto(2003)]{atkeson2003nonparametric}
Christopher~G Atkeson and Jun Morimoto.
\newblock Nonparametric representation of policies and value functions: A
  trajectory-based approach.
\newblock In \emph{Advances in neural information processing systems}, pages
  1643--1650, 2003.

\bibitem[Bagnell and Schneider(2003)]{bagnell2003covariant}
J~Andrew Bagnell and Jeff Schneider.
\newblock Covariant policy search.
\newblock IJCAI, 2003.

\bibitem[Bagnell and Schneider(2001)]{bagnell2001autonomous}
J~Andrew Bagnell and Jeff~G Schneider.
\newblock Autonomous helicopter control using reinforcement learning policy
  search methods.
\newblock In \emph{Robotics and Automation, 2001. Proceedings 2001 ICRA. IEEE
  International Conference on}, volume~2, pages 1615--1620. IEEE, 2001.

\bibitem[Bagnell et~al.(2004)Bagnell, Kakade, Schneider, and
  Ng]{bagnell2004policy}
J~Andrew Bagnell, Sham~M Kakade, Jeff~G Schneider, and Andrew~Y Ng.
\newblock Policy search by dynamic programming.
\newblock In \emph{Advances in neural information processing systems}, pages
  831--838, 2004.

\bibitem[Baxter and Bartlett(2001)]{baxter2001infinite}
Jonathan Baxter and Peter~L Bartlett.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Bertsekas and Tsitsiklis(1995)]{bertsekas1995neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Neuro-dynamic programming: an overview.
\newblock In \emph{Decision and Control, 1995., Proceedings of the 34th IEEE
  Conference on}, volume~1, pages 560--564. IEEE, 1995.

\bibitem[Finn et~al.(2016)Finn, Zhang, Fu, Tan, McCarthy, Scharff, and
  Levine]{fzftm-gpsi-16}
C.~Finn, M.~Zhang, J.~Fu, X.~Tan, Z.~McCarthy, E.~Scharff, and S.~Levine.
\newblock Guided policy search code implementation, 2016.
\newblock URL \url{http://rll.berkeley.edu/gps}.
\newblock Software available from rll.berkeley.edu/gps.

\bibitem[Gorodetsky et~al.(2015)Gorodetsky, Karaman, and
  Marzouk]{Gorodetsky-RSS-15}
Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk.
\newblock Efficient high-dimensional stochastic optimal motion control using
  tensor-train decomposition.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, Rome, Italy,
  July 2015.
\newblock \doi{10.15607/RSS.2015.XI.015}.

\bibitem[Kakade(2002)]{kakade2002natural}
Sham Kakade.
\newblock A natural policy gradient.
\newblock \emph{NIPS}, 2002.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, 2002.

\bibitem[Kocsis and Szepesv{\'a}ri(2006)]{kocsis2006bandit}
Levente Kocsis and Csaba Szepesv{\'a}ri.
\newblock Bandit based monte-carlo planning.
\newblock In \emph{European conference on machine learning}, pages 282--293.
  Springer, 2006.

\bibitem[Kwakernaak and Sivan(1972)]{kwakernaak1972linear}
Huibert Kwakernaak and Raphael Sivan.
\newblock \emph{Linear optimal control systems}, volume~1.
\newblock Wiley-Interscience New York, 1972.

\bibitem[Lazaric et~al.(2010)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2010analysis}
Alessandro Lazaric, Mohammad Ghavamzadeh, and R{\'e}mi Munos.
\newblock Analysis of a classification-based policy iteration algorithm.
\newblock In \emph{ICML-27th International Conference on Machine Learning},
  pages 607--614. Omnipress, 2010.

\bibitem[Levine and Abbeel(2014)]{levine2014learning}
Sergey Levine and Pieter Abbeel.
\newblock Learning neural network policies with guided policy search under
  unknown dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1071--1079, 2014.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Horia Mania, Aurelia Guy, and Benjamin Recht.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[Montgomery et~al.(2017)Montgomery, Ajay, Finn, Abbeel, and
  Levine]{montgomery2017reset}
William Montgomery, Anurag Ajay, Chelsea Finn, Pieter Abbeel, and Sergey
  Levine.
\newblock Reset-free guided policy search: efficient deep reinforcement
  learning with stochastic initial states.
\newblock In \emph{Robotics and Automation (ICRA), 2017 IEEE International
  Conference on}, pages 3373--3380. IEEE, 2017.

\bibitem[Montgomery and Levine(2016)]{montgomery2016guided}
William~H Montgomery and Sergey Levine.
\newblock Guided policy search via approximate mirror descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4008--4016, 2016.

\bibitem[Ross and Bagnell(2012)]{ross2012agnostic}
Stephane Ross and Drew Bagnell.
\newblock Agnostic system identification for model-based reinforcement
  learning.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning (ICML-12)}, pages 1703--1710, 2012.

\bibitem[Ross and Bagnell(2014)]{ross2014reinforcement}
Stephane Ross and J~Andrew Bagnell.
\newblock Reinforcement and imitation learning via interactive no-regret
  learning.
\newblock \emph{arXiv preprint arXiv:1406.5979}, 2014.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{Ross2011_AISTATS}
St{\'e}phane Ross, Geoffrey~J Gordon, and J.Andrew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{AISTATS}, 2011.

\bibitem[Rummery and Niranjan(1994)]{rummery1994line}
Gavin~A Rummery and Mahesan Niranjan.
\newblock \emph{On-line Q-learning using connectionist systems}, volume~37.
\newblock University of Cambridge, Department of Engineering, 1994.

\bibitem[Scherrer(2014)]{scherrer2014approximate}
Bruno Scherrer.
\newblock Approximate policy iteration schemes: a comparison.
\newblock In \emph{International Conference on Machine Learning}, pages
  1314--1322, 2014.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{ICML}, pages 1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Silver et~al.(2016)]{silver2016mastering}
David Silver et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Sun et~al.(2016)Sun, Venkatraman, Boots, and Bagnell]{sun2016learning}
Wen Sun, Arun Venkatraman, Byron Boots, and J~Andrew Bagnell.
\newblock Learning to filter with predictive state inference machines.
\newblock In \emph{ICML}, 2016.

\bibitem[Sun et~al.(2017)Sun, Venkatraman, Gordon, Boots, and
  Bagnell]{sun2017deeply}
Wen Sun, Arun Venkatraman, Geoffrey~J Gordon, Byron Boots, and J~Andrew
  Bagnell.
\newblock Deeply aggrevated: Differentiable imitation learning for sequential
  prediction.
\newblock \emph{ICML}, 2017.

\bibitem[Sutton and Barto(1998)]{sutton1998introduction}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Introduction to reinforcement learning}, volume 135.
\newblock MIT Press Cambridge, 1998.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pages 5026--5033. IEEE, 2012.

\bibitem[Venkatraman et~al.(2015)Venkatraman, Hebert, and
  Bagnell]{venkatraman2015improving}
Arun Venkatraman, Martial Hebert, and J~Andrew Bagnell.
\newblock Improving multi-step prediction of learned time series models.
\newblock \emph{AAAI}, 2015.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 1992.

\bibitem[Zhou et~al.(1996)Zhou, Doyle, Glover, et~al.]{zhou1996robust}
Kemin Zhou, John~Comstock Doyle, Keith Glover, et~al.
\newblock \emph{Robust and optimal control}, volume~40.
\newblock Prentice hall New Jersey, 1996.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock 2010.

\bibitem[Zinkevich(2003)]{Zinkevich2003_ICML}
Martin Zinkevich.
\newblock {Online Convex Programming and Generalized Infinitesimal Gradient
  Ascent}.
\newblock In \emph{ICML}, 2003.

\end{thebibliography}
