\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alabdulmohsin(2020)]{alabdulmohsin2020towards}
I.~Alabdulmohsin.
\newblock Towards a unified theory of learning and information.
\newblock \emph{Entropy}, 22\penalty0 (4):\penalty0 438, 2020.

\bibitem[Alabdulmohsin(2015)]{alabdulmohsin2015algorithmic}
I.~M. Alabdulmohsin.
\newblock Algorithmic stability and uniform generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 19--27, 2015.

\bibitem[Bassily et~al.(2016)Bassily, Nissim, Smith, Steinke, Stemmer, and
  Ullman]{bassily2016algorithmic}
R.~Bassily, K.~Nissim, A.~Smith, T.~Steinke, U.~Stemmer, and J.~Ullman.
\newblock Algorithmic stability for adaptive data analysis.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 1046--1059, 2016.

\bibitem[Bassily et~al.(2018)Bassily, Moran, Nachum, Shafer, and
  Yehudayoff]{bassily2018learners}
R.~Bassily, S.~Moran, I.~Nachum, J.~Shafer, and A.~Yehudayoff.
\newblock Learners that use little information.
\newblock In \emph{Algorithmic Learning Theory}, pages 25--55. PMLR, 2018.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bu et~al.(2020)Bu, Zou, and Veeravalli]{bu2020tightening}
Y.~Bu, S.~Zou, and V.~V. Veeravalli.
\newblock Tightening mutual information-based bounds on generalization error.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 121--130, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Esposito et~al.(2021)Esposito, Gastpar, and
  Issa]{esposito2021generalization}
A.~R. Esposito, M.~Gastpar, and I.~Issa.
\newblock Generalization error bounds via r{\'e}nyi-, f-divergences and maximal
  leakage.
\newblock \emph{IEEE Transactions on Information Theory}, 2021.

\bibitem[Feldman and Steinke(2018)]{feldman2018calibrating}
V.~Feldman and T.~Steinke.
\newblock Calibrating noise to variance in adaptive data analysis.
\newblock In \emph{Conference On Learning Theory}, pages 535--544, 2018.

\bibitem[Gelfand and Mitter(1991)]{gelfand1991recursive}
S.~B. Gelfand and S.~K. Mitter.
\newblock Recursive stochastic algorithms for global optimization in r\^{}d.
\newblock \emph{SIAM Journal on Control and Optimization}, 29\penalty0
  (5):\penalty0 999--1018, 1991.

\bibitem[Gray(2011)]{gray2011entropy}
R.~M. Gray.
\newblock \emph{Entropy and information theory}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Gupta et~al.(2021)Gupta, Stripelis, Lam, Thompson, Ambite, and
  Steeg]{gupta2021membership}
U.~Gupta, D.~Stripelis, P.~K. Lam, P.~Thompson, J.~L. Ambite, and G.~V. Steeg.
\newblock Membership inference attacks on deep regression models for
  neuroimaging.
\newblock In \emph{Medical Imaging with Deep Learning}, 2021.

\bibitem[Hafez-Kolahi et~al.(2020)Hafez-Kolahi, Golgooni, Kasaei, and
  Soleymani]{hafez2020conditioning}
H.~Hafez-Kolahi, Z.~Golgooni, S.~Kasaei, and M.~Soleymani.
\newblock Conditioning and processing: Techniques to improve
  information-theoretic generalization bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{haghifam2020sharpened}
M.~Haghifam, J.~Negrea, A.~Khisti, D.~M. Roy, and G.~K. Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 9925--9935. Curran Associates, Inc., 2020.

\bibitem[Harutyunyan et~al.(2021)Harutyunyan, Achille, Paolini, Majumder,
  Ravichandran, Bhotika, and Soatto]{harutyunyan2021estimating}
H.~Harutyunyan, A.~Achille, G.~Paolini, O.~Majumder, A.~Ravichandran,
  R.~Bhotika, and S.~Soatto.
\newblock Estimating informativeness of samples with smooth unique information.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Jiang* et~al.(2020)Jiang*, Neyshabur*, Mobahi, Krishnan, and
  Bengio]{Jiang*2020Fantastic}
Y.~Jiang*, B.~Neyshabur*, H.~Mobahi, D.~Krishnan, and S.~Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgIPJBFvH}.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Nasr et~al.(2019)Nasr, Shokri, and Houmansadr]{nasr2019comprehensive}
M.~Nasr, R.~Shokri, and A.~Houmansadr.
\newblock Comprehensive privacy analysis of deep learning: Passive and active
  white-box inference attacks against centralized and federated learning.
\newblock In \emph{2019 IEEE symposium on security and privacy (SP)}, pages
  739--753. IEEE, 2019.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{negrea2019information}
J.~Negrea, M.~Haghifam, G.~K. Dziugaite, A.~Khisti, and D.~M. Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11015--11025, 2019.

\bibitem[Neu(2021)]{neu2021information}
G.~Neu.
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:2102.00931}, 2021.

\bibitem[Paninski(2003)]{paninski2003estimation}
L.~Paninski.
\newblock Estimation of entropy and mutual information.
\newblock \emph{Neural computation}, 15\penalty0 (6):\penalty0 1191--1253,
  2003.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and Loh]{pensia2018generalization}
A.~Pensia, V.~Jog, and P.-L. Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE, 2018.

\bibitem[Raginsky et~al.(2016)Raginsky, Rakhlin, Tsao, Wu, and
  Xu]{raginsky2016information}
M.~Raginsky, A.~Rakhlin, M.~Tsao, Y.~Wu, and A.~Xu.
\newblock Information-theoretic analysis of stability and bias of learning
  algorithms.
\newblock In \emph{2016 IEEE Information Theory Workshop (ITW)}, pages 26--30.
  IEEE, 2016.

\bibitem[Raginsky et~al.(2021)Raginsky, Rakhlin, and Xu]{raginsky202110}
M.~Raginsky, A.~Rakhlin, and A.~Xu.
\newblock Information-theoretic stability and generalization.
\newblock \emph{Information-Theoretic Methods in Data Science}, page 302, 2021.

\bibitem[Russo and Zou(2019)]{russo2019much}
D.~Russo and J.~Zou.
\newblock How much does your data exploration overfit? controlling bias via
  information usage.
\newblock \emph{IEEE Transactions on Information Theory}, 66\penalty0
  (1):\penalty0 302--323, 2019.

\bibitem[Sauer(1972)]{sauer1972density}
N.~Sauer.
\newblock On the density of families of sets.
\newblock \emph{Journal of Combinatorial Theory, Series A}, 13\penalty0
  (1):\penalty0 145--147, 1972.

\bibitem[Shelah(1972)]{shelah1972combinatorial}
S.~Shelah.
\newblock A combinatorial problem; stability and order for models and theories
  in infinitary languages.
\newblock \emph{Pacific Journal of Mathematics}, 41\penalty0 (1):\penalty0
  247--261, 1972.

\bibitem[Shokri et~al.(2017)Shokri, Stronati, Song, and
  Shmatikov]{shokri2017membership}
R.~Shokri, M.~Stronati, C.~Song, and V.~Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pages
  3--18. IEEE, 2017.

\bibitem[Steinke and Zakynthinou(2020)]{steinke2020reasoning}
T.~Steinke and L.~Zakynthinou.
\newblock {R}easoning {A}bout {G}eneralization via {C}onditional {M}utual
  {I}nformation.
\newblock In J.~Abernethy and S.~Agarwal, editors, \emph{Proceedings of Thirty
  Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of
  Machine Learning Research}, pages 3437--3452. PMLR, 09--12 Jul 2020.

\bibitem[Vapnik(1998)]{vapnik1998statistical}
V.~Vapnik.
\newblock Statistical learning theory new york.
\newblock \emph{NY: Wiley}, 1998.

\bibitem[Verdu and Weissman(2008)]{verdu2008information}
S.~Verdu and T.~Weissman.
\newblock The information lost in erasures.
\newblock \emph{IEEE Transactions on Information Theory}, 54\penalty0
  (11):\penalty0 5030--5058, 2008.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wang et~al.(2016)Wang, Lei, and Fienberg]{wang2016average}
Y.-X. Wang, J.~Lei, and S.~E. Fienberg.
\newblock On-average kl-privacy and its equivalence to generalization for
  max-entropy mechanisms.
\newblock In \emph{International Conference on Privacy in Statistical
  Databases}, pages 121--134. Springer, 2016.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem[Xu and Raginsky(2017)]{xu2017information}
A.~Xu and M.~Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2524--2533, 2017.

\bibitem[Yeom et~al.(2018)Yeom, Giacomelli, Fredrikson, and
  Jha]{yeom2018privacy}
S.~Yeom, I.~Giacomelli, M.~Fredrikson, and S.~Jha.
\newblock Privacy risk in machine learning: Analyzing the connection to
  overfitting.
\newblock In \emph{2018 IEEE 31st Computer Security Foundations Symposium
  (CSF)}, pages 268--282. IEEE, 2018.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
