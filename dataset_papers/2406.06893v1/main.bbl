\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38546--38556, 2022.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock \emph{arXiv preprint arXiv:2306.04637}, 2023.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21750--21764, 2022.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Candes and Tao(2005)]{candes2005decoding}
Emmanuel Candes and Terence Tao.
\newblock Decoding by linear programming, 2005.

\bibitem[Chen et~al.(2024)Chen, Sheen, Wang, and Yang]{chen2024training}
Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang.
\newblock Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality.
\newblock \emph{arXiv preprint arXiv:2402.19442}, 2024.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Damian et~al.(2022)Damian, Nichani, and Lee]{damian2022self}
Alex Damian, Eshaan Nichani, and Jason~D Lee.
\newblock Self-stabilization: The implicit bias of gradient descent at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2209.15594}, 2022.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pages 7480--7512. PMLR, 2023.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dubois et~al.(2019)Dubois, Dagan, Hupkes, and Bruni]{dubois2019location}
Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni.
\newblock Location attention for extrapolation to longer sequences.
\newblock \emph{arXiv preprint arXiv:1911.03872}, 2019.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pages 5793--5831. PMLR, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1, 2021.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30583--30598, 2022.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock \emph{arXiv preprint arXiv:2301.13196}, 2023.

\bibitem[Hahn(2020)]{hahn2020theoretical}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 156--171, 2020.

\bibitem[Haviv et~al.(2022)Haviv, Ram, Press, Izsak, and Levy]{haviv2022transformer}
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy.
\newblock Transformer language models without positional encodings still learn positional information.
\newblock \emph{arXiv preprint arXiv:2203.16634}, 2022.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Hupkes et~al.(2020)Hupkes, Dankers, Mul, and Bruni]{hupkes2020compositionality}
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni.
\newblock Compositionality decomposed: How do neural networks generalise?
\newblock \emph{Journal of Artificial Intelligence Research}, 67:\penalty0 757--795, 2020.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
Samy Jelassi, Michael Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37822--37836, 2022.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov, Ronneberger, Tunyasuvunakool, Bates, Z{\'i}dek, Potapenko, Bridgland, Meyer, Kohl, Ballard, Cowie, Romera-Paredes, Nikolov, Jain, Adler, Back, Petersen, Reiman, Clancy, Zielinski, Steinegger, Pacholska, Berghammer, Bodenstein, Silver, Vinyals, Senior, Kavukcuoglu, Kohli, and Hassabis]{Jumper2021HighlyAP}
John~M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Z{\'i}dek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A~A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David~A. Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew~W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596:\penalty0 583 -- 589, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:235959867}.

\bibitem[Kazemnejad et~al.(2023)Kazemnejad, Padhi, Ramamurthy, Das, and Reddy]{kazemnejad2023impact}
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan~Natesan Ramamurthy, Payel Das, and Siva Reddy.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock \emph{arXiv preprint arXiv:2305.19466}, 2023.

\bibitem[Kim and Suzuki(2024)]{kim2024transformers}
Juno Kim and Taiji Suzuki.
\newblock Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape.
\newblock \emph{arXiv preprint arXiv:2402.01258}, 2024.

\bibitem[Li et~al.(2023)Li, Wang, Liu, and Chen]{li2023theoretical}
Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen.
\newblock A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity.
\newblock \emph{arXiv preprint arXiv:2302.06015}, 2023.

\bibitem[Li et~al.(2022)Li, Wang, and Li]{li2022analyzing}
Zhouzi Li, Zixuan Wang, and Jian Li.
\newblock Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.12678}, 2022.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and Weller]{likhosherstov2021expressive}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the expressive power of self-attention matrices.
\newblock \emph{arXiv preprint arXiv:2106.03764}, 2021.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{mahankali2023one}
Arvind Mahankali, Tatsunori~B Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock \emph{arXiv preprint arXiv:2307.03576}, 2023.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Liberum, Smith, and Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[Newman et~al.(2020)Newman, Hewitt, Liang, and Manning]{newman2020eos}
Benjamin Newman, John Hewitt, Percy Liang, and Christopher~D Manning.
\newblock The eos decision and length extrapolation.
\newblock \emph{arXiv preprint arXiv:2010.07174}, 2020.

\bibitem[Nichani et~al.(2024)Nichani, Damian, and Lee]{nichani2024transformers}
Eshaan Nichani, Alex Damian, and Jason~D Lee.
\newblock How transformers learn causal structure with gradient descent.
\newblock \emph{arXiv preprint arXiv:2402.14735}, 2024.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Panigrahi et~al.(2023)Panigrahi, Malladi, Xia, and Arora]{panigrahi2023trainable}
Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora.
\newblock Trainable transformer in transformer.
\newblock \emph{arXiv preprint arXiv:2307.01189}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Köpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library, 2019.

\bibitem[P{\'e}rez et~al.(2019)P{\'e}rez, Marinkovi{\'c}, and Barcel{\'o}]{perez2019turing}
Jorge P{\'e}rez, Javier Marinkovi{\'c}, and Pablo Barcel{\'o}.
\newblock On the turing completeness of modern neural network architectures.
\newblock \emph{arXiv preprint arXiv:1901.03429}, 2019.

\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{press2021train}
Ofir Press, Noah~A Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock \emph{arXiv preprint arXiv:2108.12409}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Ruis et~al.(2020)Ruis, Andreas, Baroni, Bouchacourt, and Lake]{ruis2020benchmark}
Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden~M Lake.
\newblock A benchmark for systematic generalization in grounded language understanding.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 19861--19872, 2020.

\bibitem[Ruoss et~al.(2023)Ruoss, Del{\'e}tang, Genewein, Grau-Moya, Csord{\'a}s, Bennani, Legg, and Veness]{ruoss2023randomized}
Anian Ruoss, Gr{\'e}goire Del{\'e}tang, Tim Genewein, Jordi Grau-Moya, R{\'o}bert Csord{\'a}s, Mehdi Bennani, Shane Legg, and Joel Veness.
\newblock Randomized positional encodings boost length generalization of transformers.
\newblock \emph{arXiv preprint arXiv:2305.16843}, 2023.

\bibitem[Sanford et~al.(2023)Sanford, Hsu, and Telgarsky]{sanford2023representational}
Clayton Sanford, Daniel Hsu, and Matus Telgarsky.
\newblock Representational strengths and limitations of transformers.
\newblock \emph{arXiv preprint arXiv:2306.02896}, 2023.

\bibitem[Shen et~al.(2023)Shen, Bubeck, Eldan, Lee, Li, and Zhang]{shen2023positional}
Ruoqi Shen, S{\'e}bastien Bubeck, Ronen Eldan, Yin~Tat Lee, Yuanzhi Li, and Yi~Zhang.
\newblock Positional description matters for transformers arithmetic.
\newblock \emph{arXiv preprint arXiv:2311.14737}, 2023.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv e-prints}, pages arXiv--2104, 2021.

\bibitem[Tarzanagh et~al.(2023)Tarzanagh, Li, Thrampoulidis, and Oymak]{tarzanagh2023transformers}
Davoud~Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak.
\newblock Transformers as support vector machines.
\newblock \emph{arXiv preprint arXiv:2308.16898}, 2023.

\bibitem[Tian et~al.(2023{\natexlab{a}})Tian, Wang, Chen, and Du]{tian2023scan}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock \emph{arXiv preprint arXiv:2305.16380}, 2023{\natexlab{a}}.

\bibitem[Tian et~al.(2023{\natexlab{b}})Tian, Wang, Zhang, Chen, and Du]{tian2023joma}
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention.
\newblock \emph{arXiv preprint arXiv:2310.00535}, 2023{\natexlab{b}}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and Salakhutdinov]{tsai2019transformer}
Yao-Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.
\newblock Transformer dissection: a unified understanding of transformer's attention via the lens of kernel.
\newblock \emph{arXiv preprint arXiv:1908.11775}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 35151--35174. PMLR, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wortsman et~al.(2024)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, Pennington, Sohl-Dickstein, Xu, Lee, Gilmer, and Kornblith]{wortsman2024smallscale}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie~E Everett, Alexander~A Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=d8w0pmvXbZ}.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock \emph{arXiv preprint arXiv:2105.11115}, 2021.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and Wagner]{zhang2206unveiling}
Yi~Zhang, Arturs Backurs, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task, 2022.
\newblock \emph{URL https://arxiv. org/abs/2206.04301}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Panigrahi, Ge, and Arora]{zhao2023transformers}
Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora.
\newblock Do transformers parse while predicting the masked word?
\newblock \emph{arXiv preprint arXiv:2303.08117}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran]{zhou2023algorithms}
Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran.
\newblock What algorithms can transformers learn? a study in length generalization.
\newblock \emph{arXiv preprint arXiv:2310.16028}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Alon, Chen, Wang, Agarwal, and Zhou]{zhou2024transformers}
Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou.
\newblock Transformers can achieve length generalization but not robustly.
\newblock \emph{arXiv preprint arXiv:2402.09371}, 2024.

\end{thebibliography}
