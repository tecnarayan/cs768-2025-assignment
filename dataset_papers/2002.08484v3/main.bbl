\begin{thebibliography}{10}

\bibitem{trackin_code}
{TracIn Code}.
\newblock \url{ https://github.com/frederick0329/TracIn}.

\bibitem{Liang}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1885--1894, 2017.

\bibitem{representer}
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep~K Ravikumar.
\newblock Representer point selection for explaining deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9291--9301, 2018.

\bibitem{STY17}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3319--3328, 2017.

\bibitem{Lundberg2017AUA}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In {\em Advances in neural information processing systems}, pages
  4765--4774, 2017.

\bibitem{Lime}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock " {W}hy should i trust you?" {E}xplaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1135--1144, 2016.

\bibitem{Owen2}
Art~B Owen and Cl{\'e}mentine Prieur.
\newblock On shapley value for measuring importance of dependent inputs.
\newblock {\em SIAM/ASA Journal on Uncertainty Quantification}, 5(1):986--1002,
  2017.

\bibitem{Owen1}
Art~B Owen.
\newblock Sobol'indices and shapley value.
\newblock {\em SIAM/ASA Journal on Uncertainty Quantification}, 2(1):245--251,
  2014.

\bibitem{DataValuation}
Ruoxi Jia, David Dao, Boxin Wang, Frances~Ann Hubis, Nick Hynes, Nezihe~Merve
  G{\"u}rel, Bo~Li, Ce~Zhang, Dawn Song, and Costas~J Spanos.
\newblock Towards efficient data valuation based on the shapley value.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1167--1176, 2019.

\bibitem{DataShapley}
Amirata Ghorbani and James Zou.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2242--2251, 2019.

\bibitem{SGD-Influence}
Satoshi Hara, Atsushi Nitanda, and Takanori Maehara.
\newblock Data cleansing for models trained with sgd.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4215--4224, 2019.

\bibitem{SennrichHB16}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Edinburgh neural machine translation systems for {WMT} 16.
\newblock In {\em Proceedings of the First Conference on Machine Translation},
  pages 371--376, 2016.

\bibitem{LaineA17}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In {\em 5th International Conference on Learning Representations},
  2017.

\bibitem{WeiHWDC19}
Hao{-}Ran Wei, Shujian Huang, Ran Wang, Xin{-}Yu Dai, and Jiajun Chen.
\newblock Online distilling from checkpoints for neural machine translation.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em
  Proceedings of the 17th Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies},
  pages 1932--1941, 2019.

\bibitem{Shen2019LearningWB}
Yanyao Shen and Sujay Sanghavi.
\newblock Learning with bad training data via iterative trimmed loss
  minimization.
\newblock In {\em ICML}, 2019.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, Toronto, Canada, 2009.

\bibitem{california}
R~Kelley Pace and Ronald Barry.
\newblock Sparse spatial autoregressions.
\newblock {\em Statistics \& Probability Letters}, 33(3):291--297, 1997.

\bibitem{zhang2015character}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em Advances in neural information processing systems}, pages
  649--657, 2015.

\bibitem{shen2018baseline}
Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin~Renqiang Min, Qinliang Su, Yizhe
  Zhang, Chunyuan Li, Ricardo Henao, and Lawrence Carin.
\newblock Baseline needs more love: On simple word-embedding-based models and
  associated pooling mechanisms.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 440--450, 2018.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 66--71, 2018.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{fortifying}
Xiaochuang Han and Yulia Tsvetkov.
\newblock Fortifying toxic speech detectors against veiled toxicity, 2020.

\bibitem{cook1982residuals}
R~Dennis Cook and Sanford Weisberg.
\newblock {\em Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem{Pearlmutter94fastexact}
Barak~A. Pearlmutter.
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural Computation}, 6:147--160, 1994.

\bibitem{representer_theorem}
Bernhard Sch{\"o}lkopf, Ralf Herbrich, and Alex~J Smola.
\newblock A generalized representer theorem.
\newblock In {\em International conference on computational learning theory},
  pages 416--426. Springer, 2001.

\bibitem{Github:annoy}
Erik Bernhardsson.
\newblock {\em Annoy: Approximate Nearest Neighbors in C++/Python}, 2018.
\newblock Python package version 1.13.0.

\bibitem{random-proj}
David~P Woodruff et~al.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10(1--2):1--157, 2014.

\end{thebibliography}
