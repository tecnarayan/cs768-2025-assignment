\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., and Tweed, D.~B.
\newblock Deep learning without weight transport.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 976--984, 2019.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
Bartunov, S., Santoro, A., Richards, B.~A., Marris, L., Hinton, G.~E., and
  Lillicrap, T.~P.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  9390--9400, 2018.

\bibitem[Bengio(2014)]{bengio2014auto}
Bengio, Y.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock \emph{arXiv preprint arXiv:1407.7906}, 2014.

\bibitem[Bengio(2020)]{bengio2020deriving}
Bengio, Y.
\newblock Deriving differential target propagation from iterating approximate
  inverses.
\newblock \emph{arXiv preprint arXiv:2007.15139}, 2020.

\bibitem[Choromanska et~al.(2019)Choromanska, Cowen, Kumaravel, Luss, Rigotti,
  Rish, Diachille, Gurev, Kingsbury, Tejwani, et~al.]{choromanska2019beyond}
Choromanska, A., Cowen, B., Kumaravel, S., Luss, R., Rigotti, M., Rish, I.,
  Diachille, P., Gurev, V., Kingsbury, B., Tejwani, R., et~al.
\newblock Beyond backprop: Online alternating minimization with auxiliary
  variables.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1193--1202. PMLR, 2019.

\bibitem[Crafton et~al.(2019)Crafton, Parihar, Gebhardt, and
  Raychowdhury]{crafton2019direct}
Crafton, B., Parihar, A., Gebhardt, E., and Raychowdhury, A.
\newblock Direct feedback alignment with sparse connections for local learning.
\newblock \emph{Frontiers in neuroscience}, 13:\penalty0 525, 2019.

\bibitem[Debanne(2004)]{debanne2004information}
Debanne, D.
\newblock Information processing in the axon.
\newblock \emph{Nature Reviews Neuroscience}, 5\penalty0 (4):\penalty0
  304--316, 2004.

\bibitem[Debanne et~al.(2011)Debanne, Campanac, Bialowas, Carlier, and
  Alcaraz]{debanne2011axon}
Debanne, D., Campanac, E., Bialowas, A., Carlier, E., and Alcaraz, G.
\newblock Axon physiology.
\newblock \emph{Physiological reviews}, 91\penalty0 (2):\penalty0 555--602,
  2011.

\bibitem[Gauss(1877)]{gauss1877theoria}
Gauss, C.~F.
\newblock \emph{Theoria motus corporum coelestium in sectionibus conicis solem
  ambientium}, volume~7.
\newblock FA Perthes, 1877.

\bibitem[Guerguiev et~al.(2019)Guerguiev, Kording, and
  Richards]{guerguiev2019spike}
Guerguiev, J., Kording, K., and Richards, B.
\newblock Spike-based causal inference for weight alignment.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kunin et~al.(2020)Kunin, Nayebi, Sagastuy-Brena, Ganguli, Bloom, and
  Yamins]{kunin2020two}
Kunin, D., Nayebi, A., Sagastuy-Brena, J., Ganguli, S., Bloom, J., and Yamins,
  D.
\newblock Two routes to scalable credit assignment without weight symmetry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5511--5521. PMLR, 2020.

\bibitem[Lansdell et~al.(2019)Lansdell, Prakash, and
  Kording]{lansdell2019learning}
Lansdell, B.~J., Prakash, P.~R., and Kording, K.~P.
\newblock Learning to solve the credit assignment problem.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Launay et~al.(2019)Launay, Poli, and Krzakala]{launay2019principled}
Launay, J., Poli, I., and Krzakala, F.
\newblock Principled training of neural networks with direct feedback
  alignment.
\newblock \emph{arXiv preprint arXiv:1906.04554}, 2019.

\bibitem[LeCun(1987)]{yann1987modeles}
LeCun, Y.
\newblock \emph{Modeles connexionnistes de lapprentissage}.
\newblock PhD thesis, These de Doctorat, Universite Paris, 1987.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee2015difference}
Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y.
\newblock Difference target propagation.
\newblock In \emph{Joint european conference on machine learning and knowledge
  discovery in databases}, pp.\  498--515. Springer, 2015.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Lillicrap, T.~P., Cownden, D., Tweed, D.~B., and Akerman, C.~J.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature communications}, 7\penalty0 (1):\penalty0 1--10, 2016.

\bibitem[Lillicrap et~al.(2020)Lillicrap, Santoro, Marris, Akerman, and
  Hinton]{lillicrap2020backpropagation}
Lillicrap, T.~P., Santoro, A., Marris, L., Akerman, C.~J., and Hinton, G.
\newblock Backpropagation and the brain.
\newblock \emph{Nature Reviews Neuroscience}, 21\penalty0 (6):\penalty0
  335--346, 2020.

\bibitem[Manchev \& Spratling(2020)Manchev and Spratling]{manchev2020target}
Manchev, N. and Spratling, M.~W.
\newblock Target propagation in recurrent neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 7--1, 2020.

\bibitem[Meulemans et~al.(2020)Meulemans, Carzaniga, Suykens, Sacramento, and
  Grewe]{meulemans2020theoretical}
Meulemans, A., Carzaniga, F.~S., Suykens, J. A.~K., Sacramento, J., and Grewe,
  B.~F.
\newblock A theoretical framework for target propagation.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Moskovitz et~al.(2018)Moskovitz, Litwin-Kumar, and
  Abbott]{moskovitz2018feedback}
Moskovitz, T.~H., Litwin-Kumar, A., and Abbott, L.
\newblock Feedback alignment in deep convolutional networks.
\newblock \emph{arXiv preprint arXiv:1812.06488}, 2018.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
N{\o}kland, A.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 1037--1045, 2016.

\bibitem[Ororbia \& Mali(2019)Ororbia and Mali]{ororbia2019biologically}
Ororbia, A.~G. and Mali, A.
\newblock Biologically motivated algorithms for propagating local target
  representations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  4651--4658, 2019.

\bibitem[Richards et~al.(2019)Richards, Lillicrap, Beaudoin, Bengio, Bogacz,
  Christensen, Clopath, Costa, de~Berker, Ganguli, et~al.]{richards2019deep}
Richards, B.~A., Lillicrap, T.~P., Beaudoin, P., Bengio, Y., Bogacz, R.,
  Christensen, A., Clopath, C., Costa, R.~P., de~Berker, A., Ganguli, S.,
  et~al.
\newblock A deep learning framework for neuroscience.
\newblock \emph{Nature neuroscience}, 22\penalty0 (11):\penalty0 1761--1770,
  2019.

\bibitem[Roulet \& Harchaoui(2021)Roulet and Harchaoui]{roulet2021target}
Roulet, V. and Harchaoui, Z.
\newblock Target propagation via regularized inversion.
\newblock \emph{arXiv preprint arXiv:2112.01453}, 2021.

\bibitem[Sacramento et~al.(2018)Sacramento, Ponte~Costa, Bengio, and
  Senn]{NEURIPS2018_dendritic}
Sacramento, J.~a., Ponte~Costa, R., Bengio, Y., and Senn, W.
\newblock Dendritic cortical microcircuits approximate the backpropagation
  algorithm.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Scellier \& Bengio(2017)Scellier and Bengio]{scellier2017equilibrium}
Scellier, B. and Bengio, Y.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock \emph{Frontiers in computational neuroscience}, 11:\penalty0 24,
  2017.

\bibitem[van~den Oord et~al.(2016)van~den Oord, Kalchbrenner, Espeholt,
  kavukcuoglu, Vinyals, and Graves]{NIPS2016_b1301141}
van~den Oord, A., Kalchbrenner, N., Espeholt, L., kavukcuoglu, k., Vinyals, O.,
  and Graves, A.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem[Whittington \& Bogacz(2017)Whittington and
  Bogacz]{whittington2017approximation}
Whittington, J.~C. and Bogacz, R.
\newblock An approximation of the error backpropagation algorithm in a
  predictive coding network with local hebbian synaptic plasticity.
\newblock \emph{Neural computation}, 29\penalty0 (5):\penalty0 1229--1262,
  2017.

\bibitem[Xiao et~al.(2018)Xiao, Chen, Liao, and Poggio]{xiao2018biologically}
Xiao, W., Chen, H., Liao, Q., and Poggio, T.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
