\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aggarwal et~al.(2021)Aggarwal, Mandowara, Agrawal, Khandelwal, Singla, and Garg]{aggarwaletal2021ecqa}
Aggarwal, S., Mandowara, D., Agrawal, V., Khandelwal, D., Singla, P., and Garg, D.
\newblock {E}xplanations for {C}ommonsense{QA}: {N}ew {D}ataset and {M}odels.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3050--3065. Association for Computational Linguistics, 2021.
\newblock URL \url{https://aclanthology.org/2021.acl-long.238/}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.12712}.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and Niculescu-Mizil]{bucilua.c.2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining}, pp.\  535--541, 2006.
\newblock URL \url{https://dl.acm.org/doi/abs/10.1145/1150402.1150464}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Shu, Shareghi, Collier, Narasimhan, and Yao]{chen2023fireact}
Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S.
\newblock Fireact: Toward language agent fine-tuning.
\newblock \emph{arXiv preprint arXiv:2310.05915}, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.05915}.

\bibitem[Chen \& Yang(2021)Chen and Yang]{chen2021structure}
Chen, J. and Yang, D.
\newblock Structure-aware abstractive conversation summarization via discourse and action graphs.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1380--1391, 2021.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.109/}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Saha, and Bansal]{chen2023reconcile}
Chen, J. C.-Y., Saha, S., and Bansal, M.
\newblock Reconcile: Round-table conference improves reasoning via consensus among diverse llms.
\newblock \emph{arXiv preprint arXiv:2309.13007}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2309.13007}.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Swersky, Norouzi, and Hinton]{chen.t.2020big}
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G.~E.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22243--22255, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.10029}.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{chen2022program}
Chen, W., Ma, X., Wang, X., and Cohen, W.~W.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.12588}.

\bibitem[Chenglin et~al.(2023)Chenglin, Qianglong, Caiyu, and Yin]{chenglin.l.2023mixed}
Chenglin, L., Qianglong, C., Caiyu, W., and Yin, Z.
\newblock Mixed distillation helps smaller language model better reasoning.
\newblock \emph{arXiv preprint arXiv:2312.10730}, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.10730}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark-etal-2019-boolq}
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no questions.
\newblock In Burstein, J., Doran, C., and Solorio, T. (eds.), \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  2924--2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1300}.
\newblock URL \url{https://aclanthology.org/N19-1300}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.05457}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, C. and Vapnik, V.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20:\penalty0 273--297, 1995.

\bibitem[Deng et~al.(2023)Deng, Prasad, Fernandez, Smolensky, Chaudhary, and Shieber]{deng2023implicit}
Deng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaudhary, V., and Shieber, S.
\newblock Implicit chain of thought reasoning via knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2311.01460}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.01460}.

\bibitem[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch]{du2023improving}
Du, Y., Li, S., Torralba, A., Tenenbaum, J.~B., and Mordatch, I.
\newblock Improving factuality and reasoning in language models through multiagent debate, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.14325}.

\bibitem[Fu et~al.(2023)Fu, Peng, Ou, Sabharwal, and Khot]{fu2023specializing}
Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T.
\newblock Specializing smaller language models towards multi-step reasoning.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  10421--10430. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/fu23d.html}.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and Berant]{geva-etal-2021-aristotle}
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J.
\newblock Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 346--361, 2021.
\newblock \doi{10.1162/tacl_a_00370}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.21}.

\bibitem[Ghosal et~al.(2019)Ghosal, Majumder, Poria, Chhaya, and Gelbukh]{ghosal2019dialoguegcn}
Ghosal, D., Majumder, N., Poria, S., Chhaya, N., and Gelbukh, A.
\newblock Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  154--164, 2019.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycksmath2021}
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{NeurIPS}, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.03874}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.
\newblock URL \url{https://arxiv.org/abs/1503.02531}.

\bibitem[Ho et~al.(2023)Ho, Schmid, and Yun]{ho2022large}
Ho, N., Schmid, L., and Yun, S.-Y.
\newblock Large language models are reasoning teachers.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  14852--14882, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.830}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.830}.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh2023distilling}
Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), \emph{Findings of the Association for Computational Linguistics: ACL 2023}, July 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.06825}.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{kipf2017semisupervised}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJU4ayYgl}.

\bibitem[Kirchhoff \& Ostendorf(2003)Kirchhoff and Ostendorf]{kirchhoff2003directions}
Kirchhoff, K. and Ostendorf, M.
\newblock Directions for multi-party human-computer interaction research.
\newblock In \emph{Proceedings of the HLT-NAACL 2003 Workshop on Research Directions in Dialogue Processing}, pp.\  7--9, 2003.
\newblock URL \url{https://aclanthology.org/W03-0703/}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11916}.

\bibitem[Leifeld(2018)]{leifeld2018discourse}
Leifeld, P.
\newblock Discourse network analysis. policy debates as dynamic networks,[w:] jn victor, ah montgomery, m. lubell (red.), 2018.

\bibitem[Li et~al.(2023)Li, Hessel, Yu, Ren, Chang, and Choi]{li-etal-2023-symbolic}
Li, L.~H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and Choi, Y.
\newblock Symbolic chain-of-thought distillation: Small models can also {``}think{''} step-by-step.
\newblock In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  2665--2679, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.150}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.150}.

\bibitem[Liang et~al.(2023)Liang, He, Jiao, Wang, Wang, Wang, Yang, Tu, and Shi]{liang2023encouraging}
Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S.
\newblock Encouraging divergent thinking in large language models through multi-agent debate.
\newblock \emph{arXiv preprint arXiv:2305.19118}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.19118}.

\bibitem[Liu et~al.(2023)Liu, Bubeck, Eldan, Kulkarni, Li, Nguyen, Ward, and Zhang]{liu.b.2023tinygsm}
Liu, B., Bubeck, S., Eldan, R., Kulkarni, J., Li, Y., Nguyen, A., Ward, R., and Zhang, Y.
\newblock Tinygsm: achieving $>$80\% on gsm8k with small language models.
\newblock \emph{arXiv preprint arXiv:2312.09241}, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.09241}.

\bibitem[Magister et~al.(2023)Magister, Mallinson, Adamek, Malmi, and Severyn]{magister2022teaching}
Magister, L.~C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A.
\newblock Teaching small language models to reason.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1773--1781, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-short.151}.
\newblock URL \url{https://aclanthology.org/2023.acl-short.151}.

\bibitem[Mitra et~al.(2023)Mitra, Del~Corro, Mahajan, Codas, Simoes, Agarwal, Chen, Razdaibiedina, Jones, Aggarwal, et~al.]{mitra2023orca}
Mitra, A., Del~Corro, L., Mahajan, S., Codas, A., Simoes, C., Agarwal, S., Chen, X., Razdaibiedina, A., Jones, E., Aggarwal, K., et~al.
\newblock Orca 2: Teaching small language models how to reason.
\newblock \emph{arXiv preprint arXiv:2311.11045}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.11045}.

\bibitem[Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi, and Awadallah]{mukherjee2023orca}
Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4.
\newblock \emph{arXiv preprint arXiv:2306.02707}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.02707}.

\bibitem[Ouyang et~al.(2021)Ouyang, Zhang, and Zhao]{ouyang2021dialogue}
Ouyang, S., Zhang, Z., and Zhao, H.
\newblock Dialogue graph modeling for conversational machine reading.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.\  3158--3169, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.14827}.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel-etal-2021-nlp}
Patel, A., Bhattamishra, S., and Goyal, N.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  2080--2094, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.168}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.168}.

\bibitem[Qin et~al.(2021)Qin, Li, Che, Ni, and Liu]{qin2021co}
Qin, L., Li, Z., Che, W., Ni, M., and Liu, T.
\newblock Co-gat: A co-interactive graph attention network for joint dialog act recognition and sentiment classification.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  13709--13717, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.13260}.

\bibitem[Saha et~al.(2023)Saha, Hase, and Bansal]{saha2023can}
Saha, S., Hase, P., and Bansal, M.
\newblock Can language models teach weaker agents? teacher explanations improve students via personalization.
\newblock In \emph{NeurIPS}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.09299}.

\bibitem[Shen et~al.(2021)Shen, Wu, Yang, and Quan]{shen2021directed}
Shen, W., Wu, S., Yang, Y., and Quan, X.
\newblock Directed acyclic graph network for conversational emotion recognition.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  1551--1560, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.12907}.

\bibitem[Shridhar et~al.(2023)Shridhar, Stolfo, and Sachan]{shridhar2023distilling}
Shridhar, K., Stolfo, A., and Sachan, M.
\newblock Distilling reasoning capabilities into smaller language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  7059--7073, 2023.
\newblock URL \url{https://arxiv.org/abs/2212.00193}.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and McCallum]{strubell2019energy}
Strubell, E., Ganesh, A., and McCallum, A.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  3645--3650, 2019.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor-etal-2019-commonsenseqa}
Talmor, A., Herzig, J., Lourie, N., and Berant, J.
\newblock {C}ommonsense{QA}: A question answering challenge targeting commonsense knowledge.
\newblock In Burstein, J., Doran, C., and Solorio, T. (eds.), \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4149--4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock URL \url{https://aclanthology.org/N19-1421}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{taori.r.2023stanford}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang.y.2022self}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  13484--13508, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.754}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.754}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2023chainofthought}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.11903}.

\bibitem[Wei et~al.(2023)Wei, Shuster, Szlam, Weston, Urbanek, and Komeili]{wei2023multi}
Wei, J., Shuster, K., Szlam, A., Weston, J., Urbanek, J., and Komeili, M.
\newblock Multi-party chat: Conversational agents in group settings with humans and models.
\newblock \emph{arXiv preprint arXiv:2304.13835}, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.13835}.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang, Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv preprint arXiv:2112.04359}, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.04359}.

\bibitem[West et~al.(2022)West, Bhagavatula, Hessel, Hwang, Jiang, Le~Bras, Lu, Welleck, and Choi]{west-etal-2022-symbolic}
West, P., Bhagavatula, C., Hessel, J., Hwang, J., Jiang, L., Le~Bras, R., Lu, X., Welleck, S., and Choi, Y.
\newblock Symbolic knowledge distillation: from general language models to commonsense models.
\newblock In Carpuat, M., de~Marneffe, M.-C., and Meza~Ruiz, I.~V. (eds.), \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  4602--4625, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.341}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.341}.

\bibitem[Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Zhang, Zhu, Li, Jiang, Zhang, and Wang]{wu2023autogen}
Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C.
\newblock Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
\newblock \emph{arXiv preprint arXiv:2308.08155}, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.08155}.

\bibitem[Yang et~al.(2020)Yang, Shou, Gong, Lin, and Jiang]{yang2020model}
Yang, Z., Shou, L., Gong, M., Lin, W., and Jiang, D.
\newblock Model compression with two-stage multi-teacher knowledge distillation for web question answering system.
\newblock In \emph{Proceedings of the 13th International Conference on Web Search and Data Mining}, pp.\  690--698, 2020.
\newblock URL \url{https://arxiv.org/abs/1910.08381}.

\bibitem[Yin et~al.(2023)Yin, Brahman, Ravichander, Chandu, Chang, Choi, and Lin]{yin2023lumos}
Yin, D., Brahman, F., Ravichander, A., Chandu, K., Chang, K.-W., Choi, Y., and Lin, B.~Y.
\newblock Lumos: Learning agents with unified data, modular design, and open-source llms.
\newblock \emph{arXiv preprint arXiv:2311.05657}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.05657}.

\bibitem[You et~al.(2017)You, Xu, Xu, and Tao]{you2017learning}
You, S., Xu, C., Xu, C., and Tao, D.
\newblock Learning from multiple teacher networks.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pp.\  1285--1294, 2017.
\newblock URL \url{https://dl.acm.org/doi/pdf/10.1145/3097983.3098135}.

\end{thebibliography}
