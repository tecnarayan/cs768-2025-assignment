\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Baldassarre}
L.~Baldassarre and M.~Pontil.
\newblock Advanced topics in machine learning part {II} 5. proximal methods.
\newblock University Lecture,
  \url{http://www0.cs.ucl.ac.uk/staff/l.baldassarre/lectures/baldassarre_proximal_methods.pdf}.

\bibitem{baldassi2015subdominant}
C.~Baldassi, A.~Ingrosso, C.~Lucibello, L.~Saglietti, and R.~Zecchina.
\newblock Subdominant dense clusters allow for simple learning and high
  computational performance in neural networks with discrete synapses.
\newblock {\em Physical review letters}, 115(12):128101, 2015.

\bibitem{bauschke1996projection}
H.~H. Bauschke and J.~M. Borwein.
\newblock On projection algorithms for solving convex feasibility problems.
\newblock {\em SIAM review}, 38(3):367--426, 1996.

\bibitem{bengio2014auto}
Y.~Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv preprint arXiv:1407.7906}, 2014.

\bibitem{bottou2012stochastic}
L.~Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In {\em Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem{bottou2016optimization}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em arXiv preprint arXiv:1606.04838}, 2016.

\bibitem{boyd2011distributed}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  3(1):1--122, 2011.

\bibitem{chaudhari2016entropy}
P.~Chaudhari, A.~Choromanska, S.~Soatto, and Y.~LeCun.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em arXiv preprint arXiv:1611.01838}, 2016.

\bibitem{chaudhari2017deep}
P.~Chaudhari, A.~Oberman, S.~Osher, S.~Soatto, and G.~Carlier.
\newblock Deep relaxation: partial differential equations for optimizing deep
  neural networks.
\newblock {\em arXiv preprint arXiv:1704.04932}, 2017.

\bibitem{choromanska2015loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~B. Arous, and Y.~LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em AISTATS}, 2015.

\bibitem{dauphin2014identifying}
Y.~N. Dauphin, R.~Pascanu, C.~Gulcehre, K.~Cho, S.~Ganguli, and Y.~Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In {\em NIPS}, pages 2933--2941, 2014.

\bibitem{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em JMLR}, 12(Jul):2121--2159, 2011.

\bibitem{erikssonapplied}
K.~Eriksson, D.~Estep, and C.~Johnson.
\newblock {\em Applied Mathematics Body and Soul: Vol I-III}.
\newblock Springer-Verlag Publishing, 2003.

\bibitem{gal2015modern}
Y.~Gal and Z.~Ghahramani.
\newblock On modern deep learning and variational inference.
\newblock In {\em Advances in Approximate Bayesian Inference workshop, NIPS},
  2015.

\bibitem{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em AISTATS}, pages 249--256, 2010.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{chapter-gradient-flow-2001}
S.~Hochreiter, Y.~Bengio, and P.~Frasconi.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies.
\newblock In J.~Kolen and S.~Kremer, editors, {\em Field Guide to Dynamical
  Recurrent Networks}. IEEE Press, 2001.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{jia2014caffe}
Y.~Jia, E.~Shelhamer, J.~Donahue, S.~Karayev, J.~Long, R.~Girshick,
  S.~Guadarrama, and T.~Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In {\em ACM Multimedia}, pages 675--678. ACM, 2014.

\bibitem{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2015variational}
D.~P. Kingma, T.~Salimans, and M.~Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In {\em NIPS}, pages 2575--2583, 2015.

\bibitem{krogh1991simple}
A.~Krogh and J.~A. Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In {\em NIPS}, pages 950--957, 1991.

\bibitem{lanckriet2009convergence}
G.~R. Lanckriet and B.~K. Sriperumbudur.
\newblock On the convergence of the concave-convex procedure.
\newblock In {\em NIPS}, pages 1759--1767, 2009.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{lecun1998mnist}
Y.~LeCun, C.~Cortes, and C.~J. Burges.
\newblock The mnist database of handwritten digits, 1998.

\bibitem{nesterov2012efficiency}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nesterov1994interior}
Y.~Nesterov and A.~Nemirovskii.
\newblock {\em Interior-point polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem{nishihara2015general}
R.~Nishihara, L.~Lessard, B.~Recht, A.~Packard, and M.~I. Jordan.
\newblock A general analysis of the convergence of admm.
\newblock In {\em ICML}, pages 343--352, 2015.

\bibitem{nocedal99}
J.~Nocedal and S.~J. Wright.
\newblock {\em {Numerical optimization}}.
\newblock Springer, 1st. ed. 1999. corr. 2nd printing edition, Aug. 1999.

\bibitem{razaviyayn2014parallel}
M.~Razaviyayn, M.~Hong, Z.-Q. Luo, and J.-S. Pang.
\newblock Parallel successive convex approximation for nonsmooth nonconvex
  optimization.
\newblock In {\em NIPS}, pages 1440--1448, 2014.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em JMLR}, 15(1):1929--1958, 2014.

\bibitem{sutskever2013importance}
I.~Sutskever, J.~Martens, G.~E. Dahl, and G.~E. Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em ICML}, pages 1139--1147, 2013.

\bibitem{taylor2016training}
G.~Taylor, R.~Burmeister, Z.~Xu, B.~Singh, A.~Patel, and T.~Goldstein.
\newblock Training neural networks without gradients: A scalable admm approach.
\newblock In {\em ICML}, 2016.

\bibitem{tieleman2012lecture}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2), 2012.

\bibitem{wang2015global}
Y.~Wang, W.~Yin, and J.~Zeng.
\newblock Global convergence of admm in nonconvex nonsmooth optimization.
\newblock {\em arXiv preprint arXiv:1511.06324}, 2015.

\bibitem{willoughby1979solutions}
R.~A. Willoughby.
\newblock Solutions of ill-posed problems (an tikhonov and vy arsenin).
\newblock {\em SIAM Review}, 21(2):266, 1979.

\bibitem{xu2013block}
Y.~Xu and W.~Yin.
\newblock A block coordinate descent method for regularized multiconvex
  optimization with applications to nonnegative tensor factorization and
  completion.
\newblock {\em SIAM Journal on imaging sciences}, 6(3):1758--1789, 2013.

\bibitem{xu2014globally}
Y.~Xu and W.~Yin.
\newblock A globally convergent algorithm for nonconvex optimization based on
  block coordinate update.
\newblock {\em arXiv preprint arXiv:1410.1386}, 2014.

\bibitem{zeiler2012adadelta}
M.~D. Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\bibitem{Zhang_2016_CVPR}
Z.~Zhang, Y.~Chen, and V.~Saligrama.
\newblock Efficient training of very deep neural networks for supervised
  hashing.
\newblock In {\em CVPR}, June 2016.

\end{thebibliography}
