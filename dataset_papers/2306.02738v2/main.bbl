% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{Abdar2021-zq}{article}{}
      \name{author}{12}{}{%
        {{un=0,uniquepart=base,hash=ca983cd25720ec2c2d73c39bacb24f30}{%
           family={Abdar},
           familyi={A\bibinitperiod},
           given={Moloud},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=66a6347247c12ba791d3cb13e5b7635c}{%
           family={Pourpanah},
           familyi={P\bibinitperiod},
           given={Farhad},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4e95b9bf653b8e7cdfe7de05c18345ca}{%
           family={Hussain},
           familyi={H\bibinitperiod},
           given={Sadiq},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e21aee18db148918e35156aca616aab8}{%
           family={Rezazadegan},
           familyi={R\bibinitperiod},
           given={Dana},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fa3e4da50b3416b1a04c39a932b2344c}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8e24dffc0a2bc85a7ea0c453c0cd2cc5}{%
           family={Ghavamzadeh},
           familyi={G\bibinitperiod},
           given={Mohammad},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=becd80fc91500ab9a0d8c3354d996b0d}{%
           family={Fieguth},
           familyi={F\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=80b4b73a59bf06845ed4c13852177c41}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Xiaochun},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6fd74ce4facf770405e5aa929aa7dd56}{%
           family={Khosravi},
           familyi={K\bibinitperiod},
           given={Abbas},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8656a591cea01cf0808ba7b2ce58786c}{%
           family={Acharya},
           familyi={A\bibinitperiod},
           given={U\bibnamedelima Rajendra},
           giveni={U\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=39e85e7b8ae099b87481b440057e0b7a}{%
           family={Makarenkov},
           familyi={M\bibinitperiod},
           given={Vladimir},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5345efddc35011fcd0912e307fa90dab}{%
           family={Nahavandi},
           familyi={N\bibinitperiod},
           given={Saeid},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{9011afea887f73771688fa87a3c26e65}
      \strng{fullhash}{9e632e89846ef393916dfc2c489d906b}
      \strng{bibnamehash}{9011afea887f73771688fa87a3c26e65}
      \strng{authorbibnamehash}{9011afea887f73771688fa87a3c26e65}
      \strng{authornamehash}{9011afea887f73771688fa87a3c26e65}
      \strng{authorfullhash}{9e632e89846ef393916dfc2c489d906b}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
      \field{issn}{1566-2535}
      \field{journaltitle}{An international journal on information fusion}
      \field{month}{12}
      \field{title}{A review of uncertainty quantification in deep learning: Techniques, applications and challenges}
      \field{volume}{76}
      \field{year}{2021}
      \field{pages}{243\bibrangedash 297}
      \range{pages}{55}
      \verb{doi}
      \verb 10.1016/j.inffus.2021.05.008
      \endverb
      \verb{file}
      \verb All Papers/A/Abdar et al. 2021 - A review of uncertainty quantification in deep learning - Techniques, applications and challenges.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1566253521001081
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1566253521001081
      \endverb
      \keyw{Artificial intelligence; Uncertainty quantification; Deep learning; Machine learning; Bayesian statistics; Ensemble learning;Uncertainty quantification;Review}
    \endentry
    \entry{Benavoli2016-mh}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=a530c79da7054c7e81488962a667a481}{%
           family={Benavoli},
           familyi={B\bibinitperiod},
           given={Alessio},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=492b0dfcd7b1de44b70c059e04a40bca}{%
           family={Corani},
           familyi={C\bibinitperiod},
           given={Giorgio},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a7c3dff644a45463166796db4c0ee3f3}{%
           family={Mangili},
           familyi={M\bibinitperiod},
           given={Francesca},
           giveni={F\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{64393a98e994f50a58d731472292374e}
      \strng{fullhash}{f748f913170b39ef8b050dd25f8340d4}
      \strng{bibnamehash}{f748f913170b39ef8b050dd25f8340d4}
      \strng{authorbibnamehash}{f748f913170b39ef8b050dd25f8340d4}
      \strng{authornamehash}{64393a98e994f50a58d731472292374e}
      \strng{authorfullhash}{f748f913170b39ef8b050dd25f8340d4}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1532-4435, 1533-7928}
      \field{journaltitle}{Journal of machine learning research: JMLR}
      \field{number}{5}
      \field{title}{Should We Really Use {Post-Hoc} Tests Based on {Mean-Ranks}?}
      \field{volume}{17}
      \field{year}{2016}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/B/Benavoli et al. 2016 - Should We Really Use Post-Hoc Tests Based on Mean-Ranks.pdf
      \endverb
      \verb{urlraw}
      \verb https://jmlr.org/papers/v17/benavoli16a.html
      \endverb
      \verb{url}
      \verb https://jmlr.org/papers/v17/benavoli16a.html
      \endverb
    \endentry
    \entry{Blondel2020-sf}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=d3b6c8582cea4e8b6576137b6a72cadc}{%
           family={Blondel},
           familyi={B\bibinitperiod},
           given={Mathieu},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b33444af396b31f38a47964b0f031d7}{%
           family={Teboul},
           familyi={T\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a5a45875c180d5136325798b472b95d8}{%
           family={Berthet},
           familyi={B\bibinitperiod},
           given={Quentin},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ad3b7f61a5635ccd63eddb8543c8f5f}{%
           family={Djolonga},
           familyi={D\bibinitperiod},
           given={Josip},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=2dc8d8eba3b07ccb6bda6a754b49b05a}{%
           family={Iii},
           familyi={I\bibinitperiod},
           given={Hal\bibnamedelima Daumé},
           giveni={H\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=d77c73a14b0f048484c1d34aba3071d4}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Aarti},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{b64b242b43cd72977ed6b8defdcca401}
      \strng{fullhash}{6a5023a8833c0b7a1400c73c9c6413b0}
      \strng{bibnamehash}{b64b242b43cd72977ed6b8defdcca401}
      \strng{authorbibnamehash}{b64b242b43cd72977ed6b8defdcca401}
      \strng{authornamehash}{b64b242b43cd72977ed6b8defdcca401}
      \strng{authorfullhash}{6a5023a8833c0b7a1400c73c9c6413b0}
      \strng{editorbibnamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editornamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editorfullhash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the $O(n \log n)$ time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with $O(n \log n)$ time and $O(n)$ space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank correlation coefficient and least trimmed squares.}
      \field{booktitle}{Proceedings of the 37th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Fast Differentiable Sorting and Ranking}
      \field{volume}{119}
      \field{year}{2020}
      \field{pages}{950\bibrangedash 959}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/B/Blondel et al. 2020 - Fast Differentiable Sorting and Ranking.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v119/blondel20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v119/blondel20a.html
      \endverb
    \endentry
    \entry{Bracher2021-ut}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=bf4e0b1559f41753a5f8f0e193bf5f98}{%
           family={Bracher},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d16a29b1c1bff8d354890977bae93ae5}{%
           family={Ray},
           familyi={R\bibinitperiod},
           given={Evan\bibnamedelima L},
           giveni={E\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cf94799d213c0ab2fd2012473da7440b}{%
           family={Gneiting},
           familyi={G\bibinitperiod},
           given={Tilmann},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=68b4dbf033ecb04ab5bf0af268e2da21}{%
           family={Reich},
           familyi={R\bibinitperiod},
           given={Nicholas\bibnamedelima G},
           giveni={N\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {journals.plos.org}%
      }
      \strng{namehash}{057518eeab9da06b877bc329dd050981}
      \strng{fullhash}{65a2317506fc68bba293dee27218077f}
      \strng{bibnamehash}{057518eeab9da06b877bc329dd050981}
      \strng{authorbibnamehash}{057518eeab9da06b877bc329dd050981}
      \strng{authornamehash}{057518eeab9da06b877bc329dd050981}
      \strng{authorfullhash}{65a2317506fc68bba293dee27218077f}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.}
      \field{issn}{1553-734X, 1553-7358}
      \field{journaltitle}{PLoS computational biology}
      \field{month}{2}
      \field{number}{2}
      \field{title}{Evaluating epidemic forecasts in an interval format}
      \field{volume}{17}
      \field{year}{2021}
      \field{pages}{e1008618}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1371/journal.pcbi.1008618
      \endverb
      \verb{file}
      \verb All Papers/B/Bracher et al. 2021 - Evaluating epidemic forecasts in an interval format.pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1371/journal.pcbi.1008618
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1371/journal.pcbi.1008618
      \endverb
      \keyw{Intervals;Scoring rules}
    \endentry
    \entry{Chernozhukov2021-sg}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=c75cbe2cefd1e532f636a50892605d06}{%
           family={Chernozhukov},
           familyi={C\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bd3e82975b4354cd5bb4c09b14ce1024}{%
           family={Wüthrich},
           familyi={W\bibinitperiod},
           given={Kaspar},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=56a4f4e9ee4f67fcb0d528cbce811e14}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Yinchu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {National Acad Sciences}%
      }
      \strng{namehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{fullhash}{288a6977ce38d8c7d98365423ce036a1}
      \strng{bibnamehash}{288a6977ce38d8c7d98365423ce036a1}
      \strng{authorbibnamehash}{288a6977ce38d8c7d98365423ce036a1}
      \strng{authornamehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{authorfullhash}{288a6977ce38d8c7d98365423ce036a1}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems, including cross-sectional prediction, k-step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral transform and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under heteroskedasticity. We establish approximate conditional validity under consistent estimation and provide approximate unconditional validity under model misspecification, under overfitting, and with time series data. We also propose a simple ``shape'' adjustment of our baseline method that yields optimal prediction intervals.}
      \field{issn}{0027-8424, 1091-6490}
      \field{journaltitle}{Proceedings of the National Academy of Sciences of the United States of America}
      \field{month}{11}
      \field{number}{48}
      \field{title}{Distributional conformal prediction}
      \field{volume}{118}
      \field{year}{2021}
      \verb{doi}
      \verb 10.1073/pnas.2107794118
      \endverb
      \verb{file}
      \verb All Papers/C/Chernozhukov et al. 2021 - 1909.07889.pdf;All Papers/C/Chernozhukov et al. 2021 - Distributional conformal prediction.pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1073/pnas.2107794118
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1073/pnas.2107794118
      \endverb
      \keyw{conditional validity; distribution regression; model-free validity; prediction intervals; quantile regression}
    \endentry
    \entry{Chung2021-rh}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=382b5780156723c48ab6568349e574f5}{%
           family={Chung},
           familyi={C\bibinitperiod},
           given={Youngseog},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cb1889b535f5e3414808403a8cb0c85c}{%
           family={Neiswanger},
           familyi={N\bibinitperiod},
           given={Willie},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=af008715a70386ee3deaaea97458c65e}{%
           family={Char},
           familyi={C\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b53b06645585eb933eb46040b8426df7}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Jeff},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{5}{}{%
        {{hash=6cb5af0e649387de5bbfdb150cc7a786}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={M},
           giveni={M\bibinitperiod}}}%
        {{hash=02854da982edf66ffa27f4fc1c738779}{%
           family={Beygelzimer},
           familyi={B\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod}}}%
        {{hash=5e4dc483044b6dbd4496a818b8b6c67a}{%
           family={Dauphin},
           familyi={D\bibinitperiod},
           given={Y},
           giveni={Y\bibinitperiod}}}%
        {{hash=12f9558e0c25864defe6eaed94a9011b}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={P\bibnamedelima S},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=3d379bd6370f69ebc79dc5337ed96d13}{%
           family={Vaughan},
           familyi={V\bibinitperiod},
           given={J\bibnamedelima Wortman},
           giveni={J\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{6d86ab228176a113085faa2357eb58fb}
      \strng{fullhash}{bedecc047688475ab16d8301b6b47396}
      \strng{bibnamehash}{6d86ab228176a113085faa2357eb58fb}
      \strng{authorbibnamehash}{6d86ab228176a113085faa2357eb58fb}
      \strng{authornamehash}{6d86ab228176a113085faa2357eb58fb}
      \strng{authorfullhash}{bedecc047688475ab16d8301b6b47396}
      \strng{editorbibnamehash}{6013a44c5dec47e5e51fdd72b3f8b5eb}
      \strng{editornamehash}{6013a44c5dec47e5e51fdd72b3f8b5eb}
      \strng{editorfullhash}{ad1ea98aa644fe4cbc55fbe24d1f78a9}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification}
      \field{volume}{34}
      \field{year}{2021}
      \field{pages}{10971\bibrangedash 10984}
      \range{pages}{14}
      \verb{file}
      \verb All Papers/C/Chung et al. 2021 - Beyond Pinball Loss - Quantile Methods for Calibrated Uncertainty Quantification.pdf;All Papers/C/Chung et al. 2021 - Beyond Pinball Loss - Quantile Methods for Calibrated Uncertainty Quantification.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf
      \endverb
    \endentry
    \entry{Cuturi2019-dv}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=a771427291cb72a39e026a68b102335d}{%
           family={Cuturi},
           familyi={C\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b33444af396b31f38a47964b0f031d7}{%
           family={Teboul},
           familyi={T\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0df2e0c86d89bfaf4f3726c0bc382d4f}{%
           family={Vert},
           familyi={V\bibinitperiod},
           given={Jean-Philippe},
           giveni={J\bibinithyphendelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{23774c5784dc380f2890737ecf0741c6}
      \strng{fullhash}{e78891e9d6e100e0dececdad71b6eccb}
      \strng{bibnamehash}{e78891e9d6e100e0dececdad71b6eccb}
      \strng{authorbibnamehash}{e78891e9d6e100e0dececdad71b6eccb}
      \strng{authornamehash}{23774c5784dc380f2890737ecf0741c6}
      \strng{authorfullhash}{e78891e9d6e100e0dececdad71b6eccb}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (CDFs), quantiles, or to select closest neighbors and labels. The sorting function is however piece-wise constant (the sorting permutation of a vector does not change if the entries of that vector are infinitesimally perturbed) and therefore has no gradient information to back-propagate. We propose a framework to sort elements that is algorithmically differentiable. We leverage the fact that sorting can be seen as a particular instance of the optimal transport (OT) problem on $\mathbb\{R\}$, from input values to a predefined array of sorted values (e.g. $1,2,\dots,n$ if the input array has $n$ elements). Building upon this link , we propose generalized CDFs and quantile operators by varying the size and weights of the target presorted array. Because this amounts to using the so-called Kantorovich formulation of OT, we call these quantities K-sorts, K-CDFs and K-quantiles. We recover differentiable algorithms by adding to the OT problem an entropic regularization, and approximate it using a few Sinkhorn iterations. We call these operators S-sorts, S-CDFs and S-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neuralsort [Grover et al. 2019], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{28~5}
      \field{title}{Differentiable Ranks and Sorting using Optimal Transport}
      \field{year}{2019}
      \verb{eprint}
      \verb 1905.11885
      \endverb
      \verb{file}
      \verb All Papers/C/Cuturi et al. 2019 - Differentiable Ranks and Sorting using Optimal Transport.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2019/file/d8c24ca8f23c562a5600876ca2a550ce-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2019/file/d8c24ca8f23c562a5600876ca2a550ce-Paper.pdf
      \endverb
    \endentry
    \entry{Dawid1984-gn}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=4016275a6a7dc086e196dd10c2f7520d}{%
           family={Dawid},
           familyi={D\bibinitperiod},
           given={A\bibnamedelima P},
           giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {[Royal Statistical Society, Wiley]}%
      }
      \strng{namehash}{4016275a6a7dc086e196dd10c2f7520d}
      \strng{fullhash}{4016275a6a7dc086e196dd10c2f7520d}
      \strng{bibnamehash}{4016275a6a7dc086e196dd10c2f7520d}
      \strng{authorbibnamehash}{4016275a6a7dc086e196dd10c2f7520d}
      \strng{authornamehash}{4016275a6a7dc086e196dd10c2f7520d}
      \strng{authorfullhash}{4016275a6a7dc086e196dd10c2f7520d}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{[The prequential approach is founded on the premises that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.]}
      \field{issn}{0035-9238}
      \field{journaltitle}{Journal of the Royal Statistical Society. Series A}
      \field{number}{2}
      \field{title}{Present Position and Potential Developments: Some Personal Views: Statistical Theory: The Prequential Approach}
      \field{volume}{147}
      \field{year}{1984}
      \field{pages}{278\bibrangedash 292}
      \range{pages}{15}
      \verb{doi}
      \verb 10.2307/2981683
      \endverb
      \verb{file}
      \verb All Papers/D/Dawid 1984 - Present Position and Potential Developments - Some Personal Views - Statistical Theory - The Prequential Approach.pdf
      \endverb
      \verb{urlraw}
      \verb http://www.jstor.org/stable/2981683
      \endverb
      \verb{url}
      \verb http://www.jstor.org/stable/2981683
      \endverb
    \endentry
    \entry{Demsar2006-ed}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=4f1562f6c926af2ba7e34e919ea14f1d}{%
           family={Demšar},
           familyi={D\bibinitperiod},
           given={Janez},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {JMLR.org}%
      }
      \strng{namehash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \strng{fullhash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \strng{bibnamehash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \strng{authorbibnamehash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \strng{authornamehash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \strng{authorfullhash}{4f1562f6c926af2ba7e34e919ea14f1d}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of machine learning research: JMLR}
      \field{month}{12}
      \field{title}{Statistical Comparisons of Classifiers over Multiple Data Sets}
      \field{volume}{7}
      \field{year}{2006}
      \field{pages}{1\bibrangedash 30}
      \range{pages}{30}
      \verb{file}
      \verb All Papers/D/Demšar 2006 - Statistical Comparisons of Classifiers over Multiple Data Sets.pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.5555/1248547.1248548
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.5555/1248547.1248548
      \endverb
    \endentry
    \entry{Dua2017-ut}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=eea1505410ff15d1a2b081186aad5dbb}{%
           family={Dua},
           familyi={D\bibinitperiod},
           given={Dheeru},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=00949829ae497e5a94a73f886a7d0b51}{%
           family={Graff},
           familyi={G\bibinitperiod},
           given={Casey},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{2}{%
        {University of California, Irvine, School of Information}%
        {Computer Sciences}%
      }
      \strng{namehash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \strng{fullhash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \strng{bibnamehash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \strng{authorbibnamehash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \strng{authornamehash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \strng{authorfullhash}{43f1da911d4e973a5ddbb30a2509e1f4}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{UCI} Machine Learning Repository}
      \field{year}{2017}
      \verb{urlraw}
      \verb http://archive.ics.uci.edu/ml
      \endverb
      \verb{url}
      \verb http://archive.ics.uci.edu/ml
      \endverb
    \endentry
    \entry{Fakoor2021-os}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=2f32d4327b8ff95f3121bb65e795f323}{%
           family={Fakoor},
           familyi={F\bibinitperiod},
           given={Rasool},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c4d33d0b97a56909145265ec540c595}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Taesup},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=18bfb87c822518f97f37f8c7feab27a9}{%
           family={Mueller},
           familyi={M\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=82d61e31b4f7f82ad59ff887349bdfe3}{%
           family={Smola},
           familyi={S\bibinitperiod},
           given={Alexander\bibnamedelima J},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b43d82ea7435e420b39c05f1ccd1d885}{%
           family={Tibshirani},
           familyi={T\bibinitperiod},
           given={Ryan\bibnamedelima J},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f00b178b6e1ae138ceef6f874e0135ba}
      \strng{fullhash}{7f52088be31265252e4b5223b2a1bed9}
      \strng{bibnamehash}{f00b178b6e1ae138ceef6f874e0135ba}
      \strng{authorbibnamehash}{f00b178b6e1ae138ceef6f874e0135ba}
      \strng{authornamehash}{f00b178b6e1ae138ceef6f874e0135ba}
      \strng{authorfullhash}{7f52088be31265252e4b5223b2a1bed9}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Quantile regression is a fundamental problem in statistical learning motivated by the need to quantify uncertainty in predictions, or to model a diverse population without being overly reductive. For instance, epidemiological forecasts, cost estimates, and revenue predictions all benefit from being able to quantify the range of possible values accurately. As such, many models have been developed for this problem over many years of research in econometrics, statistics, and machine learning. Rather than proposing yet another (new) algorithm for quantile regression we adopt a meta viewpoint: we investigate methods for aggregating any number of conditional quantile models, in order to improve accuracy and robustness. We consider weighted ensembles where weights may vary over not only individual models, but also over quantile levels, and feature values. All of the models we consider in this paper can be fit using modern deep learning toolkits, and hence are widely accessible (from an implementation point of view) and scalable. To improve the accuracy of the predicted quantiles (or equivalently, prediction intervals), we develop tools for ensuring that quantiles remain monotonically ordered, and apply conformal calibration methods. These can be used without any modification of the original library of base models. We also review some basic theory surrounding quantile aggregation and related scoring rules, and contribute a few new results to this literature (for example, the fact that post sorting or post isotonic regression can only improve the weighted interval score). Finally, we provide an extensive suite of empirical comparisons across 34 data sets from two different benchmark repositories.}
      \field{eprintclass}{stat.ML}
      \field{eprinttype}{arXiv}
      \field{month}{2}
      \field{title}{Flexible Model Aggregation for Quantile Regression}
      \field{year}{2021}
      \verb{eprint}
      \verb 2103.00083
      \endverb
      \verb{file}
      \verb All Papers/F/Fakoor et al. 2021 - Flexible Model Aggregation for Quantile Regression.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2103.00083
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2103.00083
      \endverb
      \keyw{Quantiles}
    \endentry
    \entry{Feldman2021-ew}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=3502aaae402618bb93559712c8b7d533}{%
           family={{Feldman}},
           familyi={F\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=c1bdcc7d5b47d60286c6c12b9d2e89da}{%
           family={{Bates}},
           familyi={B\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=323f0bdd77c5be9d0e31f98f75f5fa08}{%
           family={{Romano}},
           familyi={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {proceedings.neurips.cc}%
      }
      \strng{namehash}{c34483f8232d375d071f828ea1a0cfcb}
      \strng{fullhash}{842bb05ae9b26eb61ea83b70d691710b}
      \strng{bibnamehash}{842bb05ae9b26eb61ea83b70d691710b}
      \strng{authorbibnamehash}{842bb05ae9b26eb61ea83b70d691710b}
      \strng{authornamehash}{c34483f8232d375d071f828ea1a0cfcb}
      \strng{authorfullhash}{842bb05ae9b26eb61ea83b70d691710b}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{… estimate the conditional quantiles with quantile regression ---it is … We find in experiments that traditional quantile regression … quantiles , these two quantities are independent ( orthogonal ), …}
      \field{issn}{1049-5258}
      \field{journaltitle}{Advances in neural information processing systems}
      \field{title}{Improving conditional coverage via orthogonal quantile regression}
      \field{year}{2021}
      \verb{file}
      \verb All Papers/F/Feldman et al. 2021 - 2106.00394.pdf;All Papers/F/Feldman et al. 2021 - Improving conditional coverage via orthogonal quantile regression.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html
      \endverb
    \endentry
    \entry{Foygel_Barber2021-ig}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=1b97dd88d1fccd71e9a7656383b72338}{%
           family={Foygel\bibnamedelima Barber},
           familyi={F\bibinitperiod\bibinitdelim B\bibinitperiod},
           given={Rina},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ae404faac8c8429af059d0185dcc9643}{%
           family={Candès},
           familyi={C\bibinitperiod},
           given={Emmanuel\bibnamedelima J},
           giveni={E\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1397f973e97cb40b67654aac778db5ec}{%
           family={Ramdas},
           familyi={R\bibinitperiod},
           given={Aaditya},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b43d82ea7435e420b39c05f1ccd1d885}{%
           family={Tibshirani},
           familyi={T\bibinitperiod},
           given={Ryan\bibnamedelima J},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Oxford Academic}%
      }
      \strng{namehash}{5296a0003e5666d10a62aea7753ec499}
      \strng{fullhash}{85948c19e431739fc88f7ad933ff21ce}
      \strng{bibnamehash}{5296a0003e5666d10a62aea7753ec499}
      \strng{authorbibnamehash}{5296a0003e5666d10a62aea7753ec499}
      \strng{authornamehash}{5296a0003e5666d10a62aea7753ec499}
      \strng{authorfullhash}{85948c19e431739fc88f7ad933ff21ce}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract. We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally r}
      \field{journaltitle}{Information and Inference: A Journal of the IMA}
      \field{month}{15~6}
      \field{number}{2}
      \field{title}{The limits of distribution-free conditional predictive inference}
      \field{volume}{10}
      \field{year}{2021}
      \field{pages}{455\bibrangedash 482}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1093/imaiai/iaaa017
      \endverb
      \verb{file}
      \verb All Papers/F/Foygel Barber et al. 2021 - The limits of distribution-free conditional predictive inference.pdf
      \endverb
      \verb{urlraw}
      \verb https://academic.oup.com/imaiai/article-pdf/10/2/455/38549621/iaaa017.pdf
      \endverb
      \verb{url}
      \verb https://academic.oup.com/imaiai/article-pdf/10/2/455/38549621/iaaa017.pdf
      \endverb
    \endentry
    \entry{Friedman1940-vi}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=ed599767cda235bc9b7bb0e787374d60}{%
           family={Friedman},
           familyi={F\bibinitperiod},
           given={Milton},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Institute of Mathematical Statistics}%
      }
      \strng{namehash}{ed599767cda235bc9b7bb0e787374d60}
      \strng{fullhash}{ed599767cda235bc9b7bb0e787374d60}
      \strng{bibnamehash}{ed599767cda235bc9b7bb0e787374d60}
      \strng{authorbibnamehash}{ed599767cda235bc9b7bb0e787374d60}
      \strng{authornamehash}{ed599767cda235bc9b7bb0e787374d60}
      \strng{authorfullhash}{ed599767cda235bc9b7bb0e787374d60}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Annals of Mathematical Statistics}
      \field{issn}{0003-4851, 2168-8990}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{month}{3}
      \field{number}{1}
      \field{title}{A Comparison of Alternative Tests of Significance for the Problem of $m$ Rankings}
      \field{volume}{11}
      \field{year}{1940}
      \field{pages}{86\bibrangedash 92}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1214/aoms/1177731944
      \endverb
      \verb{file}
      \verb All Papers/F/Friedman 1940 - A Comparison of Alternative Tests of Significance for the Problem of $m$ Rankings.pdf
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-11/issue-1/A-Comparison-of-Alternative-Tests-of-Significance-for-the-Problem/10.1214/aoms/1177731944.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-11/issue-1/A-Comparison-of-Alternative-Tests-of-Significance-for-the-Problem/10.1214/aoms/1177731944.full
      \endverb
    \endentry
    \entry{Gal2016-xn}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=d4f394bd4030e20e7b1399e0abe7dc6a}{%
           family={Gal},
           familyi={G\bibinitperiod},
           given={Yarin},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bf5cf30deddddad00a6341ad7a414445}{%
           family={Ghahramani},
           familyi={G\bibinitperiod},
           given={Zoubin},
           giveni={Z\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=024bfb76c2f5540f5021fb377901cc25}{%
           family={Balcan},
           familyi={B\bibinitperiod},
           given={Maria\bibnamedelima Florina},
           giveni={M\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=68a0238356fbd88b34be8886f25938a7}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={Kilian\bibnamedelima Q},
           giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, USA}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{fullhash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{bibnamehash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{authorbibnamehash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{authornamehash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{authorfullhash}{ea98ec178dc61973ac54967bc78906b7}
      \strng{editorbibnamehash}{574fba25a54bf082817211b5d522e7ef}
      \strng{editornamehash}{574fba25a54bf082817211b5d522e7ef}
      \strng{editorfullhash}{574fba25a54bf082817211b5d522e7ef}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.}
      \field{booktitle}{Proceedings of The 33rd International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}
      \field{volume}{48}
      \field{year}{2016}
      \field{pages}{1050\bibrangedash 1059}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/G/Gal and Ghahramani 2016 - Dropout as a Bayesian Approximation - Representing Model Uncertainty in Deep Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v48/gal16.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v48/gal16.html
      \endverb
      \keyw{Variational}
    \endentry
    \entry{Gawlikowski2021-ty}{article}{}
      \name{author}{14}{}{%
        {{un=0,uniquepart=base,hash=c7b9939dd7cf821607c0cf453a170baf}{%
           family={Gawlikowski},
           familyi={G\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cfec487698cec0755cf48f6fdbf2c88c}{%
           family={Tassi},
           familyi={T\bibinitperiod},
           given={Cedrique\bibnamedelimb Rovile\bibnamedelima Njieutcheu},
           giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c1de696d3a8d95f8f7a0c49ddff64c0f}{%
           family={Ali},
           familyi={A\bibinitperiod},
           given={Mohsin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8d074ee955c33190bb25727a6e310b51}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jongseok},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc7eb309d27edb6edbeeb3c0149ca1c8}{%
           family={Humt},
           familyi={H\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cf7e48609498fdd16eadafcb88590659}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Jianxiang},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6baa0277df8aa8696f9880cc66242f6d}{%
           family={Kruspe},
           familyi={K\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e8ad581b48403dea2b06971054fe9ff1}{%
           family={Triebel},
           familyi={T\bibinitperiod},
           given={Rudolph},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c8c73bfb7060212cd42508f1f1495516}{%
           family={Jung},
           familyi={J\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dfa9d61dde3c7fc246f107f4f9786186}{%
           family={Roscher},
           familyi={R\bibinitperiod},
           given={Ribana},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6f2884ff580e1af3b02da17360b3c4ac}{%
           family={Shahzad},
           familyi={S\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c245130418606dd988ed12a97044d62}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Wen},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b86c5a1293f66bbd036f3e3fdf5909b}{%
           family={Bamler},
           familyi={B\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5e6aebfefcbe0d7c5db556334617b20}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Xiao\bibnamedelima Xiang},
           giveni={X\bibinitperiod\bibinitdelim X\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{fullhash}{6a1983024099d69ba26f86229a810b60}
      \strng{bibnamehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{authorbibnamehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{authornamehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{authorfullhash}{6a1983024099d69ba26f86229a810b60}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Due to their increasing spread, confidence in neural network predictions became more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over or under confidence. Many researchers have been working on understanding and quantifying uncertainty in a neural network's prediction. As a result, different types and sources of uncertainty have been identified and a variety of approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. A comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and implementations. Different examples from the wide spectrum of challenges in different fields give an idea of the needs and challenges regarding uncertainties in practical applications. Additionally, the practical limitations of current methods for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{title}{A Survey of Uncertainty in Deep Neural Networks}
      \field{year}{2021}
      \verb{eprint}
      \verb 2107.03342
      \endverb
      \verb{file}
      \verb All Papers/G/Gawlikowski et al. 2021 - A Survey of Uncertainty in Deep Neural Networks.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2107.03342
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2107.03342
      \endverb
      \keyw{Uncertainty quantification}
    \endentry
    \entry{Gijsbers2019-xk}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=fc85e272fcb76555940abdb9ea6a6067}{%
           family={Gijsbers},
           familyi={G\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2fcc96504beb74c2f01059f45627530f}{%
           family={LeDell},
           familyi={L\bibinitperiod},
           given={Erin},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=50f63940b6a88b8360d6cb0988ceb5a9}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Janek},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=56bd62966c40494eab97c3abba3dca7c}{%
           family={Poirier},
           familyi={P\bibinitperiod},
           given={Sébastien},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c5695b464b6059e7047553d017275f65}{%
           family={Bischl},
           familyi={B\bibinitperiod},
           given={Bernd},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4ce3afeba4f8f866dbad8e5b02118406}{%
           family={Vanschoren},
           familyi={V\bibinitperiod},
           given={Joaquin},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f9e2fb66b5acf60594c444be3c2220c2}
      \strng{fullhash}{a57f64a13bdd7a825f724d4a8bc4866e}
      \strng{bibnamehash}{f9e2fb66b5acf60594c444be3c2220c2}
      \strng{authorbibnamehash}{f9e2fb66b5acf60594c444be3c2220c2}
      \strng{authornamehash}{f9e2fb66b5acf60594c444be3c2220c2}
      \strng{authorfullhash}{a57f64a13bdd7a825f724d4a8bc4866e}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, an active field of research has developed around automated machine learning (AutoML). Unfortunately, comparing different AutoML systems is hard and often done incorrectly. We introduce an open, ongoing, and extensible benchmark framework which follows best practices and avoids common mistakes. The framework is open-source, uses public datasets and has a website with up-to-date results. We use the framework to conduct a thorough comparison of 4 AutoML systems across 39 datasets and analyze the results.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{title}{An Open Source {AutoML} Benchmark}
      \field{year}{2019}
      \verb{eprint}
      \verb 1907.00909
      \endverb
      \verb{file}
      \verb All Papers/G/Gijsbers et al. 2019 - An Open Source AutoML Benchmark.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.00909
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.00909
      \endverb
    \endentry
    \entry{Gneiting2021-vu}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=cf94799d213c0ab2fd2012473da7440b}{%
           family={Gneiting},
           familyi={G\bibinitperiod},
           given={Tilmann},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=90483b568a161f441bef204a5d975fad}{%
           family={Resin},
           familyi={R\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ac7f36a9a902f152894596924852a281}
      \strng{fullhash}{ac7f36a9a902f152894596924852a281}
      \strng{bibnamehash}{ac7f36a9a902f152894596924852a281}
      \strng{authorbibnamehash}{ac7f36a9a902f152894596924852a281}
      \strng{authornamehash}{ac7f36a9a902f152894596924852a281}
      \strng{authorfullhash}{ac7f36a9a902f152894596924852a281}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Model diagnostics and forecast evaluation are two sides of the same coin. A common principle is that fitted or predicted distributions ought to be calibrated or reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary responses, this is the universal concept of reliability. For real-valued outcomes, a general theory of calibration has been elusive, despite a recent surge of interest in distributional regression and machine learning. We develop a framework rooted in probability theory, which gives rise to hierarchies of calibration, and applies to both predictive distributions and stand-alone point forecasts. In a nutshell, a prediction - distributional or single-valued - is conditionally T-calibrated if it can be taken at face value in terms of the functional T. Whenever T is defined via an identification function - as in the cases of threshold (non) exceedance probabilities, quantiles, expectiles, and moments - auto-calibration implies T-calibration. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration (MCB), discrimination (DSC), and uncertainty (UNC). In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination, $$\textbackslashtext\{R\}^\textbackslashast = \textbackslashfrac\{\textbackslashtext\{DSC\}-\textbackslashtext\{MCB\}\}\{\textbackslashtext\{UNC\}\},$$ that nests and reinterprets the classical $\text\{R\}^2$ in least squares (mean) regression and its natural analogue $\text\{R\}^1$ in quantile regression, yet applies to T-regression in general, with MCB $\geq 0$, DSC $\geq 0$, and $\text\{R\}^\ast \in [0,1]$ under modest conditions.}
      \field{eprintclass}{stat.ME}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{title}{Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination}
      \field{year}{2021}
      \verb{eprint}
      \verb 2108.03210
      \endverb
      \verb{file}
      \verb All Papers/G/Gneiting and Resin 2021 - Regression Diagnostics meets Forecast Evaluati ... l Calibration, Reliability Diagrams, and Coefficient of Determination.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2108.03210
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2108.03210
      \endverb
    \endentry
    \entry{Gneiting2023-at}{article}{}
      \name{author}{11}{ul=2}{%
        {{un=0,uniquepart=base,hash=cf94799d213c0ab2fd2012473da7440b}{%
           family={Gneiting},
           familyi={G\bibinitperiod},
           given={Tilmann},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=01787ba399a052cabbc9e04a9ab355a6}{%
           family={Wolffram},
           familyi={W\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=90483b568a161f441bef204a5d975fad}{%
           family={Resin},
           familyi={R\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=09a3185bf721c312dc19780a7f98704a}{%
           family={Kraus},
           familyi={K\bibinitperiod},
           given={Kristof},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bf4e0b1559f41753a5f8f0e193bf5f98}{%
           family={Bracher},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1b778782737a7ed8deaf6db021fa6615}{%
           family={Dimitriadis},
           familyi={D\bibinitperiod},
           given={Timo},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c0a6e0e4c74b88370d303d86e05c7deb}{%
           family={Hagenmeyer},
           familyi={H\bibinitperiod},
           given={Veit},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6e6203cf6092da4296004fc1b9f70381}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Alexander\bibnamedelima I},
           giveni={A\bibinitperiod\bibinitdelim I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=370fe5897cfb28e5b74c141f9d27c121}{%
           family={Lerch},
           familyi={L\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d0e86d1ac9f04873b7e04525d552fde7}{%
           family={Phipps},
           familyi={P\bibinitperiod},
           given={Kaleb},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ce6fe92a26d6db892948f5a55523d9}{%
           family={Schienle},
           familyi={S\bibinitperiod},
           given={Melanie},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Annual Reviews}%
      }
      \strng{namehash}{5b8fc77034f26786e69c97c60c0fb61d}
      \strng{fullhash}{d0c09b17abf1ecc4782c1d3e14642a56}
      \strng{bibnamehash}{5b8fc77034f26786e69c97c60c0fb61d}
      \strng{authorbibnamehash}{5b8fc77034f26786e69c97c60c0fb61d}
      \strng{authornamehash}{5b8fc77034f26786e69c97c60c0fb61d}
      \strng{authorfullhash}{d0c09b17abf1ecc4782c1d3e14642a56}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Model diagnostics and forecast evaluation are closely related tasks, with the former concerning in-sample goodness (or lack) of fit and the latter addressing predictive performance out-of-sample. We review the ubiquitous setting in which forecasts are cast in the form of quantiles or quantile-bounded prediction intervals. We distinguish unconditional calibration, which corresponds to classical coverage criteria, from the stronger notion of conditional calibration, as can be visualized in quantile reliability diagrams. Consistent scoring functions?including, but not limited to, the widely used asymmetric piecewise linear score or pinball loss?provide for comparative assessment and ranking, and link to the coefficient of determination and skill scores. We illustrate the use of these tools on Engel's food expenditure data, the Global Energy Forecasting Competition 2014, and the US COVID-19 Forecast Hub. Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.}
      \field{issn}{2326-8298}
      \field{journaltitle}{Annual Review of Statistics and Its Application}
      \field{month}{7~3}
      \field{title}{Model Diagnostics and Forecast Evaluation for Quantiles}
      \field{year}{2023}
      \verb{doi}
      \verb 10.1146/annurev-statistics-032921-020240
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1146/annurev-statistics-032921-020240
      \endverb
      \verb{url}
      \verb https://doi.org/10.1146/annurev-statistics-032921-020240
      \endverb
    \endentry
    \entry{Grimit2006-dg}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=b1c3738d3543ea4205b6bd0cfcaa36d2}{%
           family={Grimit},
           familyi={G\bibinitperiod},
           given={E\bibnamedelima P},
           giveni={E\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=96bd8f79dfbdedfeb0a434710528895d}{%
           family={Gneiting},
           familyi={G\bibinitperiod},
           given={T},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a812d26393d9702d296af06542f7762e}{%
           family={Berrocal},
           familyi={B\bibinitperiod},
           given={V\bibnamedelima J},
           giveni={V\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0e304d42deaa89660432257944836337}{%
           family={Johnson},
           familyi={J\bibinitperiod},
           given={N\bibnamedelima A},
           giveni={N\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Wiley}%
      }
      \strng{namehash}{90170975368cc9979ad4191caf896c5c}
      \strng{fullhash}{ee104b6523607064dd9e2747f8e2fcf7}
      \strng{bibnamehash}{90170975368cc9979ad4191caf896c5c}
      \strng{authorbibnamehash}{90170975368cc9979ad4191caf896c5c}
      \strng{authornamehash}{90170975368cc9979ad4191caf896c5c}
      \strng{authorfullhash}{ee104b6523607064dd9e2747f8e2fcf7}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract An analogue of the linear continuous ranked probability score is introduced that applies to probabilistic forecasts of circular quantities, such as wind direction. This scoring rule is proper and thereby discourages hedging. The circular continuous ranked probability score reduces to angular distance when the forecast is deterministic, just as the linear continuous ranked probability score generalizes the absolute error. Furthermore, the circular continuous ranked probability score provides a direct way of comparing deterministic forecasts, discrete forecast ensembles, and post-processed forecast ensembles that can take the form of circular probability density functions. The circular continuous ranked probability score is used in this study to compare predictions of 10 m wind direction for 361 cases of mesoscale, short-range ensemble forecasts over the North American Pacific Northwest. Simple, calibrated probability forecasts based on the ensemble mean and its forecast error history over the period outperform probability forecasts constructed directly from the ensemble sample statistics. These results suggest that short-term forecast uncertainty is not yet well predicted at mesoscale resolutions near the surface, despite the inclusion of multi-scheme physics diversity and surface boundary parameter perturbations in the mesoscale ensemble design. Copyright ? 2006 Royal Meteorological Society}
      \field{issn}{0035-9009, 1477-870X}
      \field{journaltitle}{Quarterly Journal of the Royal Meteorological Society}
      \field{month}{10}
      \field{number}{621C}
      \field{title}{The continuous ranked probability score for circular variables and its application to mesoscale forecast ensemble verification}
      \field{volume}{132}
      \field{year}{2006}
      \field{pages}{2925\bibrangedash 2942}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1256/qj.05.235
      \endverb
      \verb{file}
      \verb All Papers/G/Grimit et al. 2006 - The continuous ranked probability score for circula ... ables and its application to mesoscale forecast ensemble verification.pdf
      \endverb
      \verb{urlraw}
      \verb http://doi.wiley.com/10.1256/qj.05.235
      \endverb
      \verb{url}
      \verb http://doi.wiley.com/10.1256/qj.05.235
      \endverb
    \endentry
    \entry{Grinsztajn2022-nu}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=3cbce71ea99fefab28b3a1f15c56086e}{%
           family={Grinsztajn},
           familyi={G\bibinitperiod},
           given={Léo},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b3a56e54ad6b57a9b6e2d9cd76c329ab}{%
           family={Oyallon},
           familyi={O\bibinitperiod},
           given={Edouard},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cfa3239dded54488f9a82a5718d5b927}{%
           family={Varoquaux},
           familyi={V\bibinitperiod},
           given={Gaël},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7a755f429af584d3d4520b48da09fefe}
      \strng{fullhash}{ed4d997d878c48f754c0a0ed32f81c2c}
      \strng{bibnamehash}{ed4d997d878c48f754c0a0ed32f81c2c}
      \strng{authorbibnamehash}{ed4d997d878c48f754c0a0ed32f81c2c}
      \strng{authornamehash}{7a755f429af584d3d4520b48da09fefe}
      \strng{authorfullhash}{ed4d997d878c48f754c0a0ed32f81c2c}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{title}{Why do tree-based models still outperform deep learning on tabular data?}
      \field{year}{2022}
      \verb{eprint}
      \verb 2207.08815
      \endverb
      \verb{file}
      \verb All Papers/G/Grinsztajn et al. 2022 - Why do tree-based models still outperform deep learning on tabular data.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2207.08815
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2207.08815
      \endverb
    \endentry
    \entry{Grover2019-iw}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=0e7ecffac4441691ff61e0b68d89cd33}{%
           family={Grover},
           familyi={G\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ac5e8918d6901571229ee91377e81197}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=342d4a7c98eb661fa78efb7febb09df5}{%
           family={Zweig},
           familyi={Z\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=160929a7e6ceecdf3f7c9a13abd5ea35}{%
           family={Ermon},
           familyi={E\bibinitperiod},
           given={Stefano},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{58aa14734b6910a3df8e0ee71d9c65d0}
      \strng{fullhash}{15e89873799bc8f6aca0f913d206d7ea}
      \strng{bibnamehash}{58aa14734b6910a3df8e0ee71d9c65d0}
      \strng{authorbibnamehash}{58aa14734b6910a3df8e0ee71d9c65d0}
      \strng{authornamehash}{58aa14734b6910a3df8e0ee71d9c65d0}
      \strng{authorfullhash}{15e89873799bc8f6aca0f913d206d7ea}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Learning Representations}
      \field{title}{Stochastic Optimization of Sorting Networks via Continuous Relaxations}
      \field{year}{2019}
      \verb{file}
      \verb All Papers/G/Grover et al. 2019 - Stochastic Optimization of Sorting Networks via Continuous Relaxations.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=H1eSS3CcKX
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=H1eSS3CcKX
      \endverb
    \endentry
    \entry{Guizilini2020-tr}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=0a10a128c34ebb7a14d523264b0684f2}{%
           family={Guizilini},
           familyi={G\bibinitperiod},
           given={Vitor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=05c93b9fbf514de4c6ec52be37ed7836}{%
           family={Ambrus},
           familyi={A\bibinitperiod},
           given={Rares},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=811690b7cfd2f118925bb59c0313cfb5}{%
           family={Pillai},
           familyi={P\bibinitperiod},
           given={Sudeep},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=accc53fadc687fcb17eddf25d3c1d601}{%
           family={Raventos},
           familyi={R\bibinitperiod},
           given={Allan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4310b5568f7680f83c3e4c70d4f4d7a1}{%
           family={Gaidon},
           familyi={G\bibinitperiod},
           given={Adrien},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{53caf48420b943b85c5a046e9b8d1d1b}
      \strng{fullhash}{ffc441adffa90f1380dd0b3732af2111}
      \strng{bibnamehash}{53caf48420b943b85c5a046e9b8d1d1b}
      \strng{authorbibnamehash}{53caf48420b943b85c5a046e9b8d1d1b}
      \strng{authornamehash}{53caf48420b943b85c5a046e9b8d1d1b}
      \strng{authorfullhash}{ffc441adffa90f1380dd0b3732af2111}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})}
      \field{title}{{3D} Packing for {Self-Supervised} Monocular Depth Estimation}
      \field{year}{2020}
      \verb{file}
      \verb All Papers/G/Guizilini et al. 2020 - 3D Packing for Self-Supervised Monocular Depth Estimation.pdf
      \endverb
    \endentry
    \entry{Gulshan2016-kw}{article}{}
      \name{author}{15}{}{%
        {{un=0,uniquepart=base,hash=8422c434883611bc561c07393f2bf716}{%
           family={Gulshan},
           familyi={G\bibinitperiod},
           given={Varun},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=79113f7fef30b07d1aa3427c8d2fda7e}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Lily},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3109ccbd2b546b240e3eccdc8514563d}{%
           family={Coram},
           familyi={C\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=79140b79b39daf5bc1fd1178ef4f0a8d}{%
           family={Stumpe},
           familyi={S\bibinitperiod},
           given={Martin\bibnamedelima C},
           giveni={M\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=217abd2e04fcec2567a9371e18d72813}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Derek},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1c5e561eab817572a9e9f51c9bc7bcaa}{%
           family={Narayanaswamy},
           familyi={N\bibinitperiod},
           given={Arunachalam},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9382331d707ba963cf9f239bb1387441}{%
           family={Venugopalan},
           familyi={V\bibinitperiod},
           given={Subhashini},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9cca0562895b7ddb20c66bea2b2b320d}{%
           family={Widner},
           familyi={W\bibinitperiod},
           given={Kasumi},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3341e18501016cf914d27f765bb9a936}{%
           family={Madams},
           familyi={M\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4532b88e41f2e1f6714200fe8db1450b}{%
           family={Cuadros},
           familyi={C\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=35d95226ab64cdf5226962d4775bad64}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Ramasamy},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6be8a4feb58712401c76f9a1a613bc5f}{%
           family={Raman},
           familyi={R\bibinitperiod},
           given={Rajiv},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5ea9cf0d7ff7aae496df937b40349060}{%
           family={Nelson},
           familyi={N\bibinitperiod},
           given={Philip\bibnamedelima C},
           giveni={P\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=505a69386bb54f2656dea3bc3031a95d}{%
           family={Mega},
           familyi={M\bibinitperiod},
           given={Jessica\bibnamedelima L},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d38a80bf82831fe5c4d0f850a77ddff9}{%
           family={Webster},
           familyi={W\bibinitperiod},
           given={Dale\bibnamedelima R},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{3ce960fef398d5afca598d8c6c127cbc}
      \strng{fullhash}{5f780573e49e5be96b9ff80a9a258f64}
      \strng{bibnamehash}{3ce960fef398d5afca598d8c6c127cbc}
      \strng{authorbibnamehash}{3ce960fef398d5afca598d8c6c127cbc}
      \strng{authornamehash}{3ce960fef398d5afca598d8c6c127cbc}
      \strng{authorfullhash}{5f780573e49e5be96b9ff80a9a258f64}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Importance: Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation. Objective: To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs. Design and Setting: A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency. Exposure: Deep learning-trained algorithm. Main Outcomes and Measures: The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity. Results: The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%. Conclusions and Relevance: In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.}
      \field{issn}{0098-7484, 1538-3598}
      \field{journaltitle}{JAMA: the journal of the American Medical Association}
      \field{month}{13~12}
      \field{number}{22}
      \field{title}{Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs}
      \field{volume}{316}
      \field{year}{2016}
      \field{pages}{2402\bibrangedash 2410}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1001/jama.2016.17216
      \endverb
      \verb{file}
      \verb All Papers/G/Gulshan et al. 2016 - Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1001/jama.2016.17216
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1001/jama.2016.17216
      \endverb
    \endentry
    \entry{Guo2017-ow}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e053cf028522c39af65dd862da2d8ed1}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Chuan},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=373d04718ec0efd3786a0e2b6c0adf00}{%
           family={Pleiss},
           familyi={P\bibinitperiod},
           given={Geoff},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17b6a43f5598c7efd897f9d254090fdd}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=68a0238356fbd88b34be8886f25938a7}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={Kilian\bibnamedelima Q},
           giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=4428b76e1301b2db58587fb18bb59a38}{%
           family={Precup},
           familyi={P\bibinitperiod},
           given={Doina},
           giveni={D\bibinitperiod}}}%
        {{hash=a71fae003da84f44e31d26e868859945}{%
           family={Teh},
           familyi={T\bibinitperiod},
           given={Yee\bibnamedelima Whye},
           giveni={Y\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{203b115999f73097be6ab32411fb1f42}
      \strng{fullhash}{f3833eed8ef776fc6c89f407c7d66c34}
      \strng{bibnamehash}{203b115999f73097be6ab32411fb1f42}
      \strng{authorbibnamehash}{203b115999f73097be6ab32411fb1f42}
      \strng{authornamehash}{203b115999f73097be6ab32411fb1f42}
      \strng{authorfullhash}{f3833eed8ef776fc6c89f407c7d66c34}
      \strng{editorbibnamehash}{ee1b5a05452f2afba9cd287be3ce0338}
      \strng{editornamehash}{ee1b5a05452f2afba9cd287be3ce0338}
      \strng{editorfullhash}{ee1b5a05452f2afba9cd287be3ce0338}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.}
      \field{booktitle}{Proceedings of the 34th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{On Calibration of Modern Neural Networks}
      \field{volume}{70}
      \field{year}{2017}
      \field{pages}{1321\bibrangedash 1330}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/G/Guo et al. 2017 - 1706.04599.pdf;All Papers/G/Guo et al. 2017 - On Calibration of Modern Neural Networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v70/guo17a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v70/guo17a.html
      \endverb
    \endentry
    \entry{Hofmann2011-rg}{report}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=aab385203b295bcea3e72bd9a22eebae}{%
           family={Hofmann},
           familyi={H\bibinitperiod},
           given={Heike},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c0396cfbff5cf793d7d9baebde82e06}{%
           family={Kafadar},
           familyi={K\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1d0f6977ce2d043e298a59a1cd75b7ef}{%
           family={Wickham},
           familyi={W\bibinitperiod},
           given={Hadley},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {had.co.nz}%
      }
      \strng{namehash}{e4b48967d851326922068d3b3f4bb850}
      \strng{fullhash}{ea600a69a578acdb393d47b68a2020ab}
      \strng{bibnamehash}{ea600a69a578acdb393d47b68a2020ab}
      \strng{authorbibnamehash}{ea600a69a578acdb393d47b68a2020ab}
      \strng{authornamehash}{e4b48967d851326922068d3b3f4bb850}
      \strng{authorfullhash}{ea600a69a578acdb393d47b68a2020ab}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Letter-value plots: Boxplots for large data}
      \field{type}{techreport}
      \field{year}{2011}
    \endentry
    \entry{Holm1979-rk}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=23b6a8c4d3d9710bb3b84d15d65dc86e}{%
           family={Holm},
           familyi={H\bibinitperiod},
           given={Sture},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}%
      }
      \strng{namehash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \strng{fullhash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \strng{bibnamehash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \strng{authorbibnamehash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \strng{authornamehash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \strng{authorfullhash}{23b6a8c4d3d9710bb3b84d15d65dc86e}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{[This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.]}
      \field{issn}{0303-6898, 1467-9469}
      \field{journaltitle}{Scandinavian journal of statistics, theory and applications}
      \field{number}{2}
      \field{title}{A Simple Sequentially Rejective Multiple Test Procedure}
      \field{volume}{6}
      \field{year}{1979}
      \field{pages}{65\bibrangedash 70}
      \range{pages}{6}
      \verb{file}
      \verb All Papers/H/Holm 1979 - A Simple Sequentially Rejective Multiple Test Procedure.pdf
      \endverb
      \verb{urlraw}
      \verb http://www.jstor.org/stable/4615733
      \endverb
      \verb{url}
      \verb http://www.jstor.org/stable/4615733
      \endverb
    \endentry
    \entry{Ismail_Fawaz2019-od}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=685ec3b02f9aa64c2818d70b3d5734b9}{%
           family={Ismail\bibnamedelima Fawaz},
           familyi={I\bibinitperiod\bibinitdelim F\bibinitperiod},
           given={Hassan},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fcc574d00402a54e3441aab7924f60c6}{%
           family={Forestier},
           familyi={F\bibinitperiod},
           given={Germain},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cc9d8ad55a0e41f6e52142507ae4cf56}{%
           family={Weber},
           familyi={W\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d350ee2220fd81c99504110da25de7df}{%
           family={Idoumghar},
           familyi={I\bibinitperiod},
           given={Lhassane},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cf881384e0d7c8971ea9353d846fdaba}{%
           family={Muller},
           familyi={M\bibinitperiod},
           given={Pierre-Alain},
           giveni={P\bibinithyphendelim A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{600a51263e6c504495944b2577cb9915}
      \strng{fullhash}{3d59e100991a7a89c7ba06c317fb4599}
      \strng{bibnamehash}{600a51263e6c504495944b2577cb9915}
      \strng{authorbibnamehash}{600a51263e6c504495944b2577cb9915}
      \strng{authornamehash}{600a51263e6c504495944b2577cb9915}
      \strng{authorfullhash}{3d59e100991a7a89c7ba06c317fb4599}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.}
      \field{issn}{1384-5810, 1573-756X}
      \field{journaltitle}{Data mining and knowledge discovery}
      \field{month}{1~7}
      \field{number}{4}
      \field{title}{Deep learning for time series classification: a review}
      \field{volume}{33}
      \field{year}{2019}
      \field{pages}{917\bibrangedash 963}
      \range{pages}{47}
      \verb{doi}
      \verb 10.1007/s10618-019-00619-1
      \endverb
      \verb{file}
      \verb All Papers/I/Ismail Fawaz et al. 2019 - Deep learning for time series classification - a review.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10618-019-00619-1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10618-019-00619-1
      \endverb
    \endentry
    \entry{Izbicki2020-ed}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=c87b1d660c84fdd48a26999544b6571a}{%
           family={Izbicki},
           familyi={I\bibinitperiod},
           given={Rafael},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b14f111655befaaec2b7d240e6c5148}{%
           family={Shimizu},
           familyi={S\bibinitperiod},
           given={Gilson},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=071f5d4410272153d32a711d3ab95872}{%
           family={Stern},
           familyi={S\bibinitperiod},
           given={Rafael},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=7be343845f20ae7e5afb043f78f991fe}{%
           family={Chiappa},
           familyi={C\bibinitperiod},
           given={Silvia},
           giveni={S\bibinitperiod}}}%
        {{hash=864584c60b808b517f2a06c664ec116a}{%
           family={Calandra},
           familyi={C\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{bd5d3737c26959827fc1531d0939c243}
      \strng{fullhash}{022ebbc662b083dcfb72d0add15f850b}
      \strng{bibnamehash}{022ebbc662b083dcfb72d0add15f850b}
      \strng{authorbibnamehash}{022ebbc662b083dcfb72d0add15f850b}
      \strng{authornamehash}{bd5d3737c26959827fc1531d0939c243}
      \strng{authorfullhash}{022ebbc662b083dcfb72d0add15f850b}
      \strng{editorbibnamehash}{975244d506887ac8f8fcf58a1715e22b}
      \strng{editornamehash}{975244d506887ac8f8fcf58a1715e22b}
      \strng{editorfullhash}{975244d506887ac8f8fcf58a1715e22b}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Besides average coverage, one might also desire to control conditional coverage, that is, coverage for every new testing point. However, without strong assumptions, conditional coverage is unachievable. Given this limitation, the literature has focused on methods with asymptotical conditional coverage. In order to obtain this property, these methods require strong conditions on the dependence between the target variable and the features. We introduce two conformal methods based on conditional density estimators that do not depend on this type of assumption to obtain asymptotic conditional coverage: Dist-split and CD-split. While Dist-split asymptotically obtains optimal intervals, which are easier to interpret than general regions, CD-split obtains optimal size regions, which are smaller than intervals. CD-split also obtains local coverage by creating prediction bands locally on a partition of the features space. This partition is data-driven and scales to high-dimensional settings. In a wide variety of simulated scenarios, our methods have a better control of conditional coverage and have smaller length than previously proposed methods.}
      \field{booktitle}{Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Flexible distribution-free conditional predictive bands using density estimators}
      \field{volume}{108}
      \field{year}{2020}
      \field{pages}{3068\bibrangedash 3077}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/I/Izbicki et al. 2020 - Flexible distribution-free conditional predictive bands using density estimators.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v108/izbicki20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v108/izbicki20a.html
      \endverb
    \endentry
    \entry{Jospin2022-zi}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=05315335dee43a918e4b3046c9b6319c}{%
           family={Jospin},
           familyi={J\bibinitperiod},
           given={Laurent\bibnamedelima Valentin},
           giveni={L\bibinitperiod\bibinitdelim V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0aab48074c6e06bce38fb86d876f9e69}{%
           family={Laga},
           familyi={L\bibinitperiod},
           given={Hamid},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e05a56800a49fe37ea40a9394fbd03bb}{%
           family={Boussaid},
           familyi={B\bibinitperiod},
           given={Farid},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=97f723cbded90372d0c37e035b53bd9e}{%
           family={Buntine},
           familyi={B\bibinitperiod},
           given={Wray},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=311b43bd7e74572b53bdaa35dd1d492a}{%
           family={Bennamoun},
           familyi={B\bibinitperiod},
           given={Mohammed},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ieeexplore.ieee.org}%
      }
      \strng{namehash}{5021fa75d9168be22976a8ec0589adf0}
      \strng{fullhash}{b209615f3c9d7f8a56f3d760e3b578ef}
      \strng{bibnamehash}{5021fa75d9168be22976a8ec0589adf0}
      \strng{authorbibnamehash}{5021fa75d9168be22976a8ec0589adf0}
      \strng{authornamehash}{5021fa75d9168be22976a8ec0589adf0}
      \strng{authorfullhash}{b209615f3c9d7f8a56f3d760e3b578ef}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks, i.e., stochastic artificial neural networks trained using Bayesian methods.}
      \field{issn}{1556-6048}
      \field{journaltitle}{IEEE Computational Intelligence Magazine}
      \field{month}{5}
      \field{number}{2}
      \field{title}{{Hands-On} Bayesian Neural {Networks---A} Tutorial for Deep Learning Users}
      \field{volume}{17}
      \field{year}{2022}
      \field{pages}{29\bibrangedash 48}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1109/MCI.2022.3155327
      \endverb
      \verb{file}
      \verb All Papers/J/Jospin et al. 2022 - Hands-On Bayesian Neural Networks—A Tutorial for Deep Learning Users.pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1109/MCI.2022.3155327
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1109/MCI.2022.3155327
      \endverb
      \keyw{Deep learning;Training data;Uncertainty;Design methodology;Computational modeling;Stochastic processes;Bayes methods;Neural networks;Bayesian}
    \endentry
    \entry{Karandikar2021-ml}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=a8e70b62cbb402c31a4f6a5e803e5a8b}{%
           family={Karandikar},
           familyi={K\bibinitperiod},
           given={Archit},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ffbad46a019f0a90111c85c007abb307}{%
           family={Cain},
           familyi={C\bibinitperiod},
           given={Nicholas},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=831126fd259babc16b5bd86bbc5f0098}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Dustin},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=55c0a67ecb602e5a3063393ffee9c7fa}{%
           family={Lakshminarayanan},
           familyi={L\bibinitperiod},
           given={Balaji},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=091907acecffa990a510ca1189a4d722}{%
           family={Mozer},
           familyi={M\bibinitperiod},
           given={Michael\bibnamedelima C},
           giveni={M\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d1b27bf0c7b7a5165a04cb629e1797c4}{%
           family={Roelofs},
           familyi={R\bibinitperiod},
           given={Becca},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{477233d19b7cb7df0f725f868487b0b3}
      \strng{fullhash}{6bc71bfe2ac70be367a5567dcde2c864}
      \strng{bibnamehash}{477233d19b7cb7df0f725f868487b0b3}
      \strng{authorbibnamehash}{477233d19b7cb7df0f725f868487b0b3}
      \strng{authornamehash}{477233d19b7cb7df0f725f868487b0b3}
      \strng{authorfullhash}{6bc71bfe2ac70be367a5567dcde2c864}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1\% decrease in accuracy. For instance, we observe an 82\% reduction in ECE (70\% relative to the post-hoc rescaled ECE) in exchange for a 0.7\% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{title}{Soft Calibration Objectives for Neural Networks}
      \field{year}{2021}
      \verb{eprint}
      \verb 2108.00106
      \endverb
      \verb{file}
      \verb All Papers/K/Karandikar et al. 2021 - Soft Calibration Objectives for Neural Networks.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2108.00106
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2108.00106
      \endverb
    \endentry
    \entry{Kuleshov2022-pv}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2e7fab79c30ca590cb9a99af964a32c9}{%
           family={Kuleshov},
           familyi={K\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=01596ffae394cda3182479c7c43c28b0}{%
           family={Deshpande},
           familyi={D\bibinitperiod},
           given={Shachi},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{6}{}{%
        {{hash=050c0df95e5eb5c3218fdd148d0e17a4}{%
           family={Chaudhuri},
           familyi={C\bibinitperiod},
           given={Kamalika},
           giveni={K\bibinitperiod}}}%
        {{hash=83a44c18acfaa38fa9f6ba76cfdc0132}{%
           family={Jegelka},
           familyi={J\bibinitperiod},
           given={Stefanie},
           giveni={S\bibinitperiod}}}%
        {{hash=da5283630e781fb48ecf4859f54bca52}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Le},
           giveni={L\bibinitperiod}}}%
        {{hash=5c3318079c4819b9fff61d2e42d425e8}{%
           family={Szepesvari},
           familyi={S\bibinitperiod},
           given={Csaba},
           giveni={C\bibinitperiod}}}%
        {{hash=86a8bc54ad4440c98093d56ca91cc28a}{%
           family={Niu},
           familyi={N\bibinitperiod},
           given={Gang},
           giveni={G\bibinitperiod}}}%
        {{hash=0dafa6bad2e178837ff918538e24226a}{%
           family={Sabato},
           familyi={S\bibinitperiod},
           given={Sivan},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{fullhash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{bibnamehash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{authorbibnamehash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{authornamehash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{authorfullhash}{850a6297ae795ab8a8c3bd661b76574b}
      \strng{editorbibnamehash}{4d1e7770366aa87da47b1c8393d68dc6}
      \strng{editornamehash}{4d1e7770366aa87da47b1c8393d68dc6}
      \strng{editorfullhash}{a844824dc9c7ca0f339a33d2aaf1cce2}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Accurate probabilistic predictions can be characterized by two properties---calibration and sharpness. However, standard maximum likelihood training yields models that are poorly calibrated and thus inaccurate---a 90\% confidence interval typically does not contain the true outcome 90\% of the time. This paper argues that calibration is important in practice and is easy to maintain by performing low-dimensional density estimation. We introduce a simple training procedure based on recalibration that yields calibrated models without sacrificing overall performance; unlike previous approaches, ours ensures the most general property of distribution calibration and applies to any model, including neural networks. We formally prove the correctness of our procedure assuming that we can estimate densities in low dimensions and we establish uniform convergence bounds. Our results yield empirical performance improvements on linear and deep Bayesian models and suggest that calibration should be increasingly leveraged across machine learning.}
      \field{booktitle}{Proceedings of the 39th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation}
      \field{volume}{162}
      \field{year}{2022}
      \field{pages}{11683\bibrangedash 11693}
      \range{pages}{11}
      \verb{file}
      \verb All Papers/K/Kuleshov and Deshpande 2022 - Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v162/kuleshov22a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v162/kuleshov22a.html
      \endverb
    \endentry
    \entry{Kuleshov2018-tb}{inproceedings}{}
      \name{author}{3}{ul=2}{%
        {{un=0,uniquepart=base,hash=2e7fab79c30ca590cb9a99af964a32c9}{%
           family={Kuleshov},
           familyi={K\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e70631cd0ab3faa5d58f1619e625f688}{%
           family={Fenner},
           familyi={F\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=160929a7e6ceecdf3f7c9a13abd5ea35}{%
           family={Ermon},
           familyi={E\bibinitperiod},
           given={Stefano},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=dc770b9b0d58d3008bbb3906497d898c}{%
           family={Dy},
           familyi={D\bibinitperiod},
           given={Jennifer},
           giveni={J\bibinitperiod}}}%
        {{hash=112eb0b147c4a7f674d015a86e5dea70}{%
           family={Krause},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{a1659300f99fd027c70580e6b133f35e}
      \strng{fullhash}{47dc8af3970573fe7090eb38a7c7a0bf}
      \strng{bibnamehash}{47dc8af3970573fe7090eb38a7c7a0bf}
      \strng{authorbibnamehash}{47dc8af3970573fe7090eb38a7c7a0bf}
      \strng{authornamehash}{a1659300f99fd027c70580e6b133f35e}
      \strng{authorfullhash}{47dc8af3970573fe7090eb38a7c7a0bf}
      \strng{editorbibnamehash}{83be554d58af5be1788b5c3616f0e92a}
      \strng{editornamehash}{83be554d58af5be1788b5c3616f0e92a}
      \strng{editorfullhash}{83be554d58af5be1788b5c3616f0e92a}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate --- for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.}
      \field{booktitle}{Proceedings of the 35th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Accurate Uncertainties for Deep Learning Using Calibrated Regression}
      \field{volume}{80}
      \field{year}{2018}
      \field{pages}{2796\bibrangedash 2804}
      \range{pages}{9}
      \verb{file}
      \verb All Papers/K/Kuleshov et al. 2018 - Accurate Uncertainties for Deep Learning Using Calibrated Regression.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v80/kuleshov18a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v80/kuleshov18a.html
      \endverb
      \keyw{Calibration;Quantiles}
    \endentry
    \entry{Lakshminarayanan2017-zg}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=a1d0c876430d0e1a43f09ef428b443f2}{%
           family={{Lakshminarayanan}},
           familyi={L\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=a62bc5b8622c3cf0de1a6074a6dcfa23}{%
           family={{Pritzel}},
           familyi={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {proceedings.neurips.cc}%
      }
      \strng{namehash}{e375428ccd48ecc2fbcadd370e41c310}
      \strng{fullhash}{e375428ccd48ecc2fbcadd370e41c310}
      \strng{bibnamehash}{e375428ccd48ecc2fbcadd370e41c310}
      \strng{authorbibnamehash}{e375428ccd48ecc2fbcadd370e41c310}
      \strng{authornamehash}{e375428ccd48ecc2fbcadd370e41c310}
      \strng{authorfullhash}{e375428ccd48ecc2fbcadd370e41c310}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to …}
      \field{issn}{1049-5258}
      \field{journaltitle}{Advances in neural information processing systems}
      \field{title}{Simple and scalable predictive uncertainty estimation using deep ensembles}
      \field{year}{2017}
      \verb{file}
      \verb All Papers/L/Lakshminarayanan et al. 2017 - Simple and scalable predictive uncertainty estimation using deep ensembles.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html
      \endverb
      \keyw{Ensembling}
    \endentry
    \entry{Minderer2021-xw}{inproceedings}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=06829adce8713f0ace444279dcd7328f}{%
           family={Minderer},
           familyi={M\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ad3b7f61a5635ccd63eddb8543c8f5f}{%
           family={Djolonga},
           familyi={D\bibinitperiod},
           given={Josip},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c6657bca0876195d7dfd6cbc48587dab}{%
           family={Romijnders},
           familyi={R\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f2fa457f3f10debc36c2b5695ba3898a}{%
           family={Hubis},
           familyi={H\bibinitperiod},
           given={Frances},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=00b950307c7d096765bb6df00c7c5c3a}{%
           family={Zhai},
           familyi={Z\bibinitperiod},
           given={Xiaohua},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=493a532418540859dca92cf3b579de2a}{%
           family={Houlsby},
           familyi={H\bibinitperiod},
           given={Neil},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=831126fd259babc16b5bd86bbc5f0098}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Dustin},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=26b9b9f1c61e9425fb38070e6d2c9f31}{%
           family={Lucic},
           familyi={L\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{5}{}{%
        {{hash=6cb5af0e649387de5bbfdb150cc7a786}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={M},
           giveni={M\bibinitperiod}}}%
        {{hash=02854da982edf66ffa27f4fc1c738779}{%
           family={Beygelzimer},
           familyi={B\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod}}}%
        {{hash=5e4dc483044b6dbd4496a818b8b6c67a}{%
           family={Dauphin},
           familyi={D\bibinitperiod},
           given={Y},
           giveni={Y\bibinitperiod}}}%
        {{hash=12f9558e0c25864defe6eaed94a9011b}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={P\bibnamedelima S},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=3d379bd6370f69ebc79dc5337ed96d13}{%
           family={Vaughan},
           familyi={V\bibinitperiod},
           given={J\bibnamedelima Wortman},
           giveni={J\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{b109f8a0ef1a4010ae8cb4f81fa54dcc}
      \strng{fullhash}{4598be9793746d616c9705d210a72d29}
      \strng{bibnamehash}{b109f8a0ef1a4010ae8cb4f81fa54dcc}
      \strng{authorbibnamehash}{b109f8a0ef1a4010ae8cb4f81fa54dcc}
      \strng{authornamehash}{b109f8a0ef1a4010ae8cb4f81fa54dcc}
      \strng{authorfullhash}{4598be9793746d616c9705d210a72d29}
      \strng{editorbibnamehash}{6013a44c5dec47e5e51fdd72b3f8b5eb}
      \strng{editornamehash}{6013a44c5dec47e5e51fdd72b3f8b5eb}
      \strng{editorfullhash}{ad1ea98aa644fe4cbc55fbe24d1f78a9}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Revisiting the Calibration of Modern Neural Networks}
      \field{volume}{34}
      \field{year}{2021}
      \field{pages}{15682\bibrangedash 15694}
      \range{pages}{13}
      \verb{file}
      \verb All Papers/M/Minderer et al. 2021 - 2106.07998.pdf;All Papers/M/Minderer et al. 2021 - Revisiting the Calibration of Modern Neural Networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf
      \endverb
      \keyw{Calibration;Classification}
    \endentry
    \entry{Pearce2018-lo}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=da626de55eba17c06969cc59bb077138}{%
           family={Pearce},
           familyi={P\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f1054809874cfb112aa15035d122a879}{%
           family={Brintrup},
           familyi={B\bibinitperiod},
           given={Alexandra},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=39ac7e01579d2d3ca29a8e40469130f8}{%
           family={Zaki},
           familyi={Z\bibinitperiod},
           given={Mohamed},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9640e7c10d70ae6e8beb0485f97671d6}{%
           family={Neely},
           familyi={N\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=dc770b9b0d58d3008bbb3906497d898c}{%
           family={Dy},
           familyi={D\bibinitperiod},
           given={Jennifer},
           giveni={J\bibinitperiod}}}%
        {{hash=112eb0b147c4a7f674d015a86e5dea70}{%
           family={Krause},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{8a20e861beecdc7d34b207fbc670fd78}
      \strng{fullhash}{d9687f7b05c90f2ae40370b259f5fec2}
      \strng{bibnamehash}{8a20e861beecdc7d34b207fbc670fd78}
      \strng{authorbibnamehash}{8a20e861beecdc7d34b207fbc670fd78}
      \strng{authornamehash}{8a20e861beecdc7d34b207fbc670fd78}
      \strng{authorfullhash}{d9687f7b05c90f2ae40370b259f5fec2}
      \strng{editorbibnamehash}{83be554d58af5be1788b5c3616f0e92a}
      \strng{editornamehash}{83be554d58af5be1788b5c3616f0e92a}
      \strng{editorfullhash}{83be554d58af5be1788b5c3616f0e92a}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10\%.}
      \field{booktitle}{Proceedings of the 35th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{{High-Quality} Prediction Intervals for Deep Learning: A {Distribution-Free}, Ensembled Approach}
      \field{volume}{80}
      \field{year}{2018}
      \field{pages}{4075\bibrangedash 4084}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/P/Pearce et al. 2018 - 1802.07167.pdf;All Papers/P/Pearce et al. 2018 - High-Quality Prediction Intervals for Deep Learning - A Distribution-Free, Ensembled Approach.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v80/pearce18a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v80/pearce18a.html
      \endverb
      \keyw{Intervals}
    \endentry
    \entry{Pinson2012-ba}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=f868152a252ba8f399a537f3a95cf9f3}{%
           family={Pinson},
           familyi={P\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2a4ad3604750b0d6b6ec01a0bac8c027}{%
           family={Hagedorn},
           familyi={H\bibinitperiod},
           given={Renate},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Wiley}%
      }
      \strng{namehash}{46cb50f63491d0e1df623f32f50a96b4}
      \strng{fullhash}{46cb50f63491d0e1df623f32f50a96b4}
      \strng{bibnamehash}{46cb50f63491d0e1df623f32f50a96b4}
      \strng{authorbibnamehash}{46cb50f63491d0e1df623f32f50a96b4}
      \strng{authornamehash}{46cb50f63491d0e1df623f32f50a96b4}
      \strng{authorfullhash}{46cb50f63491d0e1df623f32f50a96b4}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract A framework for the verification of ensemble forecasts of near-surface wind speed is described. It is based on existing scores and diagnostic tools, though considering observations from synoptic stations as reference instead of the analysis. This approach is motivated by the idea of having a user-oriented view of verification, for instance with the wind power applications in mind. The verification framework is specifically applied to the case of ECMWF ensemble forecasts and over Europe. Dynamic climatologies are derived at the various stations, serving as a benchmark. The impact of observational uncertainty on scores and diagnostic tools is also considered. The interest of this framework is demonstrated from its application to the routine evaluation of ensemble forecasts and to the assessment of the quality improvements brought in by the recent change in horizontal resolution of the ECMWF ensemble prediction system. The most important conclusions cover (1) the generally high skill of these ensemble forecasts of near-surface wind speed when evaluated at synoptic stations, (2) the noteworthy improvement of scores brought by the change of horizontal resolution, and, (3) the scope for further improvements of reliability and skill of wind speed ensemble forecasts by appropriate post-processing. Copyright ? 2011 Royal Meteorological Society}
      \field{issn}{1350-4827, 1469-8080}
      \field{journaltitle}{Meteorological Applications}
      \field{month}{12}
      \field{number}{4}
      \field{title}{Verification of the {ECMWF} ensemble forecasts of wind speed against analyses and observations}
      \field{volume}{19}
      \field{year}{2012}
      \field{pages}{484\bibrangedash 500}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1002/met.283
      \endverb
      \verb{file}
      \verb All Papers/P/Pinson and Hagedorn 2012 - Verification of the ECMWF ensemble forecasts of wind speed against analyses and observations.pdf
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/10.1002/met.283
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/10.1002/met.283
      \endverb
    \endentry
    \entry{Popordanoska2022-jz}{unpublished}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=80f8d38facc04fffa4ef913e52d5bd97}{%
           family={Popordanoska},
           familyi={P\bibinitperiod},
           given={Teodora},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fa7c1f5085613031eda4beb62784d20e}{%
           family={Sayer},
           familyi={S\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f2f7238bf44e1a8e4fa33f9907ec94c}{%
           family={Blaschko},
           familyi={B\bibinitperiod},
           given={Matthew\bibnamedelima B},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8e870330fc0baa3a6e3d79c81b15b5f1}
      \strng{fullhash}{3f84470047bb78dd949ea7b9d3610799}
      \strng{bibnamehash}{3f84470047bb78dd949ea7b9d3610799}
      \strng{authorbibnamehash}{3f84470047bb78dd949ea7b9d3610799}
      \strng{authornamehash}{8e870330fc0baa3a6e3d79c81b15b5f1}
      \strng{authorfullhash}{3f84470047bb78dd949ea7b9d3610799}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Calibrated probabilistic classifiers are models whose predicted probabilities can directly be interpreted as uncertainty estimates. It has been shown recently that deep neural networks are poorly calibrated and tend to output overconfident predictions. As a remedy, we propose a low-bias, trainable calibration error estimator based on Dirichlet kernel density estimates, which asymptotically converges to the true $L_p$ calibration error. This novel estimator enables us to tackle the strongest notion of multiclass calibration, called canonical (or distribution) calibration, while other common calibration methods are tractable only for top-label and marginal calibration. The computational complexity of our estimator is $\mathcal\{O\}(n^2)$, the convergence rate is $\mathcal\{O\}(n^\{-1/2\})$, and it is unbiased up to $\mathcal\{O\}(n^\{-2\})$, achieved by a geometric series debiasing scheme. In practice, this means that the estimator can be applied to small subsets of data, enabling efficient estimation and mini-batch updates. The proposed method has a natural choice of kernel, and can be used to generate consistent estimates of other quantities based on conditional expectation, such as the sharpness of a probabilistic classifier. Empirical results validate the correctness of our estimator, and demonstrate its utility in canonical calibration error estimation and calibration error regularized risk minimization.}
      \field{month}{10}
      \field{title}{A Consistent and Differentiable Lp Canonical Calibration Error Estimator}
      \field{year}{2022}
      \verb{file}
      \verb All Papers/P/Popordanoska et al. 2022 - 2210.07810.pdf;All Papers/P/Popordanoska et al. 2022 - A Consistent and Differentiable Lp Canonical Calibration Error Estimator.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/pdf?id=HMs5pxZq1If
      \endverb
      \verb{url}
      \verb https://openreview.net/pdf?id=HMs5pxZq1If
      \endverb
    \endentry
    \entry{Romano2019-kp}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=323f0bdd77c5be9d0e31f98f75f5fa08}{%
           family={{Romano}},
           familyi={R\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=19e4f8b476cd9b3e76bf1ad608df834e}{%
           family={{Patterson}},
           familyi={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {proceedings.neurips.cc}%
      }
      \strng{namehash}{056378d8e1a046fec61526b2d840596d}
      \strng{fullhash}{056378d8e1a046fec61526b2d840596d}
      \strng{bibnamehash}{056378d8e1a046fec61526b2d840596d}
      \strng{authorbibnamehash}{056378d8e1a046fec61526b2d840596d}
      \strng{authornamehash}{056378d8e1a046fec61526b2d840596d}
      \strng{authorfullhash}{056378d8e1a046fec61526b2d840596d}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{… conformal prediction with quantile regression . The resulting method, which we call conformalized quantile regression (… , we conclude that conformal quantile regression yields shorter …}
      \field{issn}{1049-5258}
      \field{journaltitle}{Advances in neural information processing systems}
      \field{title}{Conformalized quantile regression}
      \field{year}{2019}
      \verb{file}
      \verb All Papers/R/Romano et al. 2019 - 1905.03222.pdf;All Papers/R/Romano et al. 2019 - Conformalized quantile regression.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html
      \endverb
    \endentry
    \entry{Song2019-bk}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=0d9f240d032f9e8288bb3d77160c9165}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=536dbfc21f1928f20311917b4248ba27}{%
           family={Diethe},
           familyi={D\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ad0a95639e9e4d1b4b966605f4d4ec79}{%
           family={Kull},
           familyi={K\bibinitperiod},
           given={Meelis},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=715b97044ba694410083d16b5c29b92c}{%
           family={Flach},
           familyi={F\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=050c0df95e5eb5c3218fdd148d0e17a4}{%
           family={Chaudhuri},
           familyi={C\bibinitperiod},
           given={Kamalika},
           giveni={K\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{8a1409376fd877b7b8bab66debe59b19}
      \strng{fullhash}{ee9e8867dd855d831e756e7e590bb6df}
      \strng{bibnamehash}{8a1409376fd877b7b8bab66debe59b19}
      \strng{authorbibnamehash}{8a1409376fd877b7b8bab66debe59b19}
      \strng{authornamehash}{8a1409376fd877b7b8bab66debe59b19}
      \strng{authorfullhash}{ee9e8867dd855d831e756e7e590bb6df}
      \strng{editorbibnamehash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \strng{editornamehash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \strng{editorfullhash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.}
      \field{booktitle}{Proceedings of the 36th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Distribution calibration for regression}
      \field{volume}{97}
      \field{year}{2019}
      \field{pages}{5897\bibrangedash 5906}
      \range{pages}{10}
      \verb{file}
      \verb All Papers/S/Song et al. 2019 - Distribution calibration for regression.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/song19a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/song19a.html
      \endverb
    \endentry
    \entry{Tagasovska2019-ts}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=51d55108709a34918c3d462d18ab0db8}{%
           family={{Tagasovska}},
           familyi={T\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=34332e175b6b215969192a88d336c7e0}{%
           family={{Lopez-Paz}},
           familyi={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {proceedings.neurips.cc}%
      }
      \strng{namehash}{b1581b237afa01a6319a74bb10949338}
      \strng{fullhash}{b1581b237afa01a6319a74bb10949338}
      \strng{bibnamehash}{b1581b237afa01a6319a74bb10949338}
      \strng{authorbibnamehash}{b1581b237afa01a6319a74bb10949338}
      \strng{authornamehash}{b1581b237afa01a6319a74bb10949338}
      \strng{authorfullhash}{b1581b237afa01a6319a74bb10949338}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We provide single-model estimates of aleatoric and epistemic uncertainty for deep neural networks. To estimate aleatoric uncertainty, we propose Simultaneous Quantile Regression (SQR), a loss function to learn all the conditional quantiles of a given target variable. These quantiles can be used to compute well-calibrated prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse non-constant functions that map all training samples to zero. These certificates map out-of-distribution …}
      \field{issn}{1049-5258}
      \field{journaltitle}{Advances in neural information processing systems}
      \field{title}{Single-model uncertainties for deep learning}
      \field{year}{2019}
      \verb{file}
      \verb All Papers/T/Tagasovska and Lopez-Paz 2019 - Single-model uncertainties for deep learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2019/hash/73c03186765e199c116224b68adc5fa0-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2019/hash/73c03186765e199c116224b68adc5fa0-Abstract.html
      \endverb
      \keyw{Epistemic;Quantiles}
    \endentry
    \entry{Thiagarajan2020-gm}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=91392c8a5ceb7911befd6fa9f787b0df}{%
           family={Thiagarajan},
           familyi={T\bibinitperiod},
           given={Jayaraman\bibnamedelima J},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c100e49cc47d561582dcd8f7014b98c1}{%
           family={Venkatesh},
           familyi={V\bibinitperiod},
           given={Bindya},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6e8d232cdb54e0b15e9f7373f53405b9}{%
           family={Anirudh},
           familyi={A\bibinitperiod},
           given={Rushil},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5a8992c8cb0857ce8fcc9f6f2068affe}{%
           family={Bremer},
           familyi={B\bibinitperiod},
           given={Peer-Timo},
           giveni={P\bibinithyphendelim T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2e8ee2898dec4dbfffdbb24ca021cdcd}{%
           family={Gaffney},
           familyi={G\bibinitperiod},
           given={Jim},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1cc5e9a5488e931bb31acb549b803dec}{%
           family={Anderson},
           familyi={A\bibinitperiod},
           given={Gemma},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3cb217eab9aa941b09300455b04ee07b}{%
           family={Spears},
           familyi={S\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{46ece27cc27a484ae740691b063db7b1}
      \strng{fullhash}{7353449010ba463373d77bf4ddd7ba87}
      \strng{bibnamehash}{46ece27cc27a484ae740691b063db7b1}
      \strng{authorbibnamehash}{46ece27cc27a484ae740691b063db7b1}
      \strng{authornamehash}{46ece27cc27a484ae740691b063db7b1}
      \strng{authorfullhash}{7353449010ba463373d77bf4ddd7ba87}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.}
      \field{issn}{2041-1723}
      \field{journaltitle}{Nature communications}
      \field{month}{6~11}
      \field{number}{1}
      \field{title}{Designing accurate emulators for scientific processes using calibration-driven deep models}
      \field{volume}{11}
      \field{year}{2020}
      \field{pages}{5622}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41467-020-19448-8
      \endverb
      \verb{file}
      \verb All Papers/T/Thiagarajan et al. 2020 - Designing accurate emulators for scientific processes using calibration-driven deep models.pdf
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1038/s41467-020-19448-8
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1038/s41467-020-19448-8
      \endverb
    \endentry
    \entry{Utpala2020-nw}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=ef7fde832691c9f30ca8e74474ca1f08}{%
           family={Utpala},
           familyi={U\bibinitperiod},
           given={Saiteja},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cc4b060938757c9ab08f254e41270e91}{%
           family={Rai},
           familyi={R\bibinitperiod},
           given={Piyush},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \strng{fullhash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \strng{bibnamehash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \strng{authorbibnamehash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \strng{authornamehash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \strng{authorfullhash}{033b1a1fbac58af9dd78c37ee4d2728a}
      \field{sortinit}{U}
      \field{sortinithash}{6901a00e45705986ee5e7ca9fd39adca}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent works have shown that most deep learning models are often poorly calibrated, i.e., they may produce overconfident predictions that are wrong. It is therefore desirable to have models that produce predictive uncertainty estimates that are reliable. Several approaches have been proposed recently to calibrate classification models. However, there is relatively little work on calibrating regression models. We present a method for calibrating regression models based on a novel quantile regularizer defined as the cumulative KL divergence between two CDFs. Unlike most of the existing approaches for calibrating regression models, which are based on post-hoc processing of the model's output and require an additional dataset, our method is trainable in an end-to-end fashion without requiring an additional dataset. The proposed regularizer can be used with any training objective for regression. We also show that post-hoc calibration methods like Isotonic Calibration sometimes compound miscalibration whereas our method provides consistently better calibrations. We provide empirical results demonstrating that the proposed quantile regularizer significantly improves calibration for regression models trained using approaches, such as Dropout VI and Deep Ensembles.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{2}
      \field{title}{Quantile Regularization: Towards Implicit Calibration of Regression Models}
      \field{year}{2020}
      \verb{eprint}
      \verb 2002.12860
      \endverb
      \verb{file}
      \verb All Papers/U/Utpala and Rai 2020 - Quantile Regularization - Towards Implicit Calibration of Regression Models.pdf;All Papers/U/Utpala and Rai 2020 - quantile_regularization_toward.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2002.12860
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2002.12860
      \endverb
      \keyw{Calibration;Quantiles}
    \endentry
    \entry{Vasicek1976-fa}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=0aa13a6f2036d537b33195b22d087aab}{%
           family={Vasicek},
           familyi={V\bibinitperiod},
           given={Oldrich},
           giveni={O\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {[Royal Statistical Society, Wiley]}%
      }
      \strng{namehash}{0aa13a6f2036d537b33195b22d087aab}
      \strng{fullhash}{0aa13a6f2036d537b33195b22d087aab}
      \strng{bibnamehash}{0aa13a6f2036d537b33195b22d087aab}
      \strng{authorbibnamehash}{0aa13a6f2036d537b33195b22d087aab}
      \strng{authornamehash}{0aa13a6f2036d537b33195b22d087aab}
      \strng{authorfullhash}{0aa13a6f2036d537b33195b22d087aab}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{[A test of the composite hypothesis of normality is introduced. The test is based on the property of the normal distribution that its entropy exceeds that of any other distribution with a density that has the same variance. The test statistic is based on a class of estimators of entropy constructed here. The test is shown to be a consistent test of the null hypothesis for all alternatives without a singular continuous part. The power of the test is estimated against several alternatives. It is observed that the test compares favourably with other tests for normality.]}
      \field{issn}{1369-7412, 0035-9246}
      \field{journaltitle}{Journal of the Royal Statistical Society. Series B, Statistical methodology}
      \field{number}{1}
      \field{title}{A Test for Normality Based on Sample Entropy}
      \field{volume}{38}
      \field{year}{1976}
      \field{pages}{54\bibrangedash 59}
      \range{pages}{6}
      \verb{file}
      \verb All Papers/V/Vasicek 1976 - A Test for Normality Based on Sample Entropy.pdf
      \endverb
      \verb{urlraw}
      \verb http://www.jstor.org/stable/2984828
      \endverb
      \verb{url}
      \verb http://www.jstor.org/stable/2984828
      \endverb
    \endentry
    \entry{Vovk2020-pg}{inproceedings}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=bef6907ff5daebb69843cf35c8af1a83}{%
           family={Vovk},
           familyi={V\bibinitperiod},
           given={Vladimir},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a6a3ca48fb425092e31b66d8d38e590c}{%
           family={Petej},
           familyi={P\bibinitperiod},
           given={Ivan},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=61e84aa5a6863d7d9a5aca18359d2eca}{%
           family={Toccaceli},
           familyi={T\bibinitperiod},
           given={Paolo},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ca87923b059d26a133029c97697f174d}{%
           family={Gammerman},
           familyi={G\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e3c4e42a167a5f3b2fc50b06889ac84a}{%
           family={Ahlberg},
           familyi={A\bibinitperiod},
           given={Ernst},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=abbea684a97c045f9039fe46a1d109e2}{%
           family={Carlsson},
           familyi={C\bibinitperiod},
           given={Lars},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{5}{}{%
        {{hash=ca87923b059d26a133029c97697f174d}{%
           family={Gammerman},
           familyi={G\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=bef6907ff5daebb69843cf35c8af1a83}{%
           family={Vovk},
           familyi={V\bibinitperiod},
           given={Vladimir},
           giveni={V\bibinitperiod}}}%
        {{hash=c54ab8c161f7185b263ec9443ce75437}{%
           family={Luo},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=efe413550c582b5e00b70b1a8b9bf1b5}{%
           family={Smirnov},
           familyi={S\bibinitperiod},
           given={Evgueni},
           giveni={E\bibinitperiod}}}%
        {{hash=bae81377eed53618c4de75b94c99e7a1}{%
           family={Cherubin},
           familyi={C\bibinitperiod},
           given={Giovanni},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{62b5b7ea6ed782e2231121575c724591}
      \strng{fullhash}{4e4b2d9b9cbca3961a89103367af180f}
      \strng{bibnamehash}{62b5b7ea6ed782e2231121575c724591}
      \strng{authorbibnamehash}{62b5b7ea6ed782e2231121575c724591}
      \strng{authornamehash}{62b5b7ea6ed782e2231121575c724591}
      \strng{authorfullhash}{4e4b2d9b9cbca3961a89103367af180f}
      \strng{editorbibnamehash}{983446079483b8b663392ce58a680829}
      \strng{editornamehash}{983446079483b8b663392ce58a680829}
      \strng{editorfullhash}{6666d10214291ddc41b447030946042c}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most existing examples of full conformal predictive systems, split conformal predictive systems, and cross-conformal predictive systems impose severe restrictions on the adaptation of predictive distributions to the test object at hand. In this paper we develop split conformal predictive systems that are fully adaptive. Our method consists in calibrating existing predictive systems; the input predictive system is not supposed to satisfy any properties of validity, whereas the output predictive system is guaranteed to be calibrated in probability.}
      \field{booktitle}{Proceedings of the Ninth Symposium on Conformal and Probabilistic Prediction and Applications}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Conformal calibrators}
      \field{volume}{128}
      \field{year}{2020}
      \field{pages}{84\bibrangedash 99}
      \range{pages}{16}
      \verb{file}
      \verb All Papers/V/Vovk et al. 2020 - Conformal calibrators.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v128/vovk20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v128/vovk20a.html
      \endverb
    \endentry
    \entry{Wilcoxon1945-cp}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=405b308b1e8e33ef3275bf6cb9c9b281}{%
           family={Wilcoxon},
           familyi={W\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {[International Biometric Society, Wiley]}%
      }
      \strng{namehash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \strng{fullhash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \strng{bibnamehash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \strng{authorbibnamehash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \strng{authornamehash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \strng{authorfullhash}{405b308b1e8e33ef3275bf6cb9c9b281}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0099-4987}
      \field{journaltitle}{Biometrics Bulletin}
      \field{number}{6}
      \field{title}{Individual Comparisons by Ranking Methods}
      \field{volume}{1}
      \field{year}{1945}
      \field{pages}{80\bibrangedash 83}
      \range{pages}{4}
      \verb{doi}
      \verb 10.2307/3001968
      \endverb
      \verb{file}
      \verb All Papers/W/Wilcoxon 1945 - Individual Comparisons by Ranking Methods.pdf
      \endverb
      \verb{urlraw}
      \verb http://www.jstor.org/stable/3001968
      \endverb
      \verb{url}
      \verb http://www.jstor.org/stable/3001968
      \endverb
    \endentry
    \entry{Yoon2023-ds}{inproceedings}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=88c9a2b1f19a1f1eb9670a48f016b1e5}{%
           family={Yoon},
           familyi={Y\bibinitperiod},
           given={Hee\bibnamedelima Suk},
           giveni={H\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=db1ebed6722467c7563f138d533a6847}{%
           family={Tee},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelimb Tian\bibnamedelima Jin},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=77a1b26345d54327bed084ec2f9011ef}{%
           family={Yoon},
           familyi={Y\bibinitperiod},
           given={Eunseop},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=991dc3366d9be318275e0b474c210363}{%
           family={Yoon},
           familyi={Y\bibinitperiod},
           given={Sunjae},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c413834723d0db50fda1766238e583ad}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Gwangsu},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=400e4d5630f4e518634cc6e80b62b62f}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yingzhen},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4a06639929d0d2f21e2fa121caa8dd26}{%
           family={Yoo},
           familyi={Y\bibinitperiod},
           given={Chang\bibnamedelima D},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{29538c039adf8844104a00efc21f6388}
      \strng{fullhash}{e345eabc2bc0a0f33c0287dba34be2a9}
      \strng{bibnamehash}{29538c039adf8844104a00efc21f6388}
      \strng{authorbibnamehash}{29538c039adf8844104a00efc21f6388}
      \strng{authornamehash}{29538c039adf8844104a00efc21f6388}
      \strng{authorfullhash}{e345eabc2bc0a0f33c0287dba34be2a9}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The Eleventh International Conference on Learning Representations}
      \field{title}{{{ESD}}: Expected Squared Difference as a {Tuning-Free} Trainable Calibration Measure}
      \field{year}{2023}
      \verb{file}
      \verb All Papers/Y/Yoon et al. 2023 - ESD - Expected Squared Difference as a Tuning-Free Trainable Calibration Measure.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=bHW9njOSON
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=bHW9njOSON
      \endverb
    \endentry
    \entry{Zhao2020-ze}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=6a92c8ce44e2c757c2d165938a3eb7e4}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Shengjia},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5bd4a91e09b499d15e82bc929acf1e34}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Tengyu},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=160929a7e6ceecdf3f7c9a13abd5ea35}{%
           family={Ermon},
           familyi={E\bibinitperiod},
           given={Stefano},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=2dc8d8eba3b07ccb6bda6a754b49b05a}{%
           family={Iii},
           familyi={I\bibinitperiod},
           given={Hal\bibnamedelima Daumé},
           giveni={H\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=d77c73a14b0f048484c1d34aba3071d4}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Aarti},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{89e7f3fa5b51702868fb9a043e232d4b}
      \strng{fullhash}{46c9e58b4fcb59ba954820250f5716b4}
      \strng{bibnamehash}{46c9e58b4fcb59ba954820250f5716b4}
      \strng{authorbibnamehash}{46c9e58b4fcb59ba954820250f5716b4}
      \strng{authornamehash}{89e7f3fa5b51702868fb9a043e232d4b}
      \strng{authorfullhash}{46c9e58b4fcb59ba954820250f5716b4}
      \strng{editorbibnamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editornamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editorfullhash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.}
      \field{booktitle}{Proceedings of the 37th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Individual Calibration with Randomized Forecasting}
      \field{volume}{119}
      \field{year}{2020}
      \field{pages}{11387\bibrangedash 11397}
      \range{pages}{11}
      \verb{file}
      \verb All Papers/Z/Zhao et al. 2020 - 2006.10288.pdf;All Papers/Z/Zhao et al. 2020 - Individual Calibration with Randomized Forecasting.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v119/zhao20e.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v119/zhao20e.html
      \endverb
      \keyw{Calibration}
    \endentry
    \entry{Zhou2021-hx}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e7214ecdeebda3aed731191b6fc8cc76}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Tianhui},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=938d594260fb0313950d9490a63d9ad8}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yitong},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f19c4fff5adda4e55f61a3f77d7386a4}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f1eb6186939a333f5ba82f4c19026985}{%
           family={Carlson},
           familyi={C\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{5876bbb5b7ebf7008fabfd4946abe7ef}
      \strng{fullhash}{5f17983a2fc74044f075abcb3f307b91}
      \strng{bibnamehash}{5876bbb5b7ebf7008fabfd4946abe7ef}
      \strng{authorbibnamehash}{5876bbb5b7ebf7008fabfd4946abe7ef}
      \strng{authornamehash}{5876bbb5b7ebf7008fabfd4946abe7ef}
      \strng{authorfullhash}{5f17983a2fc74044f075abcb3f307b91}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Effective decision making requires understanding the uncertainty inherent in a prediction. In regression, this uncertainty can be estimated by a variety of methods; however, many of these methods are laborious to tune, generate overconfident uncertainty intervals, or lack sharpness (give imprecise intervals). We address these challenges by proposing a novel method to capture predictive distributions in regression by defining two neural networks with two distinct loss functions. Specifically, one network approximates the cumulative distribution function, and the second network approximates its inverse. We refer to this method as Collaborating Networks (CN). Theoretical analysis demonstrates that a fixed point of the optimization is at the idealized solution, and that the method is asymptotically consistent to the ground truth distribution. Empirically, learning is straightforward and robust. We benchmark CN against several common approaches on two synthetic and six real-world datasets, including forecasting A1c values in diabetic patients from electronic health records, where uncertainty is critical. In the synthetic data, the proposed approach essentially matches ground truth. In the real-world datasets, CN improves results on many performance metrics, including log-likelihood estimates, mean absolute errors, coverage estimates, and prediction interval widths.}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of machine learning research: JMLR}
      \field{month}{1}
      \field{title}{Estimating Uncertainty Intervals from Collaborating Networks}
      \field{volume}{22}
      \field{year}{2021}
      \verb{file}
      \verb All Papers/Z/Zhou et al. 2021 - Estimating Uncertainty Intervals from Collaborating Networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.ncbi.nlm.nih.gov/pubmed/35754923
      \endverb
      \verb{url}
      \verb https://www.ncbi.nlm.nih.gov/pubmed/35754923
      \endverb
      \keyw{calibration; conditional distributions; consistency; neural networks; uncertainty estimation}
    \endentry
  \enddatalist
\endrefsection
\endinput

