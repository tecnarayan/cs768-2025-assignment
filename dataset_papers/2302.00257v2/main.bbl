\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani et~al.(2020)Advani, Saxe, and Sompolinsky]{advani2020high}
Madhu~S Advani, Andrew~M Saxe, and Haim Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{Neural Networks}, 132:\penalty0 428--446, 2020.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Azulay et~al.(2021)Azulay, Moroshko, Nacson, Woodworth, Srebro,
  Globerson, and Soudry]{azulay2021implicit}
Shahar Azulay, Edward Moroshko, Mor~Shpigel Nacson, Blake~E Woodworth, Nathan
  Srebro, Amir Globerson, and Daniel Soudry.
\newblock On the implicit bias of initialization shape: Beyond infinitesimal
  mirror descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  468--477. PMLR, 2021.

\bibitem[Baraniuk et~al.(2008)Baraniuk, Davenport, DeVore, and
  Wakin]{baraniuk2008simple}
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin.
\newblock A simple proof of the restricted isometry property for random
  matrices.
\newblock \emph{Constructive Approximation}, 28\penalty0 (3):\penalty0
  253--263, 2008.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Bickel et~al.(2009)Bickel, Ritov, and
  Tsybakov]{bickel2009simultaneous}
Peter~J Bickel, Ya’acov Ritov, and Alexandre~B Tsybakov.
\newblock Simultaneous analysis of lasso and dantzig selector.
\newblock \emph{The Annals of statistics}, 37\penalty0 (4):\penalty0
  1705--1732, 2009.

\bibitem[Candes and Tao(2005)]{candes2005decoding}
Emmanuel~J Candes and Terence Tao.
\newblock Decoding by linear programming.
\newblock \emph{IEEE transactions on information theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.

\bibitem[Chatterji and Long(2022)]{chatterji2022foolish}
Niladri~S Chatterji and Philip~M Long.
\newblock Foolish crowds support benign overfitting.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (125):\penalty0 1--12, 2022.

\bibitem[Chinot et~al.(2022)Chinot, Löffler, and van~de
  Geer]{chinot_robustness_2022}
Geoffrey Chinot, Matthias Löffler, and Sara van~de Geer.
\newblock On the robustness of minimum norm interpolators and regularized
  empirical risk minimizers.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (4), August 2022.
\newblock ISSN 0090-5364.
\newblock \doi{10.1214/22-AOS2190}.

\bibitem[Dar et~al.(2021)Dar, Muthukumar, and Baraniuk]{dar2021farewell}
Yehuda Dar, Vidya Muthukumar, and Richard~G Baraniuk.
\newblock A farewell to the bias-variance tradeoff? an overview of the theory
  of overparameterized machine learning.
\newblock \emph{arXiv preprint arXiv:2109.02355}, 2021.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Frei et~al.(2023)Frei, Vardi, Bartlett, Srebro, and
  Hu]{frei2023implicit}
Spencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro, and Wei Hu.
\newblock Implicit bias in leaky relu networks trained on high-dimensional
  data.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=JpbLyEI5EwW}.

\bibitem[Ge et~al.(2021)Ge, Ren, Wang, and Zhou]{ge2021understanding}
Rong Ge, Yunwei Ren, Xiang Wang, and Mo~Zhou.
\newblock Understanding deflation process in over-parametrized tensor
  decomposition.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1299--1311, 2021.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Gissin et~al.(2020)Gissin, Shalev-Shwartz, and Daniely]{gissin2020the}
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1lj0nNFwB}.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2022surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (2):\penalty0 949--986,
  2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ju et~al.(2020)Ju, Lin, and Liu]{ju2020overfitting}
Peizhong Ju, Xiaojun Lin, and Jia Liu.
\newblock Overfitting can be harmless for basis pursuit, but only to a degree.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7956--7967, 2020.

\bibitem[Koehler et~al.(2021)Koehler, Zhou, Sutherland, and
  Srebro]{koehler2021uniform}
Frederic Koehler, Lijia Zhou, Danica~J Sutherland, and Nathan Srebro.
\newblock Uniform convergence of interpolators: Gaussian width, norm bounds and
  benign overfitting.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20657--20668, 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Nguyen, Hegde, and
  Wong]{li2021implicit}
Jiangyuan Li, Thanh Nguyen, Chinmay Hegde, and Ka~Wai Wong.
\newblock Implicit sparse regularization: The impact of depth and early
  stopping.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28298--28309, 2021{\natexlab{a}}.

\bibitem[Li and Wei(2021)]{li2021minimum}
Yue Li and Yuting Wei.
\newblock Minimum {$\ell_1$}-norm interpolators: Precise asymptotics and
  multiple descent.
\newblock \emph{arXiv preprint arXiv:2110.09502}, 2021.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Luo, and Lyu]{li2021towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=AHOs7Sm5H7R}.

\bibitem[Li et~al.(2022)Li, Wang, Lee, and Arora]{li2022implicit}
Zhiyuan Li, Tianhao Wang, Jason~D Lee, and Sanjeev Arora.
\newblock Implicit bias of gradient descent on reparametrized models: On
  equivalence to mirror descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 34626--34640, 2022.

\bibitem[Lounici et~al.(2011)Lounici, Pontil, Van De~Geer, and
  Tsybakov]{lounici2011oracle}
Karim Lounici, Massimiliano Pontil, Sara Van De~Geer, and Alexandre~B Tsybakov.
\newblock Oracle inequalities and optimal inference under group sparsity.
\newblock \emph{The annals of statistics}, 39\penalty0 (4):\penalty0
  2164--2204, 2011.

\bibitem[Mitra(2019)]{mitra2019understanding}
Partha~P Mitra.
\newblock Understanding overfitting peaks in generalization error: Analytical
  risk curves for {$\ell_2$} and {$\ell_1$} penalized interpolation.
\newblock \emph{arXiv preprint arXiv:1906.03667}, 2019.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{muthukumar2020harmless}
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 67--83, 2020.

\bibitem[Negrea et~al.(2020)Negrea, Dziugaite, and Roy]{negrea2020defense}
Jeffrey Negrea, Gintare~Karolina Dziugaite, and Daniel Roy.
\newblock In defense of uniform convergence: Generalization via derandomization
  with an application to interpolating predictors.
\newblock In \emph{International Conference on Machine Learning}, pages
  7263--7272. PMLR, 2020.

\bibitem[Raskutti et~al.(2011)Raskutti, Wainwright, and
  Yu]{raskutti2011minimax}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Minimax rates of estimation for high-dimensional linear regression
  over {$\ell_q$}-balls.
\newblock \emph{IEEE transactions on information theory}, 57\penalty0
  (10):\penalty0 6976--6994, 2011.

\bibitem[Razin et~al.(2021)Razin, Maman, and Cohen]{razin2021implicit}
Noam Razin, Asaf Maman, and Nadav Cohen.
\newblock Implicit regularization in tensor factorization.
\newblock In \emph{International Conference on Machine Learning}, pages
  8913--8924. PMLR, 2021.

\bibitem[Razin et~al.(2022)Razin, Maman, and Cohen]{razin2022implicit}
Noam Razin, Asaf Maman, and Nadav Cohen.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  18422--18462. PMLR, 2022.

\bibitem[Timor et~al.(2023)Timor, Vardi, and Shamir]{timor2023implicit}
Nadav Timor, Gal Vardi, and Ohad Shamir.
\newblock Implicit regularization towards rank minimization in relu networks.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 1429--1459. PMLR, 2023.

\bibitem[Tsigler and Bartlett(2020)]{tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Vardi(2022)]{vardi2022implicit}
Gal Vardi.
\newblock On the implicit bias in deep-learning algorithms.
\newblock \emph{arXiv preprint arXiv:2208.12591}, 2022.

\bibitem[Vaskevicius et~al.(2019)Vaskevicius, Kanade, and
  Rebeschini]{vaskevicius2019implicit}
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini.
\newblock Implicit regularization for optimal sparse recovery.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2022)Wang, Donhauser, and Yang]{wang2022tight}
Guillaume Wang, Konstantin Donhauser, and Fanny Yang.
\newblock Tight bounds for minimum {$\ell_1$}-norm interpolation of noisy data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 10572--10602. PMLR, 2022.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, pages 3635--3673. PMLR,
  2020.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{yun2021a}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ZsZM-4iMQkH}.

\bibitem[Zhou et~al.(2020)Zhou, Sutherland, and Srebro]{zhou2020uniform}
Lijia Zhou, Danica~J Sutherland, and Nati Srebro.
\newblock On uniform convergence and low-norm interpolation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6867--6877, 2020.

\end{thebibliography}
