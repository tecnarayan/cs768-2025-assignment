\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bapna and Firat(2019)]{bapna2019simple}
Ankur Bapna and Orhan Firat.
\newblock Simple, scalable adaptation for neural machine translation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 1538--1548, 2019.

\bibitem[Blackwood et~al.(2018)Blackwood, Ballesteros, and
  Ward]{Blackwood2018MultilingualNM}
Graeme~W. Blackwood, Miguel Ballesteros, and Todd Ward.
\newblock Multilingual neural machine translation with task-specific attention.
\newblock In \emph{Proceedings of the 27th International Conference on
  Computational Linguistics, {COLING} 2018, Santa Fe, New Mexico, USA, August
  20-26, 2018}, pages 3112--3122. Association for Computational Linguistics,
  2018.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and
  Rabinovich]{Chen2018GradNormGN}
Zhao Chen, Vijay Badrinarayanan, Chen{-}Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, \emph{Proceedings of
  the 35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 793--802. {PMLR},
  2018.

\bibitem[Dabre et~al.(2020)Dabre, Chu, and Kunchukuttan]{Dabre2020ASO}
Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.
\newblock A survey of multilingual neural machine translation.
\newblock \emph{ACM Computing Surveys (CSUR)}, 53:\penalty0 1 -- 38, 2020.
\newblock URL \url{https://dl.acm.org/doi/pdf/10.1145/3406095}.

\bibitem[Dakwale and Monz(2017)]{Dakwale2017FineTuningFN}
Praveen Dakwale and Christof Monz.
\newblock Finetuning for neural machine translation with limited degradation
  across in-and out-of-domain data.
\newblock \emph{Proceedings of the XVI Machine Translation Summit}, 117, 2017.

\bibitem[Gangi et~al.(2019)Gangi, Negri, and
  Turchi]{DBLP:conf/interspeech/GangiNT19}
Mattia Antonino~Di Gangi, Matteo Negri, and Marco Turchi.
\newblock Adapting transformer to end-to-end spoken language translation.
\newblock In Gernot Kubin and Zdravko Kacic, editors, \emph{Interspeech 2019,
  20th Annual Conference of the International Speech Communication Association,
  Graz, Austria, 15-19 September 2019}, pages 1133--1137. {ISCA}, 2019.

\bibitem[Geng et~al.(2020)Geng, Wang, Wang, Qin, Liu, and Tu]{Geng2020HowDS}
Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, and Zhaopeng Tu.
\newblock How does selective mechanism improve self-attention networks?
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault,
  editors, \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pages
  2986--2995. Association for Computational Linguistics, 2020.

\bibitem[Gu et~al.(2018)Gu, Hassan, Devlin, and Li]{gu2018universal}
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor~OK Li.
\newblock Universal neural machine translation for extremely low resource
  languages.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 344--354, 2018.

\bibitem[Guo et~al.(2020)Guo, Lee, and Ulbricht]{guo2020learning}
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht.
\newblock Learning to branch for multi-task learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3854--3863. PMLR, 2020.

\bibitem[Heigold et~al.(2013)Heigold, Vanhoucke, Senior, Nguyen, Ranzato,
  Devin, and Dean]{Heigold2013MultilingualAM}
Georg Heigold, Vincent Vanhoucke, Andrew~W. Senior, Patrick Nguyen,
  Marc'Aurelio Ranzato, Matthieu Devin, and Jeffrey Dean.
\newblock Multilingual acoustic models using distributed deep neural networks.
\newblock \emph{2013 IEEE International Conference on Acoustics, Speech and
  Signal Processing}, pages 8619--8623, 2013.

\bibitem[Inaguma et~al.(2019{\natexlab{a}})Inaguma, Duh, Kawahara, and
  Watanabe]{Inaguma2019MultilingualES}
H.~Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe.
\newblock Multilingual end-to-end speech translation.
\newblock \emph{2019 IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pages 570--577, 2019{\natexlab{a}}.

\bibitem[Inaguma et~al.(2019{\natexlab{b}})Inaguma, Duh, Kawahara, and
  Watanabe]{DBLP:conf/asru/InagumaDKW19}
Hirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe.
\newblock Multilingual end-to-end speech translation.
\newblock In \emph{{IEEE} Automatic Speech Recognition and Understanding
  Workshop, {ASRU} 2019, Singapore, December 14-18, 2019}, pages 570--577.
  {IEEE}, 2019{\natexlab{b}}.

\bibitem[Iranzo-S{\'a}nchez et~al.(2020)Iranzo-S{\'a}nchez,
  Silvestre-Cerd{\`a}, Jorge, Rosell{\'o}, Gim{\'e}nez, Sanchis, Civera, and
  Juan]{iranzo2020europarl}
Javier Iranzo-S{\'a}nchez, Joan~Albert Silvestre-Cerd{\`a}, Javier Jorge,
  Nahuel Rosell{\'o}, Adri{\`a} Gim{\'e}nez, Albert Sanchis, Jorge Civera, and
  Alfons Juan.
\newblock Europarl-st: A multilingual corpus for speech translation of
  parliamentary debates.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 8229--8233. IEEE, 2020.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Johnson et~al.(2017)Johnson, Schuster, Le, Krikun, Wu, Chen, Thorat,
  Vi{\'e}gas, Wattenberg, Corrado, Hughes, and Dean]{Johnson2017GooglesMN}
M.~Johnson, M.~Schuster, Quoc~V. Le, M.~Krikun, Y.~Wu, Z.~Chen, Nikhil Thorat,
  Fernanda~B. Vi{\'e}gas, M.~Wattenberg, G.~Corrado, Macduff Hughes, and
  J.~Dean.
\newblock Googleâ€™s multilingual neural machine translation system: Enabling
  zero-shot translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  5:\penalty0 339--351, 2017.

\bibitem[Khayrallah et~al.(2018)Khayrallah, Thompson, Duh, and
  Koehn]{Khayrallah2018RegularizedTO}
Huda Khayrallah, Brian Thompson, Kevin Duh, and Philipp Koehn.
\newblock Regularized training objective for continued training for domain
  adaptation in neural machine translation.
\newblock In Alexandra Birch, Andrew~M. Finch, Minh{-}Thang Luong, Graham
  Neubig, and Yusuke Oda, editors, \emph{Proceedings of the 2nd Workshop on
  Neural Machine Translation and Generation, NMT@ACL 2018, Melbourne,
  Australia, July 20, 2018}, pages 36--44. Association for Computational
  Linguistics, 2018.

\bibitem[Klakow and Peters(2002)]{klakow2002testing}
Dietrich Klakow and Jochen Peters.
\newblock Testing the correlation of word error rate and perplexity.
\newblock \emph{Speech Communication}, 38\penalty0 (1-2):\penalty0 19--28,
  2002.

\bibitem[Kobus et~al.(2017)Kobus, Crego, and Senellart]{Kobus2017DomainCF}
Catherine Kobus, Josep~Maria Crego, and Jean Senellart.
\newblock Domain control for neural machine translation.
\newblock In \emph{Proceedings of the International Conference Recent Advances
  in Natural Language Processing, {RANLP} 2017, Varna, Bulgaria, September 2 -
  8, 2017}, pages 372--378. {INCOMA} Ltd., 2017.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{Lepikhin2020GShardSG}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock \emph{CoRR}, abs/2006.16668, 2020.

\bibitem[Li et~al.(2020)Li, Wang, Tang, Tran, Tang, Pino, Baevski, Conneau, and
  Auli]{Li2020MultilingualST}
X.~Li, Changhan Wang, Y.~Tang, C.~Tran, Yuqing Tang, J.~Pino, Alexei Baevski,
  Alexis Conneau, and Michael Auli.
\newblock Multilingual speech translation with efficient finetuning of
  pretrained models.
\newblock \emph{arXiv: Computation and Language}, 2020.

\bibitem[Liu et~al.(2021)Liu, Cao, and Zhao]{Liu2021GumbelAttentionFM}
Pengbo Liu, Hailong Cao, and Tiejun Zhao.
\newblock Gumbel-attention for multi-modal machine translation.
\newblock \emph{CoRR}, abs/2103.08862, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.08862}.

\bibitem[Liu et~al.(2020)Liu, Gu, Goyal, Li, Edunov, Ghazvininejad, Lewis, and
  Zettlemoyer]{liu2020multilingual}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 726--742, 2020.

\bibitem[Moritz et~al.(2020)Moritz, Hori, and Le]{moritz2020streaming}
Niko Moritz, Takaaki Hori, and Jonathan Le.
\newblock Streaming automatic speech recognition with the transformer model.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 6074--6078. IEEE, 2020.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: {A} fast, extensible toolkit for sequence modeling.
\newblock In Waleed Ammar, Annie Louis, and Nasrin Mostafazadeh, editors,
  \emph{Proceedings of the 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations},
  pages 48--53. Association for Computational Linguistics, 2019.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Pham et~al.(2021)Pham, Crego, and Yvon]{pham2021revisiting}
MinhQuang Pham, Josep~Maria Crego, and Fran{\c{c}}ois Yvon.
\newblock Revisiting multi-domain machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 17--35, 2021.

\bibitem[Pratap et~al.(2020)Pratap, Sriram, Tomasello, Hannun, Liptchinsky,
  Synnaeve, and Collobert]{Pratap2020MassivelyMA}
Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni~Y. Hannun, Vitaliy
  Liptchinsky, Gabriel Synnaeve, and Ronan Collobert.
\newblock Massively multilingual asr: 50 languages, 1 model, 1 billion
  parameters.
\newblock In \emph{INTERSPEECH}, 2020.

\bibitem[Sachan and Neubig(2018)]{Sachan2018ParameterSM}
Devendra~Singh Sachan and Graham Neubig.
\newblock Parameter sharing methods for multilingual self-attentional
  translation models.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1,
  2018}, pages 261--271. Association for Computational Linguistics, 2018.

\bibitem[Salesky et~al.(2021)Salesky, Wiesner, Bremerman, Cattoni, Negri,
  Turchi, Oard, and Post]{salesky2021mtedx}
Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo
  Negri, Marco Turchi, Douglas~W. Oard, and Matt Post.
\newblock The multilingual tedx corpus for speech recognition and translation.
\newblock \emph{CoRR}, abs/2102.01757, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.01757}.

\bibitem[Saunders(2021)]{Saunders2021DomainAA}
Danielle Saunders.
\newblock Domain adaptation and multi-domain adaptation for neural machine
  translation: {A} survey.
\newblock \emph{CoRR}, abs/2104.06951, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.06951}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{Shazeer2017OutrageouslyLN}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le,
  Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[Thompson et~al.(2019)Thompson, Gwinnup, Khayrallah, Duh, and
  Koehn]{Thompson2019OvercomingCF}
Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn.
\newblock Overcoming catastrophic forgetting during domain adaptation of neural
  machine translation.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
  Short Papers)}, pages 2062--2068. Association for Computational Linguistics,
  2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{DBLP:conf/nips/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
  {USA}}, pages 5998--6008, 2017.

\bibitem[Vila et~al.(2018)Vila, Escolano, Fonollosa, and
  Costa{-}juss{\`{a}}]{DBLP:conf/iberspeech/VilaEFC18}
Laura~Cross Vila, Carlos Escolano, Jos{\'{e}} A.~R. Fonollosa, and Marta~R.
  Costa{-}juss{\`{a}}.
\newblock End-to-end speech translation with the transformer.
\newblock In Jordi Luque, Antonio Bonafonte, Francesc~Al{\'{\i}}as Pujol, and
  Ant{\'{o}}nio J.~S. Teixeira, editors, \emph{Fourth International Conference,
  IberSPEECH 2018, Barcelona, Spain, 21-23 November 2018, Proceedings}, pages
  60--63. {ISCA}, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Tang, Ma, Wu, Okhonko, and
  Pino]{wang2020fairseq}
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino.
\newblock Fairseq s2t: Fast speech-to-text modeling with fairseq.
\newblock In \emph{Proceedings of the 1st Conference of the Asia-Pacific
  Chapter of the Association for Computational Linguistics and the 10th
  International Joint Conference on Natural Language Processing: System
  Demonstrations}, pages 33--39, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Tang, Ma, Wu, Okhonko, and
  Pino]{wang2020fairseqs2t}
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino.
\newblock fairseq s2t: Fast speech-to-text modeling with fairseq.
\newblock In \emph{Proceedings of the 2020 Conference of the Asian Chapter of
  the Association for Computational Linguistics (AACL): System Demonstrations},
  2020{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{c}})Wang, Wu, and Pino]{Wang2020CoVoST2A}
Changhan Wang, Anne Wu, and Juan Pino.
\newblock Covost 2: {A} massively multilingual speech-to-text translation
  corpus.
\newblock \emph{CoRR}, abs/2007.10310, 2020{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2007.10310}.

\bibitem[Wang et~al.(2020{\natexlab{d}})Wang, Wang, Shi, Li, and
  Tu]{wang2020go}
Yong Wang, Longyue Wang, Shuming Shi, Victor~OK Li, and Zhaopeng Tu.
\newblock Go from the general to the particular: Multi-domain translation with
  domain transformation networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 9233--9241, 2020{\natexlab{d}}.

\bibitem[Wang et~al.(2020{\natexlab{e}})Wang, Lipton, and
  Tsvetkov]{wang2020negative}
Zirui Wang, Zachary~C Lipton, and Yulia Tsvetkov.
\newblock On negative interference in multilingual language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4438--4450, 2020{\natexlab{e}}.

\bibitem[Wu et~al.(2019)Wu, Zhang, and R{\'e}]{wu2019understanding}
Sen Wu, Hongyang~R Zhang, and Christopher R{\'e}.
\newblock Understanding and improving information transfer in multi-task
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zeng et~al.(2018{\natexlab{a}})Zeng, Su, Wen, Liu, Xie, Yin, and
  Zhao]{Zeng2018MultiDomainNM}
Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing Yin, and
  Jianqiang Zhao.
\newblock Multi-domain neural machine translation with word-level domain
  context discrimination.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing, Brussels, Belgium, October 31 - November 4,
  2018}, pages 447--457. Association for Computational Linguistics,
  2018{\natexlab{a}}.

\bibitem[Zeng et~al.(2018{\natexlab{b}})Zeng, Su, Wen, Liu, Xie, Yin, and
  Zhao]{zeng2018multi}
Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing Yin, and
  Jianqiang Zhao.
\newblock Multi-domain neural machine translation with word-level domain
  context discrimination.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 447--457, 2018{\natexlab{b}}.

\end{thebibliography}
