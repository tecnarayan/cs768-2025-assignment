\begin{thebibliography}{10}

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.
\newblock Palm 2 technical report.
\newblock {\em arXiv preprint arXiv:2305.10403}, 2023.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{romera2023mathematical}
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M~Pawan Kumar, Emilien Dupont, Francisco~JR Ruiz, Jordan~S Ellenberg, Pengming Wang, Omar Fawzi, et~al.
\newblock Mathematical discoveries from program search with large language models.
\newblock {\em Nature}, pages 1--3, 2023.

\bibitem{yang2023large}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen.
\newblock Large language models as optimizers.
\newblock {\em arXiv preprint arXiv:2309.03409}, 2023.

\bibitem{hu2023evaluation}
Mengzhou Hu, Sahar Alkhairy, Ingoo Lee, Rudolf~T Pillich, Robin Bachelder, Trey Ideker, and Dexter Pratt.
\newblock Evaluation of large language models for discovery of gene set function.
\newblock {\em Research Square}, 2023.

\bibitem{trinh2024solving}
Trieu~H Trinh, Yuhuai Wu, Quoc~V Le, He~He, and Thang Luong.
\newblock Solving olympiad geometry without human demonstrations.
\newblock {\em Nature}, 625(7995):476--482, 2024.

\bibitem{hosseini2014learning}
Mohammad~Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.
\newblock Learning to solve arithmetic word problems with verb categorization.
\newblock In {\em Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 523--533, 2014.

\bibitem{roy2015reasoning}
Subhro Roy, Tim Vieira, and Dan Roth.
\newblock Reasoning about quantities in natural language.
\newblock {\em Transactions of the Association for Computational Linguistics}, 3:1--13, 2015.

\bibitem{roy2016solving}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems.
\newblock {\em arXiv preprint arXiv:1608.01413}, 2016.

\bibitem{patel2021nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are nlp models really able to solve simple math word problems?
\newblock {\em arXiv preprint arXiv:2103.07191}, 2021.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset, 2021.

\bibitem{he2023solving}
Joy He-Yueya, Gabriel Poesia, Rose~E Wang, and Noah~D Goodman.
\newblock Solving math word problems by combining language models with symbolic solvers.
\newblock {\em arXiv preprint arXiv:2304.09102}, 2023.

\bibitem{flavell76}
JH~Flavell.
\newblock Metacognitive aspects of problem solving.
\newblock In {\em The Nature of Intelligence}. Routledge, 1976.

\bibitem{Corbett1997}
A.~Corbett, K.~Koedinger, and J.~Anderson.
\newblock Intelligent tutoring systems.
\newblock In M.~Helander~T. Landauer and P.~Prabhu, editors, {\em Handbook of Human Computer Interaction}, pages 849--874. Elsevier Science, Amsterdam, 1997.

\bibitem{Koedinger2012}
K.~Koedinger, A.~Corbett, and C.~Perfetti.
\newblock The knowledge-learning-instruction framework: Bridging the science-practice chasm to enhance robust student learning, 2012.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Qixuan Liu, Bingtian Yang, Xinchi Dong, Huang Huang, and William Wang.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em arXiv}, abs/2201.11903, 2022.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems}, 35:22199--22213, 2022.

\bibitem{ask-llm2024}
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed~H Chi, James Caverlee, Julian McAuley, and Derek~Zhiyuan Cheng.
\newblock How to train data-efficient llms.
\newblock {\em arXiv preprint arXiv:2402.09668}, 2024.

\bibitem{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In {\em International Conference on Machine Learning}, pages 10764--10799. PMLR, 2023.

\bibitem{Cen2006}
H.~Cen, K.~Koedinger, and B.~Junker.
\newblock Learning factors analysis: A general method for cognitive model evaluation and improvement.
\newblock In M.~Ikeda, K.~Ashley, and T.~Chan, editors, {\em Intelligent Tutoring Systems (volume 4053 of Lec. Notes in Comp. Sci.)}, pages {164--175}. 2006.

\bibitem{lindsey2014automatic}
Robert~V Lindsey, Mohammad Khajah, and Michael~C Mozer.
\newblock Automatic discovery of cognitive skills to improve the prediction of student learning.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{Brunskill2011}
Emma Brunskill.
\newblock Estimating prerequisite structure from noisy data.
\newblock In {\em Educational Data Mining}, pages 217--222, 2011.

\bibitem{Liang2018}
Chen Liang, Jianbo Ye, Shuting Wang, Bart Pursel, and C~Lee Giles.
\newblock Investigating active learning for concept prerequisite learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~32, 2018.

\bibitem{Pan2017}
Liangming Pan, Chengjiang Li, Juanzi Li, and Jie Tang.
\newblock Prerequisite relation learning for concepts in moocs.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1447--1456, 2017.

\bibitem{chen2023skillit}
Mayee~F. Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce~Zhang, Frederic Sala, and Christopher Ré.
\newblock Skill-it! a data-driven skills framework for understanding and training language models, 2023.

\bibitem{yu2023skillmix}
Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora.
\newblock Skill-mix: a flexible and expandable family of evaluations for ai models, 2023.

\bibitem{arora2023theory}
Sanjeev Arora and Anirudh Goyal.
\newblock A theory for emergence of complex skills in language models, 2023.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.

\bibitem{jiang2024mixtral}
Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mixtral of experts, 2024.

\bibitem{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models, 2023.

\bibitem{fu2023complexitybased}
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot.
\newblock Complexity-based prompting for multi-step reasoning, 2023.

\bibitem{xu2023latent}
Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Peter Stone, and Yanjun Qi.
\newblock Latent skill discovery for chain-of-thought reasoning, 2023.

\bibitem{miao2021diverse}
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
\newblock A diverse corpus for evaluating and developing english math word problem solvers, 2021.

\bibitem{koncel-kedziorski-etal-2016-mawps}
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
\newblock {MAWPS}: A math word problem repository.
\newblock In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, {\em Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 1152--1157, San Diego, California, June 2016. Association for Computational Linguistics.

\bibitem{zhang2023cumulative}
Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao.
\newblock Cumulative reasoning with large language models, 2023.

\bibitem{zhao-etal-2023-automatic}
James Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Michael Xie.
\newblock Automatic model selection with large language models for reasoning.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 758--783, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{zheng2023progressivehint}
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu~Li.
\newblock Progressive-hint prompting improves reasoning in large language models, 2023.

\end{thebibliography}
