\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barron(1994)]{barron1994approximation}
Barron, A.~R.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock \emph{Machine learning}, 14\penalty0 (1):\penalty0 115--133, 1994.

\bibitem[Beise \& Da~Cruz(2020)Beise and Da~Cruz]{Beise2020Expressiveness}
Beise, H.-P. and Da~Cruz, S.~D.
\newblock Expressiveness of {Neural} {Networks} {Having} {Width} {Equal} or
  {Below} the {Input} {Dimension}.
\newblock \emph{arxiv:2011.04923}, 2020.

\bibitem[Beise et~al.(2021)Beise, Da~Cruz, and
  Schr{\"{o}}der]{Beise2021decision}
Beise, H.-P., Da~Cruz, S.~D., and Schr{\"{o}}der, U.
\newblock On decision regions of narrow deep neural networks.
\newblock \emph{Neural networks}, 140:\penalty0 121--129, 2021.

\bibitem[Brenier \& Gangbo(2003)Brenier and Gangbo]{Brenier2003Approximation}
Brenier, Y. and Gangbo, W.
\newblock \${L}{\textasciicircum}p\$ {Approximation} of maps by
  diffeomorphisms.
\newblock \emph{Calculus of Variations and Partial Differential Equations},
  16\penalty0 (2):\penalty0 147--164, 2003.

\bibitem[Cai(2022)]{cai2022achieve}
Cai, Y.
\newblock Achieve the minimum width of neural networks for universal
  approximation.
\newblock \emph{arXiv preprint arXiv:2209.11395}, 2022.

\bibitem[Caponigro(2011)]{caponigro2011orientation}
Caponigro, M.
\newblock Orientation preserving diffeomorphisms and flows of control-affine
  systems.
\newblock \emph{IFAC Proceedings Volumes}, 44\penalty0 (1):\penalty0
  8016--8021, 2011.

\bibitem[Chong(2020)]{Chong2020closer}
Chong, K. F.~E.
\newblock A closer look at the approximation capabilities of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Daniely(2017)]{daniely2017depth}
Daniely, A.
\newblock Depth separation for neural networks.
\newblock In \emph{Conference on Learning Theory}, 2017.

\bibitem[Duan et~al.(2022)Duan, Li, Ji, and Cai]{duan2022vanilla}
Duan, Y., Li, L., Ji, G., and Cai, Y.
\newblock Vanilla feedforward neural networks as a discretization of dynamic
  systems.
\newblock \emph{arXiv preprint arXiv:2209.10909}, 2022.

\bibitem[Hanin \& Sellke(2018)Hanin and Sellke]{Hanin2018Approximating}
Hanin, B. and Sellke, M.
\newblock Approximating {Continuous} {Functions} by {ReLU} {Nets} of {Minimal}
  {Width}.
\newblock \emph{arXiv preprint arXiv:1710.11278}, 2018.

\bibitem[Hirsch(1976)]{Hirsch1976Differential}
Hirsch, M.~W.
\newblock \emph{Differential Topology}.
\newblock Springer New York, 1976.

\bibitem[Hornik(1991)]{hornik1991approximation}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{Hornik1989Multilayer}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{Huang2018Neural}
Huang, C.-W., Krueger, D., Lacoste, A., and Courville, A.
\newblock Neural {Autoregressive} {Flows}.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2018.

\bibitem[Hwang(2023)]{hwang2023minimum}
Hwang, G.
\newblock Minimum width for deep, narrow mlp: A diffeomorphism and the whitney
  embedding theorem approach.
\newblock \emph{arXiv preprint arXiv:2308.15873}, 2023.

\bibitem[Johnson(2019)]{Johnson2019Deep}
Johnson, J.
\newblock Deep, {Skinny} {Neural} {Networks} are not {Universal}
  {Approximators}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kim et~al.(2023)Kim, Min, and Park]{kim2023minimum}
Kim, N., Min, C., and Park, S.
\newblock Minimum width for universal approximation using relu networks on
  compact domain.
\newblock \emph{arXiv preprint arXiv:2309.10402}, 2023.

\bibitem[Kong \& Chaudhuri(2021)Kong and Chaudhuri]{Kong2021Universal}
Kong, Z. and Chaudhuri, K.
\newblock Universal {Approximation} of {Residual} {Flows} in {Maximum} {Mean}
  {Discrepancy}.
\newblock \emph{International Conference on Machine Learning workshop}, 2021.

\bibitem[Le~Roux \& Bengio(2008)Le~Roux and Bengio]{LeRoux2008Representational}
Le~Roux, N. and Bengio, Y.
\newblock Representational {Power} of {Restricted} {Boltzmann} {Machines} and
  {Deep} {Belief} {Networks}.
\newblock \emph{Neural Computation}, 20\penalty0 (6):\penalty0 1631--1649,
  2008.

\bibitem[Leshno et~al.(1993)Leshno, Lin, Pinkus, and
  Schocken]{Leshno1993Multilayer}
Leshno, M., Lin, V.~Y., Pinkus, A., and Schocken, S.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 861--867, 1993.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{Lu2017Expressive}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.
\newblock The {Expressive} {Power} of {Neural} {Networks}: {A} {View} from the
  {Width}.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Montufar(2014)]{Montufar2014Universal}
Montufar, G.~F.
\newblock Universal approximation depth and errors of narrow belief networks
  with discrete units.
\newblock \emph{Neural Computation}, 26\penalty0 (7):\penalty0 1386--1407,
  2014.

\bibitem[Nguyen et~al.(2018)Nguyen, Mukkamala, and Hein]{Nguyen2018Neural}
Nguyen, Q., Mukkamala, M.~C., and Hein, M.
\newblock Neural {Networks} {Should} {Be} {Wide} {Enough} to {Learn}
  {Disconnected} {Decision} {Regions}.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Park et~al.(2021)Park, Yun, Lee, and Shin]{Park2021Minimum}
Park, S., Yun, C., Lee, J., and Shin, J.
\newblock Minimum {Width} for {Universal} {Approximation}.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Rousseau \& Fablet(2018)Rousseau and Fablet]{rousseau2018residual}
Rousseau, F. and Fablet, R.
\newblock Residual networks as geodesic flows of diffeomorphisms.
\newblock \emph{arXiv preprint arXiv:1805.09585}, 2018.

\bibitem[Ruiz-Balet \& Zuazua(2021)Ruiz-Balet and Zuazua]{Ruiz-Balet2021Neural}
Ruiz-Balet, D. and Zuazua, E.
\newblock Neural {ODE} control for classification, approximation and transport.
\newblock \emph{arXiv: 2104.05278}, 2021.

\bibitem[Sutskever \& Hinton(2008)Sutskever and Hinton]{Sutskever2008Deep}
Sutskever, I. and Hinton, G.~E.
\newblock Deep, {Narrow} {Sigmoid} {Belief} {Networks} {Are} {Universal}
  {Approximators}.
\newblock \emph{Neural Computation}, 20\penalty0 (11):\penalty0 2629--2636,
  2008.

\bibitem[Tabuada \& Gharesifard(2023)Tabuada and
  Gharesifard]{Tabuada2020Universal}
Tabuada, P. and Gharesifard, B.
\newblock Universal approximation power of deep residual neural networks
  through the lens of control.
\newblock \emph{IEEE Transactions on Automatic Control}, 68, 2023.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock In \emph{Conference on learning theory}, 2016.

\bibitem[Teshima et~al.(2020{\natexlab{a}})Teshima, Ishikawa, Tojo, Oono,
  Ikeda, and Sugiyama]{Teshima2020Couplingbased}
Teshima, T., Ishikawa, I., Tojo, K., Oono, K., Ikeda, M., and Sugiyama, M.
\newblock Coupling-based {Invertible} {Neural} {Networks} {Are} {Universal}
  {Diffeomorphism} {Approximators}.
\newblock In \emph{Neural Information Processing Systems}, 2020{\natexlab{a}}.

\bibitem[Teshima et~al.(2020{\natexlab{b}})Teshima, Tojo, Ikeda, Ishikawa, and
  Oono]{Teshima2020Universal}
Teshima, T., Tojo, K., Ikeda, M., Ishikawa, I., and Oono, K.
\newblock Universal {Approximation} {Property} of {Neural} {Ordinary}
  {Differential} {Equations}.
\newblock \emph{Neural Information Processing Systems 2020 Workshop on
  Differential Geometry meets Deep Learning}, 2020{\natexlab{b}}.

\bibitem[Whitney(1944)]{whitney1944Self}
Whitney, H.
\newblock The {Self}-{Intersections} of a {Smooth} n-{Manifold} in 2n-{Space}.
\newblock \emph{Annals of Mathematics}, 45\penalty0 (2):\penalty0 220--246,
  1944.

\bibitem[Zhang et~al.(2019)Zhang, Gao, Unterman, and
  Arodz]{Zhang2019Approximation}
Zhang, H., Gao, X., Unterman, J., and Arodz, T.
\newblock Approximation {Capabilities} of {Neural} {ODEs} and {Invertible}
  {Residual} {Networks}.
\newblock In \emph{{International Conference on Machine Learning}}, 2019.

\end{thebibliography}
