\begin{thebibliography}{195}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.09540}.

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl, et~al.]{abdin2024phi}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{ArXiv preprint}, abs/2404.14219, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.14219}.

\bibitem[Agarwal et~al.(2009)Agarwal, Koppula, Leela, Chitrapura, Garg, GM, Haty, Roy, and Sasturkar]{10.1145/1645953.1646283}
Amit Agarwal, Hema~Swetha Koppula, Krishna~P. Leela, Krishna~Prasad Chitrapura, Sachin Garg, Pavan~Kumar GM, Chittaranjan Haty, Anirban Roy, and Amit Sasturkar.
\newblock Url normalization for de-duplication of web pages.
\newblock In \emph{ACM Conference on Information and Knowledge Management}, 2009.
\newblock \url{https://doi.org/10.1145/1645953.1646283}.

\bibitem[AI(2024)]{llama3}
Meta AI.
\newblock Introducing meta llama 3: The most capable openly available llm to date, 2024.
\newblock \url{https://ai.meta.com/blog/meta-llama-3/}.

\bibitem[Albalak et~al.(2022)Albalak, Tuan, Jandaghi, Pryor, Yoffe, Ramachandran, Getoor, Pujara, and Wang]{albalak-etal-2022-feta}
Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, and William~Yang Wang.
\newblock {FETA}: A benchmark for few-sample task transfer in open-domain dialogue.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  10936--10953, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.751}.

\bibitem[Albalak et~al.(2023{\natexlab{a}})Albalak, Pan, Raffel, and Wang]{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock \emph{ArXiv preprint}, abs/2312.02406, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2312.02406}.

\bibitem[Albalak et~al.(2023{\natexlab{b}})Albalak, Raffel, and Wang]{albalak2023improving}
Alon Albalak, Colin Raffel, and William~Yang Wang.
\newblock Improving few-shot generalization by exploring and exploiting auxiliary data.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023{\natexlab{b}}.
\newblock \url{https://openreview.net/forum?id=JDnLXc4NOn}.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang, Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et~al.
\newblock A survey on data selection for language models.
\newblock \emph{ArXiv preprint}, abs/2402.16827, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.16827}.

\bibitem[Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey, et~al.]{allal2023santacoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al.
\newblock Santacoder: don't reach for the stars!
\newblock \emph{ArXiv preprint}, abs/2301.03988, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.03988}.

\bibitem[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Hesslow, Launay, Malartic, Mazzotta, Noune, Pannier, and Penedo]{Almazrouei2023TheFS}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra-Aim{\'e}e Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.
\newblock The falcon series of open language models.
\newblock \emph{arXiv preprint arXiv:2311.16867}, 2023.

\bibitem[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi]{mathqa}
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
\newblock {M}ath{QA}: Towards interpretable math word problem solving with operation-based formalisms.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  2357--2367, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1245}.
\newblock URL \url{https://aclanthology.org/N19-1245}.

\bibitem[Ankner et~al.(2024)Ankner, Blakeney, Sreenivasan, Marion, Leavitt, and Paul]{ankner2024perplexed}
Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew~L. Leavitt, and Mansheej Paul.
\newblock Perplexed by perplexity: Perplexity-based data pruning with small reference models.
\newblock \emph{ArXiv preprint}, abs/2405.20541, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.20541}.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Berard, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kirsch, Lazos, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala]{pytorch2}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, David Berard, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang, Jason Liang, Yinghai Lu, CK~Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu~Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala.
\newblock Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation.
\newblock In \emph{International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}, 2024.
\newblock \url{https://pytorch.org/blog/pytorch-2-paper-tutorial}.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics.
\newblock \emph{ArXiv preprint}, abs/2310.10631, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.10631}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv preprint}, abs/1607.06450, 2016.
\newblock URL \url{https://arxiv.org/abs/1607.06450}.

\bibitem[Bansal et~al.(2024)Bansal, Suvarna, Bhatt, Peng, Chang, and Grover]{bansal2024comparing}
Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, and Aditya Grover.
\newblock Comparing bad apples to good oranges: Aligning large language models via joint preference optimization.
\newblock \emph{arXiv preprint arXiv:2404.00530}, 2024.

\bibitem[Barbaresi(2021)]{trafilatura}
Adrien Barbaresi.
\newblock Trafilatura: {A} web scraping library and command-line tool for text discovery and extraction.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations}, pp.\  122--131, Online, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-demo.15}.
\newblock URL \url{https://aclanthology.org/2021.acl-demo.15}.

\bibitem[bench authors(2023)]{srivastava2023beyond}
BIG bench authors.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock In \emph{Transactions on Machine Learning Research (TMLR)}, 2023.
\newblock \url{https://openreview.net/forum?id=uyTL5Bvosj}.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and Shmitchell]{Bender2021OnTD}
Emily~M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency}, 2021.

\bibitem[Bevendorff et~al.(2018)Bevendorff, Stein, Hagen, and Potthast]{resiliparse1}
Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast.
\newblock {Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl}.
\newblock In \emph{European Conference on Information Retrieval Research (ECIR)}, 2018.
\newblock \url{https://github.com/chatnoir-eu/chatnoir-resiliparse}.

\bibitem[Bevendorff et~al.(2021)Bevendorff, Potthast, and Stein]{resiliparse2}
Janek Bevendorff, Martin Potthast, and Benno Stein.
\newblock {FastWARC: Optimizing Large-Scale Web Archive Analytics}.
\newblock In \emph{International Symposium on Open Search Technology (OSSYM)}, 2021.
\newblock \url{https://github.com/chatnoir-eu/chatnoir-resiliparse}.

\bibitem[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu, Gao, Gao, Gao, Ge, Guan, Guo, Guo, Hao, Hao, He, Hu, Huang, Li, Li, Li, Li, Li, Liang, Lin, Liu, Liu, Liu, Liu, Liu, Liu, Lu, Lu, Luo, Ma, Nie, Pei, Piao, Qiu, Qu, Ren, Ren, Ruan, Sha, Shao, Song, Su, Sun, Sun, Tang, Wang, Wang, Wang, Wang, Wang, Wu, Wu, Xie, Xie, Xie, Xiong, Xu, Xu, Xu, Yang, mei You, Yu, yuan Yu, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhao, Zhao, Zhou, Zhou, Zhu, and Zou]{Bi2024DeepSeekLS}
DeepSeek-AI~Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.~K. Li, Wenfeng Liang, Fangyun Lin, A.~X. Liu, Bo~Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu~Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi~Xiong, Hanwei Xu, Ronald~X Xu, Yanhong Xu, Dejian Yang, Yu~mei You, Shuiping Yu, Xin yuan Yu, Bo~Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao
  Zhu, and Yuheng Zou.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint arXiv:2401.02954}, 2024.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, LeBras, Gao, and Choi]{piqa}
Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pp.\  7432--7439. {AAAI} Press, 2020.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/6239}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang, and Weinbach]{neox}
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.
\newblock {GPT}-{N}eo{X}-20{B}: An open-source autoregressive language model.
\newblock In \emph{Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models}, pp.\  95--136, virtual+Dublin, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.bigscience-1.9}.
\newblock URL \url{https://aclanthology.org/2022.bigscience-1.9}.

\bibitem[Bloom(1970{\natexlab{a}})]{10.1145/362686.362692}
Burton~H. Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock \emph{Communications of the ACM}, 1970{\natexlab{a}}.
\newblock \url{https://doi.org/10.1145/362686.362692}.

\bibitem[Bloom(1970{\natexlab{b}})]{bloom1970space}
Burton~H Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock \emph{Communications of the ACM}, 13\penalty0 (7):\penalty0 422--426, 1970{\natexlab{b}}.

\bibitem[Brandfonbrener et~al.(2024)Brandfonbrener, Zhang, Kirsch, Schwarz, and Kakade]{brandfonbrener2024color}
David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan~Richard Schwarz, and Sham~M Kakade.
\newblock Color-filter: Conditional loss reduction filtering for targeted language model pre-training.
\newblock \emph{arXiv preprint}, 2024.

\bibitem[Broder(1997{\natexlab{a}})]{minhash}
Andrei~Z. Broder.
\newblock On the resemblance and containment of documents.
\newblock In \emph{Compression and Complexity of Sequences}, 1997{\natexlab{a}}.

\bibitem[Broder(1997{\natexlab{b}})]{666900}
A.Z. Broder.
\newblock On the resemblance and containment of documents.
\newblock In \emph{Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)}, pp.\  21--29, 1997{\natexlab{b}}.
\newblock \doi{10.1109/SEQUEN.1997.666900}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Bugliarello et~al.(2022)Bugliarello, Liu, Pfeiffer, Reddy, Elliott, Ponti, and Vulic]{pmlr-v162-bugliarello22a}
Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo~Maria Ponti, and Ivan Vulic.
\newblock {IGLUE:} {A} benchmark for transfer learning across modalities, tasks, and languages.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  2370--2392. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/bugliarello22a.html}.

\bibitem[Calandriello et~al.(2024)Calandriello, Guo, Munos, Rowland, Tang, Pires, Richemond, Lan, Valko, Liu, et~al.]{calandriello2024human}
Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo~Avila Pires, Pierre~Harvey Richemond, Charline~Le Lan, Michal Valko, Tianqi Liu, et~al.
\newblock Human alignment of large language models through online preference optimisation.
\newblock \emph{arXiv preprint arXiv:2403.08635}, 2024.

\bibitem[Carlini et~al.(2023)Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang]{carlini2023quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Ponde, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Babuschkin, Balaji, Jain, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{Chen2021EvaluatingLL}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, David~W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew~M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{ArXiv preprint}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chen et~al.(2023)Chen, Roberts, Bhatia, WANG, Zhang, Sala, and R\'{e}]{chen2023skillit}
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce~Zhang, Frederic Sala, and Christopher R\'{e}.
\newblock Skill-it! a data-driven skills framework for understanding and training language models.
\newblock In A.~Oh, T.~Neumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine (eds.), \emph{Advances in Neural Information Processing Systems}, volume~36, pp.\  36000--36040. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
  and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock In \emph{Journal of Machine Learning Research (JMLR)}, 2022.
\newblock \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv preprint}, abs/2210.11416, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no questions.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  2924--2936, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1300}.
\newblock URL \url{https://aclanthology.org/N19-1300}.

\bibitem[Clark et~al.(2021)Clark, August, Serrano, Haduong, Gururangan, and Smith]{Clark2021AllT}
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah~A. Smith.
\newblock All that{'}s {`}human{'} is not gold: Evaluating human evaluation of generated text.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  7282--7296, Online, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.565}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.565}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{ArXiv preprint}, abs/1803.05457, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.05457}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{ArXiv preprint}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[{Common Crawl}({2007})]{commoncrawl}
{Common Crawl}.
\newblock {Common Crawl}, {2007}.
\newblock \url{https://commoncrawl.org}.

\bibitem[Computer(2023)]{rpjv2}
Together Computer.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Conneau \& Lample(2019)Conneau and Lample]{NEURIPS2019_c04c19c2}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  7057--7067, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html}.

\bibitem[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback}
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun.
\newblock Ultrafeedback: Boosting language models with high-quality feedback, 2023.

\bibitem[Cui et~al.(2022)Cui, Wang, Si, and Hsieh]{cui2022dcbench}
Justin Cui, Ruochen Wang, Si~Si, and Cho-Jui Hsieh.
\newblock {DC}-{BENCH}: Dataset condensation benchmark.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Bs8iFQ7AM6}.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.
\newblock \url{https://proceedings.mlr.press/v202/dehghani23a.html}.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco, Groeneveld, Mitchell, and Gardner]{c4_ai2}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  1286--1305, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.98}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.98}.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du2022glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier{-}Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  5547--5569. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length}
Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Length-controlled alpacaeval: A simple way to debias automatic evaluators.
\newblock \emph{arXiv preprint arXiv:2404.04475}, 2024.

\bibitem[Elazar et~al.(2023)Elazar, Bhagia, Magnusson, Ravichander, Schwenk, Suhr, Walsh, Groeneveld, Soldaini, Singh, Hajishirzi, Smith, and Dodge]{Elazar2023WhatsIM}
Yanai Elazar, Akshita Bhagia, Ian~H. Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah~A. Smith, and Jesse Dodge.
\newblock What's in my big data?
\newblock \emph{ArXiv preprint}, abs/2310.20707, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.20707}.

\bibitem[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto}
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.
\newblock Kto: Model alignment as prospect theoretic optimization.
\newblock \emph{ArXiv preprint}, abs/2402.01306, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.01306}.

\bibitem[Face(2023)]{hfllmleaderboard}
Hugging Face.
\newblock What's going on with the open llm leaderboard?
\newblock \url{https://huggingface.co/blog/open-llm-leaderboard-mmlu}, 2023.

\bibitem[Fan et~al.(2023)Fan, Pagliardini, and Jaggi]{fan2023doge}
Simin Fan, Matteo Pagliardini, and Martin Jaggi.
\newblock Doge: Domain reweighting with generalization estimation.
\newblock \emph{ArXiv preprint}, abs/2310.15393, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.15393}.

\bibitem[Fang et~al.(2023)Fang, Jose, Jain, Schmidt, Toshev, and Shankar]{fang2023dfn}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
\newblock Data filtering networks.
\newblock \emph{ArXiv preprint}, abs/2309.17425, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.17425}.

\bibitem[Fourrier et~al.(2023)Fourrier, Habib, Wolf, and Tunstall]{lighteval}
Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall.
\newblock Lighteval: A lightweight framework for llm evaluation, 2023.
\newblock URL \url{https://github.com/huggingface/lighteval}.

\bibitem[Gadre et~al.(2024{\natexlab{a}})Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2024datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~36, 2024{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/2304.14108}.

\bibitem[Gadre et~al.(2024{\natexlab{b}})Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, Xin, Nezhurina, Vasiljevic, Jitsev, Dimakis, Ilharco, Song, Kollar, Carmon, Dave, Heckel, Muennighoff, and Schmidt]{gadre2024LanguageMS}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros~G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{ArXiv preprint}, abs/2403.08540, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2403.08540}.

\bibitem[Gao et~al.(2021)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv preprint}, abs/2101.00027, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Ge et~al.(2024)Ge, Ma, Chen, Li, and Ding]{ge2024data}
Ce~Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding.
\newblock Data mixing made efficient: A bivariate scaling law for language model pretraining.
\newblock \emph{ArXiv preprint}, abs/2405.14908, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.14908}.

\bibitem[Geva et~al.(2019)Geva, Goldberg, and Berant]{Geva2019AreWM}
Mor Geva, Yoav Goldberg, and Jonathan Berant.
\newblock Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  1161--1166, Hong Kong, China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1107}.
\newblock URL \url{https://aclanthology.org/D19-1107}.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and Berant]{Geva2021DidAU}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.
\newblock Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 346--361, 2021.
\newblock \doi{10.1162/tacl_a_00370}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.21}.

\bibitem[Gillick \& Liu(2010)Gillick and Liu]{Gillick2010NonExpertEO}
Dan Gillick and Yang Liu.
\newblock Non-expert evaluation of summarization systems is risky.
\newblock In \emph{Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk}, pp.\  148--151, Los Angeles, 2010. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W10-0722}.

\bibitem[Glorioso et~al.(2024)Glorioso, Anthony, Tokpanov, Whittington, Pilault, Ibrahim, and Millidge]{glorioso2024zamba}
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge.
\newblock Zamba: A compact 7b ssm hybrid model.
\newblock \emph{ArXiv preprint}, abs/2405.16712, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.16712}.

\bibitem[Gokaslan et~al.(2019)Gokaslan, Cohen, Pavlick, and Tellex]{openwebtext}
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.
\newblock Openwebtext corpus, 2019.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}.

\bibitem[Grave et~al.(2018)Grave, Bojanowski, Gupta, Joulin, and Mikolov]{grave-etal-2018-learning}
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.
\newblock Learning word vectors for 157 languages.
\newblock In \emph{Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)}, Miyazaki, Japan, 2018. European Language Resources Association (ELRA).
\newblock URL \url{https://aclanthology.org/L18-1550}.

\bibitem[Groeneveld(2023)]{bff}
Dirk Groeneveld.
\newblock The big friendly filter.
\newblock \url{https://github.com/allenai/bff}, 2023.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{ArXiv preprint}, abs/2402.00838, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.00838}.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Cesar, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Singh~Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio Cesar, Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh~Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock \emph{Preprint}, 2023.
\newblock \url{https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need}.

\bibitem[Gururangan et~al.(2023)Gururangan, Wortsman, Gadre, Dave, Kilian, Shi, Mercat, Smyrnis, Ilharco, Jordan, Heckel, Dimakis, Farhadi, Shankar, and Schmidt]{open_lm}
Suchin Gururangan, Mitchell Wortsman, Samir~Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt.
\newblock {OpenLM}: a minimal but performative language modeling (lm) repository, 2023.
\newblock \url{https://github.com/mlfoundations/open_lm}.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, Zhang, Thai, Zhang, Wang, Yao, Zhao, Zhou, Cai, Zhai, Ding, Jia, Zeng, Li, Liu, and Sun]{hu2024minicpm}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng~Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024.

\bibitem[Ibrahim et~al.(2024)Ibrahim, Th{\'e}rien, Gupta, Richter, Anthony, Lesort, Belilovsky, and Rish]{ibrahim2024simple}
Adam Ibrahim, Benjamin Th{\'e}rien, Kshitij Gupta, Mats~L Richter, Quentin Anthony, Timoth{\'e}e Lesort, Eugene Belilovsky, and Irina Rish.
\newblock Simple and scalable strategies to continually pre-train large language models.
\newblock \emph{arXiv preprint arXiv:2403.08763}, 2024.

\bibitem[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2.
\newblock \emph{arXiv preprint arXiv:2311.10702}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Diego de~las Casas, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Florian~Bressand Diego de~las Casas, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{ArXiv preprint}, abs/2310.06825, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.06825}.

\bibitem[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{pubmed}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
\newblock {P}ub{M}ed{QA}: A dataset for biomedical research question answering.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2567--2577, Hong Kong, China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1259}.
\newblock URL \url{https://aclanthology.org/D19-1259}.

\bibitem[Johnson et~al.(2017)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{ArXiv preprint}, abs/1702.08734, 2017.
\newblock URL \url{https://arxiv.org/abs/1702.08734}.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1601--1611, Vancouver, Canada, 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1147}.
\newblock URL \url{https://aclanthology.org/P17-1147}.

\bibitem[Joulin et~al.(2017)Joulin, Grave, Bojanowski, and Mikolov]{joulin2017bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock In \emph{Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers}, pp.\  427--431, Valencia, Spain, 2017. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/E17-2068}.

\bibitem[Kaddour(2023)]{kaddour2023minipile}
Jean Kaddour.
\newblock The minipile challenge for data-efficient language models.
\newblock \emph{ArXiv preprint}, abs/2304.08442, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.08442}.

\bibitem[kaggle200000Jeopardy(2019)]{kaggle200000Jeopardy}
kaggle200000Jeopardy.
\newblock 200,000+ {J}eopardy! {Q}uestions --- kaggle.com.
\newblock \url{https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions}, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{ArXiv preprint}, abs/2001.08361, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.08361}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Lauren\c{c}on et~al.(2022)Lauren\c{c}on, Saulnier, Wang, Akiki, Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz\'{a}lez~Ponferrada, Nguyen, Frohberg, \v{S}a\v{s}ko, Lhoest, McMillan-Major, Dupont, Biderman, Rogers, Ben~allal, De~Toni, Pistilli, Nguyen, Nikpoor, Masoud, Colombo, de~la Rosa, Villegas, Thrush, Longpre, Nagel, Weber, Mu\~{n}oz, Zhu, Van~Strien, Alyafeai, Almubarak, Vu, Gonzalez-Dios, Soroa, Lo, Dey, Ortiz~Suarez, Gokaslan, Bose, Adelani, Phan, Tran, Yu, Pai, Chim, Lepercq, Ilic, Mitchell, Luccioni, and Jernite]{NEURIPS2022_ce9e92e3}
Hugo Lauren\c{c}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz\'{a}lez~Ponferrada, Huu Nguyen, J\"{o}rg Frohberg, Mario \v{S}a\v{s}ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben~allal, Francesco De~Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Mu\~{n}oz, Jian Zhu, Daniel Van~Strien, Zaid Alyafeai, Khalid Almubarak, Minh~Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz~Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha~Alexandra Luccioni, and Yacine Jernite.
\newblock The bigscience roots corpus: A 1.6tb composite multilingual dataset.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  31809--31826. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets_and_Benchmarks.pdf}.

\bibitem[Le~Scao et~al.(2022)Le~Scao, Wang, Hesslow, Bekman, Bari, Biderman, Elsahar, Muennighoff, Phang, Press, Raffel, Sanh, Shen, Sutawika, Tae, Yong, Launay, and Beltagy]{scao2022language}
Teven Le~Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M~Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng~Xin Yong, Julien Launay, and Iz~Beltagy.
\newblock What language model to train if you have one million {GPU} hours?
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pp.\  765--782, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.54}.

\bibitem[Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{Lee2021DeduplicatingTD}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8424--8445, Dublin, Ireland, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.577}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.577}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{International conference on the principles of knowledge representation and reasoning}, 2012.
\newblock \url{https://aaai.org/papers/59-4492-the-winograd-schema-challenge}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{ArXiv preprint}, abs/2305.06161, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2305.06161}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee]{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{ArXiv preprint}, abs/2309.05463, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2309.05463}.

\bibitem[Lin et~al.(2024)Lin, Gou, Gong, Liu, Shen, Xu, Lin, Yang, Jiao, Duan, and Chen]{lin2024rho1}
Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen.
\newblock Rho-1: Not all tokens are what you need.
\newblock \emph{ArXiv preprint}, abs/2404.07965, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.07965}.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom]{ling2017program}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: Learning to solve and explain algebraic word problems.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  158--167, Vancouver, Canada, 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1015}.
\newblock URL \url{https://aclanthology.org/P17-1015}.

\bibitem[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{Liu2020LogiQAAC}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
\newblock Logiqa: {A} challenge dataset for machine reading comprehension with logical reasoning.
\newblock In Christian Bessiere (ed.), \emph{Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI} 2020}, pp.\  3622--3628. ijcai.org, 2020.
\newblock \doi{10.24963/ijcai.2020/501}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2020/501}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2024lost}
Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang, Zhou, et~al.]{liu2024best}
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al.
\newblock Best practices and lessons learned on synthetic data for language models.
\newblock \emph{ArXiv preprint}, abs/2404.07503, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2404.07503}.

\bibitem[Liu et~al.(2023)Liu, Qiao, Neiswanger, Wang, Tan, Tao, Li, Wang, Sun, Pangarkar, Fan, Gu, Miller, Zhuang, He, Li, Koto, Tang, Ranjan, Shen, Ren, Iriondo, Mu, Hu, Schulze, Nakov, Baldwin, and Xing]{liu2023llm360}
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi~Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric~P. Xing.
\newblock Llm360: Towards fully transparent open-source llms, 2023.

\bibitem[Longpre et~al.(2023{\natexlab{a}})Longpre, Mahari, Chen, Obeng-Marnu, Sileo, Brannon, Muennighoff, Khazam, Kabbara, Perisetla, et~al.]{longpre2023data}
Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et~al.
\newblock The data provenance initiative: A large scale audit of dataset licensing \& attribution in ai.
\newblock \emph{ArXiv preprint}, abs/2310.16787, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.16787}.

\bibitem[Longpre et~al.(2023{\natexlab{b}})Longpre, Yauney, Reif, Lee, Roberts, Zoph, Zhou, Wei, Robinson, Mimno, and Ippolito]{longpre2023pretrainers}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito.
\newblock A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock \emph{ArXiv preprint}, abs/2305.13169, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2305.13169}.

\bibitem[Lozhkov et~al.(2024{\natexlab{a}})Lozhkov, Ben~Allal, von Werra, and Wolf]{lozhkov2024fineweb-edu}
Anton Lozhkov, Loubna Ben~Allal, Leandro von Werra, and Thomas Wolf.
\newblock Fineweb-edu, 2024{\natexlab{a}}.
\newblock URL \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu}.

\bibitem[Lozhkov et~al.(2024{\natexlab{b}})Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, Liu, Tian, Kocetkov, Zucker, Belkada, Wang, Liu, Abulkhanov, Paul, Li, Li, Risdal, Li, Zhu, Zhuo, Zheltonozhskii, Dade, Yu, Krauß, Jain, Su, He, Dey, Abati, Chai, Muennighoff, Tang, Oblokulov, Akiki, Marone, Mou, Mishra, Gu, Hui, Dao, Zebaze, Dehaene, Patry, Xu, McAuley, Hu, Scholak, Paquet, Robinson, Anderson, Chapados, Patwary, Tajbakhsh, Jernite, Ferrandis, Zhang, Hughes, Wolf, Guha, von Werra, and de~Vries]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos~Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock \emph{ArXiv preprint}, abs/2402.19173, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2402.19173}.

\bibitem[Luccioni \& Viviano(2021)Luccioni and Viviano]{Luccioni2021WhatsIT}
Alexandra~Sasha Luccioni and Joseph~D. Viviano.
\newblock What’s in the box? an analysis of undesirable content in the common crawl corpus.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics}, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:233864521}.

\bibitem[Luukkonen et~al.(2023)Luukkonen, Komulainen, Luoma, Eskelinen, Kanerva, Kupari, Ginter, Laippala, Muennighoff, Piktus, Wang, Tazi, Scao, Wolf, Suominen, Sairanen, Merioksa, Heinonen, Vahtola, Antao, and Pyysalo]{luukkonen-etal-2023-fingpt}
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, and Sampo Pyysalo.
\newblock {F}in{GPT}: Large generative models for a small language.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  2710--2726, Singapore, 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.164}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.164}.

\bibitem[Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and Jaitly]{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock \emph{ArXiv preprint}, abs/2401.16380, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.16380}.

\bibitem[Manber \& Myers(1993)Manber and Myers]{manber1993suffix}
Udi Manber and Gene Myers.
\newblock Suffix arrays: a new method for on-line string searches.
\newblock \emph{siam Journal on Computing}, 22\penalty0 (5):\penalty0 935--948, 1993.

\bibitem[Mazumder et~al.(2022)Mazumder, Banbury, Yao, Karlaš, Rojas, Diamos, Diamos, He, Kiela, Jurado, Kanter, Mosquera, Ciro, Aroyo, Acun, Eyuboglu, Ghorbani, Goodman, Kane, Kirkpatrick, Kuo, Mueller, Thrush, Vanschoren, Warren, Williams, Yeung, Ardalani, Paritosh, Zhang, Zou, Wu, Coleman, Ng, Mattson, and Reddi]{dataperf}
Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karlaš, William~Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera, Juan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman, Tariq Kane, Christine~R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen Paritosh, Ce~Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson, and Vijay~Janapa Reddi.
\newblock Dataperf: Benchmarks for data-centric ai development.
\newblock \emph{ArXiv preprint}, abs/2207.10062, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.10062}.

\bibitem[Meng et~al.(2024)Meng, Xia, and Chen]{meng2024simpo}
Yu~Meng, Mengzhou Xia, and Danqi Chen.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock \emph{arXiv preprint arXiv:2405.14734}, 2024.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  2381--2391, Brussels, Belgium, 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1260}.
\newblock URL \url{https://aclanthology.org/D18-1260}.

\bibitem[MosaicML(2023{\natexlab{a}})]{mosaicml}
MosaicML.
\newblock Llm evaluation scores, 2023{\natexlab{a}}.
\newblock \url{https://www.mosaicml.com/llm-evaluation}.

\bibitem[MosaicML(2023{\natexlab{b}})]{mosailMLarithmetic}
MosaicML.
\newblock llm-foundry/scripts/eval/local\_data/{E}{V}{A}{L}\_{G}{A}{U}{N}{T}{L}{E}{T}.md at main · mosaicml/llm-foundry --- github.com.
\newblock \url{https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md}, 2023{\natexlab{b}}.

\bibitem[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf, et~al.]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al.
\newblock Crosslingual generalization through multitask finetuning.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2022.
\newblock \url{https://aclanthology.org/2023.acl-long.891}.

\bibitem[Muennighoff et~al.(2023{\natexlab{a}})Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo, Singh, Tang, Von~Werra, and Longpre]{muennighoff2023octopack}
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry~Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von~Werra, and Shayne Longpre.
\newblock Octopack: Instruction tuning code large language models.
\newblock \emph{ArXiv preprint}, abs/2308.07124, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2308.07124}.

\bibitem[Muennighoff et~al.(2023{\natexlab{b}})Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models.
\newblock In \emph{Advances in Neural Information Processing Systems (NeuIPS)}, 2023{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2305.16264}.

\bibitem[Muennighoff et~al.(2024)Muennighoff, Su, Wang, Yang, Wei, Yu, Singh, and Kiela]{muennighoff2024generative}
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.
\newblock Generative representational instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2402.09906, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.09906}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv preprint}, abs/2303.08774, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1525--1534, Berlin, Germany, 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1144}.
\newblock URL \url{https://aclanthology.org/P16-1144}.

\bibitem[Parrish et~al.(2022)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel Bowman.
\newblock {BBQ}: A hand-built bias benchmark for question answering.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pp.\  2086--2105, Dublin, Ireland, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.165}.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.165}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K{\"{o}}pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  8024--8035, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel2021nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  2080--2094, Online, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.168}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.168}.

\bibitem[PatronusAI(2023)]{patronusPatronusPatronus}
PatronusAI.
\newblock {P}atronus {A}{I} | {P}atronus {A}{I} launches {E}nterprise{P}{I}{I}, the industry’s first {L}{L}{M} dataset for detecting business-sensitive information --- patronus.ai.
\newblock \url{https://www.patronus.ai/announcements/patronus-ai-launches-enterprisepii-the-industrys-first-llm-dataset-for-detecting-business-sensitive-information}, 2023.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only.
\newblock \emph{ArXiv preprint}, abs/2306.01116, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.01116}.

\bibitem[Penedo et~al.(2024)Penedo, Kydlíček, von Werra, and Wolf]{penedo2024fineweb}
Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, and Thomas Wolf.
\newblock Fineweb, 2024.
\newblock URL \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb}.
\newblock Software.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman, Cao, Cheng, Chung, Derczynski, Du, Grella, Gv, He, Hou, Kazienko, Kocon, Kong, Koptyra, Lau, Lin, Mantri, Mom, Saito, Song, Tang, Wind, Wo{\'z}niak, Zhang, Zhou, Zhu, and Zhu]{peng-etal-2023-rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart{\l}omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri~Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis{\l}aw Wo{\'z}niak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock {RWKV}: Reinventing {RNN}s for the transformer era.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2023{\natexlab{a}}.
\newblock \url{https://aclanthology.org/2023.findings-emnlp.936}.

\bibitem[Peng et~al.(2024)Peng, Goldstein, Anthony, Albalak, Alcaide, Biderman, Cheah, Du, Ferdinan, Hou, Kazienko, GV, Kocoń, Koptyra, Krishna, au2, Muennighoff, Obeid, Saito, Song, Tu, Woźniak, Zhang, Zhao, Zhao, Zhou, Zhu, and Zhu]{peng2024eagle}
Bo~Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi~Kiran GV, Jan Kocoń, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland~Jr. au2, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanisław Woźniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.
\newblock \emph{ArXiv preprint}, abs/2404.05892, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.05892}.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023{\natexlab{b}}.

\bibitem[Pouransari et~al.(2024)Pouransari, Li, Chang, Vasu, Koc, Shankar, and Tuzel]{pouransari2024dataset}
Hadi Pouransari, Chun-Liang Li, Jen-Hao~Rick Chang, Pavan Kumar~Anasosalu Vasu, Cem Koc, Vaishaal Shankar, and Oncel Tuzel.
\newblock Dataset decomposition: Faster llm training with variable sequence length curriculum.
\newblock \emph{arXiv preprint arXiv:2405.13226}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{Preprint}, 2019.
\newblock \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F.~J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L.~Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N.~K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris
  Jones, James Bradbury, Matthew~G. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason Gabriel, William~S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem~W. Ayoub, Jeff Stanway, L.~L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{ArXiv preprint}, abs/2112.11446, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Raffel et~al.(2020{\natexlab{a}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{c4}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Raffel et~al.(2020{\natexlab{b}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020{\natexlab{b}}.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajani et~al.(2023)Rajani, Tunstall, Beeching, Lambert, Rush, and Wolf]{no_robots}
Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander~M. Rush, and Thomas Wolf.
\newblock No robots.
\newblock \url{https://huggingface.co/datasets/HuggingFaceH4/no_robots}, 2023.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, pp.\  2383--2392, Austin, Texas, 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1264}.
\newblock URL \url{https://aclanthology.org/D16-1264}.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy-etal-2019-coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock {C}o{QA}: A conversational question answering challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 249--266, 2019.
\newblock \doi{10.1162/tacl_a_00266}.
\newblock URL \url{https://aclanthology.org/Q19-1016}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark.
\newblock \emph{ArXiv preprint}, abs/2311.12022, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.12022}.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, , and Gordon]{copa}
Melissa Roemmele, Cosmin~Adrian Bejan, , and Andrew~S. Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium}, 2011.
\newblock \url{https://people.ict.usc.edu/~gordon/copa.html}.

\bibitem[Romac et~al.(2021)Romac, Portelas, Hofmann, and Oudeyer]{pmlr-v139-romac21a}
Cl{\'{e}}ment Romac, R{\'{e}}my Portelas, Katja Hofmann, and Pierre{-}Yves Oudeyer.
\newblock Teachmyagent: a benchmark for automatic curriculum learning in deep {RL}.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  9052--9063. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/romac21a.html}.

\bibitem[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{winogender}
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.
\newblock Gender bias in coreference resolution.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, pp.\  8--14, New Orleans, Louisiana, 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-2002}.
\newblock URL \url{https://aclanthology.org/N18-2002}.

\bibitem[Sachdeva et~al.(2024)Sachdeva, Coleman, Kang, Ni, Hong, Chi, Caverlee, McAuley, and Cheng]{sachdeva2024train}
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed~H. Chi, James Caverlee, Julian~J. McAuley, and Derek~Zhiyuan Cheng.
\newblock How to train data-efficient llms.
\newblock \emph{ArXiv preprint}, abs/2402.09668, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.09668}.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pp.\  8732--8740. {AAAI} Press, 2020.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/6399}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{siqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi.
\newblock Social {IQ}a: Commonsense reasoning about social interactions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4463--4473, Hong Kong, China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1454}.
\newblock URL \url{https://aclanthology.org/D19-1454}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shazeer(2020)]{swiglu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{ArXiv preprint}, abs/2002.05202, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.05202}.

\bibitem[Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2023detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock \emph{ArXiv preprint}, abs/2310.16789, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.16789}.

\bibitem[Shilov et~al.(2024)Shilov, Meeus, and de~Montjoye]{shilov2024mosaic}
Igor Shilov, Matthieu Meeus, and Yves-Alexandre de~Montjoye.
\newblock Mosaic memory: Fuzzy duplication in copyright traps for large language models.
\newblock \emph{ArXiv preprint}, abs/2405.15523, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.15523}.

\bibitem[Singh et~al.(2024{\natexlab{a}})Singh, Co-Reyes, Agarwal, Anand, Patil, Garcia, Liu, Harrison, Lee, Xu, Parisi, Kumar, Alemi, Rizkowsky, Nova, Adlam, Bohnet, Elsayed, Sedghi, Mordatch, Simpson, Gur, Snoek, Pennington, Hron, Kenealy, Swersky, Mahajan, Culp, Xiao, Bileschi, Constant, Novak, Liu, Warkentin, Bansal, Dyer, Neyshabur, Sohl-Dickstein, and Fiedel]{singh2024beyond}
Avi Singh, John~D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter~J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron~T Parisi, Abhishek Kumar, Alexander~A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin~Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura~A Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel.
\newblock Beyond human data: Scaling self-training for problem-solving with language models.
\newblock \emph{Transactions on Machine Learning Research}, 2024{\natexlab{a}}.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=lNAyUngGFK}.
\newblock Expert Certification.

\bibitem[Singh et~al.(2024{\natexlab{b}})Singh, Vargus, Dsouza, Karlsson, Mahendiran, Ko, Shandilya, Patel, Mataciunas, OMahony, et~al.]{singh2024aya}
Shivalika Singh, Freddie Vargus, Daniel Dsouza, B{\"o}rje~F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et~al.
\newblock Aya dataset: An open-access collection for multilingual instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2402.06619, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2402.06619}.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Soldaini \& Lo(2023)Soldaini and Lo]{peS2o}
Luca Soldaini and Kyle Lo.
\newblock {peS2o (Pretraining Efficiently on S2ORC) Dataset}.
\newblock Technical report, {Allen Institute for AI}, 2023.
\newblock ODC-By, \url{https://github.com/allenai/pes2o}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{ArXiv preprint}, abs/2402.00159, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.00159}.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S. Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.
\newblock \url{https://openreview.net/forum?id=UmvSlP-PyV}.

\bibitem[Su et~al.(2021)Su, Ahmed, Lu, Pan, Bo, and Liu]{rope}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{ArXiv preprint}, abs/2104.09864, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.09864}.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting commonsense knowledge.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4149--4158, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock URL \url{https://aclanthology.org/N19-1421}.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivière, Kale, Love, Tafti, Hussenot, Sessa, Chowdhery, Roberts, Barua, Botev, Castro-Ros, Slone, Héliou, Tacchetti, Bulanova, Paterson, Tsai, Shahriari, Lan, Choquette-Choo, Crepy, Cer, Ippolito, Reid, Buchatskaya, Ni, Noland, Yan, Tucker, Muraru, Rozhdestvenskiy, Michalewski, Tenney, Grishchenko, Austin, Keeling, Labanowski, Lespiau, Stanway, Brennan, Chen, Ferret, Chiu, Mao-Jones, Lee, Yu, Millican, Sjoesund, Lee, Dixon, Reid, Mikuła, Wirth, Sharman, Chinaev, Thain, Bachem, Chang, Wahltinez, Bailey, Michel, Yotov, Chaabouni, Comanescu, Jana, Anil, McIlroy, Liu, Mullins, Smith, Borgeaud, Girgin, Douglas, Pandya, Shakeri, De, Klimenko, Hennigan, Feinberg, Stokowiec, hui Chen, Ahmed, Gong, Warkentin, Peran, Giang, Farabet, Vinyals, Dean, Kavukcuoglu, Hassabis, Ghahramani, Eck, Barral, Pereira, Collins, Joulin, Fiedel, Senter, Andreev, and Kenealy]{gemmateam2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier~Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline~Le Lan, Christopher~A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars~Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem,
  Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel~L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu~hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{ArXiv preprint}, abs/2403.08295, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.08295}.

\bibitem[Team(2024)]{k2llm360}
K2~Development Team.
\newblock Llm360 k2-65b: Scaling up fully transparent open-source llms.
\newblock Technical report, LLM360, 2024.
\newblock \url{https://www.llm360.ai/paper2.pdf}.

\bibitem[Team(2023)]{MosaicML2023Introducing}
MosaicML~NLP Team.
\newblock Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
\newblock \url{www.mosaicml.com/blog/mpt-7b}.

\bibitem[Teknium(2023)]{openhermes}
Teknium.
\newblock Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
\newblock \url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}.

\bibitem[Thudi \& Maddison(2024)Thudi and Maddison]{thudi2024finding}
Anvith Thudi and Chris~J. Maddison.
\newblock Finding optimally robust data mixtures via concave maximization.
\newblock \emph{ArXiv preprint}, abs/2406.01477, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.01477}.

\bibitem[Tirumala et~al.(2024)Tirumala, Simig, Aghajanyan, and Morcos]{tirumala2024d4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
\newblock D4: Improving llm pretraining via document de-duplication and diversification.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[{Together Computer}(2023)]{rpj}
{Together Computer}.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock {{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}.
\newblock \emph{ArXiv preprint}, abs/2302.13971, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}.
\newblock \emph{ArXiv preprint}, abs/2307.09288, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2307.09288}.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}, 2023.

\bibitem[{\"U}st{\"u}n et~al.(2024){\"U}st{\"u}n, Aryabumi, Yong, Ko, D'souza, Onilude, Bhandari, Singh, Ooi, Kayid, et~al.]{ustun2024aya}
Ahmet {\"U}st{\"u}n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et~al.
\newblock Aya model: An instruction finetuned open-access multilingual language model.
\newblock \emph{ArXiv preprint}, abs/2402.07827, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.07827}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\  5998--6008, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap, et~al.]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva Naik, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.
\newblock \emph{arXiv preprint arXiv:2204.07705}, 2022.

\bibitem[Warstadt et~al.(2023)Warstadt, Mueller, Choshen, Wilcox, Zhuang, Ciro, Mosquera, Paranjabe, Williams, Linzen, and Cotterell]{warstadt-etal-2023-findings}
Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell.
\newblock Findings of the {B}aby{LM} challenge: Sample-efficient pretraining on developmentally plausible corpora.
\newblock In Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell (eds.), \emph{Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning}, pp.\  1--34, Singapore, 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.conll-babylm.1}.
\newblock URL \url{https://aclanthology.org/2023.conll-babylm.1}.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2024)Wei, Cassano, Liu, Ding, Jain, de~Vries, von Werra, Guha, and Zhang]{starcoder2instruct}
Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de~Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang.
\newblock Starcoder2-instruct: Fully transparent and permissive self-alignment for code generation.
\newblock \url{https://huggingface.co/blog/sc2-instruct}, 2024.

\bibitem[Wenzek et~al.(2020)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave]{wenzek-etal-2020-ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock {CCN}et: Extracting high quality monolingual datasets from web crawl data.
\newblock In \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pp.\  4003--4012, Marseille, France, 2020. European Language Resources Association.
\newblock ISBN 979-10-95546-34-4.
\newblock URL \url{https://aclanthology.org/2020.lrec-1.494}.

\bibitem[Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen]{wettig2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.
\newblock Qurating: Selecting high-quality data for training language models.
\newblock \emph{ArXiv preprint}, abs/2402.09739, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.09739}.

\bibitem[Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, et~al.]{workshop2022bloom}
BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv preprint}, abs/2211.05100, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.05100}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and Schmidt]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs, Raphael~Gontijo Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  23965--23998. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/wortsman22a.html}.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{ArXiv preprint}, abs/2309.14322, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.14322}.

\bibitem[Xia et~al.(2024)Xia, Gao, Zeng, and Chen]{xia2023sheared}
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.
\newblock Sheared {LL}a{MA}: Accelerating language model pre-training via structured pruning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=09iOdaeOzp}.

\bibitem[Xiao et~al.(2023)Xiao, Liu, Zhang, and Muennighoff]{bge_embedding}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
\newblock C-pack: Packaged resources to advance general chinese embedding, 2023.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=lXuByUeHhd}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=uPSQv0leAu}.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, et~al.]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, et~al.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  483--498, Online, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.41}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.41}.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Chiang, Zheng, Gonzalez, and Stoica]{Yang2023RethinkingBA}
Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Rethinking benchmark and contamination for language models with rephrased samples.
\newblock \emph{ArXiv preprint}, abs/2311.04850, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2311.04850}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Chiang, Zheng, Gonzalez, and Stoica]{yang2023rethinking}
Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph~E Gonzalez, and Ion Stoica.
\newblock Rethinking benchmark and contamination for language models with rephrased samples.
\newblock \emph{ArXiv preprint}, abs/2311.04850, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2311.04850}.

\bibitem[Yin et~al.(2023)Yin, Liu, Yin, Zhong, Bansal, Han, and Chang]{yin2023dynosaur}
Da~Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang.
\newblock Dynosaur: A dynamic growth paradigm for instruction-tuning data curation.
\newblock \emph{arXiv preprint arXiv:2305.14327}, 2023.

\bibitem[Yue et~al.(2024)Yue, Zheng, Zhang, and Chen]{yue2024mammoth2}
Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen.
\newblock Mammoth2: Scaling instructions from the web.
\newblock \emph{arXiv preprint arXiv:2405.03548}, 2024.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{ArXiv preprint}, abs/2203.14465, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.14465}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  4791--4800, Florence, Italy, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1472}.
\newblock URL \url{https://aclanthology.org/P19-1472}.

\bibitem[Zhang et~al.(2019)Zhang, Titov, and Sennrich]{zhang-etal-2019-improving}
Biao Zhang, Ivan Titov, and Rico Sennrich.
\newblock Improving deep transformer with depth-scaled initialization and merged attention.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  898--909, Hong Kong, China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1083}.
\newblock URL \url{https://aclanthology.org/D19-1083}.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Huang, Zhou, Li, and Ouyang]{zhang2024accessing}
Di~Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang.
\newblock Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Qu, Liu, Zhang, Lin, Yu, Pan, Cheng, Liu, Lin, Yuan, Zheng, Pang, Du, Liang, Ma, Li, Ma, Lin, Benetos, Yang, Zhou, Ma, Liu, Niu, Wang, Que, Liu, Liu, Guo, Gao, Zhou, Zhang, Zhou, Wang, Bai, Zhang, Zhang, Wang, Yang, Zhao, Zhang, Ouyang, Huang, and Chen]{zhang2024mapneo}
Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen.
\newblock Map-neo: Highly capable and transparent bilingual large language model series.
\newblock \emph{arXiv preprint arXiv:2405.19327}, 2024{\natexlab{b}}.

\bibitem[Zhao et~al.(2024)Zhao, Ren, Hessel, Cardie, Choi, and Deng]{zhao2024wildchat}
Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng.
\newblock Wildchat: 1m chat{GPT} interaction logs in the wild.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Bl8u7ZRlbM}.

\bibitem[Zhao et~al.(2023)Zhao, Gu, Varma, Luo, chin Huang, Xu, Wright, Shojanazeri, Ott, Shleifer, Desmaison, Balioglu, Nguyen, Chauhan, Hao, and Li]{Zhao2023PyTorchFE}
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.
\newblock Pytorch fsdp: Experiences on scaling fully sharded data parallel.
\newblock In \emph{Very Large Data Bases Conference (VLDB)}, 2023.
\newblock \url{https://dl.acm.org/doi/10.14778/3611540.3611569}.

\bibitem[Zheng et~al.(2024)Zheng, Zhang, Shen, Liu, Lin, Fu, Chen, and Yue]{zheng2024opencodeinterpreter}
Tianyu Zheng, Ge~Zhang, Tianhao Shen, Xueling Liu, Bill~Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue.
\newblock Opencodeinterpreter: Integrating code generation with execution and refinement.
\newblock \emph{arXiv preprint arXiv:2402.14658}, 2024.

\bibitem[Zhong et~al.(2024)Zhong, Wang, Xu, Liu, Ding, Du, and Tao]{zhong2024achieving}
Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo~Du, and Dacheng Tao.
\newblock Achieving> 97\% on gsm8k: Deeply understanding the problems makes llms perfect reasoners.
\newblock \emph{arXiv preprint arXiv:2404.14963}, 2024.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{ArXiv preprint}, abs/2304.06364, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.06364}.

\bibitem[Zhu et~al.(2023)Zhu, Frick, Wu, Zhu, and Jiao]{starling2023}
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao.
\newblock Starling-7b: Improving llm helpfulness \& harmlessness with rlaif, November 2023.

\end{thebibliography}
