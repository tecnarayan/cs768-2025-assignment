\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 16)}, 2016.

\bibitem[Adlam et~al.(2020)Adlam, Lee, Xiao, Pennington, and
  Snoek]{adlam2020exploring}
Adlam, B., Lee, J., Xiao, L., Pennington, J., and Snoek, J.
\newblock Exploring the uncertainty properties of neural networks’ implicit
  priors in the infinite-width limit.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Arfian(2022)]{vecteeze}
Arfian, W.
\newblock Ukraine vectors by vecteezy.
\newblock
  \url{https://www.vecteezy.com/vector-art/7506324-stand-with-ukraine-text-with-ukraine-flag-ribbon-and-ukraine-map-vector-design-on-a-dark-blue-background},
  2022.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019on}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8141--8150. Curran Associates, Inc., 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{Arora2020Harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkl8sJBYvH}.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Fantacci, Godwin, Jones, Hennigan,
  Hessel, Kapturowski, Keck, Kemaev, King, Martens, Mikulik, Norman, Quan,
  Papamakarios, Ring, Ruiz, Sanchez, Schneider, Sezener, Spencer, Srinivasan,
  Stokowiec, and Viola]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Fantacci, C., Godwin, J.,
  Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I.,
  King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G.,
  Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S.,
  Srinivasan, S., Stokowiec, W., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/deepmind}.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bai et~al.(2019)Bai, Lu, Zhang, et~al.]{bai2019}
Bai, J., Lu, F., Zhang, K., et~al.
\newblock Onnx: Open neural network exchange.
\newblock \url{https://github.com/onnx/onnx}, 2019.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Borovykh(2018)]{borovykh2018gaussian}
Borovykh, A.
\newblock A gaussian process perspective on convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1810.10798}, 2018.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., and Wanderman-Milne, S.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brock et~al.(2021{\natexlab{a}})Brock, De, and
  Smith]{brock2021characterizing}
Brock, A., De, S., and Smith, S.~L.
\newblock Characterizing signal propagation to close the performance gap in
  unnormalized resnets.
\newblock \emph{arXiv preprint arXiv:2101.08692}, 2021{\natexlab{a}}.

\bibitem[Brock et~al.(2021{\natexlab{b}})Brock, De, Smith, and
  Simonyan]{brock2021high}
Brock, A., De, S., Smith, S.~L., and Simonyan, K.
\newblock High-performance large-scale image recognition without normalization.
\newblock \emph{arXiv preprint arXiv:2102.06171}, 2021{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Gong, and Wang]{chen2020tenas}
Chen, W., Gong, X., and Wang, Z.
\newblock Neural architecture search on imagenet in four gpu hours: A
  theoretically inspired perspective.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Hsieh, and Gong]{chen2021vision}
Chen, X., Hsieh, C.-J., and Gong, B.
\newblock When vision transformers outperform resnets without pretraining or
  strong data augmentations, 2021{\natexlab{b}}.

\bibitem[Dauphin \& Schoenholz(2019)Dauphin and
  Schoenholz]{dauphin2019metainit}
Dauphin, Y.~N. and Schoenholz, S.
\newblock Metainit: Initializing learning by learning to initialize.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Du et~al.(2019)Du, Hou, Salakhutdinov, Poczos, Wang, and
  Xu]{du2019graph}
Du, S.~S., Hou, K., Salakhutdinov, R.~R., Poczos, B., Wang, R., and Xu, K.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{pmlr-v70-finn17a}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1126--1135. PMLR, 06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/finn17a.html}.

\bibitem[Franceschi et~al.(2021)Franceschi, de~B{\'e}zenac, Ayed, Chen,
  Lamprier, and Gallinari]{franceschi2021neural}
Franceschi, J.-Y., de~B{\'e}zenac, E., Ayed, I., Chen, M., Lamprier, S., and
  Gallinari, P.
\newblock A neural tangent kernel perspective of gans.
\newblock \emph{arXiv preprint arXiv:2106.05566}, 2021.

\bibitem[Frostig et~al.(2021)Frostig, Johnson, Maclaurin, Paszke, and
  Radul]{frostig2021decomposing}
Frostig, R., Johnson, M.~J., Maclaurin, D., Paszke, A., and Radul, A.
\newblock Decomposing reverse-mode automatic differentiation.
\newblock \emph{arXiv preprint arXiv:2105.09469}, 2021.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Aitchison, and
  Rasmussen]{garriga2018deep}
Garriga-Alonso, A., Aitchison, L., and Rasmussen, C.~E.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{evaluating_derivatives}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating Derivatives}.
\newblock Society for Industrial and Applied Mathematics, second edition, 2008.
\newblock \doi{10.1137/1.9780898717761}.
\newblock URL \url{https://epubs.siam.org/doi/abs/10.1137/1.9780898717761}.

\bibitem[Grosse(2021)]{grosse2021neural}
Grosse, R.
\newblock Neural net training dynamics, January 2021.
\newblock URL
  \url{https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf}.

\bibitem[Hanin \& Nica(2020)Hanin and Nica]{Hanin2020Finite}
Hanin, B. and Nica, M.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgndT4KwB}.

\bibitem[He et~al.(2020)He, Lakshminarayanan, and Teh]{he2020bayesian}
He, B., Lakshminarayanan, B., and Teh, Y.~W.
\newblock Bayesian deep ensembles via the neural tangent kernel.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Horace~He(2021)]{functorch2021}
Horace~He, R.~Z.
\newblock functorch: Jax-like composable function transforms for pytorch.
\newblock \url{https://github.com/pytorch/functorch}, 2021.

\bibitem[Hron et~al.(2020{\natexlab{a}})Hron, Bahri, Novak, Pennington, and
  Sohl-Dickstein]{hron2020exact}
Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl-Dickstein, J.
\newblock Exact posterior distributions of wide bayesian neural networks,
  2020{\natexlab{a}}.

\bibitem[Hron et~al.(2020{\natexlab{b}})Hron, Bahri, Sohl-Dickstein, and
  Novak]{hron2020}
Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{b}}.

\bibitem[Hu et~al.(2020)Hu, Shen, Yang, and Shao]{Hu2020InfinitelyWG}
Hu, J., Shen, J., Yang, B., and Shao, L.
\newblock Infinitely wide graph convolutional networks: semi-supervised
  learning via gaussian processes.
\newblock \emph{arXiv preprint arXiv:2002.12168}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot2018ntk}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Khan et~al.(2019)Khan, Immer, Abedi, and Korzepa]{khan2019approximate}
Khan, M. E.~E., Immer, A., Abedi, E., and Korzepa, M.
\newblock Approximate inference turns deep networks into gaussian processes.
\newblock In \emph{Advances in neural information processing systems}, 2019.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-dickstein]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and
  Sohl-dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15156--15172, 2020.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015autograd}
Maclaurin, D., Duvenaud, D., and Adams, R.~P.
\newblock Autograd: Effortless gradients in numpy.
\newblock In \emph{ICML 2015 AutoML Workshop}, 2015.
\newblock URL \url{https://github.com/HIPS/autograd}.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417. PMLR, 2015.

\bibitem[Matthews et~al.(2018)Matthews, Hron, Rowland, Turner, and
  Ghahramani]{matthews2018}
Matthews, A., Hron, J., Rowland, M., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Müntz(1913)]{powermethod}
Müntz, H.
\newblock Solution directe de l’équation séculaire et de quelques
  problèmes analogues transcendants.
\newblock \emph{C. R. Acad. Sci. Paris}, 156:\penalty0 43--46, 1913.

\bibitem[Naumann(2004)]{naumann2004optimal}
Naumann, U.
\newblock Optimal accumulation of jacobian matrices by elimination methods on
  the dual computational graph.
\newblock \emph{Mathematical Programming}, 99\penalty0 (3):\penalty0 399--421,
  2004.

\bibitem[Naumann(2008)]{naumann2008optimal}
Naumann, U.
\newblock Optimal jacobian accumulation is np-complete.
\newblock \emph{Mathematical Programming}, 112\penalty0 (2):\penalty0 427--441,
  2008.

\bibitem[Neal(1994)]{neal}
Neal, R.~M.
\newblock Priors for infinite networks (tech. rep. no. crg-tr-94-1).
\newblock \emph{University of Toronto}, 1994.

\bibitem[Nguyen et~al.(2020)Nguyen, Chen, and Lee]{nguyen2020dataset}
Nguyen, T., Chen, Z., and Lee, J.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock \emph{arXiv preprint arXiv:2011.00050}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{nguyen2021dataset}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock \emph{arXiv preprint arXiv:2107.13034}, 2021.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2018bayesian}
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A.~A., Sohl-Dickstein, J., and
  Schoenholz, S.~S.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Oreshkin et~al.(2018)Oreshkin, L{\'o}pez, and
  Lacoste]{Oreshkin2018TADAMTD}
Oreshkin, B.~N., L{\'o}pez, P.~R., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Park et~al.(2020)Park, Lee, Peng, Cao, and
  Sohl-Dickstein]{park2020towards}
Park, D.~S., Lee, J., Peng, D., Cao, Y., and Sohl-Dickstein, J.
\newblock Towards nngp-guided neural architecture search.
\newblock \emph{arXiv preprint arXiv:2011.06006}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 32}, pp.\  8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Pennington \& Bahri(2017)Pennington and Bahri]{pmlr-v70-pennington17a}
Pennington, J. and Bahri, Y.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2798--2806. PMLR, 06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/pennington17a.html}.

\bibitem[Radul et~al.(2022)Radul, Paszke, Frostig, Johnson, and
  Maclaurin]{radul2022you}
Radul, A., Paszke, A., Frostig, R., Johnson, M., and Maclaurin, D.
\newblock You only linearize once: Tangents transpose to gradients.
\newblock \emph{arXiv preprint arXiv:2204.10923}, 2022.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Spigler et~al.(2019)Spigler, Geiger, d’Ascoli, Sagun, Biroli, and
  Wyart]{spigler2019jamming}
Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G., and Wyart, M.
\newblock A jamming transition from under-to over-parametrization affects
  generalization in deep learning.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  52\penalty0 (47):\penalty0 474001, 2019.

\bibitem[Steiner et~al.(2021)Steiner, Kolesnikov, Zhai, Wightman, Uszkoreit,
  and Beyer]{steiner2021augreg}
Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., and Beyer,
  L.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.10270}, 2021.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourfeat}
Tancik, M., Srinivasan, P.~P., Mildenhall, B., Fridovich-Keil, S., Raghavan,
  N., Singhal, U., Ramamoorthi, R., Barron, J.~T., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, Lucic, and
  Dosovitskiy]{tolstikhin2021mlpmixer}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,
  T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and
  Dosovitskiy, A.
\newblock Mlp-mixer: An all-mlp architecture for vision, 2021.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao18a}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and
  Schoenholz]{xiao2019disentangling}
Xiao, L., Pennington, J., and Schoenholz, S.~S.
\newblock Disentangling trainability and generalization in deep learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Yaida(2020)]{yaida2019non}
Yaida, S.
\newblock Non-{G}aussian processes and neural networks at finite widths.
\newblock In \emph{Mathematical and Scientific Machine Learning Conference},
  2020.

\bibitem[Yang(2019)]{yang2019scaling}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
Yang, G.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Yang et~al.(2019)Yang, Pennington, Rao, Sohl-Dickstein, and
  Schoenholz]{yang2018a}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock A mean field theory of batch normalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference}, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Wang, Xian, Chen, and Xu]{zhou2021metalearning}
Zhou, Y., Wang, Z., Xian, J., Chen, C., and Xu, J.
\newblock Meta-learning with neural tangent kernels.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ti87Pv5Oc8}.

\bibitem[Zoph \& Le(2016)Zoph and Le]{zoph2017neural}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01578}.

\end{thebibliography}
