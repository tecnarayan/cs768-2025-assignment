\begin{thebibliography}{10}

\bibitem{abnar2020quantifying}
Samira Abnar and Willem Zuidema.
\newblock Quantifying attention flow in transformers.
\newblock {\em arXiv preprint arXiv:2005.00928}, 2020.

\bibitem{akbari2021vatt}
Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock Vatt: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock {\em NeurIPS}, 2021.

\bibitem{alayrac2020self}
Jean-Baptiste Alayrac, Adri{\`a} Recasens, Rosalia Schneider, Relja
  Arandjelovi{\'c}, Jason Ramapuram, Jeffrey De~Fauw, Lucas Smaira, Sander
  Dieleman, and Andrew Zisserman.
\newblock Self-supervised multimodal versatile networks.
\newblock In {\em NeurIPS}, 2020.

\bibitem{arandjelovic2017look}
Relja Arandjelovic and Andrew Zisserman.
\newblock Look, listen and learn.
\newblock In {\em ICCV}, 2017.

\bibitem{arandjelovic2018objects}
Relja Arandjelovic and Andrew Zisserman.
\newblock Objects that sound.
\newblock In {\em ECCV}, 2018.

\bibitem{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock {\em ICCV}, 2021.

\bibitem{aytar2016soundnet}
Yusuf Aytar, Carl Vondrick, and Antonio Torralba.
\newblock Soundnet: Learning sound representations from unlabeled video.
\newblock In {\em NeurIPS}, 2016.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock {\em ICCV}, 2021.

\bibitem{bian2017revisiting}
Yunlong Bian, Chuang Gan, Xiao Liu, Fu~Li, Xiang Long, Yandong Li, Heng Qi, Jie
  Zhou, Shilei Wen, and Yuanqing Lin.
\newblock Revisiting the effectiveness of off-the-shelf temporal modeling
  approaches for large-scale video classification.
\newblock {\em arXiv preprint arXiv:1708.03805}, 2017.

\bibitem{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em CVPR}, 2017.

\bibitem{chen2021localizing}
Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi,
  and Andrew Zisserman.
\newblock Localizing visual sounds the hard way.
\newblock In {\em CVPR}, 2021.

\bibitem{chen2020vggsound}
Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman.
\newblock {VGGSound}: A large-scale audio-visual dataset.
\newblock In {\em ICASSP}, 2020.

\bibitem{chen1998audio}
Tsuhan Chen and Ram~R Rao.
\newblock Audio-visual integration in multimodal communication.
\newblock {\em Proceedings of the IEEE}, 86(5):837--852, 1998.

\bibitem{damen2020rescaling}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Antonino Furnari,
  Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett,
  Will Price, et~al.
\newblock Rescaling egocentric vision.
\newblock {\em arXiv preprint arXiv:2006.13256}, 2020.

\bibitem{dehghani2021scenic}
Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and
  Yi~Tay.
\newblock {Scenic}: A {JAX} library for computer vision research and beyond.
\newblock {\em arXiv preprint arXiv:2110.11403}, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, pages 248--255. Ieee, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{ephrat2018looking}
Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan
  Hassidim, William~T Freeman, and Michael Rubinstein.
\newblock Looking to listen at the cocktail party: a speaker-independent
  audio-visual model for speech separation.
\newblock {\em ACM Transactions on Graphics (TOG)}, 37(4):1--11, 2018.

\bibitem{fan2019more}
Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, and David Cox.
\newblock More is less: Learning efficient video representations by big-little
  network and depthwise temporal aggregation.
\newblock {\em arXiv preprint arXiv:1912.00869}, 2019.

\bibitem{fayek2020large}
Haytham~M Fayek and Anurag Kumar.
\newblock Large scale audiovisual learning of sounds with weakly labeled data.
\newblock {\em arXiv preprint arXiv:2006.01595}, 2020.

\bibitem{feichtenhofer2019slowfast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In {\em ICCV}, pages 6202--6211, 2019.

\bibitem{gabeur2020multi}
Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.
\newblock Multi-modal transformer for video retrieval.
\newblock In {\em ECCV}, volume~5. Springer, 2020.

\bibitem{gemmeke2017audio}
Jort~F Gemmeke, Daniel~PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,
  R~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In {\em ICASSP}, pages 776--780. IEEE, 2017.

\bibitem{ghanem2018activitynet}
Bernard Ghanem, Juan~Carlos Niebles, Cees Snoek, Fabian~Caba Heilbron, Humam
  Alwassel, Victor Escorcia, Ranjay Krishna, Shyamal Buch, and Cuong~Duc Dao.
\newblock The activitynet large-scale activity recognition challenge 2018
  summary.
\newblock {\em arXiv preprint arXiv:1808.03766}, 2018.

\bibitem{gong2021ast}
Yuan Gong, Yu-An Chung, and James Glass.
\newblock {AST:} audio spectrogram transformer.
\newblock {\em arXiv preprint arXiv:2104.01778}, 2021.

\bibitem{harwath2017unsupervised}
David Harwath, Antonio Torralba, and James~R Glass.
\newblock Unsupervised learning of spoken language with visual context.
\newblock {\em NeurIPS}, 2017.

\bibitem{hendricks2021decoupling}
Lisa~Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and
  Aida Nematzadeh.
\newblock Decoupling the role of data, attention, and losses in multimodal
  transformers.
\newblock {\em arXiv preprint arXiv:2102.00529}, 2021.

\bibitem{hershey2017cnn}
Shawn Hershey, Sourish Chaudhuri, Daniel~PW Ellis, Jort~F Gemmeke, Aren Jansen,
  R~Channing Moore, Manoj Plakal, Devin Platt, Rif~A Saurous, Bryan Seybold,
  et~al.
\newblock {CNN} architectures for large-scale audio classification.
\newblock In {\em ICASSP}, pages 131--135. IEEE, 2017.

\bibitem{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em ECCV}, 2016.

\bibitem{iashin2020multi}
Vladimir Iashin and Esa Rahtu.
\newblock Multi-modal dense video captioning.
\newblock In {\em CVPR Workshops}, pages 958--959, 2020.

\bibitem{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock {\em arXiv preprint arXiv:2103.03206}, 2021.

\bibitem{jansen2020coincidence}
Aren Jansen, Daniel~PW Ellis, Shawn Hershey, R~Channing Moore, Manoj Plakal,
  Ashok~C Popat, and Rif~A Saurous.
\newblock Coincidence, categorization, and consolidation: Learning to recognize
  sounds with minimal supervision.
\newblock In {\em ICASSP}, 2020.

\bibitem{jiang2019stm}
Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan.
\newblock Stm: Spatiotemporal and motion encoding for action recognition.
\newblock In {\em ICCV}, pages 2000--2009, 2019.

\bibitem{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
  Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock {\em arXiv preprint arXiv:1705.06950}, 2017.

\bibitem{kazakos2019epic}
Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen.
\newblock Epic-fusion: Audio-visual temporal binding for egocentric action
  recognition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5492--5501, 2019.

\bibitem{kazakos2021slow}
Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen.
\newblock Slow-fast auditory streams for audio recognition.
\newblock In {\em ICASSP}, pages 855--859. IEEE, 2021.

\bibitem{kim2013deep}
Yelin Kim, Honglak Lee, and Emily~Mower Provost.
\newblock Deep learning for robust feature generation in audiovisual emotion
  recognition.
\newblock In {\em ICASSP}. IEEE, 2013.

\bibitem{lee2020parameter}
Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, and Yale Song.
\newblock Parameter efficient multimodal transformers for video representation
  learning.
\newblock {\em arXiv preprint arXiv:2012.04124}, 2020.

\bibitem{li2019entangled}
Guang Li, Linchao Zhu, Ping Liu, and Yi~Yang.
\newblock Entangled transformer for image captioning.
\newblock In {\em ICCV}, pages 8928--8937, 2019.

\bibitem{li2020learning}
Jiaman Li, Yihang Yin, Hang Chu, Yi~Zhou, Tingwu Wang, Sanja Fidler, and Hao
  Li.
\newblock Learning to generate diverse dance motions with transformer.
\newblock {\em arXiv preprint arXiv:2008.08171}, 2020.

\bibitem{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock {\em arXiv preprint arXiv:1908.03557}, 2019.

\bibitem{li2021learn}
Ruilong Li, Shan Yang, David~A Ross, and Angjoo Kanazawa.
\newblock Learn to dance with aist++: Music conditioned 3d dance generation.
\newblock {\em arXiv preprint arXiv:2101.08779}, 2021.

\bibitem{li2020tea}
Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang.
\newblock Tea: Temporal excitation and aggregation for action recognition.
\newblock In {\em CVPR}, pages 909--918, 2020.

\bibitem{lin2019temporal}
Ji~Lin, Chuang Gan, and Song Han.
\newblock Temporal shift module for efficient video understanding. 2019 ieee.
\newblock In {\em ICCV}, pages 7082--7092, 2019.

\bibitem{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{monfort2019moments}
Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah~Adel
  Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et~al.
\newblock Moments in time dataset: one million videos for event understanding.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  42(2):502--508, 2019.

\bibitem{ngiam2011multimodal}
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew~Y
  Ng.
\newblock Multimodal deep learning.
\newblock In {\em ICML}, 2011.

\bibitem{owens2018audio}
Andrew Owens and Alexei~A Efros.
\newblock Audio-visual scene analysis with self-supervised multisensory
  features.
\newblock In {\em ECCV}, 2018.

\bibitem{park2019specaugment}
Daniel~S Park, William Chan, Yu~Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin~D
  Cubuk, and Quoc~V Le.
\newblock Specaugment: A simple data augmentation method for automatic speech
  recognition.
\newblock {\em arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{et_iccv2021}
Alexander Pashevich, Cordelia Schmid, and Chen Sun.
\newblock Episodic transformer for vision-and-language navigation.
\newblock In {\em ICCV}, 2021.

\bibitem{qiu2019learning}
Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei.
\newblock Learning spatio-temporal representation with local and global
  diffusion.
\newblock In {\em CVPR}, 2019.

\bibitem{ramachandram2017deep}
Dhanesh Ramachandram and Graham~W Taylor.
\newblock Deep multimodal learning: A survey on recent advances and trends.
\newblock {\em IEEE Signal Processing Magazine}, 34(6):96--108, 2017.

\bibitem{ryoo2019assemblenet}
Michael~S Ryoo, AJ~Piergiovanni, Mingxing Tan, and Anelia Angelova.
\newblock Assemblenet: Searching for multi-stream neural connectivity in video
  architectures.
\newblock {\em arXiv preprint arXiv:1905.13209}, 2019.

\bibitem{salamon2017deep}
Justin Salamon and Juan~Pablo Bello.
\newblock Deep convolutional neural networks and data augmentation for
  environmental sound classification.
\newblock {\em IEEE Signal Processing Letters}, 24(3):279--283, 2017.

\bibitem{seo2021look}
Paul~Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.
\newblock Look before you speak: Visually contextualized utterances.
\newblock In {\em CVPR}, 2021.

\bibitem{smith2005development}
Linda Smith and Michael Gasser.
\newblock The development of embodied cognition: Six lessons from babies.
\newblock {\em Artificial life}, 11(1-2):13--29, 2005.

\bibitem{sun2019learning}
Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid.
\newblock Learning video representations using contrastive bidirectional
  transformer.
\newblock {\em arXiv preprint arXiv:1906.05743}, 2019.

\bibitem{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In {\em ICCV}, 2019.

\bibitem{tzinis2020into}
Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez,
  Daniel~PW Ellis, and John~R Hershey.
\newblock Into the wild with audioscope: Unsupervised audio-visual separation
  of on-screen sounds.
\newblock {\em arXiv preprint arXiv:2011.01143}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{wang2016temporal}
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu~Qiao, Dahua Lin, Xiaoou Tang, and Luc
  Van~Gool.
\newblock Temporal segment networks: Towards good practices for deep action
  recognition.
\newblock In {\em ECCV}. Springer, 2016.

\bibitem{wang2020makes}
Weiyao Wang, Du~Tran, and Matt Feiszli.
\newblock What makes training multi-modal classification networks hard?
\newblock In {\em CVPR}, pages 12695--12705, 2020.

\bibitem{xiao2020audiovisual}
Fanyi Xiao, Yong~Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph
  Feichtenhofer.
\newblock Audiovisual slowfast networks for video recognition.
\newblock {\em arXiv preprint arXiv:2001.08740}, 2020.

\bibitem{xie2018rethinking}
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
\newblock Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
  in video classification.
\newblock In {\em ECCV}, 2018.

\bibitem{ye2019cross}
Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
\newblock Cross-modal self-attention network for referring image segmentation.
\newblock In {\em CVPR}, 2019.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhou2018temporal}
Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba.
\newblock Temporal relational reasoning in videos.
\newblock In {\em ECCV}, pages 803--818, 2018.

\end{thebibliography}
