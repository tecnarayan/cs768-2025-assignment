\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abu-Mostafa(1989)]{abu1989information}
Abu-Mostafa, Y.~S.
\newblock Information theory, complexity and neural networks.
\newblock \emph{IEEE Communications Magazine}, 27\penalty0 (11):\penalty0
  25--28, 1989.

\bibitem[Banino et~al.(2020)Banino, Badia, K{\"o}ster, Chadwick, Zambaldi,
  Hassabis, Barry, Botvinick, Kumaran, and Blundell]{banino2020memo}
Banino, A., Badia, A.~P., K{\"o}ster, R., Chadwick, M.~J., Zambaldi, V.,
  Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., and Blundell, C.
\newblock Memo: A deep network for flexible combination of episodic memories.
\newblock \emph{arXiv preprint arXiv:2001.10913}, 2020.

\bibitem[Boll{\'e} et~al.(2000)Boll{\'e}, Dominguez, and
  Amari]{bolle2000mutual}
Boll{\'e}, D., Dominguez, D., and Amari, S.-i.
\newblock Mutual information of sparsely coded associative memory with
  self-control and ternary neurons.
\newblock \emph{Neural Networks}, 13\penalty0 (4-5):\penalty0 455--462, 2000.

\bibitem[Chaudhuri \& Fiete(2019)Chaudhuri and Fiete]{chaudhuri2019bipartite}
Chaudhuri, R. and Fiete, I.
\newblock Bipartite expander hopfield networks as self-decoding high-capacity
  error correcting codes.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Crisanti et~al.(1986)Crisanti, Amit, and
  Gutfreund]{crisanti1986saturation}
Crisanti, A., Amit, D.~J., and Gutfreund, H.
\newblock Saturation level of the hopfield model for neural network.
\newblock \emph{EPL (Europhysics Letters)}, 2\penalty0 (4):\penalty0 337, 1986.

\bibitem[Demircigil et~al.(2017)Demircigil, Heusel, L{\"o}we, Upgang, and
  Vermet]{demircigil2017model}
Demircigil, M., Heusel, J., L{\"o}we, M., Upgang, S., and Vermet, F.
\newblock On a model of associative memory with huge storage capacity.
\newblock \emph{Journal of Statistical Physics}, 168\penalty0 (2):\penalty0
  288--299, 2017.

\bibitem[Dominguez et~al.(2007)Dominguez, Koroutchev, Serrano, and
  Rodr{\'\i}guez]{dominguez2007information}
Dominguez, D., Koroutchev, K., Serrano, E., and Rodr{\'\i}guez, F.~B.
\newblock Information and topology in attractor neural networks.
\newblock \emph{Neural computation}, 19\penalty0 (4):\penalty0 956--973, 2007.

\bibitem[Dragoi \& Tonegawa(2011)Dragoi and Tonegawa]{dragoi2011preplay}
Dragoi, G. and Tonegawa, S.
\newblock Preplay of future place cell sequences by hippocampal cellular
  assemblies.
\newblock \emph{Nature}, 469\penalty0 (7330):\penalty0 397--401, 2011.

\bibitem[Eichenbaum \& Cohen(2014)Eichenbaum and Cohen]{eichenbaum2014can}
Eichenbaum, H. and Cohen, N.~J.
\newblock Can we reconcile the declarative memory and spatial navigation views
  on hippocampal function?
\newblock \emph{Neuron}, 83\penalty0 (4):\penalty0 764--770, 2014.

\bibitem[Frolov \& H{\'u}sek(2000)Frolov and H{\'u}sek]{frolov2000convergence}
Frolov, A.~A. and H{\'u}sek, D.
\newblock Convergence time in hopfield network.
\newblock In \emph{Proceedings of the IEEE-INNS-ENNS International Joint
  Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges
  and Perspectives for the New Millennium}, volume~5, pp.\  622--626. IEEE,
  2000.

\bibitem[Fusi \& Abbott(2007)Fusi and Abbott]{fusi2007limits}
Fusi, S. and Abbott, L.
\newblock Limits on the memory storage capacity of bounded synapses.
\newblock \emph{Nature neuroscience}, 10\penalty0 (4):\penalty0 485--493, 2007.

\bibitem[Gardner(1988)]{gardner1988space}
Gardner, E.
\newblock The space of interactions in neural network models.
\newblock \emph{Journal of physics A: Mathematical and general}, 21\penalty0
  (1):\penalty0 257, 1988.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwi{\'n}ska, Colmenarejo, Grefenstette, Ramalho, Agapiou,
  et~al.]{graves2016hybrid}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
  Grabska-Barwi{\'n}ska, A., Colmenarejo, S.~G., Grefenstette, E., Ramalho, T.,
  Agapiou, J., et~al.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471--476, 2016.

\bibitem[Hertz et~al.(2018)Hertz, Krogh, and Palmer]{hertz2018introduction}
Hertz, J., Krogh, A., and Palmer, R.~G.
\newblock \emph{Introduction to the theory of neural computation}.
\newblock CRC Press, 2018.

\bibitem[Hopfield(1982)]{hopfield1982neural}
Hopfield, J.~J.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock \emph{Proceedings of the national academy of sciences}, 79\penalty0
  (8):\penalty0 2554--2558, 1982.

\bibitem[Hopfield(1984)]{hopfield1984neurons}
Hopfield, J.~J.
\newblock Neurons with graded response have collective computational properties
  like those of two-state neurons.
\newblock \emph{Proceedings of the national academy of sciences}, 81\penalty0
  (10):\penalty0 3088--3092, 1984.

\bibitem[Jaeger(2001)]{jaeger2001echo}
Jaeger, H.
\newblock The “echo state” approach to analysing and training recurrent
  neural networks-with an erratum note.
\newblock \emph{Bonn, Germany: German National Research Center for Information
  Technology GMD Technical Report}, 148\penalty0 (34):\penalty0 13, 2001.

\bibitem[Kanter \& Sompolinsky(1987)Kanter and
  Sompolinsky]{kanter1987associative}
Kanter, I. and Sompolinsky, H.
\newblock Associative recall of memory without errors.
\newblock \emph{Physical Review A}, 35\penalty0 (1):\penalty0 380, 1987.

\bibitem[Kohring(1990)]{kohring1990convergence}
Kohring, G.
\newblock Convergence time and finite size effects in neural networks.
\newblock \emph{Journal of Physics A: Mathematical and General}, 23\penalty0
  (11):\penalty0 2237, 1990.

\bibitem[Krotov \& Hopfield(2020)Krotov and Hopfield]{krotov2020large}
Krotov, D. and Hopfield, J.
\newblock Large associative memory problem in neurobiology and machine
  learning.
\newblock \emph{arXiv preprint arXiv:2008.06996}, 2020.

\bibitem[Krotov \& Hopfield(2016)Krotov and Hopfield]{krotov2016dense}
Krotov, D. and Hopfield, J.~J.
\newblock Dense associative memory for pattern recognition.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 1172--1180, 2016.

\bibitem[Le et~al.(2019)Le, Tran, and Venkatesh]{le2019neural}
Le, H., Tran, T., and Venkatesh, S.
\newblock Neural stored-program memory.
\newblock \emph{arXiv preprint arXiv:1906.08862}, 2019.

\bibitem[Luko{\v{s}}evi{\v{c}}ius \& Jaeger(2009)Luko{\v{s}}evi{\v{c}}ius and
  Jaeger]{lukovsevivcius2009reservoir}
Luko{\v{s}}evi{\v{c}}ius, M. and Jaeger, H.
\newblock Reservoir computing approaches to recurrent neural network training.
\newblock \emph{Computer Science Review}, 3\penalty0 (3):\penalty0 127--149,
  2009.

\bibitem[Majani et~al.(1988)Majani, Erlanson, and Abu-Mostafa]{majani1988k}
Majani, E., Erlanson, R., and Abu-Mostafa, Y.
\newblock On the k-winners-take-all network.
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Manns \& Eichenbaum(2006)Manns and Eichenbaum]{manns2006evolution}
Manns, J.~R. and Eichenbaum, H.
\newblock Evolution of declarative memory.
\newblock \emph{Hippocampus}, 16\penalty0 (9):\penalty0 795--808, 2006.

\bibitem[Mulders et~al.(2021)Mulders, Yim, Lee, Lee, Taillefumier, and
  Fiete]{Mulders2021.11.20.469406}
Mulders, D., Yim, M.~Y., Lee, J.~S., Lee, A.~K., Taillefumier, T., and Fiete,
  I.~R.
\newblock A structured scaffold underlies activity in the hippocampus.
\newblock \emph{bioRxiv}, 2021.

\bibitem[Nadal et~al.(1986)Nadal, Toulouse, Changeux, and
  Dehaene]{nadal1986networks}
Nadal, J., Toulouse, G., Changeux, J., and Dehaene, S.
\newblock Networks of formal neurons and memory palimpsests.
\newblock \emph{EPL (Europhysics Letters)}, 1\penalty0 (10):\penalty0 535,
  1986.

\bibitem[Parisi(1986)]{parisi1986memory}
Parisi, G.
\newblock A memory which forgets.
\newblock \emph{Journal of Physics A: Mathematical and General}, 19\penalty0
  (10):\penalty0 L617, 1986.

\bibitem[Personnaz et~al.(1985)Personnaz, Guyon, and
  Dreyfus]{personnaz1985information}
Personnaz, L., Guyon, I., and Dreyfus, G.
\newblock Information storage and retrieval in spin-glass like neural networks.
\newblock \emph{Journal de Physique Lettres}, 46\penalty0 (8):\penalty0
  359--365, 1985.

\bibitem[Personnaz et~al.(1986)Personnaz, Guyon, and
  Dreyfus]{personnaz1986collective}
Personnaz, L., Guyon, I., and Dreyfus, G.
\newblock Collective computational properties of neural networks: New learning
  mechanisms.
\newblock \emph{Physical Review A}, 34\penalty0 (5):\penalty0 4217, 1986.

\bibitem[Radhakrishnan et~al.(2020)Radhakrishnan, Belkin, and
  Uhler]{radhakrishnan2020overparameterized}
Radhakrishnan, A., Belkin, M., and Uhler, C.
\newblock Overparameterized neural networks implement associative memory.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (44):\penalty0 27162--27170, 2020.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Adler, Gruber, Holzleitner, Pavlovi{\'c}, Sandve,
  et~al.]{ramsauer2020hopfield}
Ramsauer, H., Sch{\"a}fl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T.,
  Gruber, L., Holzleitner, M., Pavlovi{\'c}, M., Sandve, G.~K., et~al.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Refregier \& Vignolle(1989)Refregier and
  Vignolle]{refregier1989improved}
Refregier, P. and Vignolle, J.-M.
\newblock An improved version of the pseudo-inverse solution for classification
  and neural networks.
\newblock \emph{EPL (Europhysics Letters)}, 10\penalty0 (4):\penalty0 387,
  1989.

\bibitem[Rutishauser et~al.(2011)Rutishauser, Douglas, and
  Slotine]{rutishauser2011collective}
Rutishauser, U., Douglas, R.~J., and Slotine, J.-J.
\newblock Collective stability of networks of winner-take-all circuits.
\newblock \emph{Neural computation}, 23\penalty0 (3):\penalty0 735--773, 2011.

\bibitem[Storkey(1997)]{storkey1997increasing}
Storkey, A.
\newblock Increasing the capacity of a hopfield network without sacrificing
  functionality.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  451--456. Springer, 1997.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R.
\newblock End-to-end memory networks.
\newblock \emph{arXiv preprint arXiv:1503.08895}, 2015.

\bibitem[Tapson \& van Schaik(2013)Tapson and van Schaik]{tapson2013learning}
Tapson, J. and van Schaik, A.
\newblock Learning the pseudoinverse solution to network weights.
\newblock \emph{Neural Networks}, 45:\penalty0 94--100, 2013.

\bibitem[Tsodyks \& Feigel'man(1988)Tsodyks and
  Feigel'man]{tsodyks1988enhanced}
Tsodyks, M.~V. and Feigel'man, M.~V.
\newblock The enhanced storage capacity in neural networks with low activity
  level.
\newblock \emph{EPL (Europhysics Letters)}, 6\penalty0 (2):\penalty0 101, 1988.

\bibitem[Tyulmankov et~al.(2021)Tyulmankov, Fang, Vadaparty, and
  Yang]{tyulmankov2021biological}
Tyulmankov, D., Fang, C., Vadaparty, A., and Yang, G.~R.
\newblock Biological learning in key-value memory networks.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=sMRdrUIrZbT}.

\bibitem[Van~Rossum et~al.(2012)Van~Rossum, Shippi, and Barrett]{van2012soft}
Van~Rossum, M.~C., Shippi, M., and Barrett, A.~B.
\newblock Soft-bound synaptic plasticity increases storage capacity.
\newblock \emph{PLoS computational biology}, 8\penalty0 (12):\penalty0
  e1002836, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang \& Slotine(2003)Wang and Slotine]{wang2003k}
Wang, W. and Slotine, J.-J.~E.
\newblock K-winners-take-all computation with neural oscillators.
\newblock \emph{arXiv preprint q-bio/0401001}, 2003.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yang \& Chen(1997)Yang and Chen]{yang1997dynamic}
Yang, J.-F. and Chen, C.-M.
\newblock A dynamic k-winners-take-all neural network.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 27\penalty0 (3):\penalty0 523--526, 1997.

\bibitem[Yim et~al.(2021)Yim, Sadun, Fiete, and
  Taillefumier]{10.7554/eLife.62702}
Yim, M.~Y., Sadun, L.~A., Fiete, I.~R., and Taillefumier, T.
\newblock Place-cell capacity and volatility with grid-like inputs.
\newblock \emph{eLife}, 10:\penalty0 e62702, may 2021.
\newblock ISSN 2050-084X.
\newblock \doi{10.7554/eLife.62702}.
\newblock URL \url{https://doi.org/10.7554/eLife.62702}.

\end{thebibliography}
