\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{abdelfattah2020zero}
M.~S. Abdelfattah, A.~Mehrotra, {\L}.~Dudziak, and N.~D. Lane.
\newblock Zero-cost proxies for lightweight nas.
\newblock In {\em ICLR}, 2021.

\bibitem{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em ICML}, pages 242--252. PMLR, 2019.

\bibitem{bergstra2011algorithms}
J.~Bergstra, R.~Bardenet, Y.~Bengio, and B.~K{\'e}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock In {\em NeurIPS}, volume~24, 2011.

\bibitem{brutzkus2017globally}
A.~Brutzkus and A.~Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em ICML}, pages 605--614. PMLR, 2017.

\bibitem{chen2018searching}
L.-C. Chen, M.~Collins, Y.~Zhu, G.~Papandreou, B.~Zoph, F.~Schroff, H.~Adam,
  and J.~Shlens.
\newblock Searching for efficient multi-scale architectures for dense image
  prediction.
\newblock In {\em NeurIPS}, volume~31, 2018.

\bibitem{chen2021neural}
W.~Chen, X.~Gong, and Z.~Wang.
\newblock Neural architecture search on imagenet in four gpu hours: A
  theoretically inspired perspective.
\newblock In {\em ICLR}, 2020.

\bibitem{das2021data}
D.~Das, Y.~Bhalgat, and F.~Porikli.
\newblock Data-driven weight initialization with sylvester solvers.
\newblock {\em arXiv preprint arXiv:2105.10335}, 2021.

\bibitem{dauphin2019metainit}
Y.~N. Dauphin and S.~Schoenholz.
\newblock Metainit: Initializing learning by learning to initialize.
\newblock In {\em NeurIPS}, volume~32, 2019.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, pages 248--255, 2009.

\bibitem{devries2017improved}
T.~DeVries and G.~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2020.

\bibitem{du2018gradient}
S.~Du, J.~Lee, Y.~Tian, A.~Singh, and B.~Poczos.
\newblock Gradient descent learns one-hidden-layer cnn: Donâ€™t be afraid of
  spurious local minima.
\newblock In {\em ICML}, pages 1339--1348. PMLR, 2018.

\bibitem{feurer2019hyperparameter}
M.~Feurer and F.~Hutter.
\newblock Hyperparameter optimization.
\newblock In {\em Automated machine learning}, pages 3--33. Springer, Cham,
  2019.

\bibitem{ge2017learning}
R.~Ge, J.~D. Lee, and T.~Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em ICLR}, 2018.

\bibitem{gilmer2021loss}
J.~Gilmer, B.~Ghorbani, A.~Garg, S.~Kudugunta, B.~Neyshabur, D.~Cardoze,
  G.~Dahl, Z.~Nado, and O.~Firat.
\newblock A loss curvature perspective on training instability in deep
  learning.
\newblock {\em arXiv preprint arXiv:2110.04369}, 2021.

\bibitem{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em AISTATS}, pages 249--256, 2010.

\bibitem{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em ICCV}, pages 1026--1034, 2015.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, pages 4700--4708, 2017.

\bibitem{huang2020explicitly}
T.~Huang, S.~You, Y.~Yang, Z.~Tu, F.~Wang, C.~Qian, and C.~Zhang.
\newblock Explicitly learning topology for differentiable neural architecture
  search.
\newblock {\em arXiv preprint arXiv:2011.09300}, 2020.

\bibitem{huang2020improving}
X.~S. Huang, F.~Perez, J.~Ba, and M.~Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In {\em ICML}, pages 4475--4483. PMLR, 2020.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, pages 448--456. PMLR, 2015.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em NeurIPS}, volume~31, 2018.

\bibitem{kingma2015adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lee2018snip}
N.~Lee, T.~Ajanthan, and P.~H. Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em ICLR}, 2019.

\bibitem{li2018optimization}
H.~Li, Y.~Yang, D.~Chen, and Z.~Lin.
\newblock Optimization algorithm inspired deep neural network structure design.
\newblock In {\em ACML}, 2018.

\bibitem{li2017convergence}
Y.~Li and Y.~Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em NeurIPS}, volume~30, 2017.

\bibitem{liu2019variance}
L.~Liu, H.~Jiang, P.~He, W.~Chen, X.~Liu, J.~Gao, and J.~Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In {\em ICLR}, 2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em ICCV}, pages 10012--10022, 2021.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In {\em ICLR}, 2017.

\bibitem{mellor2021neural}
J.~Mellor, J.~Turner, A.~Storkey, and E.~J. Crowley.
\newblock Neural architecture search without training.
\newblock In {\em ICML}, pages 7588--7598. PMLR, 2021.

\bibitem{meuleau2002ant}
N.~Meuleau and M.~Dorigo.
\newblock Ant colony optimization and stochastic gradient descent.
\newblock {\em Artificial life}, 8(2):103--121, 2002.

\bibitem{mishkin2015all}
D.~Mishkin and J.~Matas.
\newblock All you need is a good init.
\newblock {\em arXiv preprint arXiv:1511.06422}, 2015.

\bibitem{oymak2019overparameterized}
S.~Oymak and M.~Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In {\em ICML}, pages 4951--4960. PMLR, 2019.

\bibitem{real2019regularized}
E.~Real, A.~Aggarwal, Y.~Huang, and Q.~V. Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em AAAI}, volume~33, pages 4780--4789, 2019.

\bibitem{ruder2016overview}
S.~Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em arXiv preprint arXiv:1609.04747}, 2016.

\bibitem{saxe2013exact}
A.~M. Saxe, J.~L. McClelland, and S.~Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}, 2013.

\bibitem{shu2021nasi}
Y.~Shu, S.~Cai, Z.~Dai, B.~C. Ooi, and B.~K.~H. Low.
\newblock Nasi: Label-and data-agnostic neural architecture search at
  initialization.
\newblock In {\em ICLR}, 2022.

\bibitem{simon2021neural}
J.~B. Simon, M.~Dickens, and M.~R. DeWeese.
\newblock Neural tangent kernel eigenvalues accurately predict generalization.
\newblock {\em arXiv preprint arXiv:2110.03922}, 2021.

\bibitem{soltanolkotabi2017learning}
M.~Soltanolkotabi.
\newblock Learning relus via gradient descent.
\newblock In {\em NeurIPS}, volume~30, 2017.

\bibitem{tanaka2020pruning}
H.~Tanaka, D.~Kunin, D.~L. Yamins, and S.~Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In {\em NeurIPS}, volume~33, pages 6377--6389, 2020.

\bibitem{theis2018faster}
L.~Theis, I.~Korshunova, A.~Tejani, and F.~Husz{\'a}r.
\newblock Faster gaze prediction with dense networks and fisher pruning.
\newblock {\em arXiv preprint arXiv:1801.05787}, 2018.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em ICML}, pages 10347--10357. PMLR, 2021.

\bibitem{turner2019blockswap}
J.~Turner, E.~J. Crowley, M.~O'Boyle, A.~Storkey, and G.~Gray.
\newblock Blockswap: Fisher-guided block substitution for network compression
  on a budget.
\newblock In {\em ICLR}, 2020.

\bibitem{turner2021neural}
J.~Turner, E.~J. Crowley, and M.~F. O'Boyle.
\newblock Neural architecture search as program transformation exploration.
\newblock In {\em Proceedings of the 26th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  915--927, 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, pages 5998--6008, 2017.

\bibitem{wang2020picking}
C.~Wang, G.~Zhang, and R.~Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em ICLR}, 2020.

\bibitem{xiao2020disentangling}
L.~Xiao, J.~Pennington, and S.~Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In {\em ICML}, pages 10462--10472. PMLR, 2020.

\bibitem{xiong2020layer}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.~Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In {\em ICML}, pages 10524--10533. PMLR, 2020.

\bibitem{xu2021vitae}
Y.~Xu, Q.~Zhang, J.~Zhang, and D.~Tao.
\newblock Vitae: Vision transformer advanced by exploring intrinsic inductive
  bias.
\newblock In {\em NeurIPS}, volume~34, 2021.

\bibitem{yang2020ista}
Y.~Yang, H.~Li, S.~You, F.~Wang, C.~Qian, and Z.~Lin.
\newblock Ista-nas: Efficient and consistent neural architecture search by
  sparse coding.
\newblock In {\em NeurIPS}, 2020.

\bibitem{yang2021towards}
Y.~Yang, S.~You, H.~Li, F.~Wang, C.~Qian, and Z.~Lin.
\newblock Towards improving the consistency, efficiency, and flexibility of
  differentiable neural architecture search.
\newblock In {\em CVPR}, 2021.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{zhang2019fixup}
H.~Zhang, Y.~N. Dauphin, and T.~Ma.
\newblock Residual learning without normalization via better initialization.
\newblock In {\em ICLR}, 2019.

\bibitem{zhang2021gradsign}
Z.~Zhang and Z.~Jia.
\newblock Gradsign: Model performance inference with theoretical insights.
\newblock In {\em ICLR}, 2022.

\bibitem{zhu2021gradinit}
C.~Zhu, R.~Ni, Z.~Xu, K.~Kong, W.~R. Huang, and T.~Goldstein.
\newblock Gradinit: Learning to initialize neural networks for stable and
  efficient training.
\newblock In {\em NeurIPS}, volume~34, 2021.

\bibitem{zhuang2020adabelief}
J.~Zhuang, T.~Tang, Y.~Ding, S.~C. Tatikonda, N.~Dvornek, X.~Papademetris, and
  J.~Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock In {\em NeurIPS}, volume~33, pages 18795--18806, 2020.

\bibitem{zoph2016neural}
B.~Zoph and Q.~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em ICLR}, 2017.

\end{thebibliography}
