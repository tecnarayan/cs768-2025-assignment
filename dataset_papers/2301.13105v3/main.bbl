\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2021)Abbe, Boix-Adsera, Brennan, Bresler, and
  Nagaraj]{abbe2021staircase}
Emmanuel Abbe, Enric Boix-Adsera, Matthew Brennan, Guy Bresler, and Dheeraj
  Nagaraj.
\newblock The staircase property: How hierarchical structure can guide deep
  learning, {N}eur{I}{P}{S}, 2021.

\bibitem[Abbe et~al.(2022{\natexlab{a}})Abbe, Bengio, Cornacchia, Kleinberg,
  Lotfi, Raghu, and Zhang]{abbe2022learning}
Emmanuel Abbe, Samy Bengio, Elisabetta Cornacchia, Jon Kleinberg, Aryo Lotfi,
  Maithra Raghu, and Chiyuan Zhang.
\newblock Learning to reason with neural networks: Generalization, unseen data
  and boolean measures.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 2709--2722, 2022{\natexlab{a}}.

\bibitem[Abbe et~al.(2022{\natexlab{b}})Abbe, Boix-Adsera, and
  Misiakiewicz]{mergedstaircase}
Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient
  condition for {SGD} learning of sparse functions on two-layer neural
  networks, {COLT}, 2022{\natexlab{b}}.

\bibitem[Abbe et~al.(2022{\natexlab{c}})Abbe, Cornacchia, Hazla, and
  Marquis]{AbbeINAL}
Emmanuel Abbe, Elisabetta Cornacchia, Jan Hazla, and Christopher Marquis.
\newblock An initial alignment between neural network and target is needed for
  gradient descent to learn.
\newblock In \emph{International Conference on Machine Learning}, pages 33--52.
  PMLR, 2022{\natexlab{c}}.

\bibitem[Abbe et~al.(2023{\natexlab{a}})Abbe, Bengio, Lotfi, and
  Rizk]{icml-version}
Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk.
\newblock Generalization on the unseen, logic reasoning and degree curriculum.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pages 31--60. PMLR, 23--29
  Jul 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/abbe23a.html}.

\bibitem[Abbe et~al.(2023{\natexlab{b}})Abbe, Cornacchia, and
  Lotfi]{abbe2023provable}
Emmanuel Abbe, Elisabetta Cornacchia, and Aryo Lotfi.
\newblock Provable advantage of curriculum learning on parity targets with
  mixed inputs.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 24291--24321, 2023{\natexlab{b}}.

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and
  Zhai]{alabdulmohsin2022revisiting}
Ibrahim~M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 22300--22312, 2022.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh,
  Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring-length}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay
  Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 38546--38556, 2022.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bakhtin et~al.(2019)Bakhtin, van~der Maaten, Johnson, Gustafson, and
  Girshick]{bakhtin2019phyre}
Anton Bakhtin, Laurens van~der Maaten, Justin Johnson, Laura Gustafson, and
  Ross Girshick.
\newblock Phyre: A new benchmark for physical reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Ben-David et~al.(2006)Ben-David, Blitzer, Crammer, and
  Pereira]{ben2006analysis}
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{curriculum}
Yoshua Bengio, J\'{e}r\^{o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, ICML '09, page 41–48, New York, NY, USA, 2009.
  Association for Computing Machinery.
\newblock ISBN 9781605585161.
\newblock \doi{10.1145/1553374.1553380}.
\newblock URL \url{https://doi.org/10.1145/1553374.1553380}.

\bibitem[Carlini et~al.(2019)Carlini, Liu, Erlingsson, Kos, and
  Song]{carlini2019secret-mem1}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock The secret sharer: Evaluating and testing unintended memorization in
  neural networks.
\newblock In \emph{USENIX Security Symposium}, volume 267, 2019.

\bibitem[Carlini et~al.(2023)Carlini, Ippolito, Jagielski, Lee, Tramer, and
  Zhang]{carlini2022quantifying-mem4}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=TatRHT_1cK}.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Cornacchia and Mossel(2023)]{cornacchia2023mathematical}
Elisabetta Cornacchia and Elchanan Mossel.
\newblock A mathematical model for curriculum learning for parities.
\newblock In \emph{International Conference on Machine Learning}, pages
  6402--6423. PMLR, 2023.

\bibitem[Cox et~al.(2013)Cox, Little, and OShea]{cox2013ideals}
David Cox, John Little, and Donal OShea.
\newblock \emph{Ideals, varieties, and algorithms: an introduction to
  computational algebraic geometry and commutative algebra}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Csord\'as et~al.(2021)Csord\'as, Irie, and
  Schmidhuber]{csordas2021devil}
R\'obert Csord\'as, Kazuki Irie, and J\"urgen Schmidhuber.
\newblock The devil is in the detail: Simple tricks improve systematic
  generalization of transformers.
\newblock In \emph{Proc. Conf. on Empirical Methods in Natural Language
  Processing (EMNLP)}, Punta Cana, Dominican Republic, November 2021.

\bibitem[Daniely and Malach(2020)]{malach_parity}
Amit Daniely and Eran Malach.
\newblock Learning parities with neural networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 20356--20365. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/eaae5e04a259d09af85c108fe4d7dd0c-Paper.pdf}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Dummit and Foote(2004)]{dummit2004abstract}
David~Steven Dummit and Richard~M Foote.
\newblock \emph{Abstract algebra}, volume~3.
\newblock Wiley Hoboken, 2004.

\bibitem[Feldman and Zhang(2020)]{feldman2020neural-mem2}
Vitaly Feldman and Chiyuan Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via
  influence estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2881--2891, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Gulrajani and Lopez-Paz(2021)]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=lQdXeXDoWtI}.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Advances in neural information processing systems}, 31,
  2018{\natexlab{b}}.

\bibitem[Hartford et~al.(2018)Hartford, Graham, Leyton-Brown, and
  Ravanbakhsh]{hartford2018deep}
Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh.
\newblock Deep models of interactions across sets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1909--1918. PMLR, 2018.

\bibitem[Hupkes et~al.(2020)Hupkes, Dankers, Mul, and
  Bruni]{hupkes2020compositionality}
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni.
\newblock Compositionality decomposed: How do neural networks generalise?
\newblock \emph{Journal of Artificial Intelligence Research}, 67:\penalty0
  757--795, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jacot et~al.(2021)Jacot, Ged, {\c{S}}im{\c{s}}ek, Hongler, and
  Gabriel]{jacot2021saddle}
Arthur Jacot, Fran{\c{c}}ois Ged, Berfin {\c{S}}im{\c{s}}ek, Cl{\'e}ment
  Hongler, and Franck Gabriel.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small
  initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Johnson et~al.(2017)Johnson, Hariharan, Van Der~Maaten, Fei-Fei,
  Lawrence~Zitnick, and Girshick]{johnson2017clevr}
Justin Johnson, Bharath Hariharan, Laurens Van Der~Maaten, Li~Fei-Fei,
  C~Lawrence~Zitnick, and Ross Girshick.
\newblock Clevr: A diagnostic dataset for compositional language and elementary
  visual reasoning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2901--2910, 2017.

\bibitem[Kandpal et~al.(2022)Kandpal, Wallace, and
  Raffel]{kandpal2022deduplicating-mem3}
Nikhil Kandpal, Eric Wallace, and Colin Raffel.
\newblock Deduplicating training data mitigates privacy risks in language
  models.
\newblock In \emph{International Conference on Machine Learning}, pages
  10697--10707. PMLR, 2022.

\bibitem[Kazemnejad et~al.(2024)Kazemnejad, Padhi, Natesan~Ramamurthy, Das, and
  Reddy]{kazemnejad2023impact}
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan~Ramamurthy, Payel Das,
  and Siva Reddy.
\newblock The impact of positional encoding on length generalization in
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Kocmi and Bojar(2017)]{nlp1}
Tom Kocmi and Ond{\v{r}}ej Bojar.
\newblock Curriculum learning and minibatch bucketing in neural machine
  translation.
\newblock In Ruslan Mitkov and Galia Angelova, editors, \emph{Proceedings of
  the International Conference Recent Advances in Natural Language Processing,
  {RANLP} 2017}, pages 379--386, Varna, Bulgaria, September 2017. INCOMA Ltd.
\newblock \doi{10.26615/978-954-452-049-6_050}.
\newblock URL \url{https://doi.org/10.26615/978-954-452-049-6_050}.

\bibitem[Lake and Baroni(2018)]{lake2018generalization}
Brenden Lake and Marco Baroni.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In \emph{International conference on machine learning}, pages
  2873--2882. PMLR, 2018.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo,
  et~al.]{lewkowycz2022solving-minerva}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
  Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
  Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 3843--3857, 2022.

\bibitem[Mahdavi et~al.(2023)Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis,
  and Liao]{mahdavi2022better}
Sadegh Mahdavi, Kevin Swersky, Thomas Kipf, Milad Hashemi, Christos
  Thrampoulidis, and Renjie Liao.
\newblock Towards better out-of-distribution generalization of neural
  algorithmic reasoning tasks.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=xkrtvHlp3P}.

\bibitem[Malach et~al.(2021)Malach, Kamath, Abbe, and Srebro]{quantifying}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent
  kernels.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 7379--7389. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/malach21a.html}.

\bibitem[Mansour et~al.(2009)Mansour, Mohri, and
  Rostamizadeh]{mansour2009domain}
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Domain adaptation: Learning bounds and algorithms.
\newblock In \emph{Proceedings of The 22nd Annual Conference on Learning Theory
  (COLT 2009)}, Montr\'eal, Canada, 2009.
\newblock URL \url{http://www.cs.nyu.edu/~mohri/postscript/nadap.pdf}.

\bibitem[Mei and Montanari(2022)]{mei2022generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75\penalty0
  (4):\penalty0 667--766, 2022.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\bibitem[M{\"o}ller and Buchberger(1982)]{moller1982construction}
H~Michael M{\"o}ller and Bruno Buchberger.
\newblock The construction of multivariate polynomials with preassigned zeros.
\newblock In \emph{European Computer Algebra Conference}, pages 24--31.
  Springer, 1982.

\bibitem[Moroshko et~al.(2020)Moroshko, Woodworth, Gunasekar, Lee, Srebro, and
  Soudry]{moroshko2020implicit}
Edward Moroshko, Blake~E Woodworth, Suriya Gunasekar, Jason~D Lee, Nati Srebro,
  and Daniel Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 22182--22193, 2020.

\bibitem[O'Donnell(2014)]{o'donnell_2014}
Ryan O'Donnell.
\newblock \emph{Analysis of Boolean Functions}.
\newblock Cambridge University Press, 2014.
\newblock \doi{10.1017/CBO9781139814782}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{torch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Platanios et~al.(2019)Platanios, Stretcu, Neubig, Poczos, and
  Mitchell]{nlp3}
Emmanouil~Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos,
  and Tom Mitchell.
\newblock Competence-based curriculum learning for neural machine translation.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 1162--1172, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1119}.
\newblock URL \url{https://aclanthology.org/N19-1119}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0
  (140):\penalty0 1--67, 2020.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5301--5310. PMLR, 2019.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Ravanbakhsh et~al.(2017)Ravanbakhsh, Schneider, and
  Poczos]{ravanbakhsh2017equivariance}
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
\newblock Equivariance through parameter-sharing.
\newblock In \emph{International conference on machine learning}, pages
  2892--2901. PMLR, 2017.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21174--21187, 2020.

\bibitem[Redko et~al.(2020)Redko, Morvant, Habrard, Sebban, and
  Bennani]{redko2020survey}
Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Youn{\`e}s
  Bennani.
\newblock A survey on domain adaptation theory: learning bounds and theoretical
  guarantees.
\newblock \emph{arXiv preprint arXiv:2004.11829}, 2020.

\bibitem[Saxton et~al.(2019)Saxton, Grefenstette, Hill, and
  Kohli]{saxton2019analysing}
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.
\newblock Analysing mathematical reasoning abilities of neural models.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1gR5iR5FX}.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth
  Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9573--9585, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2017implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (70):\penalty0 1--57, 2018.

\bibitem[Spitkovsky et~al.(2010)Spitkovsky, Alshawi, and
  Jurafsky]{spitkovsky2010baby}
Valentin~I Spitkovsky, Hiyan Alshawi, and Dan Jurafsky.
\newblock From baby steps to leapfrog: How “less is more” in unsupervised
  dependency parsing.
\newblock In \emph{Human Language Technologies: The 2010 Annual Conference of
  the North American Chapter of the Association for Computational Linguistics},
  pages 751--759, 2010.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention-transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2022)Veli{\v{c}}kovi{\'c}, Badia, Budden,
  Pascanu, Banino, Dashevskiy, Hadsell, and Blundell]{velivckovic2022clrs}
Petar Veli{\v{c}}kovi{\'c}, Adri{\`a}~Puigdom{\`e}nech Badia, David Budden,
  Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles
  Blundell.
\newblock The clrs algorithmic reasoning benchmark.
\newblock In \emph{International Conference on Machine Learning}, pages
  22084--22102. PMLR, 2022.

\bibitem[Weiss et~al.(2021)Weiss, Goldberg, and Yahav]{rasp}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock Thinking like transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  11080--11090. PMLR, 2021.

\bibitem[Wiles et~al.(2022)Wiles, Gowal, Stimberg, Rebuffi, Ktena, Dvijotham,
  and Cemgil]{wiles2022a}
Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira
  Ktena, Krishnamurthy~Dj Dvijotham, and Ali~Taylan Cemgil.
\newblock A fine-grained analysis on distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Dl4LetuLdyK}.

\bibitem[Xu et~al.(2019)Xu, Zhang, Luo, Xiao, and Ma]{xu2019frequency}
Zhi-Qin~John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma.
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks, 2019.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{yun2020unifying}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ZsZM-4iMQkH}.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ~R
  Salakhutdinov, and Alexander~J Smola.
\newblock Deep sets.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zaremba and Sutskever(2014)]{zaremba2014learning}
Wojciech Zaremba and Ilya Sutskever.
\newblock Learning to execute.
\newblock \emph{arXiv preprint arXiv:1410.4615}, 2014.

\bibitem[Zhang et~al.(2021)Zhang, Raghu, Kleinberg, and
  Bengio]{Zhang2021PointerVR}
Chiyuan Zhang, Maithra Raghu, Jon~M. Kleinberg, and Samy Bengio.
\newblock Pointer value retrieval: A new benchmark for understanding the limits
  of neural network generalization.
\newblock \emph{ArXiv}, abs/2107.12580, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar,
  and Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Knowles, and Finn]{zhou2020meta}
Allan Zhou, Tom Knowles, and Chelsea Finn.
\newblock Meta-learning symmetries by reparameterization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=-QxT4mJdijq}.

\bibitem[Zhou et~al.(2024)Zhou, Bradley, Littwin, Razin, Saremi, Susskind,
  Bengio, and Nakkiran]{zhou2023algorithms}
Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua~M.
  Susskind, Samy Bengio, and Preetum Nakkiran.
\newblock What algorithms can transformers learn? a study in length
  generalization.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=AssIuHnmHX}.

\end{thebibliography}
