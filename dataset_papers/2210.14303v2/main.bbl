\begin{thebibliography}{10}

\bibitem{LiYS018}
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu.
\newblock Diffusion convolutional recurrent neural network: Data-driven traffic
  forecasting.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem{PangYZZXT18}
Yue Pang, Bo~Yao, Xiangdong Zhou, Yong Zhang, Yiming Xu, and Zijing Tan.
\newblock Hierarchical electricity time series forecasting for integrating
  consumption patterns analysis and aggregation consistency.
\newblock In {\em Proc. the International Joint Conference on Artificial
  Intelligence (IJCAI)}, 2018.

\bibitem{DingZLD15}
Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
\newblock Deep learning for event-driven stock prediction.
\newblock In {\em Proc. the International Joint Conference on Artificial
  Intelligence (IJCAI)}, 2015.

\bibitem{MatsubaraSPF14}
Yasuko Matsubara, Yasushi Sakurai, Willem~G. van Panhuis, and Christos
  Faloutsos.
\newblock {FUNNEL:} automatic mining of spatially coevolving epidemics.
\newblock In {\em Proc. the ACM SIGKDD International Conference on Knowledge
  Discovery and Data Mining (KDD)}, 2014.

\bibitem{WuXWL21}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for
  long-term series forecasting.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem{liu2022pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X. Liu, and
  Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time
  series modeling and forecasting.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem{ZhouZPZLXZ21}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
  and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In {\em Proc. the AAAI Conference on Artificial Intelligence (AAAI)},
  2021.

\bibitem{LiJXZCWY19}
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu{-}Xiang Wang,
  and Xifeng Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of
  transformer on time series forecasting.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem{KitaevKL20}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{IshidaYS0S20}
Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama.
\newblock Do we need zero training loss after achieving zero training error?
\newblock In {\em Proc. the International Conference on Machine Learning
  (ICML)}, 2020.

\bibitem{GrillSATRBDPGAP20}
Jean{-}Bastien Grill, Florian Strub, Florent Altch{\'{e}}, Corentin Tallec,
  Pierre~H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~{\'{A}}vila
  Pires, Zhaohan Guo, Mohammad~Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,
  R{\'{e}}mi Munos, and Michal Valko.
\newblock Bootstrap your own latent - {A} new approach to self-supervised
  learning.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{He0WXG20}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross~B. Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proc. of the IEEE conference on computer vision and pattern
  recognition (CVPR)}, 2020.

\bibitem{TarvainenV17}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem{LaiCYL18}
Guokun Lai, Wei{-}Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long- and short-term temporal patterns with deep neural
  networks.
\newblock In {\em SIGIR}, 2018.

\bibitem{OreshkinCCB20}
Boris~N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio.
\newblock {N-BEATS:} neural basis expansion analysis for interpretable time
  series forecasting.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{abs-1803-01271}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock {\em CoRR}, abs/1803.01271, 2018.

\bibitem{Li0TSG18}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem{NamukHowDoVision}
Namuk Park and Songkuk Kim.
\newblock How do vision transformers work?
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem{XiangningWhenVision}
Xiangning Chen, Cho{-}Jui Hsieh, and Boqing Gong.
\newblock When vision transformers outperform resnets without pretraining or
  strong data augmentations.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem{box1974some}
George~EP Box, Gwilym~M Jenkins, and John~F MacGregor.
\newblock Some recent advances in forecasting and control.
\newblock {\em Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 1974.

\bibitem{taylor2018forecasting}
Sean~J Taylor and Benjamin Letham.
\newblock Forecasting at scale.
\newblock {\em The American Statistician}, 72(1):37--45, 2018.

\bibitem{durbin2012time}
James Durbin and Siem~Jan Koopman.
\newblock {\em Time series analysis by state space methods}, volume~38.
\newblock OUP Oxford, 2012.

\bibitem{hyndman2008forecasting}
Rob Hyndman, Anne~B Koehler, J~Keith Ord, and Ralph~D Snyder.
\newblock {\em Forecasting with exponential smoothing: the state space
  approach}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{QinSCCJC17}
Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison~W.
  Cottrell.
\newblock A dual-stage attention-based recurrent neural network for time series
  prediction.
\newblock In Carles Sierra, editor, {\em Proc. the International Joint
  Conference on Artificial Intelligence (IJCAI)}, 2017.

\bibitem{wen2017multi}
Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka.
\newblock A multi-horizon quantile recurrent forecaster.
\newblock {\em arXiv preprint arXiv:1711.11053}, 2017.

\bibitem{SongRTS18}
Huan Song, Deepta Rajan, Jayaraman~J. Thiagarajan, and Andreas Spanias.
\newblock Attend and diagnose: Clinical time series analysis using attention
  models.
\newblock In {\em Proc. the AAAI Conference on Artificial Intelligence (AAAI)},
  2018.

\bibitem{Stoller0ED20}
Daniel Stoller, Mi~Tian, Sebastian Ewert, and Simon Dixon.
\newblock Seq-u-net: {A} one-dimensional causal u-net for efficient sequence
  modelling.
\newblock In {\em Proc. the International Joint Conference on Artificial
  Intelligence (IJCAI)}, 2020.

\bibitem{OordDZSVGKSK16}
A{\"{a}}ron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew~W. Senior, and Koray
  Kavukcuoglu.
\newblock Wavenet: {A} generative model for raw audio.
\newblock {ISCA}, 2016.

\bibitem{SenYD19}
Rajat Sen, Hsiang{-}Fu Yu, and Inderjit~S. Dhillon.
\newblock Think globally, act locally: {A} deep neural network approach to
  high-dimensional time series forecasting.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem{RangapuramSGSWJ18}
Syama~Sundar Rangapuram, Matthias~W. Seeger, Jan Gasthaus, Lorenzo Stella,
  Yuyang Wang, and Tim Januschowski.
\newblock Deep state space models for time series forecasting.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem{BezenacRBBKSHGJ20}
Emmanuel de~B{\'{e}}zenac, Syama~Sundar Rangapuram, Konstantinos Benidis,
  Michael Bohlke{-}Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson,
  Patrick Gallinari, and Tim Januschowski.
\newblock Normalizing kalman filters for multivariate time series analysis.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{KlushynKSCS21}
Alexej Klushyn, Richard Kurle, Maximilian Soelch, Botond Cseke, and Patrick
  van~der Smagt.
\newblock Latent matters: Learning deep state-space models.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem{KrishnanSS17}
Rahul~G. Krishnan, Uri Shalit, and David~A. Sontag.
\newblock Structured inference networks for nonlinear state space models.
\newblock In {\em Proc. the AAAI Conference on Artificial Intelligence (AAAI)},
  2017.

\bibitem{KurleRBGG20}
Richard Kurle, Syama~Sundar Rangapuram, Emmanuel de~B{\'{e}}zenac, Stephan
  G{\"{u}}nnemann, and Jan Gasthaus.
\newblock Deep rao-blackwellised particle filters for time series forecasting.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{AnsariBKTSSWJ21}
Abdul~Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali~Caner
  T{\"{u}}rkmen, Harold Soh, Alexander~J. Smola, Bernie Wang, and Tim
  Januschowski.
\newblock Deep explicit duration switching models for time series.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem{HansonP88}
Stephen~Jose Hanson and Lorien~Y. Pratt.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 1988.

\bibitem{MorganB89}
Nelson Morgan and Herv{\'{e}} Bourlard.
\newblock Generalization and parameter estimation in feedforward netws: Some
  experiments.
\newblock In {\em Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}, 1989.

\bibitem{SrivastavaHKSS14}
Nitish Srivastava, Geoffrey~E. Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{abs-2109-11649}
Mahdy Shirdel, Reza Asadi, Duc Do, and Michael Hintlian.
\newblock Deep learning with kernel flow regularization for time series
  forecasting.
\newblock {\em CoRR}, abs/2109.11649, 2021.

\bibitem{TaiebYBR17}
Souhaib~Ben Taieb, Jiafan Yu, Mateus~Neves Barreto, and Ram Rajagopal.
\newblock Regularization in hierarchical time series forecasting with
  application to electricity smart meter data.
\newblock In {\em Proc. the AAAI Conference on Artificial Intelligence (AAAI)},
  2017.

\bibitem{kim2021reversible}
Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul
  Choo.
\newblock Reversible instance normalization for accurate time-series
  forecasting against distribution shift.
\newblock In {\em Proc. the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{WuPLJZ19}
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang.
\newblock Graph wavenet for deep spatial-temporal graph modeling.
\newblock In Sarit Kraus, editor, {\em Proc. the International Joint Conference
  on Artificial Intelligence (IJCAI)}, 2019.

\bibitem{FlunkertSG17}
Valentin Flunkert, David Salinas, and Jan Gasthaus.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent
  networks.
\newblock {\em CoRR}, abs/1704.04110, 2017.

\end{thebibliography}
