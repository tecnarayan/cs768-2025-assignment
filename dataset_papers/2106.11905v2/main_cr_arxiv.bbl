\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Shedivat et~al.(2017)Al-Shedivat, Wilson, Saatchi, Hu, and
  Xing]{al2017learning}
Maruan Al-Shedivat, Andrew~Gordon Wilson, Yunus Saatchi, Zhiting Hu, and Eric~P
  Xing.
\newblock Learning scalable deep kernels with recurrent structure.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 2850--2886, 2017.

\bibitem[Arjovsky(2021)]{arjovsky2021out}
Martin Arjovsky.
\newblock Out of distribution generalization in machine learning.
\newblock \emph{arXiv preprint arXiv:2103.02667}, 2021.

\bibitem[Athiwaratkun et~al.(2018)Athiwaratkun, Finzi, Izmailov, and
  Wilson]{athiwaratkun2018there}
Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock There are many consistent explanations of unlabeled data: Why you
  should average.
\newblock \emph{arXiv preprint arXiv:1806.05594}, 2018.

\bibitem[Benton et~al.(2021)Benton, Maddox, Lotfi, and Wilson]{benton2021loss}
Gregory~W Benton, Wesley~J Maddox, Sanae Lotfi, and Andrew~Gordon Wilson.
\newblock Loss surface simplexes for mode connecting volumes and fast
  ensembling.
\newblock \emph{arXiv preprint arXiv:2102.13042}, 2021.

\bibitem[Bishop(2006)]{bishop06}
Christopher~M. Bishop.
\newblock \emph{Pattern Recognition and Machine Learning}.
\newblock Springer, 2006.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Bojarski et~al.(2016)Bojarski, Del~Testa, Dworakowski, Firner, Flepp,
  Goyal, Jackel, Monfort, Muller, Zhang, et~al.]{bojarski2016end}
Mariusz Bojarski, Davide Del~Testa, Daniel Dworakowski, Bernhard Firner, Beat
  Flepp, Prasoon Goyal, Lawrence~D Jackel, Mathew Monfort, Urs Muller, Jiakai
  Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock \emph{arXiv preprint arXiv:1604.07316}, 2016.

\bibitem[Carbone et~al.(2020)Carbone, Wicker, Laurenti, Patane, Bortolussi, and
  Sanguinetti]{carbone2020robustness}
Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi,
  and Guido Sanguinetti.
\newblock Robustness of bayesian neural networks to gradient-based attacks.
\newblock \emph{arXiv preprint arXiv:2002.04359}, 2020.

\bibitem[Carvalho et~al.(2009)Carvalho, Polson, and
  Scott]{carvalho2009handling}
Carlos~M Carvalho, Nicholas~G Polson, and James~G Scott.
\newblock Handling sparsity via the horseshoe.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 73--80. PMLR,
  2009.

\bibitem[Chang et~al.(2019)Chang, Yao, Williams-King, and
  Lipson]{chang2019ensemble}
Oscar Chang, Yuling Yao, David Williams-King, and Hod Lipson.
\newblock Ensemble model patching: A parameter-efficient variational bayesian
  neural network.
\newblock \emph{arXiv preprint arXiv:1905.09453}, 2019.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Cui et~al.(2020)Cui, Havulinna, Marttinen, and
  Kaski]{cui2020informative}
Tianyu Cui, Aki Havulinna, Pekka Marttinen, and Samuel Kaski.
\newblock Informative gaussian scale mixture priors for bayesian neural
  networks.
\newblock \emph{arXiv e-prints}, pages arXiv--2002, 2020.

\bibitem[Daum{\'e}~III(2009)]{daume2009frustratingly}
Hal Daum{\'e}~III.
\newblock Frustratingly easy domain adaptation.
\newblock \emph{arXiv preprint arXiv:0907.1815}, 2009.

\bibitem[Daume~III and Marcu(2006)]{daume2006domain}
Hal Daume~III and Daniel Marcu.
\newblock Domain adaptation for statistical classifiers.
\newblock \emph{Journal of artificial Intelligence research}, 26:\penalty0
  101--126, 2006.

\bibitem[Daxberger et~al.(2020)Daxberger, Nalisnick, Allingham, Antor{\'a}n,
  and Hern{\'a}ndez-Lobato]{daxberger2020expressive}
Erik Daxberger, Eric Nalisnick, James~Urquhart Allingham, Javier Antor{\'a}n,
  and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Expressive yet tractable bayesian deep learning via subnetwork
  inference.
\newblock \emph{arXiv preprint arXiv:2010.14689}, 2020.

\bibitem[Domingos(2000)]{domingos2000bayesian}
Pedro Domingos.
\newblock Bayesian averaging of classifiers and the overfitting problem.
\newblock In \emph{ICML}, volume 747, pages 223--230. Citeseer, 2000.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry2020efficient}
Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek,
  Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran.
\newblock Efficient and scalable bayesian neural nets with rank-1 factors.
\newblock In \emph{International conference on machine learning}, pages
  2782--2792, 2020.

\bibitem[Esteva et~al.(2017)Esteva, Kuprel, Novoa, Ko, Swetter, Blau, and
  Thrun]{esteva2017dermatologist}
Andre Esteva, Brett Kuprel, Roberto~A Novoa, Justin Ko, Susan~M Swetter,
  Helen~M Blau, and Sebastian Thrun.
\newblock Dermatologist-level classification of skin cancer with deep neural
  networks.
\newblock \emph{nature}, 542\penalty0 (7639):\penalty0 115--118, 2017.

\bibitem[Filos et~al.(2019)Filos, Farquhar, Gomez, Rudner, Kenton, Smith,
  Alizadeh, de~Kroon, and Gal]{filos2019systematic}
Angelos Filos, Sebastian Farquhar, Aidan~N Gomez, Tim~GJ Rudner, Zachary
  Kenton, Lewis Smith, Milad Alizadeh, Arnoud de~Kroon, and Yarin Gal.
\newblock A systematic comparison of bayesian deep learning robustness in
  diabetic retinopathy tasks.
\newblock \emph{arXiv preprint arXiv:1912.10481}, 2019.

\bibitem[Fortuin(2021)]{fortuin2021priors}
Vincent Fortuin.
\newblock Priors in bayesian deep learning: A review.
\newblock \emph{arXiv preprint arXiv:2105.06868}, 2021.

\bibitem[Fortuin et~al.(2021)Fortuin, Garriga-Alonso, Wenzel, R{\"a}tsch,
  Turner, van~der Wilk, and Aitchison]{fortuin2021bayesian}
Vincent Fortuin, Adri{\`a} Garriga-Alonso, Florian Wenzel, Gunnar R{\"a}tsch,
  Richard Turner, Mark van~der Wilk, and Laurence Aitchison.
\newblock Bayesian neural network priors revisited.
\newblock \emph{arXiv preprint arXiv:2102.06571}, 2021.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem[Gulshan et~al.(2016)Gulshan, Peng, Coram, Stumpe, Wu, Narayanaswamy,
  Venugopalan, Widner, Madams, Cuadros, et~al.]{gulshan2016development}
Varun Gulshan, Lily Peng, Marc Coram, Martin~C Stumpe, Derek Wu, Arunachalam
  Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge
  Cuadros, et~al.
\newblock Development and validation of a deep learning algorithm for detection
  of diabetic retinopathy in retinal fundus photographs.
\newblock \emph{Jama}, 316\penalty0 (22):\penalty0 2402--2410, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2018.

\bibitem[Izmailov et~al.(2021)Izmailov, Vikram, Hoffman, and
  Wilson]{izmailov2021bayesian}
Pavel Izmailov, Sharad Vikram, Matthew~D Hoffman, and Andrew~Gordon Wilson.
\newblock What are bayesian neural network posteriors really like?
\newblock \emph{arXiv preprint arXiv:2104.14421}, 2021.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Behrmann, Zemel, and
  Bethge]{jacobsen2018excessive}
J{\"o}rn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge.
\newblock Excessive invariance causes adversarial vulnerability.
\newblock \emph{arXiv preprint arXiv:1811.00401}, 2018.

\bibitem[Jouppi et~al.(2020)Jouppi, Yoon, Kurian, Li, Patil, Laudon, Young, and
  Patterson]{jouppi2020domain}
Norman~P Jouppi, Doe~Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James
  Laudon, Cliff Young, and David Patterson.
\newblock A domain-specific supercomputer for training deep neural networks.
\newblock \emph{Communications of the ACM}, 63\penalty0 (7):\penalty0 67--78,
  2020.

\bibitem[Kendall and Gal(2017)]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in {B}ayesian deep learning for
  computer vision?
\newblock In \emph{Advances in neural information processing systems}, pages
  5574--5584, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kessler et~al.(2019)Kessler, Nguyen, Zohren, and
  Roberts]{kessler2019hierarchical}
Samuel Kessler, Vu~Nguyen, Stefan Zohren, and Stephen Roberts.
\newblock Hierarchical indian buffet neural networks for bayesian continual
  learning.
\newblock \emph{arXiv preprint arXiv:1912.02290}, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Krizhevsky et~al.(2014)Krizhevsky, Nair, and
  Hinton]{krizhevsky2014cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock {The CIFAR-10 dataset}.
\newblock 2014.
\newblock \url{http://www. cs. toronto. edu/kriz/cifar.html}.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6402--6413, 2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, Studer, and
  Goldstein]{li2017visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1-3):\penalty0
  503--528, 1989.

\bibitem[MacKay(1995)]{mackay1995probable}
David~JC MacKay.
\newblock Probable networks and plausible predictionsâ€”a review of practical
  bayesian methods for supervised neural networks.
\newblock \emph{Network: computation in neural systems}, 6\penalty0
  (3):\penalty0 469--505, 1995.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 13153--13164, 2019.

\bibitem[Masegosa(2019)]{masegosa2019learning}
Andres~R. Masegosa.
\newblock Learning under model misspecification: Applications to variational
  and ensemble methods, 2019.

\bibitem[Michelmore et~al.(2020)Michelmore, Wicker, Laurenti, Cardelli, Gal,
  and Kwiatkowska]{michelmore2020uncertainty}
Rhiannon Michelmore, Matthew Wicker, Luca Laurenti, Luca Cardelli, Yarin Gal,
  and Marta Kwiatkowska.
\newblock Uncertainty quantification with statistical guarantees in end-to-end
  autonomous driving control.
\newblock In \emph{2020 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 7344--7350. IEEE, 2020.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: On the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\bibitem[Minka(2000)]{minka2000bayesian}
Thomas~P Minka.
\newblock {B}ayesian model averaging is not model combination.
\newblock 2000.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2498--2507. PMLR, 2017.

\bibitem[Morningstar et~al.(2020)Morningstar, Alemi, and
  Dillon]{morningstar2020pac}
Warren~R Morningstar, Alexander~A Alemi, and Joshua~V Dillon.
\newblock Pac m-bayes: Narrowing the empirical risk gap in the misspecified
  bayesian regime.
\newblock \emph{arXiv preprint arXiv:2010.09629}, 2020.

\bibitem[Mu and Gilmer(2019)]{mu2019mnist}
Norman Mu and Justin Gilmer.
\newblock Mnist-c: A robustness benchmark for computer vision.
\newblock \emph{arXiv preprint arXiv:1906.02337}, 2019.

\bibitem[Nagarajan et~al.(2020)Nagarajan, Andreassen, and
  Neyshabur]{nagarajan2020understanding}
Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
\newblock Understanding the failure modes of out-of-distribution
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.15775}, 2020.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?
\newblock \emph{arXiv preprint arXiv:2008.11687}, 2020.

\bibitem[Nocedal(1980)]{nocedal1980updating}
Jorge Nocedal.
\newblock Updating quasi-newton matrices with limited storage.
\newblock \emph{Mathematics of computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D~Sculley, Sebastian
  Nowozin, Joshua~V Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{arXiv preprint arXiv:1906.02530}, 2019.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rasmussen and Williams(2006)]{rasmussen06}
C.~E. Rasmussen and C.~K.~I. Williams.
\newblock \emph{{G}aussian processes for Machine Learning}.
\newblock The {MIT} {P}ress, 2006.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Roy et~al.(2021)Roy, Ren, Azizi, Loh, Natarajan, Mustafa, Pawlowski,
  Freyberg, Liu, Beaver, et~al.]{roy2021does}
Abhijit~Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil
  Mustafa, Nick Pawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et~al.
\newblock Does your dermatology classifier know what it doesn't know? detecting
  the long-tail of unseen conditions.
\newblock \emph{arXiv preprint arXiv:2104.03829}, 2021.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Storkey(2009)]{storkey2009training}
Amos Storkey.
\newblock When training and test sets are different: characterizing learning
  transfer.
\newblock \emph{Dataset shift in machine learning}, 30:\penalty0 3--28, 2009.

\bibitem[Storkey and Sugiyama(2007)]{storkey2007mixture}
Amos~J Storkey and Masashi Sugiyama.
\newblock Mixture regression for covariate shift.
\newblock \emph{Advances in neural information processing systems},
  19:\penalty0 1337, 2007.

\bibitem[Sugiyama et~al.(2006)Sugiyama, Blankertz, Krauledat, Dornhege, and
  M{\"u}ller]{sugiyama2006importance}
Masashi Sugiyama, Benjamin Blankertz, Matthias Krauledat, Guido Dornhege, and
  Klaus-Robert M{\"u}ller.
\newblock Importance-weighted cross-validation for covariate shift.
\newblock In \emph{Joint Pattern Recognition Symposium}, pages 354--363.
  Springer, 2006.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and
  M{\"u}ller]{sugiyama2007covariate}
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M{\"u}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (5), 2007.

\bibitem[Trask et~al.(2018)Trask, Hill, Reed, Rae, Dyer, and
  Blunsom]{trask2018neural}
Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom.
\newblock Neural arithmetic logic units.
\newblock \emph{arXiv preprint arXiv:1808.00508}, 2018.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, {\'S}wiatkowski, Tran,
  Mandt, Snoek, Salimans, Jenatton, and Nowozin]{wenzel2020good}
Florian Wenzel, Kevin Roth, Bastiaan~S Veeling, Jakub {\'S}wiatkowski, Linh
  Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and
  Sebastian Nowozin.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock \emph{arXiv preprint arXiv:2002.02405}, 2020.

\bibitem[Wicker et~al.(2021)Wicker, Laurenti, Patane, Chen, Zhang, and
  Kwiatkowska]{wicker2021bayesian}
Matthew Wicker, Luca Laurenti, Andrea Patane, Zhuotong Chen, Zheng Zhang, and
  Marta Kwiatkowska.
\newblock Bayesian inference with certifiable adversarial robustness.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2431--2439. PMLR, 2021.

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{arXiv preprint arXiv:2002.08791}, 2020.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
