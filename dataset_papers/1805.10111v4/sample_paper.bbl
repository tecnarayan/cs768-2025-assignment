\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In {\em OSDI}, volume~16, pages 265--283, 2016.

\bibitem{aji2017sparse}
A.~F. Aji and K.~Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{allen2017katyusha}
Z.~Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In {\em STOC}, pages 1200--1205, 2017.

\bibitem{bekkerman2011scaling}
R.~Bekkerman, M.~Bilenko, and J.~Langford.
\newblock {\em Scaling up machine learning: Parallel and distributed
  approaches}.
\newblock Cambridge University Press, 2011.

\bibitem{chang2011libsvm}
C.-C. Chang and C.-J. Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{chilimbi2014project}
T.~M. Chilimbi, Y.~Suzue, J.~Apacible, and K.~Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em OSDI}, volume~14, pages 571--582, 2014.

\bibitem{de2018high}
C.~De~Sa, M.~Leszczynski, J.~Zhang, A.~Marzoev, C.~R. Aberger, K.~Olukotun, and
  C.~R{\'e}.
\newblock High-accuracy low-precision training.
\newblock {\em arXiv preprint arXiv:1803.03383}, 2018.

\bibitem{de2015taming}
C.~M. De~Sa, C.~Zhang, K.~Olukotun, and C.~R{\'e}.
\newblock Taming the wild: A unified analysis of hogwild-style algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  2674--2682, 2015.

\bibitem{dean2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, A.~Senior, P.~Tucker,
  K.~Yang, Q.~V. Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1223--1231, 2012.

\bibitem{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{hien2016accelerated}
L.~T.~K. Hien, C.~V. Nguyen, H.~Xu, C.~Lu, and J.~Feng.
\newblock Accelerated stochastic mirror descent algorithms for composite
  non-strongly convex optimization.
\newblock {\em arXiv preprint arXiv:1605.06892}, 2016.

\bibitem{huffman1952method}
D.~A. Huffman.
\newblock A method for the construction of minimum-redundancy codes.
\newblock {\em Proceedings of the IRE}, 40(9):1098--1101, 1952.

\bibitem{iandola2016firecaffe}
F.~N. Iandola, M.~W. Moskewicz, K.~Ashraf, and K.~Keutzer.
\newblock Firecaffe: near-linear acceleration of deep neural network training
  on compute clusters.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2592--2600, 2016.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem{kanai2017preventing}
S.~Kanai, Y.~Fujiwara, and S.~Iwamura.
\newblock Preventing gradient explosions in gated recurrent units.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  435--444, 2017.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{li2014communication}
M.~Li, D.~G. Andersen, A.~J. Smola, and K.~Yu.
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  19--27, 2014.

\bibitem{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem{lin2015universal}
H.~Lin, J.~Mairal, and Z.~Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015.

\bibitem{lin2017deep}
Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and W.~J. Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In {\em ICLR}, 2018.

\bibitem{nesterov2013introductory}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{nesterov1983method}
Y.~E. Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. Akad. Nauk SSSR}, volume 269, pages 543--547, 1983.

\bibitem{pascanu2013difficulty}
R.~Pascanu, T.~Mikolov, and Y.~Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1318, 2013.

\bibitem{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in neural information processing systems}, pages
  693--701, 2011.

\bibitem{Reddi2016Fast}
S.~J. Reddi, S.~Sra, B.~Poczos, and A.~Smola.
\newblock Fast stochastic methods for nonsmooth nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1605.06900}, 2016.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{seide20141}
F.~Seide, H.~Fu, J.~Droppo, G.~Li, and D.~Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{Sra2012Scalable}
S.~Sra.
\newblock Scalable nonconvex inexact proximal splitting.
\newblock {\em Advances in Neural Information Processing Systems}, pages
  539--547, 2012.

\bibitem{strom2015scalable}
N.~Strom.
\newblock Scalable distributed dnn training using commodity gpu cloud
  computing.
\newblock In {\em Sixteenth Annual Conference of the International Speech
  Communication Association}, 2015.

\bibitem{wang2018atomo}
H.~Wang, S.~Sievert, Z.~Charles, D.~Papailiopoulos, and S.~Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{wangni2017gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1710.09854}, 2017.

\bibitem{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem{wu2018error}
J.~Wu, W.~Huang, J.~Huang, and T.~Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{xing2015petuum}
E.~P. Xing, Q.~Ho, W.~Dai, J.~K. Kim, J.~Wei, S.~Lee, X.~Zheng, P.~Xie,
  A.~Kumar, and Y.~Yu.
\newblock Petuum: A new platform for distributed machine learning on big data.
\newblock {\em IEEE Transactions on Big Data}, 1(2):49--67, 2015.

\bibitem{yan2015performance}
F.~Yan, O.~Ruwase, Y.~He, and T.~Chilimbi.
\newblock Performance modeling and scalability optimization of distributed deep
  learning systems.
\newblock In {\em Proceedings of the 21th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1355--1364. ACM, 2015.

\bibitem{zhang2017zipml}
H.~Zhang, J.~Li, K.~Kara, D.~Alistarh, J.~Liu, and C.~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4035--4043, 2017.

\bibitem{zhou2016dorefa}
S.~Zhou, Y.~Wu, Z.~Ni, X.~Zhou, H.~Wen, and Y.~Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
