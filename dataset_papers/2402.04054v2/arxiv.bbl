\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alquier(2024)]{alquier2021user}
Alquier, P.
\newblock User-friendly introduction to {PAC-Bayes} bounds.
\newblock \emph{Foundations and Trends in Machine Learning}, 17\penalty0 (2):\penalty0 174--303, 2024.

\bibitem[Amit \& Meir(2018)Amit and Meir]{amit2018meta}
Amit, R. and Meir, R.
\newblock Meta-learning by adjusting priors based on extended {PAC-Bayes} theory.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Baxter(2000)]{baxter2000model}
Baxter, J.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)}, 12:\penalty0 149--198, 2000.

\bibitem[Chen et~al.(2021)Chen, Shui, and Marchand]{chen2021generalization}
Chen, Q., Shui, C., and Marchand, M.
\newblock Generalization bounds for meta-learning: An information-theoretic analysis.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Denevi et~al.(2019)Denevi, Ciliberto, Grazzi, and Pontil]{denevi2019learning}
Denevi, G., Ciliberto, C., Grazzi, R., and Pontil, M.
\newblock Learning-to-learn stochastic gradient descent with biased regularization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Ding et~al.(2021)Ding, Chen, Levinboim, Goodman, and Soricut]{ding2021bridging}
Ding, N., Chen, X., Levinboim, T., Goodman, S., and Soricut, R.
\newblock Bridging the gap between practice and {PAC-Bayes} theory in few-shot meta-learning.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Farid \& Majumdar(2021)Farid and Majumdar]{farid2021generalization}
Farid, A. and Majumdar, A.
\newblock Generalization bounds for meta-learning via {PAC-Bayes} and uniform stability.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Friedman \& Meir(2023)Friedman and Meir]{friedman2023adaptive}
Friedman, L. and Meir, R.
\newblock Adaptive meta-learning via data-dependent {PAC-Bayes} bounds.
\newblock In \emph{Conference on Lifelong Learning Agents (CoLLAs)}, 2023.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2010.

\bibitem[Guan \& Lu(2022)Guan and Lu]{guan2022fast}
Guan, J. and Lu, Z.
\newblock Fast-rate {PAC-Bayesian} generalization bounds for meta-learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Guedj(2019)]{guedj2019primer}
Guedj, B.
\newblock A primer on {PAC-Bayesian} learning.
\newblock \emph{arXiv preprint arXiv:1901.05353}, 2019.

\bibitem[Hellstr{\"o}m \& Durisi(2022)Hellstr{\"o}m and Durisi]{hellstrom2022evaluated}
Hellstr{\"o}m, F. and Durisi, G.
\newblock Evaluated {CMI} bounds for meta learning: Tightness and expressiveness.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Hellstr{\"o}m et~al.(2023)Hellstr{\"o}m, Durisi, Guedj, and Raginsky]{hellstrom2023generalization}
Hellstr{\"o}m, F., Durisi, G., Guedj, B., and Raginsky, M.
\newblock Generalization bounds: Perspectives from information theory and {PAC-Bayes}.
\newblock \emph{arXiv preprint arXiv:2309.04381}, 2023.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and Conwell]{hochreiter2001learning}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks (ICANN)}, 2001.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2015.

\bibitem[LeCun \& Cortes(1998)LeCun and Cortes]{mnist}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock http://yann.lecun.com/exdb/mnist/, 1998.

\bibitem[Lee et~al.(2019)Lee, Maji, Ravichandran, and Soatto]{lee2019meta}
Lee, K., Maji, S., Ravichandran, A., and Soatto, S.
\newblock Meta-learning with differentiable convex optimization.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{li2017meta}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-{SGD}: Learning to learn quickly for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Liu et~al.(2021)Liu, Lu, Yan, and Zhang]{liu2021pac}
Liu, T., Lu, J., Yan, Z., and Zhang, G.
\newblock {PAC-Bayes} bounds for meta-learning with data-dependent prior.
\newblock \emph{arXiv preprint arXiv:2102.03748}, 2021.

\bibitem[Maurer(2004)]{maurer2004note}
Maurer, A.
\newblock A note on the {PAC} {Bayesian} theorem.
\newblock \emph{arXiv preprint arXiv:cs.LG/0411099}, 2004.

\bibitem[Maurer(2009)]{maurer2009transfer}
Maurer, A.
\newblock Transfer bounds for linear feature learning.
\newblock \emph{Machine Learning}, 75\penalty0 (3):\penalty0 327--350, 2009.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and Romera-Paredes]{maurer2016benefit}
Maurer, A., Pontil, M., and Romera-Paredes, B.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 17\penalty0 (81):\penalty0 1--32, 2016.

\bibitem[McAllester(1998)]{mcallester1998some}
McAllester, D.~A.
\newblock Some {PAC-Bayesian} theorems.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 1998.

\bibitem[Nguyen et~al.(2022)Nguyen, Do, and Carneiro]{nguyen2022pac}
Nguyen, C., Do, T.-T., and Carneiro, G.
\newblock {PAC-Bayes} meta-learning with implicit task-specific posteriors.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 45\penalty0 (1):\penalty0 841--851, 2022.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Pentina \& Lampert(2014)Pentina and Lampert]{pentina2014pac}
Pentina, A. and Lampert, C.~H.
\newblock A {PAC-Bayesian} bound for lifelong learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2014.

\bibitem[Pentina \& Lampert(2015)Pentina and Lampert]{pentina2015lifelong}
Pentina, A. and Lampert, C.~H.
\newblock Lifelong learning with non-iid tasks.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2015.

\bibitem[P{\'e}rez-Ortiz et~al.(2021)P{\'e}rez-Ortiz, Rivasplata, Shawe-Taylor, and Szepesv{\'a}ri]{perez2021tighter}
P{\'e}rez-Ortiz, M., Rivasplata, O., Shawe-Taylor, J., and Szepesv{\'a}ri, C.
\newblock Tighter risk certificates for neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 10326--10365, 2021.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{ravi2016optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2017.

\bibitem[Rezazadeh(2022)]{rezazadeh2022unified}
Rezazadeh, A.
\newblock A unified view on {PAC-Bayes} bounds for meta-learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Riou et~al.(2023)Riou, Alquier, and Ch{\'e}rief-Abdellatif]{riou2023bayes}
Riou, C., Alquier, P., and Ch{\'e}rief-Abdellatif, B.-E.
\newblock {Bayes meets Bernstein at the meta level: an analysis of fast rates in meta-learning with PAC-Bayes}.
\newblock \emph{arXiv preprint arXiv:2302.11709}, 2023.

\bibitem[Rothfuss et~al.(2021)Rothfuss, Fortuin, Josifoski, and Krause]{rothfuss2021pacoh}
Rothfuss, J., Fortuin, V., Josifoski, M., and Krause, A.
\newblock {PACOH}: {Bayes-optimal} meta-learning with {PAC-guarantees}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Rothfuss et~al.(2023)Rothfuss, Josifoski, Fortuin, and Krause]{rothfuss2022pacoh}
Rothfuss, J., Josifoski, M., Fortuin, V., and Krause, A.
\newblock {Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice}.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2023.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
Schmidhuber, J.
\newblock Evolutionary principles in self-referential learning, 1987.

\bibitem[Scott et~al.(2024)Scott, Zakerinia, and Lampert]{scott2023pefll}
Scott, J., Zakerinia, H., and Lampert, C.~H.
\newblock {PeFLL: Personalized Federated Learning by Learning to Learn}.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Seldin et~al.(2012)Seldin, Laviolette, Cesa-Bianchi, Shawe-Taylor, and Auer]{seldin2012pac}
Seldin, Y., Laviolette, F., Cesa-Bianchi, N., Shawe-Taylor, J., and Auer, P.
\newblock {PAC-Bayesian} inequalities for martingales.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0 (12):\penalty0 7086--7093, 2012.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun1998}
Thrun, S. and Pratt, L. (eds.).
\newblock \emph{Learning to Learn}.
\newblock Kluwer Academic Press, 1998.

\bibitem[Tian \& Yu(2023)Tian and Yu]{tian2023can}
Tian, P. and Yu, H.
\newblock Can we improve meta-learning model in few-shot learning by aligning data distributions?
\newblock \emph{Knowledge-Based Systems}, 277:\penalty0 110800, 2023.

\bibitem[Zhao et~al.(2020)Zhao, Kobayashi, Sacramento, and von Oswald]{zhao2020meta}
Zhao, D., Kobayashi, S., Sacramento, J., and von Oswald, J.
\newblock Meta-learning via hypernetworks.
\newblock In \emph{NeurIPS Workshop on Meta-Learning (MetaLearn)}, 2020.

\end{thebibliography}
