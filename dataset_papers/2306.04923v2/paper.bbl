\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azizian et~al.(2020)Azizian, Scieur, Mitliagkas, Lacoste-Julien, and
  Gidel]{azizian2020accelerating}
Azizian, W., Scieur, D., Mitliagkas, I., Lacoste-Julien, S., and Gidel, G.
\newblock Accelerating smooth games by manipulating spectral shapes.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  1705--1715. PMLR, 26--28 Aug 2020.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S.~P. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Cesa-Bianchi et~al.(1996)Cesa-Bianchi, Long, and
  Warmuth]{cesa1996worst}
Cesa-Bianchi, N., Long, P.~M., and Warmuth, M.~K.
\newblock Worst-case quadratic loss bounds for prediction using linear
  functions and gradient descent.
\newblock \emph{IEEE Transactions on Neural Networks}, 7\penalty0 (3):\penalty0
  604--619, 1996.

\bibitem[Cesa-Bianchi et~al.(2012)Cesa-Bianchi, Gaillard, Lugosi, and
  Stoltz]{cesa2012mirror}
Cesa-Bianchi, N., Gaillard, P., Lugosi, G., and Stoltz, G.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Chen et~al.(2021)Chen, Luo, and Wei]{chen2021impossible}
Chen, L., Luo, H., and Wei, C.-Y.
\newblock Impossible tuning made possible: A new expert algorithm and its
  applications.
\newblock In Belkin, M. and Kpotufe, S. (eds.), \emph{Proceedings of Thirty
  Fourth Conference on Learning Theory}, volume 134 of \emph{Proceedings of
  Machine Learning Research}, pp.\  1216--1259. PMLR, 15--19 Aug 2021.

\bibitem[Cutkosky(2019)]{cutkosky2019artificial}
Cutkosky, A.
\newblock Artificial constraints and hints for unbounded online learning.
\newblock In Beygelzimer, A. and Hsu, D. (eds.), \emph{Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of \emph{Proceedings
  of Machine Learning Research}, pp.\  874--894, Phoenix, USA, 25--28 Jun 2019.
  PMLR.

\bibitem[Cutkosky(2020)]{cutkosky2020parameter}
Cutkosky, A.
\newblock Parameter-free, dynamic, and strongly-adaptive online learning.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2250--2259, Virtual,
  13--18 Jul 2020. PMLR.

\bibitem[Cutkosky \& Orabona(2018)Cutkosky and Orabona]{cutkosky2018black}
Cutkosky, A. and Orabona, F.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock In Bubeck, S., Perchet, V., and Rigollet, P. (eds.),
  \emph{Proceedings of the 31st Conference On Learning Theory}, volume~75 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1493--1529. PMLR,
  06--09 Jul 2018.

\bibitem[Du et~al.(2022)Du, Gidel, Jordan, and Li]{du2022optimal}
Du, S.~S., Gidel, G., Jordan, M.~I., and Li, C.~J.
\newblock Optimal extragradient-based bilinearly-coupled saddle-point
  optimization, 2022.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{duchi10adagrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{Conference on Learning Theory}, 2010.

\bibitem[Foster et~al.(2015)Foster, Rakhlin, and Sridharan]{foster2015adaptive}
Foster, D.~J., Rakhlin, A., and Sridharan, K.
\newblock Adaptive online learning.
\newblock In \emph{Advances in Neural Information Processing Systems 28}. 2015.

\bibitem[Gyorgy \& Szepesvari(2016)Gyorgy and Szepesvari]{gyorgy2016shifting}
Gyorgy, A. and Szepesvari, C.
\newblock Shifting regret, mirror descent, and matrices.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2943--2951, New York,
  New York, USA, 20--22 Jun 2016. PMLR.

\bibitem[Hall \& Willett(2016)Hall and Willett]{hall2016online}
Hall, E.~C. and Willett, R.~M.
\newblock Online optimization in dynamic environments, 2016.

\bibitem[Hazan(2016)]{hazan2016introduction}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends® in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.
\newblock ISSN 2167-3888.
\newblock \doi{10.1561/2400000013}.

\bibitem[Hazan et~al.(2007)Hazan, Rakhlin, and Bartlett]{hazan2007adaptive}
Hazan, E., Rakhlin, A., and Bartlett, P.
\newblock Adaptive online gradient descent.
\newblock In Platt, J., Koller, D., Singer, Y., and Roweis, S. (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2007.

\bibitem[Ibrahim et~al.(2020)Ibrahim, Azizian, Gidel, and
  Mitliagkas]{ibrahim2020linear}
Ibrahim, A., Azizian, W., Gidel, G., and Mitliagkas, I.
\newblock Linear lower bounds and conditioning of differentiable games.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4583--4593. PMLR,
  13--18 Jul 2020.

\bibitem[Jacobsen \& Cutkosky(2022)Jacobsen and
  Cutkosky]{jacobsen2022parameter}
Jacobsen, A. and Cutkosky, A.
\newblock Parameter-free mirror descent.
\newblock In Loh, P.-L. and Raginsky, M. (eds.), \emph{Proceedings of Thirty
  Fifth Conference on Learning Theory}, volume 178 of \emph{Proceedings of
  Machine Learning Research}, pp.\  4160--4211. PMLR, 02--05 Jul 2022.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{jadbabaie2015online}
Jadbabaie, A., Rakhlin, A., Shahrampour, S., and Sridharan, K.
\newblock {Online Optimization : Competing with Dynamic Comparators}.
\newblock In Lebanon, G. and Vishwanathan, S. V.~N. (eds.), \emph{Proceedings
  of the Eighteenth International Conference on Artificial Intelligence and
  Statistics}, volume~38 of \emph{Proceedings of Machine Learning Research},
  pp.\  398--406, San Diego, California, USA, 09--12 May 2015. PMLR.

\bibitem[Kempka et~al.(2019)Kempka, Kotlowski, and Warmuth]{kempka2019adaptive}
Kempka, M., Kotlowski, W., and Warmuth, M.~K.
\newblock Adaptive scale-invariant online algorithms for learning linear
  models.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3321--3330. PMLR,
  09--15 Jun 2019.

\bibitem[Kivinen \& Warmuth(1997)Kivinen and Warmuth]{kivinen1997exponentiated}
Kivinen, J. and Warmuth, M.~K.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock \emph{information and computation}, 132\penalty0 (1):\penalty0 1--63,
  1997.

\bibitem[Liu \& Orabona(2022)Liu and Orabona]{liu2022initialization}
Liu, M. and Orabona, F.
\newblock On the initialization for convex-concave min-max problems.
\newblock In Dasgupta, S. and Haghtalab, N. (eds.), \emph{Proceedings of The
  33rd International Conference on Algorithmic Learning Theory}, volume 167 of
  \emph{Proceedings of Machine Learning Research}, pp.\  743--767. PMLR, 29
  Mar--01 Apr 2022.

\bibitem[Luo et~al.(2022)Luo, Zhang, Zhao, and Zhou]{luo2022corralling}
Luo, H., Zhang, M., Zhao, P., and Zhou, Z.-H.
\newblock Corralling a larger band of bandits: A case study on switching regret
  for linear bandits, 2022.

\bibitem[Mayo et~al.(2022)Mayo, Hadiji, and van Erven]{mayo2022scalefree}
Mayo, J.~J., Hadiji, H., and van Erven, T.
\newblock Scale-free unconstrained online learning for curved losses, 2022.

\bibitem[Mcmahan \& Streeter(2012)Mcmahan and Streeter]{mcmahan2012noregret}
Mcmahan, B. and Streeter, M.
\newblock No-regret algorithms for unconstrained online convex optimization.
\newblock In Pereira, F., Burges, C. J.~C., Bottou, L., and Weinberger, K.~Q.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~25.
  Curran Associates, Inc., 2012.

\bibitem[McMahan \& Streeter(2010)McMahan and Streeter]{mcmahan2010adaptive}
McMahan, H.~B. and Streeter, M.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{Conference on Learning Theory}, 2010.

\bibitem[Mhammedi \& Koolen(2020)Mhammedi and Koolen]{mhammedi2020lipschitz}
Mhammedi, Z. and Koolen, W.~M.
\newblock Lipschitz and comparator-norm adaptivity in online learning.
\newblock In Abernethy, J. and Agarwal, S. (eds.), \emph{Proceedings of Thirty
  Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of
  Machine Learning Research}, pp.\  2858--2887. PMLR, 09--12 Jul 2020.

\bibitem[Orabona(2013)]{orabona2013dimensionfree}
Orabona, F.
\newblock Dimension-free exponentiated gradient.
\newblock In Burges, C. J.~C., Bottou, L., Welling, M., Ghahramani, Z., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.

\bibitem[Orabona(2019)]{orabona2019modern}
Orabona, F.
\newblock A modern introduction to online learning.
\newblock \emph{CoRR}, abs/1912.13213, 2019.

\bibitem[Orabona \& P\'{a}l(2016)Orabona and P\'{a}l]{orabona2016coin}
Orabona, F. and P\'{a}l, D.
\newblock Coin betting and parameter-free online learning.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, NIPS'16, pp.\  577–585, Red Hook, NY, USA,
  2016. Curran Associates Inc.

\bibitem[Orabona \& Pál(2018)Orabona and Pál]{orabona2018scale}
Orabona, F. and Pál, D.
\newblock Scale-free online learning.
\newblock \emph{Theoretical Computer Science}, 716:\penalty0 50 -- 69, 2018.
\newblock ISSN 0304-3975.
\newblock \doi{https://doi.org/10.1016/j.tcs.2017.11.021}.
\newblock Special Issue on ALT 2015.

\bibitem[Orabona et~al.(2012)Orabona, Cesa-Bianchi, and
  Gentile]{orabona2012beyond}
Orabona, F., Cesa-Bianchi, N., and Gentile, C.
\newblock Beyond logarithmic bounds in online learning.
\newblock In Lawrence, N.~D. and Girolami, M. (eds.), \emph{Proceedings of the
  Fifteenth International Conference on Artificial Intelligence and
  Statistics}, volume~22 of \emph{Proceedings of Machine Learning Research},
  pp.\  823--831, La Palma, Canary Islands, 21--23 Apr 2012. PMLR.

\bibitem[Shalev-Shwartz(2011)]{shalev2011online}
Shalev-Shwartz, S.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0 (2),
  2011.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and
  Tewari]{srebro2010smoothness}
Srebro, N., Sridharan, K., and Tewari, A.
\newblock Smoothness, low noise and fast rates.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Telgarsky(2022)]{telgarsky2022stochastic}
Telgarsky, M.
\newblock Stochastic linear optimization never overfits with
  quadratically-bounded losses on general data, 2022.

\bibitem[van~der Hoeven(2019)]{van2019user}
van~der Hoeven, D.
\newblock User-specified local differential privacy in unconstrained adaptive
  online learning.
\newblock In \emph{NeurIPS}, pp.\  14080--14089, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Lu, and Zhou]{zhang2018adaptive}
Zhang, L., Lu, S., and Zhou, Z.-H.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  1330--1340, 2018.

\bibitem[Zhao et~al.(2020)Zhao, Zhang, Zhang, and Zhou]{zhao2020dynamic}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Dynamic regret of convex and smooth functions.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  12510--12520. Curran Associates, Inc., 2020.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{International Conference on Machine Learning}, 2003.

\end{thebibliography}
