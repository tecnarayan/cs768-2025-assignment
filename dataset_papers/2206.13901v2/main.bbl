\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Degrave, Bohez,
  Tassa, Belov, Heess, and Riedmiller]{abdolmaleki2018relative}
A.~Abdolmaleki, J.~T. Springenberg, J.~Degrave, S.~Bohez, Y.~Tassa, D.~Belov,
  N.~Heess, and M.~Riedmiller.
\newblock Relative entropy regularized policy iteration.
\newblock \emph{arXiv}, 1812.02256, 2018.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto2017successor}
A.~Barreto, W.~Dabney, R.~Munos, J.~J. Hunt, T.~Schaul, H.~van Hasselt, and
  D.~Silver.
\newblock {Successor Features} for transfer in reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2017.

\bibitem[Barreto et~al.(2019)Barreto, Borsa, Hou, Comanici, Ayg{\"u}n, Hamel,
  Toyama, Hunt, Mourad, Silver, and Precup]{Barreto2019TheOK}
A.~Barreto, D.~Borsa, S.~Hou, G.~Comanici, E.~Ayg{\"u}n, P.~Hamel, D.~Toyama,
  J.~Hunt, S.~Mourad, D.~Silver, and D.~Precup.
\newblock The {Option Keyboard}: Combining skills in reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2019.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, Józefowicz, Gray, Olsson, Pachocki,
  Petrov, de~Oliveira~Pinto, Raiman, Salimans, Schlatter, Schneider, Sidor,
  Sutskever, Tang, Wolski, and Zhang]{berner2019dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~D{\k{e}}biak, C.~Dennison,
  D.~Farhi, Q.~Fischer, S.~Hashme, C.~Hesse, R.~Józefowicz, S.~Gray,
  C.~Olsson, J.~Pachocki, M.~Petrov, H.~P. de~Oliveira~Pinto, J.~Raiman,
  T.~Salimans, J.~Schlatter, J.~Schneider, S.~Sidor, I.~Sutskever, J.~Tang,
  F.~Wolski, and S.~Zhang.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv}, 1912.06680, 2019.

\bibitem[Borsa et~al.(2019)Borsa, Barreto, Quan, Mankowitz, Munos, Hasselt,
  Silver, and Schaul]{Borsa2019UniversalSF}
D.~Borsa, A.~Barreto, J.~Quan, D.~J. Mankowitz, R.~Munos, H.~V. Hasselt,
  D.~Silver, and T.~Schaul.
\newblock Universal {Successor Features} approximators.
\newblock \emph{arXiv}, 1812.07626, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{arXiv}, 1606.01540, 2016.

\bibitem[Cassirer et~al.(2021)Cassirer, Barth-Maron, Brevdo, Ramos, Boyd,
  Sottiaux, and Kroiss]{reverb}
A.~Cassirer, G.~Barth-Maron, E.~Brevdo, S.~Ramos, T.~Boyd, T.~Sottiaux, and
  M.~Kroiss.
\newblock Reverb: A framework for experience replay.
\newblock \emph{arXiv}, 2102.04736, 2021.

\bibitem[Dayan(1993)]{dayan1993improving}
P.~Dayan.
\newblock Improving generalization for temporal difference learning: The
  {Successor Representation}.
\newblock \emph{Neural Computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Fatemi and Tavakoli(2022)]{fatemi2022orchestrated}
M.~Fatemi and A.~Tavakoli.
\newblock Orchestrated value mapping for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2203.07171}, 2022.

\bibitem[Fujimoto et~al.(2018)Fujimoto, {van Hoof}, and
  Meger]{fujimoto2018addressing}
S.~Fujimoto, H.~{van Hoof}, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning ({ICML})},
  2018.

\bibitem[Grimm and Singh(2019)]{Grimm2019LearningIR}
C.~Grimm and S.~Singh.
\newblock Learning independently-obtainable reward functions.
\newblock \emph{arXiv}, 1901.08649, 2019.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning ({ICML})},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and Levine]{sac}
T.~Haarnoja, A.~Zhou, K.~Hartikainen, G.~Tucker, S.~Ha, J.~Tan, V.~Kumar,
  H.~Zhu, A.~Gupta, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv}, 1812.05905, 2018{\natexlab{b}}.

\bibitem[Hasselt(2010)]{hasselt2010double}
H.~Hasselt.
\newblock {Double Q-learning}.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2010.

\bibitem[Hayes et~al.(2022)Hayes, R{\u a}dulescu, Bargiacchi,
  K{\"a}llstr{\"o}m, Macfarlane, Reymond, Verstraeten, Zintgraf, Dazeley,
  Heintz, Howley, Irissappane, Mannion, Now{\'e}, Ramos, Restelli, Vamplew, and
  Roijers]{hayes_practical_2022}
C.~F. Hayes, R.~R{\u a}dulescu, E.~Bargiacchi, J.~K{\"a}llstr{\"o}m,
  M.~Macfarlane, M.~Reymond, T.~Verstraeten, L.~M. Zintgraf, R.~Dazeley,
  F.~Heintz, E.~Howley, A.~A. Irissappane, P.~Mannion, A.~Now{\'e}, G.~Ramos,
  M.~Restelli, P.~Vamplew, and D.~M. Roijers.
\newblock A practical guide to multi-objective reinforcement learning and
  planning.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 36\penalty0
  (1):\penalty0 26, Apr. 2022.

\bibitem[Heuillet et~al.(2021)Heuillet, Couthouis, and
  D{\'\i}az-Rodr{\'\i}guez]{heuillet2021explainability}
A.~Heuillet, F.~Couthouis, and N.~D{\'\i}az-Rodr{\'\i}guez.
\newblock Explainability in deep reinforcement learning.
\newblock \emph{Knowledge-Based Systems}, 214:\penalty0 106685, 2021.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{unreal}
M.~Jaderberg, V.~Mnih, W.~M. Czarnecki, T.~Schaul, J.~Z. Leibo, D.~Silver, and
  K.~Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{arXiv}, 1611.05397, 2016.

\bibitem[Juozapaitis et~al.(2019)Juozapaitis, Koul, Fern, Erwig, and
  Doshi-Velez]{Juozapaitis2019ExplainableRL}
Z.~Juozapaitis, A.~Koul, A.~Fern, M.~Erwig, and F.~Doshi-Velez.
\newblock Explainable reinforcement learning via reward decomposition.
\newblock In \emph{{IJCAI/ECAI} Workshop on Explainable Artificial
  Intelligence}, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~L. Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv}, 1412.6980, 2014.

\bibitem[Laroche et~al.(2017)Laroche, Fatemi, Romoff, and van
  Seijen]{laroche2017multi}
R.~Laroche, M.~Fatemi, J.~Romoff, and H.~van Seijen.
\newblock Multi-advisor reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.00756}, 2017.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2016continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Liu et~al.(2021)Liu, Liu, Jin, Stone, and Liu]{cagrad}
B.~Liu, X.~Liu, X.~Jin, P.~Stone, and Q.~Liu.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2021.

\bibitem[Mills et~al.(2020)Mills, Ronagh, and Tamblyn]{mills2020finding}
K.~Mills, P.~Ronagh, and I.~Tamblyn.
\newblock Finding the ground state of spin hamiltonians with reinforcement
  learning.
\newblock \emph{Nature Machine Intelligence}, 2:\penalty0 509--517, 2020.

\bibitem[Mirowski et~al.(2016)Mirowski, Pascanu, Viola, Soyer, Ballard, Banino,
  Denil, Goroshin, Sifre, Kavukcuoglu, Kumaran, and
  Hadsell]{learning_to_navigate}
P.~Mirowski, R.~Pascanu, F.~Viola, H.~Soyer, A.~J. Ballard, A.~Banino,
  M.~Denil, R.~Goroshin, L.~Sifre, K.~Kavukcuoglu, D.~Kumaran, and R.~Hadsell.
\newblock Learning to navigate in complex environments.
\newblock \emph{arXiv}, 1611.03673, 2016.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2016.

\bibitem[Roijers et~al.(2013)Roijers, Vamplew, Whiteson, and Dazeley]{Roijers}
D.~M. Roijers, P.~Vamplew, S.~Whiteson, and R.~Dazeley.
\newblock A survey of multi-objective sequential decision-making.
\newblock \emph{Journal of Artificial Intelligence Research}, 48:\penalty0
  67--113, 2013.

\bibitem[Russell and Zimdars(2003)]{Russell2003QDecompositionFR}
S.~J. Russell and A.~Zimdars.
\newblock Q-decomposition for reinforcement learning agents.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2003.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{Schaul2015UniversalVF}
T.~Schaul, D.~Horgan, K.~Gregor, and D.~Silver.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning {ICML}}, 2015.

\bibitem[Seijen et~al.(2017)Seijen, Fatemi, Romoff, Laroche, Barnes, and
  Tsang]{HybridRA}
H.~V. Seijen, M.~Fatemi, J.~Romoff, R.~Laroche, T.~Barnes, and J.~Tsang.
\newblock Hybrid reward architecture for reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~Lillicrap, F.~Hui,
  L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of {G}o without human knowledge.
\newblock \emph{Nature}, 550:\penalty0 354--359, 2017.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
R.~S. Sutton, J.~Modayil, M.~Delp, T.~Degris, P.~M. Pilarski, A.~White, and
  D.~Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{Conference on Autonomous Agents and Multiagent Systems
  ({AAMAS})}, 2011.

\bibitem[Tunyasuvunakool et~al.(2021)Tunyasuvunakool, Adler, Wu, Green,
  Zielinski, {\v{Z}}{\'\i}dek, Bridgland, Cowie, Meyer, Laydon,
  et~al.]{tunyasuvunakool2021highly}
K.~Tunyasuvunakool, J.~Adler, Z.~Wu, T.~Green, M.~Zielinski,
  A.~{\v{Z}}{\'\i}dek, A.~Bridgland, A.~Cowie, C.~Meyer, A.~Laydon, et~al.
\newblock Highly accurate protein structure prediction for the human proteome.
\newblock \emph{Nature}, 596:\penalty0 590--596, 2021.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, Gulcehre, Wang, Pfaff, Wu, Ring, Yogatama,
  Wünsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu, Hassabis, Apps, and
  Silver]{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, J.~Oh, D.~Horgan, M.~Kroiss,
  I.~Danihelka, A.~Huang, L.~Sifre, T.~Cai, J.~P. Agapiou, M.~Jaderberg, A.~S.
  Vezhnevets, R.~Leblond, T.~Pohlen, V.~Dalibard, D.~Budden, Y.~Sulsky,
  J.~Molloy, T.~L. Paine, C.~Gulcehre, Z.~Wang, T.~Pfaff, Y.~Wu, R.~Ring,
  D.~Yogatama, D.~Wünsch, K.~McKinney, O.~Smith, T.~Schaul, T.~Lillicrap,
  K.~Kavukcuoglu, D.~Hassabis, C.~Apps, and D.~Silver.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575:\penalty0 350--354, 2019.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{2020SciPy-NMeth}
P.~Virtanen, R.~Gommers, T.~E. Oliphant, M.~Haberland, T.~Reddy, D.~Cournapeau,
  E.~Burovski, P.~Peterson, W.~Weckesser, J.~Bright, S.~J. {van der Walt},
  M.~Brett, J.~Wilson, K.~J. Millman, N.~Mayorov, A.~R.~J. Nelson, E.~Jones,
  R.~Kern, E.~Larson, C.~J. Carey, {\.I}.~Polat, Y.~Feng, E.~W. Moore,
  J.~{VanderPlas}, D.~Laxalde, J.~Perktold, R.~Cimrman, I.~Henriksen, E.~A.
  Quintero, C.~R. Harris, A.~M. Archibald, A.~H. Ribeiro, F.~Pedregosa, P.~{van
  Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.

\bibitem[Wurman et~al.(2022)Wurman, Barrett, Kawamoto, MacGlashan, Subramanian,
  Walsh, Capobianco, Devlic, Eckert, Fuchs, Gilpin, Khandelwal, Kompella, Lin,
  MacAlpine, Oller, Seno, Sherstan, Thomure, Aghabozorgi, Barrett, Douglas,
  Whitehead, Dürr, Stone, Spranger, and Kitano]{wurman2022GT}
P.~R. Wurman, S.~Barrett, K.~Kawamoto, J.~MacGlashan, K.~Subramanian, T.~J.
  Walsh, R.~Capobianco, A.~Devlic, F.~Eckert, F.~Fuchs, L.~Gilpin,
  P.~Khandelwal, V.~Kompella, H.~Lin, P.~MacAlpine, D.~Oller, T.~Seno,
  C.~Sherstan, M.~D. Thomure, H.~Aghabozorgi, L.~Barrett, R.~Douglas,
  D.~Whitehead, P.~Dürr, P.~Stone, M.~Spranger, and H.~Kitano.
\newblock Outracing champion {Gran Turismo} drivers with deep reinforcement
  learning.
\newblock \emph{Nature}, 602:\penalty0 223--228, 2022.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
T.~Yu, S.~Kumar, A.~Gupta, S.~Levine, K.~Hausman, and C.~Finn.
\newblock Gradient surgery for multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  ({NeurIPS})}, 2020.

\end{thebibliography}
