\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Allen-Zhu} and Li(2023{\natexlab{a}})]{AL2023-cfg}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Physics of Language Models: Part 1, Learning Hierarchical Language
  Structures}.
\newblock \emph{ArXiv e-prints}, abs/2305.13673, May 2023{\natexlab{a}}.
\newblock Full version available at \url{http://arxiv.org/abs/2305.13673}.

\bibitem[{Allen-Zhu} and Li(2023{\natexlab{b}})]{AL2023-knowledgeUB}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Physics of Language Models: Part 3.2, Knowledge Manipulation}.
\newblock \emph{ArXiv e-prints}, abs/2309.14402, September 2023{\natexlab{b}}.
\newblock Full version available at \url{http://arxiv.org/abs/2309.14402}.

\bibitem[{Allen-Zhu} and Li(2024)]{AL2024-knowledgeScaling}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Physics of Language Models: Part 3.3, Knowledge Capacity Scaling
  Laws}.
\newblock \emph{ArXiv e-prints}, abs/2404.05405, April 2024.
\newblock Full version available at \url{http://arxiv.org/abs/2404.05405}.

\bibitem[Anderson and Milson(1989)]{anderson1989human}
John~R Anderson and Robert Milson.
\newblock Human memory: An adaptive perspective.
\newblock \emph{Psychological Review}, 96\penalty0 (4):\penalty0 703, 1989.

\bibitem[Aspillaga et~al.(2021)Aspillaga, Mendoza, and
  Soto]{aspillaga2021inspecting}
Carlos Aspillaga, Marcelo Mendoza, and Alvaro Soto.
\newblock Inspecting the concept knowledge graph encoded by modern language
  models.
\newblock \emph{arXiv preprint arXiv:2105.13471}, 2021.

\bibitem[Baddeley(1997)]{baddeley1997human}
Alan~D Baddeley.
\newblock \emph{Human memory: Theory and practice}.
\newblock psychology press, 1997.

\bibitem[Berglund et~al.(2023)Berglund, Stickland, Balesni, Kaufmann, Tong,
  Korbak, Kokotajlo, and Evans]{berglund2023taken}
Lukas Berglund, Asa~Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong,
  Tomasz Korbak, Daniel Kokotajlo, and Owain Evans.
\newblock Taken out of context: On measuring situational awareness in llms.
\newblock \emph{arXiv preprint arXiv:2309.00667}, 2023.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach]{gpt-neox-20b}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
  USVSN~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
  Wang, and Samuel Weinbach.
\newblock {GPT-NeoX-20B}: An open-source autoregressive language model.
\newblock In \emph{Proceedings of the ACL Workshop on Challenges \&
  Perspectives in Creating Large Language Models}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.06745}.

\bibitem[Cai et~al.(2020)Cai, Chen, Song, Zhang, Zhao, and Yin]{cai2020data}
Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, and Dawei
  Yin.
\newblock Data manipulation: Towards effective instance learning for neural
  dialogue generation via learning to augment and reweight.
\newblock \emph{arXiv preprint arXiv:2004.02594}, 2020.

\bibitem[Choi et~al.(2022)Choi, Lee, Kyung, and Kim]{choi2022albert}
Byeongmin Choi, YongHyun Lee, Yeunwoong Kyung, and Eunchan Kim.
\newblock Albert with knowledge graph encoder utilizing semantic similarity for
  commonsense question answering.
\newblock \emph{arXiv preprint arXiv:2211.07065}, 2022.

\bibitem[Conneau et~al.(2018)Conneau, Kruszewski, Lample, Barrault, and
  Baroni]{conneau2018you}
Alexis Conneau, German Kruszewski, Guillaume Lample, Lo{\"\i}c Barrault, and
  Marco Baroni.
\newblock What you can cram into a single vector: Probing sentence embeddings
  for linguistic properties.
\newblock \emph{arXiv preprint arXiv:1805.01070}, 2018.

\bibitem[Craik and Jennings(1992)]{craik1992human}
Fergus~IM Craik and Janine~M Jennings.
\newblock Human memory.
\newblock 1992.

\bibitem[Dai et~al.(2021)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock \emph{arXiv preprint arXiv:2104.08696}, 2021.

\bibitem[Eldan and Li(2023)]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li.
\newblock Tinystories: How small can language models be and still speak
  coherent english?
\newblock \emph{arXiv preprint arXiv:2305.07759}, 2023.

\bibitem[Geva et~al.(2020)Geva, Schuster, Berant, and
  Levy]{geva2020transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock \emph{arXiv preprint arXiv:2012.14913}, 2020.

\bibitem[He et~al.(2020)He, Liu, Gao, and Chen]{he2020deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock \emph{arXiv preprint arXiv:2006.03654}, 2020.

\bibitem[Hernandez et~al.(2023)Hernandez, Li, and
  Andreas]{hernandez2023measuring}
Evan Hernandez, Belinda~Z Li, and Jacob Andreas.
\newblock Measuring and manipulating knowledge representations in language
  models.
\newblock \emph{arXiv preprint arXiv:2304.00740}, 2023.

\bibitem[Hu et~al.(2021)Hu, Wallis, {Allen-Zhu}, Li, Wang, Wang, Chen,
  et~al.]{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan {Allen-Zhu}, Yuanzhi Li, Shean Wang,
  Lu~Wang, Weizhu Chen, et~al.
\newblock {LoRA: Low-Rank Adaptation of Large Language Models}.
\newblock In \emph{ICLR}, 2021.

\bibitem[Jiang et~al.(2024)Jiang, Sun, Shi, Rodriguez, Zhou, Neubig, Lin, Yih,
  and Iyer]{jiang2024instruction}
Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham
  Neubig, Xi~Victoria Lin, Wen-tau Yih, and Srinivasan Iyer.
\newblock Instruction-tuned language models are better knowledge learners.
\newblock \emph{arXiv preprint arXiv:2402.12847}, 2024.

\bibitem[Kenton and Toutanova(2019)]{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem[Kobayashi(2018)]{kobayashi-2018-contextual}
Sosuke Kobayashi.
\newblock Contextual augmentation: Data augmentation by words with paradigmatic
  relations.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pages 452--457, New Orleans,
  Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-2072}.
\newblock URL \url{https://aclanthology.org/N18-2072}.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K\"{u}ttler, Lewis, Yih, Rockt\"{a}schel, Riedel, and
  Kiela]{NEURIPS2020_6b493230}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K\"{u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt\"{a}schel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 9459--9474. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf}.

\bibitem[Li et~al.(2021)Li, Nye, and Andreas]{li2021implicit}
Belinda~Z Li, Maxwell Nye, and Jacob Andreas.
\newblock Implicit representations of meaning in neural language models.
\newblock \emph{arXiv preprint arXiv:2106.00737}, 2021.

\bibitem[Liu et~al.(2020)Liu, Wang, Ji, Cheng, Zhu, Awa, He, Chen, Poon, Cao,
  and Gao]{liu2020mtmtdnn}
Xiaodong Liu, Yu~Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa,
  Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, and Jianfeng Gao.
\newblock The microsoft toolkit of multi-task deep neural networks for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:2002.07972}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv e-prints}, abs/1907.11692, July 2019.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17359--17372, 2022.

\bibitem[Naseem et~al.(2021)Naseem, Ravishankar, Mihindukulasooriya, Abdelaziz,
  Lee, Kapanipathi, Roukos, Gliozzo, and Gray]{naseem-etal-2021-semantics}
Tahira Naseem, Srinivas Ravishankar, Nandana Mihindukulasooriya, Ibrahim
  Abdelaziz, Young-Suk Lee, Pavan Kapanipathi, Salim Roukos, Alfio Gliozzo, and
  Alexander Gray.
\newblock A semantics-aware transformer model of relation linking for knowledge
  base question answering.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 2: Short Papers)}, pages 256--262,
  Online, August 2021. Association for Computational Linguistics.

\bibitem[Omar et~al.(2023)Omar, Mangukiya, Kalnis, and
  Mansour]{omar2023chatgpt}
Reham Omar, Omij Mangukiya, Panos Kalnis, and Essam Mansour.
\newblock Chatgpt versus traditional question answering for knowledge graphs:
  Current status and future directions towards knowledge graph chatbots.
\newblock \emph{arXiv preprint arXiv:2302.06466}, 2023.

\bibitem[Peng et~al.(2022)Peng, Wang, Hu, Jin, Hou, Li, Liu, and
  Liu]{peng2022copen}
Hao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin, Lei Hou, Juanzi Li, Zhiyuan
  Liu, and Qun Liu.
\newblock Copen: Probing conceptual knowledge in pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2211.04079}, 2022.

\bibitem[Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Lewis, Bakhtin, Wu,
  Miller, and Riedel]{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
  Alexander~H Miller, and Sebastian Riedel.
\newblock Language models as knowledge bases?
\newblock \emph{arXiv preprint arXiv:1909.01066}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Richardson and Sabharwal(2020)]{richardson-sabharwal-2020-qa}
Kyle Richardson and Ashish Sabharwal.
\newblock What does my {QA} model know? devising controlled probes using expert
  knowledge.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 572--588, 2020.
\newblock \doi{10.1162/tacl_a_00331}.
\newblock URL \url{https://aclanthology.org/2020.tacl-1.37}.

\bibitem[Singhal et~al.(2022)Singhal, Azizi, Tu, Mahdavi, Wei, Chung, Scales,
  Tanwani, Cole-Lewis, Pfohl, et~al.]{singhal2022large}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S~Sara Mahdavi, Jason Wei, Hyung~Won
  Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et~al.
\newblock Large language models encode clinical knowledge.
\newblock \emph{arXiv preprint arXiv:2212.13138}, 2022.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2021.

\bibitem[Sun et~al.(2023)Sun, Xu, Zha, Liu, and Dong]{sun2023head}
Kai Sun, Yifan~Ethan Xu, Hanwen Zha, Yue Liu, and Xin~Luna Dong.
\newblock Head-to-tail: How knowledgeable are large language models (llm)? aka
  will llms replace knowledge graphs?
\newblock \emph{arXiv preprint arXiv:2308.10168}, 2023.

\bibitem[Sushil et~al.(2021)Sushil, Suster, and
  Daelemans]{sushil-etal-2021-yet}
Madhumita Sushil, Simon Suster, and Walter Daelemans.
\newblock Are we there yet? exploring clinical domain knowledge of {BERT}
  models.
\newblock In \emph{Proceedings of the 20th Workshop on Biomedical Language
  Processing}, pages 41--53, Online, June 2021. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2021.bionlp-1.5}.
\newblock URL \url{https://aclanthology.org/2021.bionlp-1.5}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Ye et~al.(2024{\natexlab{a}})Ye, Xu, Li, and
  {Allen-Zhu}]{YXLZ2024-gsm1}
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan {Allen-Zhu}.
\newblock {Physics of Language Models: Part 2.1, Grade-School Math and the
  Hidden Reasoning Process}.
\newblock \emph{arXiv preprint arXiv:xxxx.xxxxx}, 2024{\natexlab{a}}.
\newblock to appear.

\bibitem[Ye et~al.(2024{\natexlab{b}})Ye, Xu, Li, and
  {Allen-Zhu}]{YXLZ2024-gsm2}
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan {Allen-Zhu}.
\newblock {Physics of Language Models: Part 2.2, How to Learn From Mistakes on
  Grade-School Math Problems}.
\newblock \emph{arXiv preprint arXiv:xxxx.xxxxx}, 2024{\natexlab{b}}.
\newblock to appear.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  et~al.]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{Bookcorpus}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, December 2015.

\bibitem[Zlotnik and Vansintjan(2019)]{zlotnik2019memory}
Gregorio Zlotnik and Aaron Vansintjan.
\newblock Memory: An extended definition.
\newblock \emph{Frontiers in psychology}, 10:\penalty0 2523, 2019.
\newblock \doi{10.3389/fpsyg.2019.02523}.

\end{thebibliography}
