\begin{thebibliography}{135}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and McGuinness]{arazo2019unsupervised}
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness.
\newblock {Unsupervised label noise modeling and loss correction}.
\newblock In \emph{International conference on machine learning}, pp.\  312--321. PMLR, 2019.

\bibitem[Atkeson \& Schaal(1997)Atkeson and Schaal]{atkeson1997robot}
Christopher~G Atkeson and Stefan Schaal.
\newblock {Robot learning from demonstration}.
\newblock In \emph{ICML}, volume~97, pp.\  12--20. Citeseer, 1997.

\bibitem[Awadalla et~al.(2022)Awadalla, Wortsman, Ilharco, Min, Magnusson, Hajishirzi, and Schmidt]{awadalla2022exploring}
Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, and Ludwig Schmidt.
\newblock {Exploring The Landscape of Distributional Robustness for Question Answering Models}.
\newblock \emph{arXiv preprint arXiv:2210.12517}, 2022.

\bibitem[Bach et~al.(2017)Bach, He, Ratner, and Ré]{bach2017learning}
Stephen~H Bach, Bryan He, Alexander Ratner, and Christopher Ré.
\newblock {Learning the structure of generative models without labeled data}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  273--282. PMLR, 2017.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock {Training a helpful and harmless assistant with reinforcement learning from human feedback}.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock {Constitutional AI: Harmlessness from AI feedback}.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bain \& Sammut(1995)Bain and Sammut]{bain1995framework}
Michael Bain and Claude Sammut.
\newblock {A Framework for Behavioural Cloning.}
\newblock In \emph{Machine Intelligence 15}, pp.\  103--129, 1995.

\bibitem[Bellamy et~al.(2018)Bellamy, Dey, Hind, Hoffman, Houde, Kannan, Lohia, Martino, Mehta, Mojsilovic, et~al.]{bellamy2018ai}
Rachel~KE Bellamy, Kuntal Dey, Michael Hind, Samuel~C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et~al.
\newblock {AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias}.
\newblock \emph{arXiv preprint arXiv:1810.01943}, 2018.

\bibitem[Bengio et~al.()Bengio, Hinton, Yao, Song, Abbeel, Harari, Zhang, Xue, Shalev-Shwartz, Hadfield, et~al.]{bengio2023managing}
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval~Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et~al.
\newblock {Managing {AI}} risks in an era of rapid progress.
\newblock \emph{arXiv preprint arXiv:2310.17688}.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot, Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin~A Raffel.
\newblock {Mixmatch: A holistic approach to semi-supervised learning}.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Beyer et~al.(2022)Beyer, Zhai, Royer, Markeeva, Anil, and Kolesnikov]{beyer2022knowledge}
Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov.
\newblock {Knowledge distillation: A good teacher is patient and consistent}.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10925--10934, 2022.

\bibitem[Bills et~al.(2023)Bills, Cammarata, Mossing, Tillman, Gao, Goh, Sutskever, Leike, Wu, and Saunders]{bills2023language}
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
\newblock {Language models can explain neurons in language models}.
\newblock \emph{OpenAI Blog}, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Le~Bras, Gao, and Choi]{Bisk2020}
Yonatan Bisk, Rowan Zellers, Ronan Le~Bras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA: Reasoning about Physical Commonsense in Natural Language}.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence}, 2020.

\bibitem[Bowman(2022)]{bowman2022alignment}
Sam Bowman.
\newblock {Artificial Sandwiching: When can we test scalable alignment protocols without humans?}
\newblock \emph{AI Alignment Forum}, 2022.

\bibitem[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner, Lukosuite, Askell, Jones, Chen, et~al.]{bowman2022measuring}
Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et~al.
\newblock {Measuring progress on scalable oversight for large language models}.
\newblock \emph{arXiv preprint arXiv:2211.03540}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock {Language models are few-shot learners}.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and Steinhardt]{burns2023discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock {Discovering Latent Knowledge in Language Models Without Supervision}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[CAIS()]{ai-risk-open-letter}
CAIS.
\newblock {Statement on {AI}} risk.

\bibitem[Carlsmith()]{carlsmith2023scheming}
Joe Carlsmith.
\newblock {Scheming {AI}}s: Will {AI}s fake alignment during training in order to get power?
\newblock \emph{arXiv preprint arXiv:2311.08379}.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, Jégou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock {Emerging properties in self-supervised vision transformers}.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  9650--9660, 2021.

\bibitem[Cha et~al.(2021)Cha, Chun, Lee, Cho, Park, Lee, and Park]{cha2021swad}
Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park.
\newblock {Swad: Domain generalization by seeking flat minima}.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22405--22418, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Swersky, Norouzi, and Hinton]{chen2020big}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton.
\newblock {Big self-supervised models are strong semi-supervised learners}.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22243--22255, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Wei, Kumar, and Ma]{chen2020self}
Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma.
\newblock {Self-training avoids using spurious features under domain shift}.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21061--21071, 2020{\natexlab{b}}.

\bibitem[Christiano(2018)]{christiano2018approval}
Paul Christiano.
\newblock {Approval-directed bootstrapping}.
\newblock \emph{AI Alignment Forum}, 2018.

\bibitem[Christiano(2019)]{christiano2019capability}
Paul Christiano.
\newblock {Capability amplification}.
\newblock \emph{AI Alignment Forum}, 2019.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock {Deep reinforcement learning from human preferences}.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Christiano et~al.(2018)Christiano, Shlegeris, and Amodei]{christiano2018supervising}
Paul Christiano, Buck Shlegeris, and Dario Amodei.
\newblock {Supervising strong learners by amplifying weak experts}.
\newblock \emph{arXiv preprint arXiv:1810.08575}, 2018.

\bibitem[Christiano et~al.(2022)Christiano, Cotra, and Xu]{christiano2022eliciting}
Paul Christiano, Ajeya Cotra, and Mark Xu.
\newblock {Eliciting latent knowledge}.
\newblock Technical report, Alignment Research Center (ARC), 2022.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}.
\newblock In \emph{NAACL}, 2019.

\bibitem[Cotra(2021)]{cotra2021sandwiching}
Ajeya Cotra.
\newblock {The case for aligning narrowly superhuman models}.
\newblock \emph{AI Alignment Forum}, 2021.

\bibitem[Dai \& Le(2015)Dai and Le]{dai2015semi}
Andrew~M Dai and Quoc~V Le.
\newblock {Semi-supervised sequence learning}.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Demski \& Garrabrant(2019)Demski and Garrabrant]{demski2019embedded}
Abram Demski and Scott Garrabrant.
\newblock {Embedded agency}.
\newblock \emph{arXiv preprint arXiv:1902.09469}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock {An image is worth 16x16 words: Transformers for image recognition at scale}.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock {A Mathematical Framework for Transformer Circuits}.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Evans et~al.(2021)Evans, Cotton-Barratt, Finnveden, Bales, Balwit, Wills, Righetti, and Saunders]{evans2021truthful}
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders.
\newblock {Truthful {AI}}: Developing and governing {AI} that does not lie.
\newblock \emph{arXiv preprint arXiv:2110.06674}, 2021.

\bibitem[French et~al.(2017)French, Mackiewicz, and Fisher]{french2017self}
Geoffrey French, Michal Mackiewicz, and Mark Fisher.
\newblock {Self-ensembling for visual domain adaptation}.
\newblock \emph{arXiv preprint arXiv:1706.05208}, 2017.

\bibitem[Frénay \& Verleysen(2013)Frénay and Verleysen]{frenay2013classification}
Beno\^it Frénay and Michel Verleysen.
\newblock {Classification in the presence of label noise: a survey}.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 25\penalty0 (5):\penalty0 845--869, 2013.

\bibitem[Furlanello et~al.(2018)Furlanello, Lipton, Tschannen, Itti, and Anandkumar]{furlanello2018born}
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
\newblock {Born again neural networks}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1607--1616. PMLR, 2018.

\bibitem[Glaese et~al.(2022)Glaese, McAleese, Trebacz, Aslanides, Firoiu, Ewalds, Rauh, Weidinger, Chadwick, Thacker, et~al.]{glaese2022improving}
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et~al.
\newblock {Improving alignment of dialogue agents via targeted human judgements}.
\newblock \emph{arXiv preprint arXiv:2209.14375}, 2022.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{gou2021knowledge}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock {Knowledge distillation: A survey}.
\newblock \emph{International Journal of Computer Vision}, 129:\penalty0 1789--1819, 2021.

\bibitem[Grandvalet \& Bengio(2004)Grandvalet and Bengio]{grandvalet2004semi}
Yves Grandvalet and Yoshua Bengio.
\newblock {Semi-supervised learning by entropy minimization}.
\newblock \emph{Advances in neural information processing systems}, 17, 2004.

\bibitem[Guo et~al.(2018)Guo, Huang, Zhang, Zhuang, Dong, Scott, and Huang]{Guo_2018_ECCV}
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew~R. Scott, and Dinglong Huang.
\newblock {CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images}.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, 2018.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and Sugiyama]{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama.
\newblock {Co-teaching: Robust training of deep neural networks with extremely noisy labels}.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep residual learning for image recognition}.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and Gimpel]{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock {Using trusted data to train deep networks on labels corrupted by severe noise}.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock {Using pre-training can improve model robustness and uncertainty}.
\newblock In \emph{International conference on machine learning}, pp.\  2712--2721. PMLR, 2019.

\bibitem[Hendrycks et~al.(2020{\natexlab{a}})Hendrycks, Burns, Basart, Critch, Li, Song, and Steinhardt]{hendrycks2020aligning}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
\newblock {Aligning {AI}} with shared human values.
\newblock \emph{arXiv preprint arXiv:2008.02275}, 2020{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2020{\natexlab{b}})Hendrycks, Liu, Wallace, Dziedzic, Krishnan, and Song]{hendrycks2020pretrained}
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.
\newblock {Pretrained transformers improve out-of-distribution robustness}.
\newblock \emph{arXiv preprint arXiv:2004.06100}, 2020{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock {Measuring Mathematical Problem Solving With the MATH Dataset}.
\newblock \emph{Sort}, 2\penalty0 (4):\penalty0 0--6, 2021.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock {Distilling the knowledge in a neural network}.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock {LoRA: Low-Rank Adaptation of Large Language Models}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Huang et~al.(2019)Huang, Le~Bras, Bhagavatula, and Choi]{huang2019cosmos}
Lifu Huang, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock {Cosmos QA: Machine reading comprehension with contextual commonsense reasoning}.
\newblock \emph{arXiv preprint arXiv:1909.00277}, 2019.

\bibitem[Hubinger et~al.(2019)Hubinger, van Merwijk, Mikulik, Skalse, and Garrabrant]{hubinger2019risks}
Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant.
\newblock {Risks from learned optimization in advanced machine learning systems}.
\newblock \emph{arXiv preprint arXiv:1906.01820}, 2019.

\bibitem[Irving et~al.(2018)Irving, Christiano, and Amodei]{irving2018ai}
Geoffrey Irving, Paul Christiano, and Dario Amodei.
\newblock {AI safety via debate}.
\newblock \emph{arXiv preprint arXiv:1805.00899}, 2018.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew~Gordon Wilson.
\newblock {Averaging weights leads to wider optima and better generalization}.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Khani et~al.(2019)Khani, Raghunathan, and Liang]{khani2019maximum}
Fereshte Khani, Aditi Raghunathan, and Percy Liang.
\newblock {Maximum weighted loss discrepancy}.
\newblock \emph{arXiv preprint arXiv:1906.03518}, 2019.

\bibitem[Khashabi et~al.(2018)Khashabi, Chaturvedi, Roth, Upadhyay, and Roth]{khashabi2018looking}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.
\newblock {Looking beyond the surface: A challenge set for reading comprehension over multiple sentences}.
\newblock In \emph{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pp.\  252--262, 2018.

\bibitem[Kim et~al.(2019)Kim, Ghorbani, and Zou]{kim2019multiaccuracy}
Michael~P Kim, Amirata Ghorbani, and James Zou.
\newblock {Multiaccuracy: Black-box post-processing for fairness in classification}.
\newblock In \emph{Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society}, pp.\  247--254, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock {Adam: {A}} method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma et~al.(2014)Kingma, Mohamed, Jimenez~Rezende, and Welling]{kingma2014semi}
Durk~P Kingma, Shakir Mohamed, Danilo Jimenez~Rezende, and Max Welling.
\newblock {Semi-supervised learning with deep generative models}.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Kirichenko et~al.(2023)Kirichenko, Izmailov, and Wilson]{kirichenko2023last}
Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock {Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
\newblock {Imagenet classification with deep convolutional neural networks}.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Krogh \& Hertz(1991)Krogh and Hertz]{krogh1991simple}
Anders Krogh and John Hertz.
\newblock {A simple weight decay can improve generalization}.
\newblock \emph{Advances in neural information processing systems}, 4, 1991.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and Liang]{kumar2022fine}
Ananya Kumar, Aditi Raghunathan, Robbie~Matthew Jones, Tengyu Ma, and Percy Liang.
\newblock {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Laine \& Aila(2016)Laine and Aila]{laine2016temporal}
Samuli Laine and Timo Aila.
\newblock {Temporal ensembling for semi-supervised learning}.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Dong-Hyun Lee et~al.
\newblock {Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks}.
\newblock In \emph{Workshop on challenges in representation learning, ICML}, volume~3, pp.\  896. Atlanta, 2013.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.
\newblock {Rlaif: Scaling reinforcement learning from human feedback with {AI}} feedback.
\newblock \emph{arXiv preprint arXiv:2309.00267}, 2023.

\bibitem[Lee et~al.(2022{\natexlab{a}})Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{lee2022surgical}
Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn.
\newblock {Surgical Fine-Tuning Improves Adaptation to Distribution Shifts}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{a}}.

\bibitem[Lee et~al.(2022{\natexlab{b}})Lee, Yao, and Finn]{lee2022diversify}
Yoonho Lee, Huaxiu Yao, and Chelsea Finn.
\newblock {Diversify and disambiguate: Learning from underspecified data}.
\newblock \emph{arXiv preprint arXiv:2202.03418}, 2022{\natexlab{b}}.

\bibitem[Leike \& Sutskever(2023)Leike and Sutskever]{superalignment}
Jan Leike and Ilya Sutskever.
\newblock {Introducing Superalignment}.
\newblock \emph{OpenAI Blog}, 2023.

\bibitem[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and Legg]{leike2018scalable}
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg.
\newblock {Scalable agent alignment via reward modeling: a research direction}.
\newblock \emph{arXiv preprint arXiv:1811.07871}, 2018.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock {Dividemix: Learning with noisy labels as semi-supervised learning}.
\newblock \emph{arXiv preprint arXiv:2002.07394}, 2020.

\bibitem[Li et~al.(2023)Li, Patel, Viégas, Pfister, and Wattenberg]{li2023inference}
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.
\newblock {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}.
\newblock \emph{arXiv preprint arXiv:2306.03341}, 2023.

\bibitem[{Lichess Team}(2023)]{lichess}
{Lichess Team}.
\newblock {Lichess Database}.
\newblock \url{https://github.com/lichess-org/database}, 2023.
\newblock Accessed: 2023.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock {Let's Verify Step by Step}.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Liu et~al.(2021)Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang, and Finn]{liu2021just}
Evan~Z Liu, Behzad Haghgoo, Annie~S Chen, Aditi Raghunathan, Pang~Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn.
\newblock {Just train twice: Improving group robustness without training group information}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6781--6792. PMLR, 2021.

\bibitem[Liu et~al.(2022)Liu, Xu, Xu, Qian, Li, Jin, Ji, and Chan]{liu2022empirical}
Ziquan Liu, Yi~Xu, Yuanhong Xu, Qi~Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni~B Chan.
\newblock {An empirical study on distribution shift robustness from the perspective of pre-training and data augmentation}.
\newblock \emph{arXiv preprint arXiv:2205.12753}, 2022.

\bibitem[Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and Bailey]{ma2020normalized}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey.
\newblock {Normalized loss functions for deep learning with noisy labels}.
\newblock In \emph{International conference on machine learning}, pp.\  6543--6553. PMLR, 2020.

\bibitem[McKenzie et~al.(2023)McKenzie, Lyzhov, Pieler, Parrish, Mueller, Prabhu, McLean, Kirtland, Ross, Liu, et~al.]{mckenzie2023inverse}
Ian~R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et~al.
\newblock {Inverse Scaling: When Bigger Isn't Better}.
\newblock \emph{arXiv preprint arXiv:2306.09479}, 2023.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock {Locating and editing factual associations in GPT}.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17359--17372, 2022.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Ngo et~al.(2022)Ngo, Chan, and Mindermann]{ngo2022alignment}
Richard Ngo, Lawrence Chan, and Sören Mindermann.
\newblock {The alignment problem from a deep learning perspective}.
\newblock \emph{arXiv preprint arXiv:2209.00626}, 2022.

\bibitem[Nie et~al.(2019)Nie, Williams, Dinan, Bansal, Weston, and Kiela]{nie2019adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
\newblock {Adversarial NLI: A new benchmark for natural language understanding}.
\newblock \emph{arXiv preprint arXiv:1910.14599}, 2019.

\bibitem[Olah et~al.(2018)Olah, Satyanarayan, Johnson, Carter, Schubert, Ye, and Mordvintsev]{olah2018the}
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev.
\newblock {The Building Blocks of Interpretability}.
\newblock \emph{Distill}, 2018.
\newblock https://distill.pub/2018/building-blocks.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI.
\newblock {GPT-4 Technical Report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock {Training language models to follow instructions with human feedback}.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pacchiardi et~al.(2023)Pacchiardi, Chan, Mindermann, Moscovitz, Pan, Gal, Evans, and Brauner]{pacchiardi2023catch}
Lorenzo Pacchiardi, Alex~J Chan, Sören Mindermann, Ilan Moscovitz, Alexa~Y Pan, Yarin Gal, Owain Evans, and Jan Brauner.
\newblock {How to catch an {AI}} liar: Lie detection in black-box llms by asking unrelated questions.
\newblock \emph{arXiv preprint arXiv:2309.15840}, 2023.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Édouard Duchesnay]{JMLR:v12:pedregosa11a}
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay.
\newblock {Scikit-learn: Machine Learning in Python}.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (85):\penalty0 2825--2830, 2011.

\bibitem[Perez et~al.(2022{\natexlab{a}})Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock {Red teaming language models with language models}.
\newblock \emph{arXiv preprint arXiv:2202.03286}, 2022{\natexlab{a}}.

\bibitem[Perez et~al.(2022{\natexlab{b}})Perez, Ringer, Lukošiūtė, Nguyen, Chen, Heiner, Pettit, Olsson, Kundu, Kadavath, et~al.]{perez2022discovering}
Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et~al.
\newblock {Discovering language model behaviors with model-written evaluations}.
\newblock \emph{arXiv preprint arXiv:2212.09251}, 2022{\natexlab{b}}.

\bibitem[Pilehvar \& Camacho-Collados(2018)Pilehvar and Camacho-Collados]{pilehvar2018wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock {WiC: the word-in-context dataset for evaluating context-sensitive meaning representations}.
\newblock \emph{arXiv preprint arXiv:1808.09121}, 2018.

\bibitem[Radford et~al.(2017)Radford, Jozefowicz, and Sutskever]{radford2017learning}
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
\newblock {Learning to generate reviews and discovering sentiment}.
\newblock \emph{arXiv preprint arXiv:1704.01444}, 2017.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock {Learning transferable visual models from natural language supervision}.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Ratner et~al.(2017)Ratner, Bach, Ehrenberg, Fries, Wu, and Ré]{ratner2017snorkel}
Alexander Ratner, Stephen~H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré.
\newblock {Snorkel: Rapid training data creation with weak supervision}.
\newblock In \emph{Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases}, volume~11, pp.\  269. NIH Public Access, 2017.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and Rabinovich]{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich.
\newblock {Training deep neural networks on noisy labels with bootstrapping}.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock {" Why should i trust you?" Explaining the predictions of any classifier}.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining}, pp.\  1135--1144, 2016.

\bibitem[Roger et~al.(2023)Roger, Greenblatt, Nadeau, Shlegeris, and Thomas]{roger2023measurement}
Fabien Roger, Ryan Greenblatt, Max Nadeau, Buck Shlegeris, and Nate Thomas.
\newblock {Measurement tampering detection benchmark}.
\newblock \emph{arXiv preprint arXiv:2308.15605}, 2023.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, Downey, and Rumshisky]{rogers2020getting}
Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky.
\newblock {Getting closer to {AI}} complete question answering: A set of prerequisite real tasks.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pp.\  8722--8731, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock {Imagenet large scale visual recognition challenge}.
\newblock \emph{International journal of computer vision}, 115:\penalty0 211--252, 2015.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock {Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization}.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Santurkar et~al.(2021)Santurkar, Tsipras, Elango, Bau, Torralba, and Madry]{santurkar2021editing}
Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry.
\newblock {Editing a classifier by rewriting its prediction rules}.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23359--23373, 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le~Bras, and Yejin Choi.
\newblock {Socialiqa: Commonsense reasoning about social interactions}.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022self}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.
\newblock {Self-critiquing models for assisting human evaluators}.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Schwarzschild et~al.(2021{\natexlab{a}})Schwarzschild, Borgnia, Gupta, Bansal, Emam, Huang, Goldblum, and Goldstein]{schwarzschild2021datasets}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Arpit Bansal, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein.
\newblock {Datasets for studying generalization from easy to hard examples}.
\newblock \emph{arXiv preprint arXiv:2108.06011}, 2021{\natexlab{a}}.

\bibitem[Schwarzschild et~al.(2021{\natexlab{b}})Schwarzschild, Borgnia, Gupta, Huang, Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein.
\newblock {Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks}.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 6695--6706, 2021{\natexlab{b}}.

\bibitem[Shu et~al.(2018)Shu, Bui, Narui, and Ermon]{shu2018dirt}
Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon.
\newblock {A DIRT-T Approach to Unsupervised Domain Adaptation}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning, Andrew~Y Ng, and Christopher Potts.
\newblock {Recursive deep models for semantic compositionality over a sentiment treebank}.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and Ré]{sohoni2020no}
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Ré.
\newblock {No subclass left behind: Fine-grained robustness in coarse-grained classification problems}.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 19339--19352, 2020.

\bibitem[Song et~al.(2022)Song, Kim, Park, Shin, and Lee]{song2022learning}
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee.
\newblock {Learning from noisy labels with deep neural networks: A survey}.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock {Dropout: a simple way to prevent neural networks from overfitting}.
\newblock \emph{The journal of machine learning research}, 15\penalty0 (1):\penalty0 1929--1958, 2014.

\bibitem[Stanton et~al.(2021)Stanton, Izmailov, Kirichenko, Alemi, and Wilson]{stanton2021does}
Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander~A Alemi, and Andrew~G Wilson.
\newblock {Does knowledge distillation really work?}
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 6906--6919, 2021.

\bibitem[Steinhardt(2022)]{steinhardt2022forecasting}
Jacob Steinhardt.
\newblock {AI Forecasting: One Year In}, 2022.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano.
\newblock {Learning to summarize with human feedback}.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Sun et~al.(2019)Sun, Yu, Chen, Yu, Choi, and Cardie]{sun2019dream}
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie.
\newblock {Dream: A challenge data set and models for dialogue-based reading comprehension}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 217--231, 2019.

\bibitem[Tafjord et~al.(2019)Tafjord, Gardner, Lin, and Clark]{quartz}
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark.
\newblock Quartz: An open-domain dataset of qualitative relationship questions.
\newblock \emph{arXiv preprint arXiv:1909.03553}, 2019.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {GLUE: A multi-task benchmark and analysis platform for natural language understanding}.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {Superglue: A stickier benchmark for general-purpose language understanding systems}.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Alex Warstadt, Amanpreet Singh, and Samuel Bowman.
\newblock {Neural network acceptability judgments}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 625--641, 2019.

\bibitem[Wei et~al.(2020)Wei, Shen, Chen, and Ma]{wei2020theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock {Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{weifinetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock {Finetuned Language Models are Zero-Shot Learners}.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock {Emergent abilities of large language models}.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner.
\newblock {Crowdsourcing multiple choice science questions}.
\newblock \emph{arXiv preprint arXiv:1707.06209}, 2017.

\bibitem[Wentworth(2020)]{wentworth2020alignment}
John Wentworth.
\newblock {Alignment by Default}.
\newblock \emph{AI Alignment Forum}, 2020.

\bibitem[Worley(2021)]{worley2021bootstrapped}
Gordon~Seidoh Worley.
\newblock {Bootstrapped Alignment}.
\newblock \emph{AI Alignment Forum}, 2021.

\bibitem[Wortsman et~al.(2022{\natexlab{a}})Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et~al.
\newblock {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  23965--23998. PMLR, 2022{\natexlab{a}}.

\bibitem[Wortsman et~al.(2022{\natexlab{b}})Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs, Lopes, Hajishirzi, Farhadi, Namkoong, et~al.]{wortsman2022robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et~al.
\newblock {Robust fine-tuning of zero-shot models}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7959--7971, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2021)Wu, Ouyang, Ziegler, Stiennon, Lowe, Leike, and Christiano]{wu2021recursively}
Jeff Wu, Long Ouyang, Daniel~M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.
\newblock {Recursively summarizing books with human feedback}.
\newblock \emph{arXiv preprint arXiv:2109.10862}, 2021.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock {Self-training with noisy student improves imagenet classification}.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10687--10698, 2020.

\bibitem[Yi \& Wu(2019)Yi and Wu]{Yi_2019_CVPR}
Kun Yi and Jianxin Wu.
\newblock {Probabilistic End-To-End Noise Correction for Learning With Noisy Labels}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {HellaSwag: Can a Machine Really Finish Your Sentence?}
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Lemoine, and Mitchell]{zhang2018mitigating}
Brian~Hu Zhang, Blake Lemoine, and Margaret Mitchell.
\newblock {Mitigating unwanted biases with adversarial learning}.
\newblock In \emph{Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, pp.\  335--340, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Baldridge, and He]{paws2019naacl}
Yuan Zhang, Jason Baldridge, and Luheng He.
\newblock {PAWS: Paraphrase Adversaries from Word Scrambling}.
\newblock In \emph{Proc. of NAACL}, 2019.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock {Generalized cross entropy loss for training deep neural networks with noisy labels}.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Khashabi, Ning, and Roth]{ZKNR19}
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth.
\newblock {“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding}.
\newblock In \emph{EMNLP}, 2019.

\end{thebibliography}
