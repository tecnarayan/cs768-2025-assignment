@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning},
  volume={2},
  pages={267--274},
  year={2002}
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}


@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={NIPS Deep Learning Workshop},
  year={2013}
}



@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={682--697},
  year={2008},
  publisher={Elsevier}
}

@article{schulman2016high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@inproceedings{vuong2019supervised,
  title={Supervised Policy Update for Deep Reinforcement Learning},
  author={Vuong, Quan and Zhang, Yiming and Ross, Keith W},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2019}
}

@misc{https://doi.org/10.48550/arxiv.1909.05011,
  doi = {10.48550/ARXIV.1909.05011},
  url = {https://arxiv.org/abs/1909.05011},
  author = {Agnihotri, Akhil and Pratyush, Sai Akula and Gupta, Amit Kumar},
  keywords = {Applied Physics (physics.app-ph), Materials Science (cond-mat.mtrl-sci), FOS: Physical sciences, FOS: Physical sciences},
  title = {A Review on Superplastic Forming of Ti-6Al-4V Alloy},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{miryoosefi2019reinforcement,
  title={Reinforcement Learning with Convex Constraints},
  author={Miryoosefi, Sobhan and Brantley, Kiant{\'e} and Daum{\'e} III, Hal and Dudik, Miroslav and Schapire, Robert},
  journal={arXiv preprint arXiv:1906.09323},
  year={2019}
}

@INPROCEEDINGS{9030307,  author={Agnihotri, Akhil and Saraf, Prathamesh and Bapnad, Kriti Rajesh},  booktitle={2019 IEEE 16th India Council International Conference (INDICON)},   title={A Convolutional Neural Network Approach Towards Self-Driving Cars},   year={2019},  volume={},  number={},  pages={1-4},  doi={10.1109/INDICON47234.2019.9030307}}



@article{borkar2005actor,
  title={An actor-critic algorithm for constrained Markov decision processes},
  author={Borkar, Vivek S},
  journal={Systems \& control letters},
  volume={54},
  number={3},
  pages={207--213},
  year={2005},
  publisher={Elsevier}
}

@article{bhatnagar2012online,
  title={An online actor--critic algorithm with function approximation for constrained markov decision processes},
  author={Bhatnagar, Shalabh and Lakshmanan, K},
  journal={Journal of Optimization Theory and Applications},
  volume={153},
  number={3},
  pages={688--708},
  year={2012},
  publisher={Springer}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}

@book{kemeny1960finite,
  title={Finite {M}arkov {C}hains},
  author={Kemeny, J.G. and Snell, I.J.},
  year={1960},
  publisher={Van Nostrand, New Jersey}
}

@book{puterman1994markov,
  title={Markov {D}ecision {P}rocesses: {D}iscrete {S}tochastic {D}ynamic {P}rogramming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons}
}


@article{zhang2020first,
  title={First Order Constrained Optimization in Policy Space},
  author={Zhang, Yiming and Vuong, Quan and Ross, Keith},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{abdolmaleki2018maximum,
  title={Maximum a posteriori policy optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={International Conference on Learning Representation (ICLR)},
  year={2018}
}

@inproceedings{yang2020projection,
  title={Projection-Based Constrained Policy Optimization},
  author={Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2020}
}

@article{tibshirani2017dykstra,
  title={Dykstra's algorithm, ADMM, and coordinate descent: Connections, insights, and extensions},
  author={Tibshirani, Ryan J},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}



@InProceedings{pmlr-v139-zhang21q,
  title = 	 {On-Policy Deep Reinforcement Learning for the Average-Reward Criterion},
  author =       {Zhang, Yiming and Ross, Keith W},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12535--12545},
  year = 	 {2021},
  month = 	 {18--24 Jul},
  abstract = 	 {We develop theory and algorithms for average-reward on-policy Reinforcement Learning (RL). We first consider bounding the difference of the long-term average reward for two policies. We show that previous work based on the discounted return (Schulman et al. 2015, Achiam et al. 2017) results in a non-meaningful lower bound in the average reward setting. By addressing the average-reward criterion directly, we then derive a novel bound which depends on the average divergence between the policies and on Kemeny’s constant. Based on this bound, we develop an iterative procedure which produces a sequence of monotonically improved policies for the average reward criterion. This iterative procedure can then be combined with classic Deep Reinforcement Learning (DRL) methods, resulting in practical DRL algorithms that target the long-run average reward criterion. In particular, we demonstrate that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.}
}


@article{calvo2023state,
  title={State augmented constrained reinforcement learning: Overcoming the limitations of learning with rewards},
  author={Calvo-Fullana, Miguel and Paternain, Santiago and Chamon, Luiz FO and Ribeiro, Alejandro},
  journal={IEEE Transactions on Automatic Control},
  year={2023},
  publisher={IEEE}
}

@misc{Bhatnagar2012,
author = {Bhatnagar, Shalabh and Lakshmanan, K.},
title = {{A}n {O}nline {A}ctor–{C}ritic {A}lgorithm with {F}unction {A}pproximation for {C}onstrained {M}arkov {D}ecision {P}rocesses},
howpublished = {\url{https://doi.org/10.1007/s10957-012-9989-5}},
year = {2012},
note = {[Accessed 08-10-2023]},
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@InProceedings{pmlr-v119-satija20a,
  title = 	 {Constrained {M}arkov Decision Processes via Backward Value Functions},
  author =       {Satija, Harsh and Amortila, Philip and Pineau, Joelle},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8502--8511},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/satija20a/satija20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/satija20a.html},
  abstract = 	 {Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{pirotta2013safe,
  title={Safe policy iteration},
  author={Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
  booktitle={International Conference on Machine Learning},
  pages={307--315},
  year={2013}
}

@inproceedings{wu2017scalable,
  title={Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle={Advances in neural information processing systems (NIPS)},
  pages={5285--5294},
  year={2017}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={International Conference on Machine Learning (ICML)},
  year={2018}
}

@misc{brockman2016openai,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@article{naik2019discounted,
  title={Discounted reinforcement learning is not an optimization problem},
  author={Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Sutton, Richard S},
  journal={NeurIPS Optimization Foundations for Reinforcement Learning Workshop},
  year={2019}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@misc{achiam2017advanced,
  author = "Joshua Achiam",
  title  = "{{UC} {B}erkeley {CS} 285 ({F}all 2017), Advanced Policy Gradients}",
  note   = "URL: \url{http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf}. 
            Last visited on 2020/05/24",
  year   = 2017,
}


@inproceedings{abbasi2019politex,
  title={POLITEX: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019}
}

@article{wei2019model,
  title={Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes},
  author={Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  journal={arXiv preprint arXiv:1910.07072},
  year={2019}
}

@article{mahadevan1996average,
  title={Average reward reinforcement learning: Foundations, algorithms, and empirical results},
  author={Mahadevan, Sridhar},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={159--195},
  year={1996},
  publisher={Springer}
}

@article{blackwell1962discrete,
  title={Discrete dynamic programming},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={719--726},
  year={1962},
  publisher={JSTOR}
}

@book{tsybakov2008introduction,
  title={Introduction to nonparametric estimation},
  author={Tsybakov, Alexandre B},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@inproceedings{schwartz1993reinforcement,
  title={A reinforcement learning method for maximizing undiscounted rewards},
  author={Schwartz, Anton},
  booktitle={Proceedings of the tenth international conference on machine learning},
  volume={298},
  pages={298--305},
  year={1993}
}


@book{bertsekas1995dynamic,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  volume={1,2},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

@book{kallenberg1983linear,
  title={Linear Programming and Finite Markovian Control Problems},
  author={Kallenberg, L.C.M.},
  year={1983},
  publisher={Centrum Voor Wiskunde en Informatica}
}

@article{abounadi2001learning,
  title={Learning algorithms for Markov decision processes with average cost},
  author={Abounadi, Jinane and Bertsekas, D and Borkar, Vivek S},
  journal={SIAM Journal on Control and Optimization},
  volume={40},
  number={3},
  pages={681--698},
  year={2001},
  publisher={SIAM}
}

@book{howard1960dynamic,
  title={Dynamic programming and markov processes.},
  author={Howard, Ronald A},
  year={1960},
  publisher={John Wiley}
}

@article{veinott1966finding,
  title={On finding optimal policies in discrete dynamic programming with no discounting},
  author={Veinott, Arthur F},
  journal={The Annals of Mathematical Statistics},
  volume={37},
  number={5},
  pages={1284--1294},
  year={1966},
  publisher={JSTOR}
}

@book{lehmann2006theory,
  title={Theory of point estimation},
  author={Lehmann, Erich L and Casella, George},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{strang2007computational,
  title={Computational science and engineering},
  author={Strang, Gilbert},
  publisher={Wellesley-Cambridge Press},
  year=2007
}

@article{ross1985constrained,
  title={Constrained Markov decision processes with queueing applications.},
  author={Ross, Keith W},
  journal={Dissertation Abstracts International Part B: Science and Engineering[DISS. ABST. INT. PT. B- SCI. \& ENG.],},
  volume={46},
  number={4},
  year={1985}
}


@inproceedings{zhao2011analysis,
  title={Analysis and improvement of policy gradient estimation},
  author={Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={262--270},
  year={2011}
}

@article{tessler2018reward,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  journal={International Conference on Learning Representation (ICLR)},
  year={2019}
}

@book{hunter2014mathematical,
  title={Mathematical techniques of applied probability: Discrete time models: Basic theory},
  author={Hunter, Jeffrey J},
  volume={1},
  year={2014},
  publisher={Academic Press}
}

@article{levene2002kemeny,
  title={Kemeny's constant and the random surfer},
  author={Levene, Mark and Loizou, George},
  journal={The American mathematical monthly},
  volume={109},
  number={8},
  pages={741--745},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{ma2021average,
  title={Average-Reward Reinforcement Learning with Trust Region Methods},
  author={Ma, Xiaoteng and Tang, Xiaohang and Xia, Li and Yang, Jun and Zhao, Qianchuan},
  journal={arXiv preprint arXiv:2106.03442},
  year={2021}
}

@inproceedings{duan2016benchmarking,
  title     = {Benchmarking deep reinforcement learning for continuous control},
  author    = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle = {International Conference on Machine Learning},
  pages     = {1329--1338},
  year      = {2016}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garcia, Javier and Fernandez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}


@inproceedings{wan2021learning,
  title={Learning and planning in average-reward markov decision processes},
  author={Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  booktitle={International Conference on Machine Learning},
  pages={10653--10662},
  year={2021},
  organization={PMLR}
}

@article{liao2022batch,
  title={Batch policy learning in average reward markov decision processes},
  author={Liao, Peng and Qi, Zhengling and Wan, Runzhe and Klasnja, Predrag and Murphy, Susan A},
  journal={The Annals of Statistics},
  volume={50},
  number={6},
  pages={3364--3387},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{senior2020improved,
  title     = {Improved protein structure prediction using potentials from deep learning},
  author    = {Senior, Andrew W and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'\i}dek, Augustin and Nelson, Alexander WR and Bridgland, Alex and others},
  journal   = {Nature},
  volume    = {577},
  number    = {7792},
  pages     = {706--710},
  year      = {2020},
  publisher = {Nature Publishing Group}
}

@article{haviv1996constrained,
  title={On constrained Markov decision processes},
  author={Haviv, Moshe},
  journal={Operations research letters},
  volume={19},
  number={1},
  pages={25--28},
  year={1996},
  publisher={Elsevier}
}

@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@inproceedings{thomas2014bias,
  title={Bias in natural actor-critic algorithms},
  author={Thomas, Philip},
  booktitle={International conference on machine learning},
  pages={441--448},
  year={2014},
  organization={PMLR}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016}
}

@inproceedings{wen2020safe,
  title={Safe reinforcement learning for autonomous vehicles through parallel constrained policy optimization},
  author={Wen, Lu and Duan, Jingliang and Li, Shengbo Eben and Xu, Shaobing and Peng, Huei},
  booktitle={2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{koch2019reinforcement,
  title={Reinforcement learning for UAV attitude control},
  author={Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
  journal={ACM Transactions on Cyber-Physical Systems},
  volume={3},
  number={2},
  pages={1--21},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@InProceedings{Hu_2022_CVPR,
    author    = {Hu, Hanjiang and Liu, Zuxin and Chitlangia, Sharad and Agnihotri, Akhil and Zhao, Ding},
    title     = {Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {2550-2559}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{wei2021learning,
  title={Learning infinite-horizon average-reward MDPs with linear function approximation},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Jain, Rahul},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3007--3015},
  year={2021},
  organization={PMLR}
}

@inproceedings{vinitsky2018benchmarks,
  title={Benchmarks for reinforcement learning in mixed-autonomy traffic},
  author={Vinitsky, Eugene and Kreidieh, Aboudy and Le Flem, Luc and Kheterpal, Nishant and Jang, Kathy and Wu, Fangyu and Liaw, Richard and Liang, Eric and Bayen, Alexandre M.},
  booktitle={Proceedings of Conference on Robot Learning},
  pages={399--409},
  year={2018}
}

@article{zhang2021average,
  title={Average-Reward Off-Policy Policy Evaluation with Function Approximation},
  author={Zhang, Shangtong and Wan, Yi and Sutton, Richard S and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2101.02808},
  year={2021}
}


@article{gao2018reinforcement,
  title={Reinforcement learning from imperfect demonstrations},
  author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
  journal={arXiv preprint arXiv:1802.05313},
  year={2018}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics},
  pages={627--635},
  year={2011}
}

@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}


@article{chow2017risk,
  title={Risk-constrained reinforcement learning with percentile risk criteria},
  author={Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6070--6120},
  year={2017},
  publisher={JMLR. org}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{zhou2020smarts,
  title={Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving},
  author={Zhou, Ming and Luo, Jun and Villella, Julian and Yang, Yaodong and Rusu, David and Miao, Jiayu and Zhang, Weinan and Alban, Montgomery and Fadakar, Iman and Chen, Zheng and others},
  journal={arXiv preprint arXiv:2010.09776},
  year={2020}
}


@article{doyle2009kemeny,
  title={The Kemeny constant of a Markov chain},
  author={Doyle, Peter G},
  journal={arXiv preprint arXiv:0909.2636},
  year={2009}
}

@article{hunter2006mixing,
  title={Mixing times with applications to perturbed Markov chains},
  author={Hunter, Jeffrey J},
  journal={Linear Algebra and its Applications},
  volume={417},
  number={1},
  pages={108--123},
  year={2006},
  publisher={Elsevier}
}

@article{hunter2018computation,
  title={The computation of the mean first passage times for Markov chains},
  author={Hunter, Jeffrey J},
  journal={Linear Algebra and its Applications},
  volume={549},
  pages={100--122},
  year={2018},
  publisher={Elsevier}
}

@article{zhang2020average,
  title={Average Reward Reinforcement Learning with Monotonic Policy Improvement},
  author={Zhang, Yiming and Ross, Keith},
  journal={Preprint},
  year={2020}
}

@article{wan2020learning,
  title={Learning and Planning in Average-Reward Markov Decision Processes},
  author={Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  journal={arXiv preprint arXiv:2006.16318},
  year={2020}
}

@article{agarwal2019theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@misc{aldous1995reversible,
  title={Reversible Markov chains and random walks on graphs},
  author={Aldous, David and Fill, James},
  year={1995},
  publisher={Berkeley}
}

@book{grinstead2012introduction,
  title={Introduction to probability},
  author={Grinstead, Charles Miller and Snell, James Laurie},
  year={2012},
  publisher={American Mathematical Soc.}
}

@article{even2009online,
  title={Online Markov decision processes},
  author={Even-Dar, Eyal and Kakade, Sham M and Mansour, Yishay},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={726--736},
  year={2009},
  publisher={INFORMS}
}

@inproceedings{neu2010online,
  title={Online Markov decision processes under bandit feedback},
  author={Neu, Gergely and Antos, Andras and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1804--1812},
  year={2010}
}

@article{cho2001comparison,
  title={Comparison of perturbation bounds for the stationary distribution of a Markov chain},
  author={Cho, Grace E and Meyer, Carl D},
  journal={Linear Algebra and its Applications},
  volume={335},
  number={1-3},
  pages={137--150},
  year={2001},
  publisher={North-Holland}
}

@article{hunter2005stationary,
  title={Stationary distributions and mean first passage times of perturbed Markov chains},
  author={Hunter, Jeffrey J},
  journal={Linear Algebra and its Applications},
  volume={410},
  pages={217--243},
  year={2005},
  publisher={Elsevier}
}

@article{meyer1975role,
  title={The role of the group generalized inverse in the theory of finite Markov chains},
  author={Meyer, Jr, Carl D},
  journal={Siam Review},
  volume={17},
  number={3},
  pages={443--464},
  year={1975},
  publisher={SIAM}}
  
@inproceedings{wei2020model,
  title={Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes},
  author={Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  booktitle={International conference on machine learning},
  year={2020}
}

@inproceedings{yang2016efficient,
  title={Efficient Average Reward Reinforcement Learning Using Constant Shifting Values.},
  author={Yang, Shangdong and Gao, Yang and An, Bo and Wang, Hao and Chen, Xingguo},
  booktitle={AAAI},
  pages={2258--2264},
  year={2016}
}

@article{baxter2001infinite,
  title={Infinite-horizon policy-gradient estimation},
  author={Baxter, Jonathan and Bartlett, Peter L},
  journal={Journal of Artificial Intelligence Research},
  volume={15},
  pages={319--350},
  year={2001}
}

@inproceedings{kakade2001optimizing,
  title={Optimizing average reward using discounted rewards},
  author={Kakade, Sham},
  booktitle={International Conference on Computational Learning Theory},
  pages={605--615},
  year={2001},
  organization={Springer}
}

@inproceedings{amit2020discount,
  title={Discount Factor as a Regularizer in Reinforcement Learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={International conference on machine learning},
  year={2020}
}

@article{andrychowicz2020matters,
  title={What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
  author={Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and others},
  journal={arXiv preprint arXiv:2006.05990},
  year={2020}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{song2020v,
  title={V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control},
  author={Song, H Francis and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Clark, Aidan and Soyer, Hubert and Rae, Jack W and Noury, Seb and Ahuja, Arun and Liu, Siqi and Tirumala, Dhruva and others},
  journal={International Conference on Learning Representations},
  year={2020}
}

@book{bremaud2020markov,
  title={Markov Chains Gibbs Fields, Monte Carlo Simulation and Queues},
  author={Br{\'e}maud, Pierre},
  year=2020,
  edition=2,
  publisher={Springer}
}

@techreport{tadepalli1994h,
  title={H-learning: A reinforcement learning method to optimize undiscounted average reward},
  author={Tadepalli, Prasad and Ok, DoKyeong},
  number={94-30-01},
  institution = {Oregon State University},
  year={1994}
}

@article{ross1986optimal,
  title={Optimal dynamic routing in Markov queueing networks},
  author={Ross, Keith W},
  journal={Automatica},
  volume={22},
  number={3},
  pages={367--370},
  year={1986},
  publisher={Elsevier}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{lillicrap2016continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{petrik2008biasing,
  title={Biasing approximate dynamic programming with a lower discount factor},
  author={Petrik, Marek and Scherrer, Bruno},
  booktitle={Twenty-Second Annual Conference on Neural Information Processing Systems-NIPS 2008},
  year={2008}
}


@InProceedings{pmlr-v162-chen22i,
  title = 	 {Learning Infinite-horizon Average-reward {M}arkov Decision Process with Constraints},
  author =       {Chen, Liyu and Jain, Rahul and Luo, Haipeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {3246--3270},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/chen22i/chen22i.pdf},
  url = 	 {https://proceedings.mlr.press/v162/chen22i.html},
  abstract = 	 {We study regret minimization for infinite-horizon average-reward Markov Decision Processes (MDPs) under cost constraints. We start by designing a policy optimization algorithm with carefully designed action-value estimator and bonus term, and show that for ergodic MDPs, our algorithm ensures $O(\sqrt{T})$ regret and constant constraint violation, where $T$ is the total number of time steps. This strictly improves over the algorithm of (Singh et al., 2020), whose regret and constraint violation are both $O(T^{2/3})$. Next, we consider the most general class of weakly communicating MDPs. Through a finite-horizon approximation, we develop another algorithm with $O(T^{2/3})$ regret and constraint violation, which can be further improved to $O(\sqrt{T})$ via a simple modification, albeit making the algorithm computationally inefficient. As far as we know, these are the first set of provable algorithms for weakly communicating MDPs with cost constraints.}
}


@inproceedings{jiang2015dependence,
  title={The dependence of effective planning horizon on model accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages={1181--1189},
  year={2015},
  organization={Citeseer}
}



@inproceedings{jiang2016structural,
  title={On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={IJCAI},
  pages={1640--1647},
  year={2016}
}

@inproceedings{lehnert2018value,
  title={On value function representation of long horizon problems},
  author={Lehnert, Lucas and Laroche, Romain and van Seijen, Harm},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

%===RAHUL ADDED BEGIN===

@article{manne1960linear,
  title={Linear programming and sequential decisions},
  author={Manne, Alan S},
  journal={Management Science},
  volume={6},
  number={3},
  pages={259--267},
  year={1960},
  publisher={INFORMS}
}

@article{hordijk1979linear,
  title={Linear programming and Markov decision chains},
  author={Hordijk, Arie and Kallenberg, LCM},
  journal={Management Science},
  volume={25},
  number={4},
  pages={352--362},
  year={1979},
  publisher={INFORMS}
}

@article{borkar1988convex,
  title={A convex analytic approach to Markov decision processes},
  author={Borkar, Vivek S},
  journal={Probability Theory and Related Fields},
  volume={78},
  number={4},
  pages={583--602},
  year={1988},
  publisher={Springer}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@article{aractingi2023controlling,
  title={Controlling the Solo12 quadruped robot with deep reinforcement learning},
  author={Aractingi, Michel and L{\'e}ziart, Pierre-Alexandre and Flayols, Thomas and Perez, Julien and Silander, Tomi and Sou{\`e}res, Philippe},
  journal={scientific Reports},
  volume={13},
  number={1},
  pages={11945},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{moskovitz2023confronting,
  title={Confronting reward model overoptimization with constrained rlhf},
  author={Moskovitz, Ted and Singh, Aaditya K and Strouse, DJ and Sandholm, Tuomas and Salakhutdinov, Ruslan and Dragan, Anca D and McAleer, Stephen},
  journal={arXiv preprint arXiv:2310.04373},
  year={2023}
}

%===RAHUL ADDED END====
