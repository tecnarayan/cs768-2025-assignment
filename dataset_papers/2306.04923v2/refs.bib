% =========
% --- A ---
% =========
@misc{abbasiyadkori2022new,
      title={A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits},
      author={Yasin Abbasi-Yadkori and Andras Gyorgy and Nevena Lazic},
      year={2022},
      eprint={2201.06532},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{abernathy2021last,
  title = 	 {Last-Iterate Convergence Rates for Min-Max Optimization: Convergence of Hamiltonian Gradient Descent and Consensus Optimization},
  author =       {Abernethy, Jacob and Lai, Kevin A. and Wibisono, Andre},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {3--47},
  year = 	 {2021},
  editor = 	 {Vitaly Feldman and Katrina Ligett and Sivan Sabato},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  abstract = {While classic work in convex-concave min-max optimization relies
                  on average-iterate convergence results, the emergence of
                  nonconvex applications such as training Generative Adversarial
                  Networks has led to renewed interest in last-iterate
                  convergence guarantees. Proving last-iterate convergence is
                  challenging because many natural algorithms, such as
                  Simultaneous Gradient Descent/Ascent, provably diverge or
                  cycle even in simple convex-concave min-max settings, and
                  there are relatively few papers that prove global last-iterate
                  convergence rates beyond the bilinear and convex-strongly
                  concave settings. In this work, we show that the Hamiltonian
                  Gradient Descent (HGD) algorithm achieves linear convergence
                  in a variety of more general settings, including
                  convex-concave problems that satisfy a "sufficiently bilinear"
                  condition. We also prove convergence rates for stochastic HGD
                  and for some parameter settings of the Consensus Optimization
                  algorithm of Mescheder et al. (2017).}
}
@article{agarwal2021efficient,
  title={Efficient Methods for Online Multiclass Logistic Regression},
  author={Agarwal, Naman and Kale, Satyen and Zimmert, Julian},
  year={2021},
  journal={arXiv preprint arXiv:2110.03020},
}
@article{asi2021private,
  author    = {Hilal Asi and
               John C. Duchi and
               Alireza Fallah and
               Omid Javidbakht and
               Kunal Talwar},
  title     = {Private Adaptive Gradient Methods for Convex Optimization},
  journal   = {CoRR},
  volume    = {abs/2106.13756},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2106.13756},
  timestamp = {Wed, 30 Jun 2021 16:14:10 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{admadi2020strategic,
  author    = {Saba Ahmadi and
               Hedyeh Beyhaghi and
               Avrim Blum and
               Keziah Naggita},
  title     = {The Strategic Perceptron},
  journal   = {CoRR},
  volume    = {abs/2008.01710},
  year      = {2020},
  archivePrefix = {arXiv},
  eprint    = {2008.01710},
  timestamp = {Fri, 07 Aug 2020 15:07:21 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{agarwal2020pcpg,
      title={PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning},
      author={Alekh Agarwal and Mikael Henaff and Sham Kakade and Wen Sun},
      year={2020},
      eprint={2007.08459},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{ahn2020fromnesterov,
  title = 	 {From Nesterov’s Estimate Sequence to Riemannian Acceleration},
  author =       {Ahn, Kwangjun and Sra, Suvrit},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {84--118},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  abstract = 	 { We propose the first global accelerated gradient method for
                  Riemannian manifolds. Toward establishing our results, we
                  revisit Nesterov’s estimate sequence technique and develop a
                  conceptually simple alternative from first principles. We then
                  extend our analysis to Riemannian acceleration, localizing the
                  key difficulty into “metric distortion.” We control this
                  distortion via a novel geometric inequality, which enables us
                  to formulate and analyze global Riemannian acceleration.}
}
@misc{arjevani2019lower,
      title={Lower Bounds for Non-Convex Stochastic Optimization},
      author={Yossi Arjevani and Yair Carmon and John C. Duchi and Dylan J. Foster and Nathan Srebro and Blake Woodworth},
      year={2019},
      eprint={1912.02365},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{arnold2019reducing,
  title={Reducing the variance in online optimization by transporting past gradients},
  author={Arnold, S{\'e}bastien MR and Manzagol, Pierre-Antoine and Babanezhad, Reza and Mitliagkas, Ioannis and Roux, Nicolas Le},
  year={2019},
  journal={arXiv preprint arXiv:1906.03532},
}
@article{asi2019stochastic,
   title={Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity},
   author={Asi, Hilal and Duchi, John C.},
   year={2019},
   volume={29},
   ISSN={1095-7189},
   DOI={10.1137/18m1230323},
   number={3},
   journal={SIAM Journal on Optimization},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   month={Jan},
   pages={2257–2290}
}
@InProceedings{abernathy2014online,
  title = 	 {Online Linear Optimization via Smoothing},
  author = 	 {Jacob Abernethy and Chansoo Lee and Abhinav Sinha and Ambuj Tewari},
  booktitle = 	 {Proceedings of The 27th Conference on Learning Theory},
  pages = 	 {807--823},
  year = 	 {2014},
  editor = 	 {Maria Florina Balcan and Vitaly Feldman and Csaba Szepesvári},
  volume = 	 {35},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Barcelona, Spain},
  month = 	 {13--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {We present a new optimization-theoretic approach to analyzing
                  Follow-the-Leader style algorithms, particularly in the
                  setting where perturbations are used as a tool for
                  regularization. We show that adding a strongly convex penalty
                  function to the decision rule and adding stochastic
                  perturbations to data correspond to deterministic and
                  stochastic smoothing operations, respectively. We establish an
                  equivalence between “Follow the Regularized Leader” and
                  “Follow the Perturbed Leader” up to the smoothness properties.
                  This intuition leads to a new generic analysis framework that
                  recovers and improves the previous known regret bounds of the
                  class of algorithms commonly known as Follow the Perturbed
                  Leader.}
}
@ARTICLE{agarwal2013generalization,
  author={A. {Agarwal} and J. C. {Duchi}},
  title={The Generalization Ability of Online Algorithms for Dependent Data},
  year={2013},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={1},
  pages={573-587},
  doi={10.1109/TIT.2012.2212414}}
@InProceedings{anava2013online,
  title = 	 {Online Learning for Time Series Prediction},
  author = 	 {Oren Anava and Elad Hazan and Shie Mannor and Ohad Shamir},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {172--184},
  year = 	 {2013},
  editor = 	 {Shai Shalev-Shwartz and Ingo Steinwart},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  abstract = 	 {In this paper, we address the problem of predicting a time
                  series using the ARMA (autoregressive moving average) model,
                  under minimal assumptions on the noise terms. Using regret
                  minimization techniques, we develop effective online learning
                  algorithms for the prediction problem, \emphwithout assuming
                  that the noise terms are Gaussian, identically distributed or
                  even independent. Furthermore, we show that our algorithm’s
                  performances asymptotically approaches the performance of the
                  best ARMA model in hindsight.}
}
@InProceedings{abbasi2012online,
  title = 	 {Online-to-Confidence-Set Conversions and Application to Sparse Stochastic Bandits},
  author = 	 {Yasin Abbasi-Yadkori and David Pal and Csaba Szepesvari},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1--9},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  abstract = 	 {We introduce a novel technique, which we call
                  online-to-confidence-set conversion. The technique allows us
                  to construct high-probability confidence sets for linear
                  prediction  with correlated inputs given the predictions of
                  any algorithm   (e.g., online LASSO, exponentiated gradient
                  algorithm,   online least-squares, p-norm algorithm)
                  targeting online learning  with linear predictors and the
                  quadratic loss. By construction, the size of the confidence
                  set is   directly governed by the regret of the online
                  learning algorithm. Constructing tight confidence sets is
                  interesting on its own, but  the new technique is given extra
                  weight by the fact   having access tight confidence sets
                  underlies a number of    important problems. The advantage of
                  our construction here is that  progress in constructing better
                  algorithms for online prediction problems   directly
                  translates into tighter confidence sets. In this paper, this
                  is demonstrated in the case of linear stochastic bandits. In
                  particular, we introduce the sparse variant of linear
                  stochastic  bandits and show that a recent online algorithm
                  together with our  online-to-confidence-set conversion allows
                  one to derive  algorithms that can exploit if the reward is a
                  function of a sparse   linear combination of the components of
                  the chosen action.}
}
@article{audibert2009exploration,
author = {Jean-Yves Audibert and Rémi Munos and Csaba Szepesvári},
title = {Exploration–exploitation tradeoff using variance estimates in multi-armed bandits},
journal = {Theoretical Computer Science},
volume = {410},
number = {19},
pages = {1876-1902},
year = {2009},
note = {Algorithmic Learning Theory},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2009.01.016},
keywords = {Exploration–exploitation tradeoff, Multi-armed bandits, Bernstein’s inequality, High-probability bound, Risk analysis},
abstract = {Algorithms based on upper confidence bounds for balancing
                  exploration and exploitation are gaining popularity since they
                  are easy to implement, efficient and effective. This paper
                  considers a variant of the basic algorithm for the stochastic,
                  multi-armed bandit problem that takes into account the
                  empirical variance of the different arms. In earlier
                  experimental works, such algorithms were found to outperform
                  the competing algorithms. We provide the first analysis of the
                  expected regret for such algorithms. As expected, our results
                  show that the algorithm that uses the variance estimates has a
                  major advantage over its alternatives that do not use such
                  estimates provided that the variances of the payoffs of the
                  suboptimal arms are low. We also prove that the regret
                  concentrates only at a polynomial rate. This holds for all the
                  upper confidence bound based algorithms and for all bandit
                  problems except those special ones where with probability one
                  the payoff obtained by pulling the optimal arm is larger than
                  the expected payoff for the second best arm. Hence, although
                  upper confidence bound bandit algorithms achieve logarithmic
                  expected regret rates, they might not be suitable for a
                  risk-averse decision maker. We illustrate some of the results
                  by computer simulations.}
}
@inproceedings{attouch1989epigraphical,
  title={Epigraphical Analysis},
  author={Attouch, Hedy and Wets, RJ-B},
  booktitle={Annales de l'Institut Henri Poincar{\'e} C, Analyse non lin{\'e}aire},
  volume={6},
  pages={73--100},
  year={1989},
  organization={Elsevier}
}
% =========
% --- B ---
% =========
@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}
@book{bellemare2022distributional,
    title={Distributional Reinforcement Learning},
    author={Marc G. Bellemare and Will Dabney and Mark Rowland},
    year={2022},
    publisher={MIT Press},
}
@inproceedings{baby2021optimal,
  title = {Optimal Dynamic Regret in Exp-Concave Online Learning},
  author = {Baby, Dheeraj and Wang, Yu-Xiang},
  booktitle = {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = {359--409},
  year = {2021},
  editor = {Belkin, Mikhail and Kpotufe, Samory},
  volume = {134},
  series = {Proceedings of Machine Learning Research},
  month = {15--19 Aug},
  publisher = {PMLR},
  abstract = {We consider the problem of the Zinkevich (2003)-style dynamic regret minimization in online learning with \emph{exp-concave} losses. We show that whenever improper learning is allowed, a Strongly Adaptive online learner achieves the dynamic regret of $\tilde O^*(n^{1/3}C_n^{2/3} \vee 1)$ where $C_n$ is the \emph{total variation} (a.k.a. \emph{path length}) of the an arbitrary sequence of comparators that may not be known to the learner ahead of time. Achieving this rate was highly nontrivial even for square losses in 1D where the best known upper bound was $O(\sqrt{nC_n} \vee \log n)$ (Yuan and Lamperski, 2019). Our new proof techniques make elegant use of the intricate structures of the primal and dual variables imposed by the KKT conditions and could be of independent interest. Finally, we apply our results to the classical statistical problem of \emph{locally adaptive non-parametric regression} (Mammen, 1991; Donoho and Johnstone, 1998) and obtain a stronger and more flexible algorithm that do not require any statistical assumptions or any hyperparameter tuning.},
}
@misc{bartlett2021deep,
      title={Deep learning: a statistical viewpoint},
      author={Peter L. Bartlett and Andrea Montanari and Alexander Rakhlin},
      year={2021},
      eprint={2103.09177},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}
@inproceedings{bhaskara2021logarithmic,
title={Logarithmic Regret from Sublinear Hints},
author={Aditya Bhaskara and Ashok Cutkosky and Ravi Kumar and Manish Purohit},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
}
@misc{bilodeau2021minimax,
      title={Minimax Rates for Conditional Density Estimation via Empirical Entropy},
      author={Blair Bilodeau and Dylan J. Foster and Daniel M. Roy},
      year={2021},
      eprint={2109.10461},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}
@inproceedings{bhaskara2020onlinelinear,
 author = {Bhaskara, Aditya and Cutkosky, Ashok and Kumar, Ravi and Purohit, Manish},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {9530--9539},
 publisher = {Curran Associates, Inc.},
 title = {Online Linear Optimization with Many Hints},
 volume = {33},
 year = {2020}
}
@InProceedings{bhaskara2020onlinelearning,
  title = 	 {Online Learning with Imperfect Hints},
  author =       {Bhaskara, Aditya and Cutkosky, Ashok and Kumar, Ravi and Purohit, Manish},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {822--831},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = 	 {We consider a variant of the classical online linear
                  optimization problem in which at every step, the online player
                  receives a “hint” vector before choosing the action for that
                  round. Rather surprisingly, it was shown that if the hint
                  vector is guaranteed to have a positive correlation with the
                  cost vector, then the online player can achieve a regret of
                  $O(\log T)$, thus significantly improving over the
                  $O(\sqrt{T})$ regret in the general setting. However, the
                  result and analysis require the correlation property at
                  \emph{all} time steps, thus raising the natural question: can
                  we design online learning algorithms that are resilient to bad
                  hints? In this paper we develop algorithms and nearly matching
                  lower bounds for online learning with imperfect hints. Our
                  algorithms are oblivious to the quality of the hints, and the
                  regret bounds interpolate between the always-correlated hints
                  case and the no-hints case. Our results also generalize,
                  simplify, and improve upon previous results on optimistic
                  regret bounds, which can be viewed as an additive version of
                  hints.}
}
@article{bresler2020least,
  title={Least Squares Regression with Markovian Data: Fundamental Limits and Algorithms},
  author={Bresler, Guy and Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth and Wu, Xian},
  journal={arXiv preprint arXiv:2006.08916},
  year={2020}
}
@article{bach2019universal,
  author    = {Francis R. Bach and
               Kfir Y. Levy},
  title     = {A Universal Algorithm for Variational Inequalities Adaptive to Smoothness
               and Noise},
  journal   = {CoRR},
  volume    = {abs/1902.01637},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1902.01637},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{bubeck2019improved,
  title = 	 {Improved Path-length Regret Bounds for Bandits},
  author =       {Bubeck, S{\'e}bastien and Li, Yuanzhi and Luo, Haipeng and Wei, Chen-Yu},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {508--528},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = 	 {We study adaptive regret bounds in terms of the variation of
                  the losses (the so-called path-length bounds) for both
                  multi-armed bandit and more generally linear bandit. We first
                  show that the seemingly suboptimal path-length bound of (Wei
                  and Luo, 2018) is in fact not improvable for adaptive
                  adversary. Despite this negative result, we then develop two
                  new algorithms, one that strictly improves over (Wei and Luo,
                  2018) with a smaller path-length measure, and the other which
                  improves over (Wei and Luo, 2018) for oblivious adversary when
                  the path-length is large. Our algorithms are based on the
                  well-studied optimistic mirror descent framework, but
                  importantly with several novel techniques, including new
                  optimistic predictions, a slight bias towards recently
                  selected arms, and the use of a hybrid regularizer similar to
                  that of (Bubeck et al., 2018). Furthermore, we extend our
                  results to linear bandit by showing a reduction to obtaining
                  dynamic regret for a full-information problem, followed by a
                  further reduction to convex body chasing. As a consequence we
                  obtain new dynamic regret results as well as the first
                  path-length regret bounds for general linear bandit.}
}
@article{bhandari2018afinite,
  author    = {Jalaj Bhandari and
               Daniel Russo and
               Raghav Singal},
  title     = {A Finite Time Analysis of Temporal Difference Learning With Linear
               Function Approximation},
  journal   = {CoRR},
  volume    = {abs/1806.02450},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1806.02450},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  booktitle={Advances in neural information processing systems},
  pages={4055--4065},
  year={2017}
}
@InProceedings{dai2017learning,
  title = 	 {{Learning from Conditional Distributions via Dual Embeddings}},
  author = 	 {Bo Dai and Niao He and Yunpeng Pan and Byron Boots and Le Song},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1458--1467},
  year = 	 {2017},
  editor = 	 {Aarti Singh and Jerry Zhu},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  abstract = {Many machine learning tasks, such as learning with invariance and
                  policy evaluation in reinforcement learning, can be
                  characterized as problems of learning from conditional
                  distributions. In such problems, each sample x itself is
                  associated with a conditional distribution $p(z|x)$
                  represented by samples $z_i_i=1^M$, and the goal is to learn a
                  function f that links these conditional distributions to
                  target values y. These problems become very challenging when
                  we only have limited samples or in the extreme case only one
                  sample from each conditional distribution. Commonly used
                  approaches either assume that z is independent of x, or
                  require an overwhelmingly large set of samples from each
                  conditional distribution. To address these challenges, we
                  propose a novel approach which employs a new min-max
                  reformulation of the learning from conditional distribution
                  problem. With such new reformulation, we only need to deal
                  with the joint distribution p(z,x). We also design an
                  efficient learning algorithm, Embedding-SGD, and establish
                  theoretical sample complexity for such problems. Finally, our
                  numerical experiments, on both synthetic and real-world
                  datasets, show that the proposed approach can significantly
                  improve over existing algorithms.}
}
@article{besbes2015nonstationary,
  author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  title = {Non-Stationary Stochastic Optimization},
  journal = {Operations Research},
  volume = {63},
  number = {5},
  pages = {1227-1244},
  year = {2015},
  doi = {10.1287/opre.2015.1408},
  abstract = { We consider a non-stationary variant of a sequential stochastic
                  optimization problem, in which the underlying cost functions
                  may change along the horizon. We propose a measure, termed
                  variation budget, that controls the extent of said change, and
                  study how restrictions on this budget impact achievable
                  performance. We identify sharp conditions under which it is
                  possible to achieve long-run average optimality and more
                  refined performance measures such as rate optimality that
                  fully characterize the complexity of such problems. In doing
                  so, we also establish a strong connection between two rather
                  disparate strands of literature: (1) adversarial online convex
                  optimization and (2) the more traditional stochastic
                  approximation paradigm (couched in a non-stationary setting).
                  This connection is the key to deriving well-performing
                  policies in the latter, by leveraging structure of optimal
                  policies in the former. Finally, tight bounds on the minimax
                  regret allow us to quantify the “price of non-stationarity,”
                  which mathematically captures the added complexity embedded in
                  a temporally changing environment versus a stationary one. }
}
@article{bauschke2012fenchel,
  title={What is a Fenchel Conjugate},
  author={Bauschke, H and Lucet, Yves},
  journal={Notices of the AMS},
  volume={59},
  number={1},
  pages={44--46},
  year={2012}
}
@article{beck2012smoothing,
author = {Beck, Amir and Teboulle, Marc},
title = {Smoothing and First Order Methods: A Unified Framework},
journal = {SIAM Journal on Optimization},
volume = {22},
number = {2},
pages = {557-580},
year = {2012},
doi = {10.1137/100818327},
URL = {https://doi.org/10.1137/100818327},
eprint = {https://doi.org/10.1137/100818327}
}
@book{bauschke2011fixed,
  title={Fixed-point algorithms for inverse problems in science and engineering},
  author={Bauschke, Heinz H and Burachik, Regina S and Combettes, Patrick L and Elser, Veit and Luke, D Russell and Wolkowicz, Henry},
  volume={49},
  year={2011},
  publisher={Springer Science \& Business Media}
}
@incollection{bauschke2011self,
  title={Self-dual smooth approximations of convex functions via the proximal average},
  author={Bauschke, Heinz H and Moffat, Sarah M and Wang, Xianfu},
  booktitle={Fixed-Point Algorithms for Inverse Problems in Science and Engineering},
  pages={23--32},
  year={2011},
  publisher={Springer}
}
@article{bach2010selfconcordant,
author = {Francis Bach},
title = {Self-concordant analysis for logistic regression},
volume = {4},
journal = {Electronic Journal of Statistics},
number = {none},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {384 -- 414},
year = {2010},
doi = {10.1214/09-EJS521},
}
@article{baes2009estimate,
  title={Estimate sequence methods: extensions and approximations},
  author={Baes, Michel},
  year={2009},
  journal={Institute for Operations Research, ETH, Z{\"u}rich, Switzerland},
  pages={5},
}
@incollection{bartlett2008high,
           title = {High-probability regret bounds for bandit online linear optimization},
          author = {Peter Bartlett and Varsha Dani and Thomas Hayes and Sham Kakade and Alexander Rakhlin and Ambuj Tewari},
       booktitle = {Proceedings of the 21st Annual Conference on Learning Theory - COLT 2008},
         address = {United States},
          editor = {T Zhang and R A Sevedio},
            year = {2008},
           pages = {335--342},
       publisher = {Omnipress},
        abstract = {We present a modification of the algorithm of Dani et al.
                  [8] for the online linear optimization problem in the bandit
                  setting, which with high probability has regret at most O ? (
                  {$\sqrt{}$} T) against an adaptive adversary. This improves on
                  the previous algorithm [8] whose regret is bounded in
                  expectation against an oblivious adversary. We obtain the same
                  dependence on the dimension (n 3/2) as that exhibited by Dani
                  et al. The results of this paper rest firmly on those of [8]
                  and the remarkable technique of Auer et al. [2] for obtaining
                  high probability bounds via optimistic estimates. This paper
                  answers an open question: it eliminates the gap between the
                  high-probability bounds obtained in the full-information vs
                  bandit settings.}
}
% =========
% --- C ---
% =========
@article{chowdhury2022bregman,
  title={Bregman Deviations of Generic Exponential Families},
  author={Chowdhury, Sayak Ray and Saux, Patrick and Maillard, Odalric-Ambrym and Gopalan, Aditya},
  journal={arXiv preprint arXiv:2201.07306},
  year={2022}
}
@misc{campolongo2021closer,
      title={A Closer Look at Temporal Variability in Dynamic Online Learning},
      author={Nicolò Campolongo and Francesco Orabona},
      year={2021},
      eprint={2102.07666},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{chen2021impossible,
  title = 	 {Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications},
  author =       {Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {1216--1259},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  abstract = 	 {We resolve the long-standing "impossible tuning" issue for the
                  classic expert problem and show that, it is in fact possible
                  to achieve regret $O\left(\sqrt{(\ln d)\sum_t
                  \ell_{t,i}^2}\right)$ simultaneously for all expert $i$ in a
                  $T$-round $d$-expert problem where $\ell_{t,i}$ is the loss
                  for expert $i$ in round $t$. Our algorithm is based on the
                  Mirror Descent framework with a correction term and a weighted
                  entropy regularizer. While natural, the algorithm has not been
                  studied before and requires a careful analysis. We also
                  generalize the bound to $O\left(\sqrt{(\ln d)\sum_t
                  (\ell_{t,i}-m_{t,i})^2}\right)$ for any prediction vector
                  $m_t$ that the learner receives, and recover or improve many
                  existing results by choosing different $m_t$. Furthermore, we
                  use the same framework to create a master algorithm that
                  combines a set of base algorithms and learns the best one with
                  little overhead. The new guarantee of our master allows us to
                  derive many new results for both the expert problem and more
                  generally Online Linear Optimization.}
}
@misc{chen2021provable,
      title={Provable Regret Bounds for Deep Online Learning and Control},
      author={Xinyi Chen and Edgar Minasyan and Jason D. Lee and Elad Hazan},
      year={2021},
      eprint={2110.07807},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{cutkosky2021highprobability,
      title={High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails},
      author={Ashok Cutkosky and Harsh Mehta},
      year={2021},
      eprint={2106.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{cai2020provably,
  title = 	 {Provably Efficient Exploration in Policy Optimization},
  author =       {Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1283--1294},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Virtual},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {While policy-based reinforcement learning (RL) achieves tremendous
                  successes in practice, it is significantly less understood in
                  theory, especially compared with value-based RL. In
                  particular, it remains elusive how to design a provably
                  efficient policy optimization algorithm that incorporates
                  exploration. To bridge such a gap, this paper proposes an
                  Optimistic variant of the Proximal Policy Optimization
                  algorithm (OPPO), which follows an “optimistic version” of the
                  policy gradient direction. This paper proves that, in the
                  problem of episodic Markov decision process with linear
                  function approximation, unknown transition, and adversarial
                  reward with full-information feedback, OPPO achieves
                  $\tilde{O}(\sqrt{d^2 H^3 T})$ regret. Here $d$ is the feature
                  dimension, $H$ is the episode horizon, and $T$ is the total
                  number of steps. To the best of our knowledge, OPPO is the
                  first provably efficient policy optimization algorithm that
                  explores.}
}
@inproceedings{campolongo2020temporal,
 author = {Campolongo, Nicol\`{o} and Orabona, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12377--12387},
 publisher = {Curran Associates, Inc.},
 title = {Temporal Variability in Implicit Online Learning},
 volume = {33},
 year = {2020}
}
@misc{chen2020finitesample,
      title={Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning},
      author={Zaiwei Chen and Sheng Zhang and Thinh T. Doan and John-Paul Clarke and Siva Theja Maguluri},
      year={2020},
      eprint={1905.11425},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@InProceedings{cheng2020reduction,
  title = 	 {A Reduction from Reinforcement Learning to No-Regret Online Learning},
  author =       {Cheng, Ching-An and des Combes, Remi Tachet and Boots, Byron and Gordon, Geoff},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3514--3524},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  abstract = 	 {We present a reduction from reinforcement learning (RL) to
                  no-regret online learning based on the saddle-point
                  formulation of RL, by which "any" online algorithm with
                  sublinear regret can generate policies with provable
                  performance guarantees. This new perspective decouples the RL
                  problem into two parts: regret minimization and function
                  approximation. The first part admits a standard
                  online-learning analysis, and the second part can be
                  quantified independently of the learning algorithm. Therefore,
                  the proposed reduction can be used as a tool to systematically
                  design new RL algorithms. We demonstrate this idea by devising
                  a simple RL algorithm based on mirror descent and the
                  generative-model oracle. For any $\gamma$-discounted tabular
                  RL problem, with probability at least $1-\delta$, it learns an
                  $\epsilon$-optimal policy using at most
                  $\tilde{O}\left(\frac{|\SS||Å|\log(\frac{1}{\delta})}{(1-\gamma)^4\epsilon^2}\right)$
                  samples. Furthermore, this algorithm admits a direct extension
                  to linearly parameterized function approximators for
                  large-scale applications, with computation and sample
                  complexities independent of $|\SS|$,$|Å|$, though at the cost
                  of potential approximation bias.}
}
@InProceedings{cutkosky2020momentum,
  title = 	 {Momentum Improves Normalized {SGD}},
  author =       {Cutkosky, Ashok and Mehta, Harsh},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2260--2268},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {We provide an improved analysis of normalized SGD showing that
                  adding momentum provably removes the need for large batch
                  sizes on non-convex objectives. Then, we consider the case of
                  objectives with bounded second derivative and show that in
                  this case a small tweak to the momentum formula allows
                  normalized SGD with momentum to find an $\epsilon$-critical
                  point in $O(1/\epsilon^{3.5})$ iterations, matching the
                  best-known rates without accruing any logarithmic factors or
                  dependence on dimension. We provide an adaptive learning rate
                  schedule that automatically improves convergence rates when
                  the variance in the gradients is small. Finally, we show that
                  our method is effective when employed on popular large scale
                  tasks such as ResNet-50 and BERT pretraining, matching the
                  performance of the disparate methods used to get
                  state-of-the-art results on both tasks.}
}
@article{cutkosky2020better,
  title={Better Full-Matrix Regret via Parameter-Free Online Learning},
  author={Cutkosky, Ashok},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@InProceedings{cutkosky2020parameter,
  title = 	 {Parameter-free, Dynamic, and Strongly-Adaptive Online Learning},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2250--2259},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Virtual},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {We provide a new online learning algorithm that for the first time
              combines several disparate notions of adaptivity. First, our
              algorithm obtains a “parameter-free” regret bound that adapts
              to the norm of the comparator and the squared norm of the size
              of the gradients it observes. Second, it obtains a
              “strongly-adaptive” regret bound, so that for any given
              interval of length $N$, the regret over the interval is
              $\tilde O(\sqrt{N})$. Finally, our algorithm obtains an
              optimal “dynamic” regret bound: for any sequence of
              comparators with path-length $P$, our algorithm obtains regret
              $\tilde O(\sqrt{PN})$ over intervals of length $N$. Our
              primary technique for achieving these goals is a new method of
              combining constrained online learning regret bounds that does
              not rely on an expert meta-algorithm to aggregate learners.}
}
@InProceedings{cutkosky2019anytime,
  title = 	 {Anytime Online-to-Batch, Optimism and Acceleration},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1446--1454},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = {A standard way to obtain convergence guarantees in stochastic
                  convex optimization is to run an online learning algorithm and
                  then output the average of its iterates: the actual iterates
                  of the online learning algorithm do not come with individual
                  guarantees. We close this gap by introducing a black-box
                  modification to any online learning algorithm whose iterates
                  converge to the optimum in stochastic scenarios. We then
                  consider the case of smooth losses, and show that combining
                  our approach with optimistic online learning algorithms
                  immediately yields a fast convergence rate of
                  $O(L/T^{3/2}+\sigma/\sqrt{T})$ on $L$-smooth problems with
                  $\sigma^2$ variance in the gradients. Finally, we provide a
                  reduction that converts any adaptive online algorithm into one
                  that obtains the optimal accelerated rate of $\tilde O(L/T^2 +
                  \sigma/\sqrt{T})$, while still maintaining $\tilde
                  O(1/\sqrt{T})$ convergence in the non-smooth setting.
                  Importantly, our algorithms adapt to $L$ and $\sigma$
                  automatically: they do not need to know either to obtain these
                  rates.}
}
@InProceedings{cutkosky2019artificial,
  title = 	 {Artificial Constraints and Hints for Unbounded Online Learning},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {874--894},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = {We provide algorithms that guarantees regret $R_T(u)\le \tilde
                  O(G\|u\|^3 + G(\|u\|+1)\sqrt{T})$ or $R_T(u)\le \tilde
                  O(G\|u\|^3T^{1/3} + GT^{1/3}+ G\|u\|\sqrt{T})$ for online
                  convex optimization with $G$-Lipschitz losses for any
                  comparison point $u$ without prior knowledge of either $G$ or
                  $\|u\|$. Previous algorithms dispense with the $O(\|u\|^3)$
                  term at the expense of knowledge of one or both of these
                  parameters, while a lower bound shows that some additional
                  penalty term over $G\|u\|\sqrt{T}$ is necessary. Previous
                  penalties were \emph{exponential} while our bounds are
                  polynomial in all quantities. Further, given a known bound
                  $\|u\|\le D$, our same techniques allow us to design
                  algorithms that adapt optimally to the unknown value of
                  $\|u\|$ without requiring knowledge of $G$.}
}
@InProceedings{cutkosky2019combining,
  title = 	 {Combining Online Learning Guarantees},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {895--913},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = {We show how to take any two parameter-free online learning
                  algorithms with different regret guarantees and obtain a
                  single algorithm whose regret is the minimum of the two base
                  algorithms. Our method is embarrassingly simple: just add the
                  iterates. This trick can generate efficient algorithms that
                  adapt to many norms simultaneously, as well as providing
                  diagonal-style algorithms that still maintain dimension-free
                  guarantees. We then proceed to show how a variant on this idea
                  yields a black-box procedure for generating \emph{optimistic}
                  online learning algorithms. This yields the first optimistic
                  regret guarantees in the unconstrained setting and generically
                  increases adaptivity. Further, our optimistic algorithms are
                  guaranteed to do no worse than their non-optimistic
                  counterparts regardless of the quality of the optimistic
                  estimates provided to the algorithm.}
}
@InProceedings{cutkosky2019matrix,
  title = 	 {Matrix-Free Preconditioning in Online Learning},
  author =       {Cutkosky, Ashok and Sarlos, Tamas},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1455--1464},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {We provide an online convex optimization algorithm with regret
                  that interpolates between the regret of an algorithm using an
                  optimal preconditioning matrix and one using a diagonal
                  preconditioning matrix. Our regret bound is never worse than
                  that obtained by diagonal preconditioning, and in certain
                  setting even surpasses that of algorithms with full-matrix
                  preconditioning. Importantly, our algorithm runs in the same
                  time and space complexity as online gradient descent. Along
                  the way we incorporate new techniques that mildly streamline
                  and improve logarithmic factors in prior regret analyses. We
                  conclude by benchmarking our algorithm on synthetic data and
                  deep learning tasks.}
}
@article{cutkosky2019momentum,
  author    = {Ashok Cutkosky and
               Francesco Orabona},
  title     = {Momentum-Based Variance Reduction in Non-Convex {SGD}},
  journal   = {CoRR},
  volume    = {abs/1905.10018},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1905.10018},
  timestamp = {Wed, 29 May 2019 11:27:50 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{cohen2018acceleration,
  title={On acceleration with noise-corrupted gradients},
  author={Cohen, Michael B and Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1805.12591},
  year={2018}
}
@book{cutkosky2018algorithms,
  title={Algorithms and Lower Bounds for Parameter-free Online Learning},
  author={Cutkosky, Ashok},
  year={2018},
  publisher={Stanford University}
}
@InProceedings{cutkosky2018black,
  title = 	 {Black-Box Reductions for Parameter-free Online Learning in Banach Spaces},
  author =       {Cutkosky, Ashok and Orabona, Francesco},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1493--1529},
  year = 	 {2018},
  editor = 	 {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  abstract = 	 {We introduce several new black-box reductions that
                  significantly improve the design of adaptive and
                  parameter-free online learning algorithms by simplifying
                  analysis, improving regret guarantees, and sometimes even
                  improving runtime. We reduce parameter-free online learning to
                  online exp-concave optimization, we reduce optimization in a
                  Banach space to one-dimensional optimization, and we reduce
                  optimization over a constrained domain to unconstrained
                  optimization. All of our reductions run as fast as online
                  gradient descent. We use our new techniques to improve upon
                  the previously best regret bounds for parameter-free learning,
                  and do so for arbitrary norms.}
}
@InProceedings{cutkosky2017online,
  title = 	 {Online Learning Without Prior Information},
  author = 	 {Cutkosky, Ashok and Boahen, Kwabena},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {643--677},
  year = 	 {2017},
  editor = 	 {Kale, Satyen and Shamir, Ohad},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--10 Jul},
  publisher =    {PMLR},
  abstract = 	 {The vast majority of optimization and online learning
                  algorithms today require some prior information about the data
                  (often in the form of bounds on gradients or on the optimal
                  parameter value). When this information is not available,
                  these algorithms require laborious manual tuning of various
                  hyperparameters, motivating the search for algorithms that can
                  adapt to the data with no prior information. We  describe a
                  frontier of new lower bounds on the performance of such
                  algorithms, reflecting a tradeoff between a term that depends
                  on the optimal parameter value and a term that depends on the
                  gradients’ rate of growth. Further, we construct a family of
                  algorithms whose performance matches any desired point on this
                  frontier, which no previous algorithm reaches.}
}
@inproceedings{cutkosky2016online,
 title = {Online Convex Optimization with Unconstrained Domains and Losses},
 author = {Cutkosky, Ashok and Boahen, Kwabena A},
 year = {2016},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 volume = {29},
}
@article{cesa2012mirror,
  title={Mirror descent meets fixed share (and feels no regret)},
  author={Cesa-Bianchi, Nicolo and Gaillard, Pierre and Lugosi, G{\'a}bor and Stoltz, Gilles},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}
@InProceedings{chiang2012online,
  title = 	 {Online Optimization with Gradual Variations},
  author = 	 {Chao-Kai Chiang and Tianbao Yang and Chia-Jung Lee and Mehrdad Mahdavi and Chi-Jen Lu and Rong Jin and Shenghuo Zhu},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {6.1--6.20},
  year = 	 {2012},
  editor = 	 {Shie Mannor and Nathan Srebro and Robert C. Williamson},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {JMLR Workshop and Conference Proceedings},
  abstract = 	 {We study the online convex optimization problem, in which an
                  online algorithm has to make repeated decisions with convex
                  loss functions and hopes to achieve a small regret. We
                  consider a natural restriction of this problem in which the
                  loss functions have a small deviation, measured by the sum of
                  the distances between every two consecutive loss functions,
                  according to some distance metrics. We show that for the
                  linear and general smooth convex loss functions, an online
                  algorithm modified from the gradient descend algorithm can
                  achieve a regret which only scales as the square root of the
                  deviation. For the closely related problem of prediction with
                  expert advice, we show that an online algorithm modified  from
                  the multiplicative update algorithm can also achieve a similar
                  regret bound for a different measure of deviation. Finally,
                  for loss functions which are strictly convex, we show that an
                  online algorithm modified from the online Newton step
                  algorithm can achieve a regret which is only logarithmic in
                  terms of the deviation, and as an application, we can also
                  have such a logarithmic regret for the portfolio management
                  problem.}
}
@Inbook{Combettes2011,
author={Combettes, Patrick L. and Pesquet, Jean-Christophe},
title={Proximal Splitting Methods in Signal Processing},
bookTitle={Fixed-Point Algorithms for Inverse Problems in Science and Engineering},
editor={Bauschke, Heinz H.
and Burachik, Regina S.
and Combettes, Patrick L.
and Elser, Veit
and Luke, D. Russell
and Wolkowicz, Henry},
year={2011},
publisher={Springer New York},
address={New York, NY},
pages={185--212},
isbn={978-1-4419-9569-8},
doi={10.1007/978-1-4419-9569-8_10},
abstract={The proximity operator of a convex function is a natural extension of
                  the notion of a projection operator onto a convex set. This
                  tool, which plays a central role in the analysis and the
                  numerical solution of convex optimization problems, has
                  recently been introduced in the arena of inverse problems and,
                  especially, in signal processing, where it has become
                  increasingly important. In this paper, we review the basic
                  properties of proximity operators which are relevant to signal
                  processing and present optimization methods based on these
                  operators. These proximal splitting methods are shown to
                  capture and extend several well-known algorithms in a unifying
                  framework. Applications of proximal methods in signal recovery
                  and synthesis are discussed.}
}
@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}
@article{combettes2005signal,
  title={Signal recovery by proximal forward-backward splitting},
  author={Combettes, Patrick L and Wajs, Val{\'e}rie R},
  year={2005},
  journal={Multiscale Modeling \& Simulation},
  volume={4},
  number={4},
  pages={1168--1200},
  publisher={SIAM}
}
@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}
% =========
% --- D ---
% =========
@misc{du2022optimal,
  title = {Optimal Extragradient-Based Bilinearly-Coupled Saddle-Point Optimization},
  author = {Du, Simon S. and Gidel, Gauthier and Jordan, Michael I. and Li, Chris Junchi},
  year = {2022},
  doi = {10.48550/ARXIV.2206.08573},
}
@misc{daspremont2021acceleration,
      title={Acceleration Methods},
      author={Alexandre d'Aspremont and Damien Scieur and Adrien Taylor},
      year={2021},
      eprint={2101.09545},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      abstract={This monograph covers some recent advances on a range of
                  acceleration techniques frequently used in convex
                  optimization. We first use quadratic optimization problems to
                  introduce two key families of methods, momentum and nested
                  optimization schemes, which coincide in the quadratic case to
                  form the Chebyshev method whose complexity is analyzed using
                  Chebyshev polynomials. We discuss momentum methods in detail,
                  starting with the seminal work of Nesterov (1983) and
                  structure convergence proofs using a few master templates,
                  such as that of \emph{optimized gradient methods} which have
                  the key benefit of showing how momentum methods maximize
                  convergence rates. We further cover proximal acceleration
                  techniques, at the heart of the \emph{Catalyst} and
                  \emph{Accelerated Hybrid Proximal Extragradient} frameworks,
                  using similar algorithmic patterns. Common acceleration
                  techniques directly rely on the knowledge of some regularity
                  parameters of the problem at hand, and we conclude by
                  discussing \emph{restart} schemes, a set of simple techniques
                  to reach nearly optimal convergence rates while adapting to
                  unobserved regularity parameters.}
}
@InProceedings{diakonikolas2021efficient,
  title = 	 {Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization},
  author =       {Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2746--2754},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  abstract = 	 { The use of min-max optimization in the adversarial training
                  of deep neural network classifiers, and the training of
                  generative adversarial networks has motivated the study of
                  nonconvex-nonconcave optimization objectives, which frequently
                  arise in these applications. Unfortunately, recent results
                  have established that even approximate first-order stationary
                  points of such objectives are intractable, even under
                  smoothness conditions, motivating the study of min-max
                  objectives with additional structure. We introduce a new class
                  of structured nonconvex-nonconcave min-max optimization
                  problems, proposing a generalization of the extragradient
                  algorithm which provably converges to a stationary point. The
                  algorithm applies not only to Euclidean spaces, but also to
                  general $\ell_p$-normed finite-dimensional real vector spaces.
                  We also discuss its stability under stochastic oracles and
                  provide bounds on its sample complexity. Our iteration
                  complexity and sample complexity bounds either match or
                  improve the best known bounds for the same or less general
                  nonconvex-nonconcave settings, such as those that satisfy
                  variational coherence or in which a weak solution to the
                  associated variational inequality problem is assumed to exist.
                  }
}
@article{diakonikolas2021generalized,
author = {Diakonikolas, Jelena and Jordan, Michael I.},
title = {Generalized Momentum-Based Methods: A Hamiltonian Perspective},
journal = {SIAM Journal on Optimization},
volume = {31},
number = {1},
pages = {915-944},
year = {2021},
doi = {10.1137/20M1322716},
eprint = {https://doi.org/10.1137/20M1322716}
}
@article{diakonikolas2021potential,
  title={Potential function-based framework for making the gradients small in convex and min-max optimization},
  author={Diakonikolas, Jelena and Wang, Puqian},
  journal={arXiv preprint arXiv:2101.12101},
  year={2021}
}
@misc{dong2021provable,
      title={Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature},
      author={Kefan Dong and Jiaqi Yang and Tengyu Ma},
      year={2021},
      eprint={2102.04168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{dorazio2021stochastic,
      title={Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize},
      author={Ryan D'Orazio and Nicolas Loizou and Issam Laradji and Ioannis Mitliagkas},
      year={2021},
      eprint={2110.15412},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{devroye2020total,
      title={The total variation distance between high-dimensional Gaussians},
      author={Luc Devroye and Abbas Mehrabian and Tommy Reddad},
      year={2020},
      eprint={1810.08693},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}
@misc{diakonikolas2020conjugate,
      title={Conjugate Gradients and Accelerated Methods Unified: The Approximate Duality Gap View},
      author={Jelena Diakonikolas and Lorenzo Orecchia},
      year={2020},
      eprint={1907.00289},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@InProceedings{diakonikolas2020halpern,
   title = {Halpern Iteration for Near-Optimal and Parameter-Free Monotone Inclusion and Strong Solutions to Variational Inequalities},
   author = {Diakonikolas, Jelena},
   booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
   pages = {1428--1451},
   year = {2020},
   editor = {Jacob Abernethy and Shivani Agarwal},
   volume = {125},
   series = {Proceedings of Machine Learning Research},
   address = {},
   month = {09--12 Jul},
   publisher = {PMLR},
   abstract = { We leverage the connections between nonexpansive maps, monotone
                  Lipschitz operators, and proximal mappings to obtain
                  near-optimal (i.e., optimal up to poly-log factors in terms of
                  iteration complexity) and parameter-free methods for solving
                  monotone inclusion problems. These results immediately
                  translate into near-optimal guarantees for approximating
                  strong solutions to variational inequality problems,
                  approximating convex-concave min-max optimization problems,
                  and minimizing the norm of the gradient in min-max
                  optimization problems. Our analysis is based on a novel and
                  simple potential-based proof of convergence of Halpern
                  iteration, a classical iteration for finding fixed points of
                  nonexpansive maps. Additionally, we provide a series of
                  algorithmic reductions that highlight connections between
                  different problem classes and lead to lower bounds that
                  certify near-optimality of the studied methods.}
}
@article{ding2020anefficient,
  author    = {Qin Ding and
               Cho{-}Jui Hsieh and
               James Sharpnack},
  title     = {An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling},
  journal   = {CoRR},
  volume    = {abs/2006.04012},
  year      = {2020},
  archivePrefix = {arXiv},
  eprint    = {2006.04012},
  timestamp = {Fri, 12 Jun 2020 14:02:57 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{doan2020convergence,
      title={Convergence Rates of Accelerated Markov Gradient Descent with Applications in Reinforcement Learning},
      author={Thinh T. Doan and Lam M. Nguyen and Nhan H. Pham and Justin Romberg},
      year={2020},
      eprint={2002.02873},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{defazio2020understanding,
  author    = {Aaron Defazio},
  title     = {Understanding the Role of Momentum in Non-Convex Optimization: Practical Insights from a Lyapunov Analysis},
  journal   = {CoRR},
  volume    = {abs/2010.00406},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2010.00406},
  timestamp = {Mon, 12 Oct 2020 17:53:10 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{defazio2019curved,
  title={On the curved geometry of accelerated optimization},
  author={Defazio, Aaron},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1766--1775},
  year={2019}
}
@article{diakonikolas2019theapproximate,
author = {Diakonikolas, Jelena and Orecchia, Lorenzo},
title = {The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {1},
pages = {660-689},
year = {2019},
doi = {10.1137/18M1172314},
URL = {https://doi.org/10.1137/18M1172314},
eprint = {https://doi.org/10.1137/18M1172314}
}
@InProceedings{dalal2018finite,
  title = 	 {Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning},
  author =       {Dalal, Gal and Thoppe, Gugan and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Mannor, Shie},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1199--1233},
  year = 	 {2018},
  editor = 	 {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  abstract = {Two-timescale Stochastic Approximation (SA) algorithms are widely
                  used in Reinforcement Learning (RL). Their iterates have two
                  parts that are updated using distinct stepsizes. In this work,
                  we develop a novel recipe for their finite sample analysis.
                  Using this, we provide a concentration bound, which is the
                  first such result for a two-timescale SA. The type of bound we
                  obtain is known as “lock-in probability”. We also introduce a
                  new projection scheme, in which the time between successive
                  projections increases exponentially. This scheme allows one to
                  elegantly transform a lock-in probability into a convergence
                  rate result for projected two-timescale SA. From this latter
                  result, we then extract key insights on stepsize selection. As
                  an application, we finally obtain convergence rates for the
                  projected two-timescale RL algorithms GTD(0), GTD2, and TDC.}
}
@article{diakonikolas2017accelerated,
  title={Accelerated extra-gradient descent: A novel accelerated first-order method},
  author={Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1706.04680},
  year={2017}
}
@inproceedings{du2017gradient,
author = {Du, Simon S. and Jin, Chi and Lee, Jason D. and Jordan, Michael I. and P\'{o}czos, Barnab\'{a}s and Singh, Aarti},
title = {Gradient Descent Can Take Exponential Time to Escape Saddle Points},
year = {2017},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
abstract = {Although gradient descent (GD) almost always escapes saddle points
                  asymptotically [Lee et al., 2016], this paper shows that even
                  with fairly natural random initialization schemes and
                  non-pathological functions, GD can be significantly slowed
                  down by saddle points, taking exponential time to escape. On
                  the other hand, gradient descent with perturbations [Ge et
                  al., 2015, Jin et al., 2017] is not slowed down by saddle
                  points—it can find an approximate local minimizer in
                  polynomial time. This result implies that GD is inherently
                  slower than perturbed GD, and justifies the importance of
                  adding perturbations for efficient non-convex optimization.
                  While our focus is theoretical, we also present experiments
                  that illustrate our theoretical findings.},
}
@InProceedings{du2017stochastic,
  title = 	 {Stochastic Variance Reduction Methods for Policy Evaluation},
  author =       {Simon S. Du and Jianshu Chen and Lihong Li and Lin Xiao and Dengyong Zhou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1049--1058},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  abstract = {Policy evaluation is concerned with estimating the value function
                  that predicts long-term values of states under a given policy.
                  It is a crucial step in many reinforcement-learning
                  algorithms. In this paper, we focus on policy evaluation with
                  linear function approximation over a fixed dataset. We first
                  transform the empirical policy evaluation problem into a
                  (quadratic) convex-concave saddle-point problem, and then
                  present a primal-dual batch gradient method, as well as two
                  stochastic variance reduction methods for solving the problem.
                  These algorithms scale linearly in both sample size and
                  feature dimension. Moreover, they achieve linear convergence
                  even when the saddle-point problem has only strong concavity
                  in the dual variables but no strong convexity in the primal
                  variables. Numerical experiments on benchmark problems
                  demonstrate the effectiveness of our methods.}
}
@InProceedings{daniely2015strongly,
  title = 	 {Strongly Adaptive Online Learning},
  author = 	 {Daniely, Amit and Gonen, Alon and Shalev-Shwartz, Shai},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1405--1411},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  abstract = 	 {Strongly adaptive algorithms are algorithms whose performance
                  on every time interval is close to optimal. We present a
                  reduction that can transform standard low-regret algorithms to
                  strongly adaptive. As a consequence, we derive simple, yet
                  efficient, strongly adaptive algorithms for a handful of
                  problems.}
}
@article{drori2014performance,
author = {Drori, Yoel and Teboulle, Marc},
title = {Performance of First-Order Methods for Smooth Convex Minimization: A Novel Approach},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {145},
number = {1–2},
issn = {0025-5610},
doi = {10.1007/s10107-013-0653-0},
journal = {Math. Program.},
month = jun,
pages = {451–482},
numpages = {32},
abstract = {We introduce a novel approach for analyzing the worst-case
                  performance of first-order black-box optimization methods. We
                  focus on smooth unconstrained convex minimization over the
                  Euclidean space. Our approach relies on the observation that
                  by definition, the worst-case behavior of a black-box
                  optimization method is by itself an optimization problem,
                  which we call the performance estimation problem (PEP). We
                  formulate and analyze the PEP for two classes of first-order
                  algorithms. We first apply this approach on the classical
                  gradient method and derive a new and tight analytical bound on
                  its performance. We then consider a broader class of
                  first-order black-box methods, which among others, include the
                  so-called heavy-ball method and the fast gradient schemes. We
                  show that for this broader class, it is possible to derive new
                  bounds on the performance of these methods by solving an
                  adequately relaxed convex semidefinite PEP. Finally, we show
                  an efficient procedure for finding optimal step sizes which
                  results in a first-order black-box method that achieves best
                  worst-case performance.}
}
@article{duchi2012ergodic,
  title={Ergodic Mirror Descent},
  author={Duchi, John C and Agarwal, Alekh and Johansson, Mikael and Jordan, Michael I},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1549--1578},
  year={2012},
  publisher={SIAM}
}
@article{duchi2012randomized,
title = {Randomized Smoothing for Stochastic Optimization},
author = {Duchi, John C. and Bartlett, Peter L. and Wainwright, Martin J.},
journal = {SIAM Journal on Optimization},
volume = {22},
number = {2},
pages = {674-701},
year = {2012},
doi = {10.1137/110831659},
URL = {https://doi.org/10.1137/110831659},
eprint = {https://doi.org/10.1137/110831659}
}
@inproceedings{duchi2010composite,
  title={Composite Objective Mirror Descent},
  author={Duchi, John C and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
  year={2010},
  booktitle={COLT},
  pages={14--26},
}
% =========
% --- E ---
% =========

% =========
% --- F ---
% =========
@misc{foster2021offline,
      title={Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation},
      author={Dylan J. Foster and Akshay Krishnamurthy and David Simchi-Levi and Yunzong Xu},
      year={2021},
      eprint={2111.10919},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{foster2021statistical,
      title={The Statistical Complexity of Interactive Decision Making},
      author={Dylan J. Foster and Sham M. Kakade and Jian Qian and Alexander Rakhlin},
      year={2021},
      eprint={2112.13487},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{fang2020online,
  title = 	 {Online mirror descent and dual averaging: keeping pace in the dynamic case},
  author =       {Fang, Huang and Harvey, Nick and Portella, Victor and Friedlander, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3008--3017},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {Online mirror descent (OMD) and dual averaging (DA)—two
                  fundamental algorithms for online convex optimization—are
                  known to have very similar (and sometimes identical)
                  performance guarantees when used with a fixed learning rate.
                  Under dynamic learning rates, however, OMD is provably
                  inferior to DA and suffers a linear regret, even in common
                  settings such as prediction with expert advice. We modify the
                  OMD algorithm through a simple technique that we call
                  stabilization. We give essentially the same abstract regret
                  bound for OMD with stabilization and for DA by modifying the
                  classical OMD convergence analysis in a careful and modular
                  way that allows for straightforward and flexible proofs.
                  Simple corollaries of these bounds show that OMD with
                  stabilization and DA enjoy the same performance guarantees in
                  many applications—even under dynamic learning rates. We also
                  shed light on the similarities between OMD and DA and show
                  simple conditions under which stabilized-OMD and DA generate
                  the same iterates.}
}
@InProceedings{foster2020open,
  title = 	 {Open Problem: Model Selection for Contextual Bandits},
  author =       {Foster, Dylan J. and Krishnamurthy, Akshay and Luo, Haipeng},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3842--3846},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  abstract = 	 {In statistical learning, algorithms for model selection allow
                  the learner to adapt to the complexity of the best hypothesis
                  class in a sequence. We ask whether similar guarantees are
                  possible for contextual bandit learning.}
}
@InProceedings{fang2019sharp,
  title = 	 {Sharp Analysis for Nonconvex SGD Escaping from Saddle Points},
  author =       {Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1192--1234},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = 	 {In this paper, we give a sharp analysis for  Stochastic
                  Gradient Descent (SGD)  and  prove that SGD is able to
                  efficiently escape from  saddle points and find an $(\epsilon,
                  O(\epsilon^{0.5}))$-approximate second-order stationary point
                  in $\tilde{O}(\epsilon^{-3.5})$ stochastic gradient
                  computations for generic nonconvex optimization problems, when
                  the objective function satisfies  gradient-Lipschitz,
                  Hessian-Lipschitz, and dispersive noise assumptions. This
                  result subverts the classical belief that  SGD requires at
                  least $O(\epsilon^{-4})$ stochastic gradient computations for
                  obtaining an $(\epsilon,O(\epsilon^{0.5}))$-approximate
                  second-order stationary point. Such SGD rate matches, up to a
                  polylogarithmic factor of problem-dependent parameters, the
                  rate of most accelerated  nonconvex stochastic optimization
                  algorithms that adopt additional techniques, such as
                  Nesterov’s momentum acceleration, negative curvature search,
                  as well as quadratic and cubic regularization tricks. Our
                  novel analysis gives new insights into nonconvex SGD and can
                  be potentially generalized to a broad class of stochastic
                  optimization algorithms.}
}
@InProceedings{foster2018online,
  title = 	 {Online Learning: Sufficient Statistics and the Burkholder Method},
  author =       {Foster, Dylan J. and Rakhlin, Alexander and Sridharan, Karthik},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {3028--3064},
  year = 	 {2018},
  editor = 	 {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  abstract = 	 {We uncover a fairly general principle in online learning: If a
                  regret inequality can be (approximately) expressed as a
                  function of certain "sufficient statistics" for the data
                  sequence, then there exists a special Burkholder function that
                  1) can be used algorithmically to achieve the regret bound and
                  2) only depends on these sufficient statistics, not the entire
                  data sequence,  so that the online strategy is only required
                  to keep the sufficient statistics in memory. This
                  characterization is achieved by bringing the full power of the
                  Burkholder Method—originally developed for certifying
                  probabilistic martingale inequalities—to bear on the online
                  learning setting. To demonstrate the scope and effectiveness
                  of the Burkholder method, we develop a novel online strategy
                  for matrix prediction that attains a regret bound
                  corresponding to the variance term in matrix concentration
                  inequalities. We also present a linear-time/space prediction
                  strategy for parameter-free supervised learning with linear
                  classes and general smooth norms.}
}
@InProceedings{freund2016open,
  title = 	 {Open Problem: Second order regret bounds based on scaling time},
  author = 	 {Freund, Yoav},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1651--1654},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  abstract = 	 {We argue that the second order bounds given in
                  Cesa-Bianchi2006, which accumulate the square of the loss of
                  each action separately, are loose. We propose a different form
                  of a second order bound and conjecture the it is satisfied by
                  NormalHedge ChaudhuriFrHs2009.}
}
% =========
% --- G ---
% =========
@misc{gutman2021perturbed,
      title={Perturbed Fenchel duality and first-order methods},
      author={David H. Gutman and Javier F. Peña},
      year={2021},
      eprint={1812.10198},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{grimm2020value,
  title={The Value Equivalence Principle for Model-Based Reinforcement Learning},
  author={Grimm, Christopher and Barreto, Andr{\'e} and Singh, Satinder and Silver, David},
  journal={arXiv preprint arXiv:2011.03506},
  year={2020}
}
@article{gunasekar2020mirrorless,
  title={Mirrorless Mirror Descent: A More Natural Discretization of Riemannian Gradient Flow},
  author={Gunasekar, Suriya and Woodworth, Blake and Srebro, Nathan},
  journal={arXiv preprint arXiv:2004.01025},
  year={2020}
}
@InProceedings{ghiassian2020gradient,
  title = 	 {Gradient Temporal-Difference Learning with Regularized Corrections},
  author =       {Ghiassian, Sina and Patterson, Andrew and Garg, Shivam and Gupta, Dhawal and White, Adam and White, Martha},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3524--3534},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {It is still common to use Q-learning and temporal difference (TD)
                  learning{—}even though they have divergence issues and sound
                  Gradient TD alternatives exist{—}because divergence seems rare
                  and they typically perform well. However, recent work with
                  large neural network learning systems reveals that instability
                  is more common than previously thought. Practitioners face a
                  difficult dilemma: choose an easy to use and performant TD
                  method, or a more complex algorithm that is more sound but
                  harder to tune and all but unexplored with non-linear function
                  approximation or control. In this paper, we introduce a new
                  method called TD with Regularized Corrections (TDRC), that
                  attempts to balance ease of use, soundness, and performance.
                  It behaves as well as TD, when TD performs well, but is sound
                  in cases where TD diverges. We empirically investigate TDRC
                  across a range of problems, for both prediction and control,
                  and for both linear and non-linear function approximation, and
                  show, potentially for the first time, that Gradient TD methods
                  could be a better alternative to TD and Q-learning.}
}
@article{gupta2019finite,
  author    = {Harsh Gupta and
               R. Srikant and
               Lei Ying},
  title     = {Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1907.06290},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1907.06290},
  timestamp = {Thu, 22 Aug 2019 13:48:06 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gemp2016online,
  author    = {Ian Gemp and
               Sridhar Mahadevan},
  title     = {Online Monotone Optimization},
  journal   = {CoRR},
  volume    = {abs/1608.07888},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1608.07888},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{gyorgy2016shifting,
  title = 	 {Shifting Regret, Mirror Descent, and Matrices},
  author = 	 {Gyorgy, Andras and Szepesvari, Csaba},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2943--2951},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  abstract = 	 {We consider the problem of online prediction in changing
                  environments. In this framework the performance of a predictor
                  is evaluated as the loss relative to an arbitrarily changing
                  predictor, whose individual components come from a base class
                  of predictors. Typical results in the literature consider
                  different base classes (experts, linear predictors on the
                  simplex, etc.) separately. Introducing an arbitrary mapping
                  inside the mirror decent algorithm, we provide a framework
                  that unifies and extends existing results. As an example, we
                  prove new shifting regret bounds for matrix prediction
                  problems.}
}
@article{ghadimi2012optimal,
author = {Ghadimi, Saeed and Lan, Guanghui},
title = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic Composite Optimization I: A Generic Algorithmic Framework},
journal = {SIAM Journal on Optimization},
volume = {22},
number = {4},
pages = {1469-1492},
year = {2012},
doi = {10.1137/110848864},
URL = { https://doi.org/10.1137/110848864},
eprint = {https://doi.org/10.1137/110848864}
}
% =========
% --- H ---
% =========
@article{howard2021time,
title = {Time-uniform, Nonparametric, Nonasymptotic Confidence Sequences},
author = {Steven R. Howard and Aaditya Ramdas and Jon McAuliffe and Jasjeet Sekhon},
volume = {49},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1055 -- 1080},
keywords = {Confidence sequence, empirical-Bernstein bound, finite LIL bound, matrix concentration, potential outcomes, sequential probability ratio test},
year = {2021},
doi = {10.1214/20-AOS1991},
URL = {https://doi.org/10.1214/20-AOS1991},
}
@article{howard2020time,
title = {Time-uniform Chernoff Bounds via Nonnegative Supermartingales},
author = {Steven R. Howard and Aaditya Ramdas and Jon McAuliffe and Jasjeet Sekhon},
year = {2020},
volume = {17},
journal = {Probability Surveys},
number = {none},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {257 -- 317},
keywords = {Exponential concentration inequalities, line crossing probability, nonnegative supermartingale},
doi = {10.1214/18-PS321},
URL = {https://doi.org/10.1214/18-PS321},
}
@InProceedings{harvey2019tight,
  title = 	 {Tight analyses for non-smooth stochastic gradient descent},
  author =       {Harvey, Nicholas J.~A. and Liaw, Christopher and Plan, Yaniv and Randhawa, Sikander},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1579--1613},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = 	 {Consider the problem of minimizing functions that are
                  Lipschitz and strongly convex, but not necessarily
                  differentiable. We prove that after $T$ steps of stochastic
                  gradient descent, the error of the final iterate is
                  $O(\log(T)/T)$ \emph{with high probability}. We also construct
                  a function from this class for which the error of the final
                  iterate of \emph{deterministic} gradient descent is
                  $\Omega(\log(T)/T)$. This shows that the upper bound is tight
                  and that, in this setting, the last iterate of stochastic
                  gradient descent has the same general error rate (with high
                  probability) as deterministic gradient descent. This resolves
                  both open questions posed by Shamir (2012). An intermediate
                  step of our analysis proves that the suffix averaging method
                  achieves error $O(1/T)$ \emph{with high probability}, which is
                  optimal (for any first-order optimization method). This
                  improves results of Rakhlin et al. (2012) and Hazan and Kale
                  (2014), both of which achieved error $O(1/T)$, but only in
                  expectation, and achieved a high probability error bound of
                  $O(\log \log(T)/T)$, which is suboptimal.}
}
@InProceedings{hazan2019provably,
  title = 	 {Provably Efficient Maximum Entropy Exploration},
  author =       {Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2681--2691},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {Suppose an agent is in a (possibly unknown) Markov Decision
                  Process in the absence of a reward signal, what might we hope
                  that an agent can efficiently learn to do? This work studies a
                  broad class of objectives that are defined solely as functions
                  of the state-visitation frequencies that are induced by how
                  the agent behaves. For example, one natural, intrinsically
                  defined, objective problem is for the agent to learn a policy
                  which induces a distribution over state space that is as
                  uniform as possible, which can be measured in an entropic
                  sense. We provide an efficient algorithm to optimize such such
                  intrinsically defined objectives, when given access to a black
                  box planning oracle (which is robust to function
                  approximation). Furthermore, when restricted to the tabular
                  setting where we have sample based access to the MDP, our
                  proposed algorithm is provably efficient, both in terms of its
                  sample and computational complexities. Key to our algorithmic
                  methodology is utilizing the conditional gradient method
                  (a.k.a. the Frank-Wolfe algorithm) which utilizes an
                  approximate MDP solver.}
}
@article{hazan2019introduction,
  author    = {Elad Hazan},
  title     = {Introduction to Online Convex Optimization},
  journal   = {CoRR},
  volume    = {abs/1909.05207},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1909.05207},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{hazan2016introduction,
  title = {Introduction to Online Convex Optimization},
  author = {Elad Hazan},
  year = {2016},
  volume = {2},
  journal = {Foundations and Trends® in Optimization},
  doi = {10.1561/2400000013},
  issn = {2167-3888},
  number = {3-4},
  pages = {157-325},
}
@article{hsieh2018mirrored,
  title     = {Mirrored Langevin Dynamics},
  author    = {Ya{-}Ping Hsieh and
               Volkan Cevher},
  journal   = {CoRR},
  volume    = {abs/1802.10174},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1802.10174},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{huang2017following,
  author  = {Ruitong Huang and Tor Lattimore and Andr{{\'a}}s Gy{{\"o}}rgy and Csaba Szepesv{{\'a}}ri},
  title   = {Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {145},
  pages   = {1-31},
}
@article{hoorfar2008inequalities,
  title={Inequalities on the Lambert W function and hyperpower function},
  author={Hoorfar, Abdolhossein and Hassani, Mehdi},
  journal={J. Inequal. Pure and Appl. Math},
  volume={9},
  number={2},
  pages={5--9},
  year={2008}
}
% =========
% --- I ---
% =========
@InProceedings{ito2021parameter,
  title = 	 {Parameter-Free Multi-Armed Bandit Algorithms with Hybrid Data-Dependent Regret Bounds},
  author =       {Ito, Shinji},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2552--2583},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  abstract = 	 {This paper presents multi-armed bandit (MAB) algorithms that
                  work well in adversarial environments and that offer improved
                  performance by exploiting inherent structures in such
                  environments, as stochastic generative models, as well as
                  small variations in loss vectors. The fundamental aim of this
                  work is to overcome the limitation of worst-case analyses in
                  MAB contexts. There can be found two basic approaches
                  achieving this purpose: best-of-both-worlds algorithms that
                  work well for both stochastic and adversarial settings, and
                  data-dependent regret bounds that work well depending on
                  certain difficulty indicators w.r.g.&nbsp;loss sequences. One
                  remarkable study w.r.t.&nbsp;the best-of-both-worlds approach
                  deals with the Tsallis-INF algorithm
                  \citep{zimmert2019optimal}, which achieves nearly optimal
                  regret bounds up to small constants in both settings, though
                  such bounds have remained unproven for a special case of a
                  stochastic setting with multiple optimal arms.  This paper
                  offers two particular contributions: (i) We show that the
                  Tsallis-INF algorithm enjoys a regret bound of a logarithmic
                  order in the number of rounds for stochastic environments,
                  even if the best arm is not unique. (ii) We provide a new
                  algorithm with a new \textit{hybrid} regret bound that implies
                  logarithmic regret in the stochastic regime and multiple
                  data-dependent regret bounds in the adversarial regime,
                  including bounds dependent on cumulative loss, total
                  variation, and loss-sequence path-length. Both our proposed
                  algorithm and the Tsallis-INF algorithm are based on a
                  follow-the-regularized-leader (FTRL) framework with a
                  time-varying regularizer. The analyses in this paper rely on
                  \textit{skewed Bregman divergence}, which provides simple
                  expressions of regret bounds for FTRL with a time-varying
                  regularizer.}
}
@misc{loshchilov2016online,
      title={Online Batch Selection for Faster Training of Neural Networks},
      author={Ilya Loshchilov and Frank Hutter},
      year={2016},
      eprint={1511.06343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% =========
% --- J ---
% =========
@InProceedings{jacobsen2022parameter,
  title = 	 {Parameter-free Mirror Descent},
  author =       {Jacobsen, Andrew and Cutkosky, Ashok},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {4160--4211},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  month = 	 {02--05 Jul},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
}
@InProceedings{jezequel2020efficient,
  title = 	 {Efficient improper learning for online logistic regression},
  author =       {J{\'e}z{\'e}quel, R{\'e}mi and Gaillard, Pierre and Rudi, Alessandro},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2085--2108},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  abstract = 	 { We consider the setting of online  logistic regression and
                  consider the regret with respect to the $\ell_2$-ball of
                  radius $B$. It is known (see Hazan et al. (2014)) that any
                  proper algorithm which has logarithmic regret in the number of
                  samples (denoted $n$) necessarily suffers an exponential
                  multiplicative constant in $B$. In this work, we design an
                  efficient improper algorithm that avoids this exponential
                  constant while preserving a logarithmic regret. Indeed, Foster
                  et al. (2018) showed that the lower bound  does not apply to
                  improper algorithms and proposed a strategy based on
                  exponential weights with prohibitive computational complexity.
                  Our new algorithm based on regularized empirical risk
                  minimization with surrogate losses satisfies a regret scaling
                  as $O(B\log(Bn))$ with a per-round time-complexity of order
                  $O(d^2 + \log(n))$.}
}
@misc{jin2021bellman,
      title={Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms},
      author={Chi Jin and Qinghua Liu and Sobhan Miryoosefi},
      year={2021},
      eprint={2102.00815},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{joulani2020simpler,
  title = 	 {A simpler approach to accelerated optimization: iterative averaging meets optimism},
  author =       {Joulani, Pooria and Raj, Anant and Gyorgy, Andras and Szepesvari, Csaba},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4984--4993},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {Recently there have been several attempts to extend Nesterov’s
                  accelerated algorithm to smooth stochastic and
                  variance-reduced optimization. In this paper, we show that
                  there is a simpler approach to acceleration: applying
                  optimistic online learning algorithms and querying the
                  gradient oracle at the online average of the intermediate
                  optimization iterates. In particular, we tighten a recent
                  result of Cutkosky (2019) to demonstrate theoretically that
                  online iterate averaging results in a reduced optimization
                  gap, independently of the algorithm involved. We show that
                  carefully combining this technique with existing generic
                  optimistic online learning algorithms yields the optimal
                  accelerated rates for optimizing strongly-convex and
                  non-strongly-convex, possibly composite objectives, with
                  deterministic as well as stochastic first-order oracles. We
                  further extend this idea to variance-reduced optimization.
                  Finally, we also provide “universal” algorithms that achieve
                  the optimal rate for smooth and non-smooth composite
                  objectives simultaneously without further tuning, generalizing
                  the results of Kavis et al. (2019) and solving a number of
                  their open problems.}
}
@misc{juditsky2020unifying,
      title={Unifying mirror descent and dual averaging},
      author={Anatoli Juditsky and Joon Kwon and Éric Moulines},
      year={2020},
      eprint={1910.13742},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{jin2019provably,
  author    = {Chi Jin and
               Zhuoran Yang and
               Zhaoran Wang and
               Michael I. Jordan},
  title     = {Provably Efficient Reinforcement Learning with Linear Function Approximation},
  journal   = {CoRR},
  volume    = {abs/1907.05388},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1907.05388},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{jun2019kernel,
  author    = {Kwang{-}Sung Jun and
               Ashok Cutkosky and
               Francesco Orabona},
  title     = {Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration},
  journal   = {CoRR},
  volume    = {abs/1905.10680},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1905.10680},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{jun2019parameter,
  title = 	 {Parameter-Free Online Convex Optimization with Sub-Exponential Noise},
  author =       {Jun, Kwang-Sung and Orabona, Francesco},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1802--1823},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = 	 { We consider the problem of unconstrained online convex
                  optimization (OCO) with sub-exponential noise, a strictly more
                  general problem than the standard OCO. In this setting, the
                  learner receives a subgradient of the loss functions corrupted
                  by sub-exponential noise and strives to achieve optimal regret
                  guarantee, without knowledge of the competitor norm, i.e., in
                  a parameter-free way. Recently, Cutkosky and Boahen (COLT
                  2017) proved that, given unbounded subgradients, it is
                  impossible to guarantee a sublinear regret due to an
                  exponential penalty. This paper shows that it is possible to
                  go around the lower bound by allowing the observed
                  subgradients to be unbounded via stochastic noise. However,
                  the presence of unbounded noise in unconstrained OCO is
                  challenging; existing algorithms do not provide near-optimal
                  regret bounds or fail to have a guarantee. So, we design a
                  novel parameter-free OCO algorithm for Banach space, which we
                  call BANCO, via a reduction to betting on noisy coins. We show
                  that BANCO achieves the optimal regret rate in our problem.
                  Finally, we show the application of our results to obtain a
                  parameter-free locally private stochastic subgradient descent
                  algorithm, and the connection to the law of iterated
                  logarithms. }
}
@InProceedings{joulani2017modular,
  title = 	 {A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, and Variational Bounds},
  author = 	 {Pooria Joulani and András György and Csaba Szepesvári},
  booktitle = 	 {Proceedings of the 28th International Conference on Algorithmic Learning Theory},
  pages = 	 {681--720},
  year = 	 {2017},
  editor = 	 {Steve Hanneke and Lev Reyzin},
  volume = 	 {76},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Kyoto University, Kyoto, Japan},
  month = 	 {15--17 Oct},
  publisher =    {PMLR},
  abstract = {Recently, much work has been done on extending the scope of online
                  learning and incremental stochastic optimization algorithms.
                  In this paper we contribute to this effort in two ways: First,
                  based on a new regret decomposition and a generalization of
                  Bregman divergences, we provide a self-contained, modular
                  analysis of the two workhorses of online learning: (general)
                  adaptive versions of Mirror Descent (MD) and the
                  Follow-the-Regularized-Leader (FTRL) algorithms. The analysis
                  is done with extra care so as not to introduce assumptions not
                  needed in the proofs and allows to combine, in a
                  straightforward way, different algorithmic ideas (e.g.,
                  adaptivity, optimism, implicit updates) and learning settings
                  (e.g., strongly convex or composite objectives). This way we
                  are able to reprove, extend and refine a large body of the
                  literature, while keeping the proofs concise. The second
                  contribution is a byproduct of this careful analysis: We
                  present algorithms with improved variational bounds for
                  smooth, composite objectives, including a new family of
                  optimistic MD algorithms with only one projection step per
                  round. Furthermore, we provide a simple extension of adaptive
                  regret bounds to practically relevant non-convex problem
                  settings with essentially no extra effort.}
}
@InProceedings{jun2017improved,
  title = 	 {{Improved Strongly Adaptive Online Learning using Coin Betting}},
  author = 	 {Jun, Kwang-Sung and Orabona, Francesco and Wright, Stephen and Willett, Rebecca},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {943--951},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  abstract = 	 {This paper describes a new parameter-free online learning
                  algorithm for changing environments.  In comparing against
                  algorithms with the same time complexity as ours, we obtain a
                  strongly adaptive regret bound that is a factor of at least
                  $\sqrt\log(T)$ better, where $T$ is the time horizon.
                  Empirical results show that our algorithm outperforms
                  state-of-the-art methods in learning with expert advice and
                  metric learning scenarios.   }
}
@InProceedings{jadbabaie2015online,
  title = 	 {{Online Optimization : Competing with Dynamic Comparators}},
  author = 	 {Jadbabaie, Ali and Rakhlin, Alexander and Shahrampour, Shahin and Sridharan, Karthik},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {398--406},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  abstract = 	 {Recent literature on online learning has focused on developing
                  adaptive algorithms that take advantage of a regularity of the
                  sequence of observations, yet retain worst-case performance
                  guarantees. A complementary direction is to develop prediction
                  methods that perform well against complex benchmarks. In this
                  paper, we address these two directions together. We present a
                  fully adaptive method that competes with dynamic benchmarks in
                  which regret guarantee scales with regularity of the sequence
                  of cost functions and comparators. Notably, the regret bound
                  adapts to the smaller complexity measure in the problem
                  environment. Finally, we apply our results to drifting
                  zero-sum, two-player games where both players achieve no
                  regret guarantees against best sequences of actions in
                  hindsight.}
}
% =========
% --- K ---
% =========
@InProceedings{kirschner2021asymptotically,
  title = 	 {Asymptotically Optimal Information-Directed Sampling},
  author =       {Kirschner, Johannes and Lattimore, Tor and Vernade, Claire and Szepesvari, Csaba},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2777--2821},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  abstract = 	 {We introduce a simple and efficient algorithm for stochastic
                  linear bandits with finitely many actions that is
                  asymptotically optimal and (nearly) worst-case optimal in
                  finite time. The approach is based on the frequentist
                  information-directed sampling (IDS) framework, with a
                  surrogate for the information gain that is informed by the
                  optimization problem that defines the asymptotic lower bound.
                  Our analysis sheds light on how IDS balances the trade-off
                  between regret and information and uncovers a surprising
                  connection between the recently proposed primal-dual methods
                  and the IDS algorithm. We demonstrate empirically that IDS is
                  competitive with UCB in finite-time, and can be significantly
                  better in the asymptotic regime.}
}
@article{kuznetsov2020discrepancy,
  title={Discrepancy-based theory and algorithms for forecasting non-stationary time series},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  journal={Annals of Mathematics and Artificial Intelligence},
  pages={1--33},
  year={2020},
  publisher={Springer}
}
@InProceedings{kempka2019adaptive,
  title = 	 {Adaptive Scale-Invariant Online Algorithms for Learning Linear Models},
  author =       {Kempka, Michal and Kotlowski, Wojciech and Warmuth, Manfred K.},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3321--3330},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = {We consider online learning with linear models, where the
                  algorithm predicts on sequentially revealed instances (feature
                  vectors), and is compared against the best linear function
                  (comparator) in hindsight. Popular algorithms in this
                  framework, such as Online Gradient Descent (OGD), have
                  parameters (learning rates), which ideally should be tuned
                  based on the scales of the features and the optimal
                  comparator, but these quantities only become available at the
                  end of the learning process. In this paper, we resolve the
                  tuning problem by proposing online algorithms making
                  predictions which are invariant under arbitrary rescaling of
                  the features. The algorithms have no parameters to tune, do
                  not require any prior knowledge on the scale of the instances
                  or the comparator, and achieve regret bounds matching (up to a
                  logarithmic factor) that of OGD with optimally tuned separate
                  learning rates per dimension, while retaining comparable
                  runtime performance.}
}
@InProceedings{kotlowski2017scale,
  title = 	 {Scale-Invariant Unconstrained Online Learning},
  author = 	 {Wojciech Kotłowski},
  booktitle = 	 {Proceedings of the 28th International Conference on Algorithmic Learning Theory},
  pages = 	 {412--433},
  year = 	 {2017},
  editor = 	 {Steve Hanneke and Lev Reyzin},
  volume = 	 {76},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Kyoto University, Kyoto, Japan},
  month = 	 {15--17 Oct},
  publisher =    {PMLR},
  abstract = {We consider a variant of online convex optimization in which both
                  the instances (input vectors) and the comparator (weight
                  vector) are unconstrained. We exploit a natural scale
                  invariance symmetry in our unconstrained setting: the
                  predictions of the optimal comparator are invariant under any
                  linear transformation of the instances. Our goal is to design
                  online algorithms which also enjoy this property, i.e. are
                  scale-invariant. We start with the case of coordinate-wise
                  invariance, in which the individual coordinates (features) can
                  be arbitrarily rescaled. We give an algorithm, which achieves
                  essentially optimal regret bound in this setup, expressed by
                  means of a coordinate-wise scale-invariant norm of the
                  comparator. We then study general invariance with respect to
                  arbitrary linear transformations. We first give a negative
                  result, showing that no algorithm can achieve a meaningful
                  bound in terms of scale-invariant norm of the comparator in
                  the worst case. Next, we compliment this result with a
                  positive one, providing an algorithm which "almost" achieves
                  the desired bound, incurring only a logarithmic overhead in
                  terms of the norm of the instances.}
}
@article{kamalaruban2016improved,
  author    = {Parameswaran Kamalaruban},
  title     = {Improved Optimistic Mirror Descent for Sparsity and Curvature},
  journal   = {CoRR},
  volume    = {abs/1609.02383},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1609.02383},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{karimi2016linear,
  author    = {Hamed Karimi and
               Julie Nutini and
               Mark Schmidt},
  title     = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-{\L}ojasiewicz Condition},
  journal   = {CoRR},
  volume    = {abs/1608.04636},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1608.04636},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{koolen2016combining,
  title     = {Combining Adversarial Guarantees and Stochastic Fast Rates in Online
               Learning},
  author    = {Wouter M. Koolen and
               Peter Gr{\"{u}}nwald and
               Tim van Erven},
  journal   = {CoRR},
  volume    = {abs/1605.06439},
  year      = {2016},
  eprint    = {1605.06439},
  timestamp = {Mon, 13 Aug 2018 16:48:31 +0200},
}
@InProceedings{kuznetsov2016timeseries,
  title = 	 {Time series prediction and online learning},
  author = 	 {Vitaly Kuznetsov and Mehryar Mohri},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1190--1213},
  year = 	 {2016},
  editor = 	 {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  abstract = 	 {We present a series of theoretical and algorithmic results
                  combining the benefits of the statistical learning approach to
                  time series prediction with that of on-line learning. We prove
                  new generalization guarantees for hypotheses derived from
                  regret minimization algorithms in the general scenario where
                  the data is generated by a non-stationary non-mixing
                  stochastic process. Our theory enables us to derive model
                  selection techniques with favorable theoretical guarantees in
                  the scenario of time series, thereby solving a problem that is
                  notoriously difficult in that scenario. It also helps us
                  devise new ensemble methods with favorable theoretical
                  guarantees for the task of forecasting non-stationary time
                  series. }
}
@InProceedings{koolen2015second,
  title = 	 {Second-order Quantile Methods for Experts and Combinatorial Games},
  author = 	 {Wouter M. Koolen and Tim Van Erven},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {1155--1175},
  year = 	 {2015},
  editor = 	 {Peter Grünwald and Elad Hazan and Satyen Kale},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher =    {PMLR},
  abstract = 	 {We aim to design strategies for sequential decision making
                  that adjust to the difficulty of the learning problem. We
                  study this question both in the setting of prediction with
                  expert advice, and for more general combinatorial decision
                  tasks. We are not satisfied with just guaranteeing minimax
                  regret rates, but we want our algorithms to perform
                  significantly better on easy data. Two popular ways to
                  formalize such adaptivity are second-order regret bounds and
                  quantile bounds. The underlying notions of ‘easy data’, which
                  may be paraphrased as “the learning problem has small
                  variance” and “multiple decisions are useful”, are synergetic.
                  But even though there are sophisticated algorithms that
                  exploit one of the two, no existing algorithm is able to adapt
                  to both. The difficulty in combining the two notions lies in
                  tuning a parameter called the learning rate, whose optimal
                  value behaves non-monotonically.  We introduce a potential
                  function for which (very surprisingly!) it is sufficient to
                  simply put a prior on learning rates; an approach that does
                  not work for any previous method. By choosing the right prior
                  we construct efficient algorithms and show that they reap both
                  benefits by proving the first bounds that are both
                  second-order and incorporate quantiles.}
}
@article{krichene2015accelerated,
  title={Accelerated mirror descent in continuous and discrete time},
  author={Krichene, Walid and Bayen, Alexandre and Bartlett, Peter L},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={2845--2853},
  year={2015}
}
@article{kuznetsov2015learning,
title = {Learning theory and algorithms for forecasting non-stationary time series},
author = {Vitaly Kuznetsov and Mehryar Mohri},
year = {2015},
language = {English (US)},
volume = {2015-January},
pages = {541--549},
journal = {Advances in Neural Information Processing Systems},
issn = {1049-5258},
abstract = {We present data-dependent learning bounds for the general scenario
                  of nonstationary non-mixing stochastic processes. Our learning
                  guarantees are expressed in terms of a data-dependent measure
                  of sequential complexity and a discrepancy measure that can be
                  estimated from data under some mild assumptions. We use our
                  learning bounds to devise new algorithms for non-stationary
                  time series forecasting for which we report some preliminary
                  experimental results.}
}
@inproceedings{kakade2008generalization,
author = {Kakade, Sham M. and Tewari, Ambuj},
title = {On the Generalization Ability of Online <i>Strongly</i> Convex Programming Algorithms},
year = {2008},
isbn = {9781605609492},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 21st International Conference on Neural Information Processing Systems},
pages = {801–808},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'08},
abstract = {This paper examines the generalization properties of online convex
                  programming algorithms when the loss function is Lipschitz and
                  strongly convex. Our main result is a sharp bound, that holds
                  with high probability, on the excess risk of the output of an
                  online algorithm in terms of the average regret. This allows
                  one to use recent algorithms with logarithmic cumulative
                  regret guarantees to achieve fast convergence rates for the
                  excess risk with high probability. As a corollary, we
                  characterize the convergence rate of PEGASOS (with high
                  probability), a recently proposed method for solving the SVM
                  optimization problem.}
}
@incollection{kiefer1987introduction,
  title={Introduction to statistical inference},
  author={Kiefer, Jack Carl},
  booktitle={Introduction to Statistical Inference},
  pages={1--3},
  year={1987},
  publisher={Springer}
}
% =========
% --- L ---
% =========
@inproceedings{liu2022initialization,
  title = {On the Initialization for Convex-Concave Min-max Problems},
  author = {Liu, Mingrui and Orabona, Francesco},
  booktitle = {Proceedings of The 33rd International Conference on Algorithmic Learning Theory},
  pages = {743--767},
  year = {2022},
  editor = {Dasgupta, Sanjoy and Haghtalab, Nika},
  volume = {167},
  series = {Proceedings of Machine Learning Research},
  month = {29 Mar--01 Apr},
  publisher = {PMLR},
  abstract = {Convex-concave min-max problems are ubiquitous in machine learning, and people usually utilize first-order methods (e.g., gradient descent ascent) to find the optimal solution. One feature which separates convex-concave min-max problems from convex minimization problems is that the best known convergence rates for min-max problems have an explicit dependence on the size of the domain, rather than on the distance between initial point and the optimal solution. This means that the convergence speed does not have any improvement even if the algorithm starts from the optimal solution, and hence, is oblivious to the initialization. Here, we show that strict-convexity-strict-concavity is sufficient to get the convergence rate to depend on the initialization. We also show how different algorithms can asymptotically achieve initialization-dependent convergence rates on this class of functions. Furthermore, we show that the so-called “parameter-free” algorithms allow to achieve improved initialization-dependent asymptotic rates without any learning rate to tune. In addition, we utilize this particular parameter-free algorithm as a subroutine to design a new algorithm, which achieves a novel non-asymptotic fast rate for strictly-convex-strictly-concave min-max problems with a growth condition and H{ö}lder continuous solution mapping. Experiments are conducted to verify our theoretical findings and demonstrate the effectiveness of the proposed algorithms. },
}
@misc{lan2021policy,
      title={Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes},
      author={Guanghui Lan},
      year={2021},
      eprint={2102.00135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{lattimore2021mirror,
  title = 	 {Mirror Descent and the Information Ratio},
  author =       {Lattimore, Tor and Gyorgy, Andras},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2965--2992},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  abstract = 	 {We establish a connection between the stability of mirror
                  descent and the information ratio by Russo and Van Roy (2014).
                  Our analysis shows that mirror descent with suitable loss
                  estimators and exploratory distributions enjoys the same bound
                  on the adversarial regret as the bounds on the Bayesian regret
                  for information-directed sampling. Along the way, we develop
                  the theory for information-directed sampling and provide an
                  efficient algorithm for adversarial bandits for which the
                  regret upper bound matches exactly the best known
                  information-theoretic upper bound. Keywords: Bandits, partial
                  monitoring, mirror descent, information theory.}
}
@article{levy2021storm+,
  title={STORM+: Fully Adaptive SGD with Momentum for Nonconvex Optimization},
  author={Levy, Kfir Y and Kavis, Ali and Cevher, Volkan},
  year={2021},
  journal={arXiv preprint arXiv:2111.01040},
}
@misc{li2021eluder,
      title={Eluder Dimension and Generalized Rank},
      author={Gene Li and Pritish Kamath and Dylan J. Foster and Nathan Srebro},
      year={2021},
      eprint={2104.06970},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{li2021iterate,
      title={On the Last Iterate Convergence of Momentum Methods},
      author={Xiaoyu Li and Mingrui Liu and Francesco Orabona},
      year={2021},
      eprint={2102.07002},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu2021parameterfree,
      title={A Parameter-free Algorithm for Convex-concave Min-max Problems},
      author={Mingrui Liu and Francesco Orabona},
      year={2021},
      eprint={2103.00284},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{liu2021equivalence,
      title={Equivalence Analysis between Counterfactual Regret Minimization and Online Mirror Descent},
      author={Weiming Liu and Huacong Jiang and Bin Li and Houqiang Li},
      year={2021},
      eprint={2110.04961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{loizou2021stochastic,
  title = 	 { Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence },
  author =       {Loizou, Nicolas and Vaswani, Sharan and Hadj Laradji, Issam and Lacoste-Julien, Simon},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1306--1314},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  abstract = 	 { We propose a stochastic variant of the classical Polyak
                  step-size (Polyak, 1987) commonly used in the subgradient
                  method. Although computing the Polyak step-size requires
                  knowledge of the optimal function values, this information is
                  readily available for typical modern machine learning
                  applications. Consequently, the proposed stochastic Polyak
                  step-size (SPS) is an attractive choice for setting the
                  learning rate for stochastic gradient descent (SGD). We
                  provide theoretical convergence guarantees for SGD equipped
                  with SPS in different settings, including strongly convex,
                  convex and non-convex functions. Furthermore, our analysis
                  results in novel convergence guarantees for SGD with a
                  constant step-size. We show that SPS is particularly effective
                  when training over-parameterized models capable of
                  interpolating the training data. In this setting, we prove
                  that SPS enables SGD to converge to the true solution at a
                  fast rate without requiring the knowledge of any
                  problem-dependent constants or additional computational
                  overhead. We experimentally validate our theoretical results
                  via extensive experiments on synthetic and real datasets. We
                  demonstrate the strong performance of SGD with SPS compared to
                  state-of-the-art optimization methods when training
                  over-parameterized models. }
}
@article{lehnert2020successor,
  title={Successor features combine elements of model-free and model-based reinforcement learning},
  author={Lehnert, Lucas and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={196},
  pages={1--53},
  year={2020}
}
@misc{liu2020temporal,
      title={Temporal Difference Learning as Gradient Splitting},
      author={Rui Liu and Alex Olshevsky},
      year={2020},
      eprint={2010.14657},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu2020adam,
      title={Adam$^+$: A Stochastic Method with Adaptive Variance Reduction},
      author={Mingrui Liu and Wei Zhang and Francesco Orabona and Tianbao Yang},
      year={2020},
      eprint={2011.11985},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{li2019onthe,
  title = 	 {On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes},
  author =       {Li, Xiaoyu and Orabona, Francesco},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {983--992},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Masashi Sugiyama},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  abstract = 	 {Stochastic gradient descent is the method of choice for large
                  scale optimization of machine learning objective functions.
                  Yet, its performance is greatly variable and heavily depends
                  on the choice of the stepsizes. This has motivated a large
                  body of research on adaptive stepsizes. However, there is
                  currently a gap in our theoretical understanding of these
                  methods, especially in the non-convex setting. In this paper,
                  we start closing this gap: we theoretically analyze in the
                  convex and non-convex settings a generalized version of the
                  AdaGrad stepsizes. We show sufficient conditions for these
                  stepsizes to achieve almost sure asymptotic convergence of the
                  gradients to zero, proving the first guarantee for generalized
                  AdaGrad stepsizes in the non-convex setting. Moreover, we show
                  that these stepsizes allow to automatically adapt to the level
                  of noise of the stochastic gradients in both the convex and
                  non-convex settings, interpolating between O(1/T) and
                  O(1/sqrt(T)), up to logarithmic terms.}
}
@article{lu2019relative,
    author = {Lu, Haihao},
    title = {"Relative Continuity" for Non-Lipschitz Nonsmooth Convex Optimization Using Stochastic (or Deterministic) Mirror Descent},
    journal = {INFORMS Journal on Optimization},
    volume = {1},
    number = {4},
    pages = {288-303},
    year = {2019},
    doi = {10.1287/ijoo.2018.0008},
    abstract = { The usual approach to developing and analyzing first-order
                  methods for nonsmooth (stochastic or deterministic) convex
                  optimization assumes that the objective function is uniformly
                  Lipschitz continuous with parameter Mf. However, in many
                  settings, the nondifferentiable convex function f is not
                  uniformly Lipschitz continuous—for example, (i) the classical
                  support vector machine problem, (ii) the problem of minimizing
                  the maximum of convex quadratic functions, and even (iii) the
                  univariate setting with f(x):=max{0,x}+x2. Herein, we develop
                  a notion of “relative continuity” that is determined relative
                  to a user-specified “reference function” h (that should be
                  computationally tractable for algorithms), and we show that
                  many nondifferentiable convex functions are relatively
                  continuous with respect to a correspondingly fairly simple
                  reference function h. We also similarly develop a notion of
                  “relative stochastic continuity” for the stochastic setting.
                  We analyze two standard algorithms—the (deterministic) mirror
                  descent algorithm and the stochastic mirror descent
                  algorithm—for solving optimization problems in these new
                  settings, providing the first computational guarantees for
                  instances where the objective function is not uniformly
                  Lipschitz continuous. This paper is a companion paper for
                  nondifferentiable convex optimization to the recent paper by
                  Lu et al. [Lu H, Freund RM, Nesterov Y (2018) Relatively
                  smooth convex optimization by first-order methods, and
                  applications. SIAM J. Optim. 28(1): 333–354.], which developed
                  analogous results for differentiable convex optimization. }
}
@article{lu2019adaptive,
  title={Adaptive and Efficient Algorithms for Tracking the Best Expert},
  author={Lu, Shiyin and Zhang, Lijun},
  journal={arXiv preprint arXiv:1909.02187},
  year={2019}
}
@inproceedings{levy2018online,
 author = {Levy, Kfir Y. and Yurtsever, Alp and Cevher, Volkan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {6500--6509},
 publisher = {Curran Associates, Inc.},
 title = {Online Adaptive Methods, Universality and Acceleration},
 volume = {31},
 year = {2018}
}
@inproceedings{levy2017online,
 author = {Levy, Kfir},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {1613--1622},
 publisher = {Curran Associates, Inc.},
 title = {Online to Offline Conversions, Universality and Adaptive Minibatch Sizes},
 volume = {30},
 year = {2017}
}
@article{liu2018proximal,
  title={Proximal gradient temporal difference learning: Stable reinforcement learning with polynomial sample complexity},
  author={Liu, Bo and Gemp, Ian and Ghavamzadeh, Mohammad and Liu, Ji and Mahadevan, Sridhar and Petrik, Marek},
  journal={Journal of Artificial Intelligence Research},
  volume={63},
  pages={461--494},
  year={2018}
}
% ==========
% --- M ---
% ==========
@article{ma2021greedy,
  author    = {Shaocong Ma and
               Ziyi Chen and
               Yi Zhou and
               Shaofeng Zou},
  title     = {Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity},
  journal   = {CoRR},
  volume    = {abs/2103.16377},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2103.16377},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{madden2021bounds,
  title={Bounds for the tracking error of first-order online optimization methods},
  author={Madden, Liam and Becker, Stephen and Dall’Anese, Emiliano},
  journal={Journal of Optimization Theory and Applications},
  volume={189},
  number={2},
  pages={437--457},
  year={2021},
  publisher={Springer}
}
@misc{mei2021leveraging,
      title={Leveraging Non-uniformity in First-order Non-convex Optimization},
      author={Jincheng Mei and Yue Gao and Bo Dai and Csaba Szepesvari and Dale Schuurmans},
      year={2021},
      eprint={2105.06072},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{muehlebach2021optimization,
  author  = {Michael Muehlebach and Michael I. Jordan},
  title   = {Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {73},
  pages   = {1-50}
}
@InProceedings{mei2020onthe,
  title = 	 {On the Global Convergence Rates of Softmax Policy Gradient Methods},
  author =       {Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6820--6829},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = 	 {We make three contributions toward better understanding policy
                  gradient methods in the tabular setting. First, we show that
                  with the true gradient, policy gradient with a softmax
                  parametrization converges at a $O(1/t)$ rate, with constants
                  depending on the problem and initialization. This result
                  significantly expands the recent asymptotic convergence
                  results. The analysis relies on two findings: that the softmax
                  policy gradient satisfies a Ł{}ojasiewicz inequality, and the
                  minimum probability of an optimal action during optimization
                  can be bounded in terms of its initial value. Second, we
                  analyze entropy regularized policy gradient and show that it
                  enjoys a significantly faster linear convergence rate
                  $O(e^{-t})$ toward softmax optimal policy. This result
                  resolves an open question in the recent literature. Finally,
                  combining the above two results and additional new
                  $\Omega(1/t)$ lower bound results, we explain how entropy
                  regularization improves policy optimization, even with the
                  true gradient, from the perspective of convergence rate. The
                  separation of rates is further explained using the notion of
                  non-uniform Ł{}ojasiewicz degree. These results provide a
                  theoretical understanding of the impact of entropy and
                  corroborate existing empirical studies.}
}
@misc{mhammedi2021efficient,
      title={Efficient Projection-Free Online Convex Optimization with Membership Oracle},
      author={Zakaria Mhammedi},
      year={2021},
      eprint={2111.05818},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{mhammedi2021risk,
  title={Risk Monotonicity in Statistical Learning},
  author={Mhammedi, Zakaria},
  year={2021},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
}
@InProceedings{mhammedi2020lipschitz,
  title = 	 {Lipschitz and Comparator-Norm Adaptivity in Online Learning},
  author =       {Mhammedi, Zakaria and Koolen, Wouter M.},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2858--2887},
  year = 	 {2020},
  editor = 	 {Jacob Abernethy and Shivani Agarwal},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  abstract = { We study Online Convex Optimization in the unbounded setting
                  where neither predictions nor gradient are constrained. The
                  goal is to simultaneously adapt to both the sequence of
                  gradients and the comparator. We first develop parameter-free
                  and scale-free algorithms for a simplified setting with hints.
                  We present two versions: the first adapts to the squared norms
                  of both comparator and gradients separately using $O(d)$ time
                  per round, the second adapts to their squared inner products
                  (which measure variance only in the comparator direction) in
                  time $O(d^3)$ per round. We then generalize two prior
                  reductions to the unbounded setting; one to not need hints,
                  and a second to deal with the range ratio problem (which
                  already arises in prior work). We discuss their optimality in
                  light of prior and new lower bounds. We apply our methods to
                  obtain sharper regret bounds for scale-invariant online
                  prediction with linear models. }
}
@InProceedings{mhammedi2019lipschitz,
  title = 	 {Lipschitz Adaptivity with Multiple Learning Rates in Online Learning},
  author =       {Mhammedi, Zakaria and Koolen, Wouter M and Van Erven, Tim},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2490--2511},
  year = 	 {2019},
  editor = 	 {Alina Beygelzimer and Daniel Hsu},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Phoenix, USA},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  abstract = {We aim to design adaptive online learning algorithms that take
                  advantage of any special structure that might be present in
                  the learning task at hand, with as little manual tuning by the
                  user as possible. A fundamental obstacle that comes up in the
                  design of such adaptive algorithms is to calibrate a so-called
                  step-size or learning rate hyperparameter depending on
                  variance, gradient norms, etc. A recent technique promises to
                  overcome this difficulty by maintaining multiple learning
                  rates in parallel. This technique has been applied in the
                  MetaGrad algorithm for online convex optimization and the
                  Squint algorithm for prediction with expert advice. However,
                  in both cases the user still has to provide in advance a
                  Lipschitz hyperparameter that bounds the norm of the
                  gradients. Although this hyperparameter is typically not
                  available in advance, tuning it correctly is crucial: if it is
                  set too small, the methods may fail completely; but if it is
                  taken too large, performance deteriorates significantly. In
                  the present work we remove this Lipschitz hyperparameter by
                  designing new versions of MetaGrad and Squint that adapt to
                  its optimal value automatically. We achieve this by
                  dynamically updating the set of active learning rates. For
                  MetaGrad, we further improve the computational efficiency of
                  handling constraints on the domain of prediction, and we
                  remove the need to specify the number of rounds in advance.}
}


@article{mertikopoulos2018mirror,
  author    = {Panayotis Mertikopoulos and
               Houssam Zenati and
               Bruno Lecouat and
               Chuan{-}Sheng Foo and
               Vijay Chandrasekhar and
               Georgios Piliouras},
  title     = {Mirror descent in saddle-point problems: Going the extra (gradient)
               mile},
  journal   = {CoRR},
  volume    = {abs/1807.02629},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1807.02629},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{mcmahan2017survey,
  author  = {H. Brendan McMahan},
  title   = {A Survey of Algorithms and Analysis for Adaptive Online Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {90},
  pages   = {1-50},
}

@article{mcmahan2017survey,
  title = {A Survey of Algorithms and Analysis for Adaptive Online Learning},
  author = {McMahan, H. Brendan},
  year = {2017},
  issue_date = {January 2017},
  publisher = {JMLR.org},
  volume = {18},
  number = {1},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  month = {jan},
  pages = {3117–3166},
  numpages = {50},
  abstract = {We present tools for the analysis of Follow-The-Regularized-Leader
                  (FTRL), Dual Averaging, and Mirror Descent algorithms when the
                  regularizer (equivalently, proxfunction or learning rate
                  schedule) is chosen adaptively based on the data. Adaptivity
                  can be used to prove regret bounds that hold on every round,
                  and also allows for data-dependent regret bounds as in
                  AdaGrad-style algorithms (e.g., Online Gradient Descent with
                  adaptive per-coordinate learning rates). We present results
                  from a large number of prior works in a unified manner, using
                  a modular and tight analysis that isolates the key arguments
                  in easily re-usable lemmas. This approach strengthens
                  previously known FTRL analysis techniques to produce bounds as
                  tight as those achieved by potential functions or primal-dual
                  analysis. Further, we prove a general and exact equivalence
                  between adaptive Mirror Descent algorithms and a corresponding
                  FTRL update, which allows us to analyze Mirror Descent
                  algorithms in the same framework. The key to bridging the gap
                  between Dual Averaging and Mirror Descent algorithms lies in
                  an analysis of the FTRL-Proximal algorithm family. Our regret
                  bounds are proved in the most general form, holding for
                  arbitrary norms and non-smooth regularizers with time-varying
                  weight.},
}
@InProceedings{mcmahan2014unconstrained,
  title = 	 {Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations},
  author = 	 {H. Brendan McMahan and Francesco Orabona},
  booktitle = 	 {Proceedings of The 27th Conference on Learning Theory},
  pages = 	 {1020--1039},
  year = 	 {2014},
  editor = 	 {Maria Florina Balcan and Vitaly Feldman and Csaba Szepesvári},
  volume = 	 {35},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Barcelona, Spain},
  month = 	 {13--15 Jun},
  publisher =    {PMLR},
  abstract = {We study algorithms for online linear optimization in Hilbert
                  spaces, focusing on the case where the player is
                  unconstrained. We develop a novel characterization of a large
                  class of minimax algorithms, recovering, and even improving,
                  several previous results as immediate corollaries. Moreover,
                  using our tools, we develop an algorithm that provides a
                  regret bound of O(U \sqrtT \log( U \sqrtT \log^2 T +1)), where
                  U is the L_2 norm of an arbitrary comparator and both T and U
                  are unknown to the player. This bound is optimal up to
                  \sqrt\log \log T terms. When T is known, we derive an
                  algorithm with an optimal regret bound (up to constant
                  factors). For both the known and unknown T case, a Normal
                  approximation to the conditional value of the game proves to
                  be the key analysis tool.}
}
@inproceedings{mcmahan2013minimax,
 title = {Minimax Optimal Algorithms for Unconstrained Linear Optimization},
 author = {McMahan, Brendan and Abernethy, Jacob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 volume = {26},
 year = {2013}
}
@inproceedings{mcmahan2012noregret,
 author = {Mcmahan, Brendan and Streeter, Matthew},
 title = {No-Regret Algorithms for Unconstrained Online Convex Optimization},
 year = {2012},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 volume = {25},
}
@InProceedings{mohri2012newanalysis,
author={Mohri, Mehryar and Mu{\~{n}}oz Medina, Andres},
title={New Analysis and Algorithm for Learning with Drifting Distributions},
booktitle={Algorithmic Learning Theory},
year={2012},
editor={Bshouty, Nader H. and Stoltz, Gilles and Vayatis, Nicolas and Zeugmann, Thomas},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={124--138},
abstract={We present a new analysis of the problem of learning with drifting
                  distributions in the batch setting using the notion of
                  discrepancy. We prove learning bounds based on the Rademacher
                  complexity of the hypothesis set and the discrepancy of
                  distributions both for a drifting PAC scenario and a tracking
                  scenario. Our bounds are always tighter and in some cases
                  substantially improve upon previous ones based on the L1
                  distance. We also present a generalization of the standard
                  on-line to batch conversion to the drifting scenario in terms
                  of the discrepancy and arbitrary convex combinations of
                  hypotheses. We introduce a new algorithm exploiting these
                  learning guarantees, which we show can be formulated as a
                  simple QP. Finally, we report the results of preliminary
                  experiments demonstrating the benefits of this algorithm.}
}
% =========
% --- N ---
% =========
@article{neu2020online,
  title={Online learning in MDPs with linear function approximation and bandit feedback},
  author={Neu, Gergely and Olkhovskaya, Julia},
  journal={arXiv preprint arXiv:2007.01612},
  year={2020}
}
@article{neu2020unifying,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  journal={arXiv preprint arXiv:2007.01891},
  year={2020}
}
@book{nesterov1994interior,
  title={Interior-point polynomial algorithms in convex programming},
  author={Nesterov, Yurii and Nemirovskii, Arkadii},
  year={1994},
  publisher={SIAM}
}

% =========
% --- O ---
% =========
@misc{orabona2021last,
  title={Last iterate of sgd converges (even in unbounded domains)},
  author={Orabona, F},
  journal={Blogpost on http://parameterfree.com},
  year={2021}
}
@article{orabona2021parameterfree,
      title={Parameter-free Stochastic Optimization of Variationally Coherent Functions},
      author={Francesco Orabona and Dávid Pál},
      year={2021},
      eprint={2102.00236},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{orabona2019modern,
  author    = {Francesco Orabona},
  title     = {A Modern Introduction to Online Learning},
  journal   = {CoRR},
  volume    = {abs/1912.13213},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1912.13213},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{ollivier2018approximate,
      title={Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies},
      author={Yann Ollivier},
      year={2018},
      eprint={1805.00869},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{orabona2018scale,
title = {Scale-free online learning},
journal = {Theoretical Computer Science},
volume = {716},
pages = {50 - 69},
year = {2018},
note = {Special Issue on ALT 2015},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2017.11.021},
author = {Francesco Orabona and Dávid Pál},
keywords = {Online algorithms, Optimization, Regret bounds, Online learning},
abstract = {We design and analyze algorithms for online linear optimization that
                  have optimal regret and at the same time do not need to know
                  any upper or lower bounds on the norm of the loss vectors. Our
                  algorithms are instances of the Follow the Regularized Leader
                  (FTRL) and Mirror Descent (MD) meta-algorithms. We achieve
                  adaptiveness to the norms of the loss vectors by scale
                  invariance, i.e., our algorithms make exactly the same
                  decisions if the sequence of loss vectors is multiplied by any
                  positive constant. The algorithm based on FTRL works for any
                  decision set, bounded or unbounded. For unbounded decisions
                  sets, this is the first adaptive algorithm for online linear
                  optimization with a non-vacuous regret bound. In contrast, we
                  show lower bounds on scale-free algorithms based on MD on
                  unbounded domains.}
}
@inproceedings{orabona2016coin,
  author = {Orabona, Francesco and P\'{a}l, D\'{a}vid},
  title = {Coin Betting and Parameter-Free Online Learning},
  year = {2016},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages = {577–585},
  numpages = {9},
  location = {Barcelona, Spain},
  series = {NIPS'16},
  abstract = {In the recent years, a number of parameter-free algorithms have been
                    developed for online linear optimization over Hilbert spaces
                    and for learning with expert advice. These algorithms achieve
                    optimal regret bounds that depend on the unknown competitors,
                    without having to tune the learning rates with oracle
                    choices.We present a new intuitive framework to design
                    parameter-free algorithms for both online linear optimization
                    over Hilbert spaces and for learning with expert advice, based
                    on reductions to betting on outcomes of adversarial coins. We
                    instantiate it using a betting algorithm based on the
                    Krichevsky-Trofimov estimator. The resulting algorithms are
                    simple, with no parameters to be tuned, and they improve or
                    match previous results in terms of regret guarantee and
                    per-round complexity.},
}
@article{orabona2015generalized,
author = {Orabona, Francesco and Crammer, Koby and Cesa-Bianchi, Nicol\`{o}},
title = {A Generalized Online Mirror Descent with Applications to Classification and Regression},
year = {2015},
issue_date = {June  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {99},
number = {3},
issn = {0885-6125},
doi = {10.1007/s10994-014-5474-8},
journal = {Mach. Learn.},
month = jun,
pages = {411–435},
numpages = {25},
abstract = {Online learning algorithms are fast, memory-efficient, easy to
                  implement, and applicable to many prediction problems,
                  including classification, regression, and ranking. Several
                  online algorithms were proposed in the past few decades, some
                  based on additive updates, like the Perceptron, and some on
                  multiplicative updates, like Winnow. A unifying perspective on
                  the design and the analysis of online algorithms is provided
                  by online mirror descent, a general prediction strategy from
                  which most first-order algorithms can be obtained as special
                  cases. We generalize online mirror descent to time-varying
                  regularizers with generic updates. Unlike standard mirror
                  descent, our more general formulation also captures second
                  order algorithms, algorithms for composite losses and
                  algorithms for adaptive filtering. Moreover, we recover, and
                  sometimes improve, known regret bounds as special cases of our
                  analysis using specific regularizers. Finally, we show the
                  power of our approach by deriving a new second order algorithm
                  with a regret bound invariant with respect to arbitrary
                  rescalings of individual features.}
}
@inproceedings{orabona2014simultaneous,
 title = {Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning},
 author = {Orabona, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 volume = {27},
 year = {2014}
}
@inproceedings{orabona2013dimensionfree,
 title = {Dimension-Free Exponentiated Gradient},
 author = {Orabona, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 volume = {26},
 year = {2013}
}
@InProceedings{orabona2012beyond,
  title = 	 {Beyond Logarithmic Bounds in Online Learning},
  author = 	 {Francesco Orabona and Nicolo Cesa-Bianchi and Claudio Gentile},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {823--831},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  abstract = 	 {We prove logarithmic regret bounds that depend on the loss L_T^* of the competitor rather than on the number T of time steps. In the general online convex optimization setting,  our bounds hold for any smooth and exp-concave loss (such as the square loss or the logistic loss). This bridges the gap between the O(ln T) regret exhibited by exp-concave losses and the O(sqrt(L_T^*)) regret exhibited by smooth losses. We also show that these bounds are tight for specific losses, thus they cannot be improved in general. For online regression with square loss, our analysis can be used to derive a sparse randomized variant of the online Newton step, whose expected number of updates scales with the algorithm’s loss. For online classification, we prove the first logarithmic mistake bounds that do not rely on prior knowledge of a bound on the competitor’s norm.}
}
% =========
% --- P ---
% =========
@misc{patterson2021investigating,
  title={Investigating Objectives for Off-policy Value Estimation in Reinforcement Learning},
  author={Patterson, Andrew and Ghiassian, Sina and Gupta, D and White, A and White, M},
  year={2021},
  publisher={Preparation}
}
@article{peng2020accelerating,
  author={Peng, Xinyu and Li, Li and Wang, Fei-Yue},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={Accelerating Minibatch Stochastic Gradient Descent Using Typicality Sampling},
  year={2020},
  volume={31},
  number={11},
  pages={4649-4659},
  doi={10.1109/TNNLS.2019.2957003}
}
@InProceedings{perdomo2020performative,
  title = 	 {Performative Prediction},
  author =       {Perdomo, Juan and Zrnic, Tijana and Mendler-D{\"u}nner, Celestine and Hardt, Moritz},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7599--7609},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {When predictions support decisions they may influence the outcome
                  they aim to predict. We call such predictions performative;
                  the prediction influences the target. Performativity is a
                  well-studied phenomenon in policy-making that has so far been
                  neglected in supervised learning. When ignored, performativity
                  surfaces as undesirable distribution shift, routinely
                  addressed with retraining. We develop a risk minimization
                  framework for performative prediction bringing together
                  concepts from statistics, game theory, and causality. A
                  conceptual novelty is an equilibrium notion we call
                  performative stability. Performative stability implies that
                  the predictions are calibrated not against past outcomes, but
                  against the future outcomes that manifest from acting on the
                  prediction. Our main results are necessary and sufficient
                  conditions for the convergence of retraining to a
                  performatively stable point of nearly minimal loss. In full
                  generality, performative prediction strictly subsumes the
                  setting known as strategic classification. We thus also give
                  the first sufficient conditions for retraining to overcome
                  strategic feedback effects.}
}
@article{popkov2005gradient,
  title={Gradient Methods for Nonstationary Unconstrained Optimization Problems},
  author={Popkov, A Yu},
  journal={Automation and Remote Control},
  volume={66},
  number={6},
  pages={883--891},
  year={2005},
  publisher={Springer}
}
% =========
% --- R ---
% =========
@article{roux2019anytime,
  title={Anytime tail averaging},
  author={Roux, Nicolas Le},
  year={2019},
  journal={arXiv preprint arXiv:1902.05083},
}
@misc{reddi2016fast,
      title={Fast Stochastic Methods for Nonsmooth Nonconvex Optimization},
      author={Sashank J. Reddi and Suvrit Sra and Barnabas Poczos and Alex Smola},
      year={2016},
      eprint={1605.06900},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@InProceedings{russo2016simple,
  title = 	 {Simple Bayesian Algorithms for Best Arm Identification},
  author = 	 {Russo, Daniel},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1417--1418},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  abstract = 	 {This paper considers the optimal adaptive allocation of
                  measurement effort for identifying the best among a finite set
                  of options or designs.  An experimenter sequentially chooses
                  designs to measure and observes noisy signals of their quality
                  with the goal of confidently identifying the best design
                  after a small number of measurements. I propose three simple
                  Bayesian algorithms for adaptively allocating measurement
                  effort. One is Top-Two Probability sampling, which computes
                  the two designs with the highest posterior probability of
                  being optimal, and then randomizes to select among these two.
                  One is a variant a top-two sampling which considers not only
                  the probability a design is optimal, but the expected amount
                  by which its quality exceeds that of other designs. The final
                  algorithm is a modified version of Thompson sampling that is
                  tailored for identifying the best design. I prove that these
                  simple algorithms satisfy a strong optimality property. In a
                  frequestist setting where the true quality of the designs is
                  fixed, one hopes the posterior definitively identifies the
                  optimal design, in the sense that that the posterior
                  probability assigned to the event that some other design is
                  optimal converges to zero as measurements are collected. I
                  show that under the proposed algorithms this convergence
                  occurs at an \emphexponential rate, and the corresponding
                  exponent is the best possible among all allocation rules.}
}
@article{raskutti2015theinformation,
  title={The Information Geometry of Mirror Descent},
  author={G. {Raskutti} and S. {Mukherjee}},
  journal={IEEE Transactions on Information Theory},
  year={2015},
  volume={61},
  number={3},
  pages={1451-1457},
  doi={10.1109/TIT.2015.2388583}
}
@InProceedings{rakhlin2013online,
  title = 	 {Online Learning with Predictable Sequences},
  author = 	 {Alexander Rakhlin and Karthik Sridharan},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {993--1019},
  year = 	 {2013},
  editor = 	 {Shai Shalev-Shwartz and Ingo Steinwart},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  abstract = 	 {We present methods for online linear optimization that take
                  advantage of benign (as opposed to worst-case) sequences.
                  Specifically if the sequence encountered by the learner is
                  described well by a known “predictable process”, the
                  algorithms presented enjoy tighter bounds as compared to the
                  typical worst case bounds. Additionally, the methods achieve
                  the usual worst-case regret bounds if the sequence is not
                  benign. Our approach can be seen as a way of adding \emphprior
                  knowledge about the sequence within the paradigm of online
                  learning. The setting is shown to encompass partial and side
                  information. Variance and path-length bounds can be seen as
                  particular examples of online learning with simple predictable
                  sequences.We further extend our methods to include competing
                  with a set of possible predictable processes (models), that is
                  “learning” the predictable process itself concurrently with
                  using it to obtain better regret guarantees. We show that such
                  model selection is possible under various assumptions on the
                  available feedback. }
}
@inproceedings{rakhlin2012relax,
 title = {Relax and Randomize : From Value to Algorithms},
 author = {Rakhlin, Sasha and Shamir, Ohad and Sridharan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 publisher = {Curran Associates, Inc.},
 volume = {25},
 year = {2012}
}
% =========
% --- S ---
% =========
@article{schlegel2021general,
  title={General value function networks},
  author={Schlegel, Matthew and Jacobsen, Andrew and Abbas, Zaheer and Patterson, Andrew and White, Adam and White, Martha},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={497--543},
  year={2021}
}
@article{salim2020dualize,
  title={Dualize, split, randomize: Fast nonsmooth optimization algorithms},
  author={Salim, Adil and Condat, Laurent and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2004.02635},
  year={2020}
}
@article{salim2020primal,
  title={Primal dual interpretation of the proximal stochastic gradient Langevin algorithm},
  author={Salim, Adil and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.09270},
  year={2020}
}
@article{salim2020wasserstein,
  title={The Wasserstein Proximal Gradient Algorithm},
  author={Salim, Adil and Korba, Anna and Luise, Giulia},
  journal={arXiv preprint arXiv:2002.03035},
  year={2020}
}
@inbook{sellke2020chasing,
author = {Mark Sellke},
title = {Chasing Convex Bodies Optimally},
booktitle = {Symposium on Discrete Algorithms},
chapter = {},
pages = {1509-1518},
doi = {10.1137/1.9781611975994.92},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975994.92},
}
@article{shariff2020efficient,
  title={Efficient Planning in Large MDPs with Weak Linear Function Approximation},
  author={Shariff, Roshan and Szepesv{\'a}ri, Csaba},
  journal={arXiv preprint arXiv:2007.06184},
  year={2020}
}
@misc{sinclair2020adaptive,
      title={Adaptive Discretization for Model-Based Reinforcement Learning},
      author={Sean R. Sinclair and Tianyu Wang and Gauri Jain and Siddhartha Banerjee and Christina Lee Yu},
      year={2020},
      eprint={2007.00717},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{sun2019generalized,
  title={Generalized self-concordant functions: a recipe for Newton-type methods},
  author={Sun, Tianxiao and Tran-Dinh, Quoc},
  year={2019},
  journal={Mathematical Programming},
  volume={178},
  number={1},
  pages={145--213},
  publisher={Springer}
}
@misc{sun2018markov,
      title={On Markov Chain Gradient Descent},
      author={Tao Sun and Yuejiao Sun and Wotao Yin},
      year={2018},
      eprint={1809.04216},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}
@article{simonetto2017prediction,
   author={A. {Simonetto} and E. {Dall’Anese}},
   journal={IEEE Transactions on Signal Processing},
   title={Prediction-Correction Algorithms for Time-Varying Constrained Optimization},
   year={2017},
   volume={65},
   number={20},
   pages={5481-5494},
   doi={10.1109/TSP.2017.2728498}
}
@article{scherrer2016improved,
author = {Scherrer, Bruno},
title = {Improved and Generalized Upper Bounds on the Complexity of Policy Iteration},
journal = {Mathematics of Operations Research},
volume = {41},
number = {3},
pages = {758-774},
year = {2016},
doi = {10.1287/moor.2015.0753},
eprint = {https://doi.org/10.1287/moor.2015.0753},
abstract = { Given a Markov decision process (MDP) with n states and a total
            number m of actions, we study the number of iterations needed
            by policy iteration (PI) algorithms to converge to the optimal
            γ-discounted policy. We consider two variations of PI:
            Howard’s PI that changes the actions in all states with a
            positive advantage, and Simplex-PI that only changes the
            action in the state with maximal advantage. We show that
            Howard’s PI terminates after at most O((m/(1 − γ))log(1/(1 −
            γ))) iterations, improving by a factor O(log n) a result by
            Hansen et al. [Hansen TD, Miltersen PB, Zwick U (2013)
            Strategy iteration is strongly polynomial for two-player
            turn-based stochastic games with a constant discount factor.
            J. ACM 60(1):1:1–1:16.], whereas Simplex-PI terminates after
            at most O((n m/(1 − γ))log(1/(1 − γ))) iterations, improving
            by a factor O(log n) a result by Ye [Ye Y (2011) The simplex
            and policy-iteration methods are strongly polynomial for the
            Markov decision problem with a fixed discount rate. Math.
            Oper. Res. 36(4):593–603.]. Under some structural properties
            of the MDP, we then consider bounds that are independent of
            the discount factor γ: quantities of interest are bounds τt
            and τr—uniform on all states and policies—respectively, on the
            expected time spent in transient states and the inverse of the
            frequency of visits in recurrent states given that the process
            starts from the uniform distribution. Indeed, we show that
            Simplex-PI terminates after at most Õ(n3m2τtτr) iterations.
            This extends a recent result for deterministic MDPs by Post
            and Ye [Post I, Ye Y (2013) The simplex method is strongly
            polynomial for deterministic Markov decision processes. Khanna
            S, ed. Proc. 24th ACM-SIAM Sympos. Discrete Algorithms, SODA
            '13 (SIAM, Philadelphia), 1465–1473.] in which τt ≤ 1 and τr ≤
            n; in particular it shows that Simplex-PI is strongly
            polynomial for a much larger class of MDPs. We explain why
            similar results seem hard to derive for Howard’s PI. Finally,
            under the additional (restrictive) assumption that the state
            space is partitioned in two sets, respectively, states that
            are transient and recurrent for all policies, we show that
            both Howard’s PI and Simplex-PI terminate after at most
            Õ(m(n2τt + nτr)) iterations. }
}
@article{simonetto2016aclass,
   author={A. {Simonetto} and A. {Mokhtari} and A. {Koppel} and G. {Leus} and A. {Ribeiro}},
   journal={IEEE Transactions on Signal Processing},
   title={A Class of Prediction-Correction Methods for Time-Varying Convex Optimization},
   year={2016},
   volume={64},
   number={17},
   pages={4576-4591},
   doi={10.1109/TSP.2016.2568161}
}
@misc{schmidt2014convergence,
  title={Convergence rate of stochastic gradient with constant step size},
  author={Schmidt, Mark},
  year={2014},
  DOI={http://dx.doi.org/10.14288/1.0050992},
  month={Sep},
  collection={Faculty Research and Publications},
  series={Faculty Research and Publications},
}
@InProceedings{silver2013gradient,
  title = 	 {Gradient Temporal Difference Networks},
  author = 	 {David Silver},
  booktitle = 	 {Proceedings of the Tenth European Workshop on Reinforcement Learning},
  pages = 	 {117--130},
  year = 	 {2013},
  editor = 	 {Marc Peter Deisenroth and Csaba Szepesvári and Jan Peters},
  volume = 	 {24},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {30 Jun--01 Jul},
  publisher =    {PMLR},
  abstract = 	 {Temporal-difference (TD) networks (Sutton and Tanner, 2004)
                  are a predictive represen- tation of state in which each node
                  is an answer to a question about future observations or
                  questions. Unfortunately, existing algorithms for learning TD
                  networks are known to diverge, even in very simple problems.
                  In this paper we present the first sound learning rule for TD
                  networks. Our approach is to develop a true gradient descent
                  algorithm that takes account of all three roles performed by
                  each node in the network: as state, as an answer, and as a
                  target for other questions. Our algorithm combines gradient
                  temporal-difference learning (Maei et al., 2009) with
                  real-time recurrent learning (Williams and Zipser, 1994). We
                  provide a generalisation of the Bellman equation that
                  corresponds to the semantics of the TD network, and prove that
                  our algorithm converges to a fixed point of this equation.}
}
@InProceedings{saha2011improved,
  title = 	 {Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit Feedback},
  author = 	 {Saha, Ankan and Tewari, Ambuj},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {636--642},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
}
@article{srebro2010smoothness,
  title={Smoothness, low noise and fast rates},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  year={2010},
  journal={Advances in neural information processing systems},
  volume={23},
}
@inproceedings{sutton2009fast,
author = {Sutton, Richard S. and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv\'{a}ri, Csaba and Wiewiora, Eric},
title = {Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1553374.1553501},
abstract = {Sutton, Szepesv\'{a}ri and Maei (2009) recently introduced the first
                  temporal-difference learning algorithm compatible with both
                  linear function approximation and off-policy training, and
                  whose complexity scales only linearly in the size of the
                  function approximator. Although their gradient temporal
                  difference (GTD) algorithm converges reliably, it can be very
                  slow compared to conventional linear TD (on on-policy problems
                  where TD is convergent), calling into question its practical
                  utility. In this paper we introduce two new related algorithms
                  with better convergence rates. The first algorithm, GTD2, is
                  derived and proved convergent just as GTD was, but uses a
                  different objective function and converges significantly
                  faster (but still not as fast as conventional TD). The second
                  new algorithm, linear TD with gradient correction, or TDC,
                  uses the same update rule as conventional TD except for an
                  additional term which is initially zero. In our experiments on
                  small test problems and in a Computer Go application with a
                  million features, the learning rate of this algorithm was
                  comparable to that of conventional TD. This algorithm appears
                  to extend linear TD to off-policy learning with no penalty in
                  performance while only doubling computational requirements.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {993–1000},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}
% =========
% --- T ---
% =========
@misc{tran2021correcting,
      title={Correcting Momentum with Second-order Information},
      author={Hoang Tran and Ashok Cutkosky},
      year={2021},
      eprint={2103.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@ARTICLE{tau2020primal,
  author={Tao, Wei and Pan, Zhisong and Wu, Gaowei and Tao, Qing},
  journal={IEEE Transactions on Cybernetics},
  title={Primal Averaging: A New Gradient Evaluation Step to Attain the Optimal Individual Convergence},
  year={2020},
  volume={50},
  number={2},
  pages={835-845},
  doi={10.1109/TCYB.2018.2874332}
}
@article{tran2019convergence,
  title={On the Convergence Proof of AMSGrad and a New Version},
  author={Tran, Phuong Thi and Phong, Le Trieu},
  year={2019},
  journal={IEEE Access},
  volume={7},
  number={},
  pages={61706-61716},
  doi={10.1109/ACCESS.2019.2916341}
}
@article{taylor2017smooth,
  title={Smooth strongly convex interpolation and exact worst-case performance of first-order methods},
  author={Taylor, Adrien B and Hendrickx, Julien M and Glineur, Fran{\c{c}}ois},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={307--345},
  year={2017},
  publisher={Springer}
}
@article{taylor2019stochastic,
  title={Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions},
  author={Taylor, Adrien and Bach, Francis},
  journal={arXiv preprint arXiv:1902.00947},
  year={2019}
}
@incollection{tran2015composite,
  title={Composite convex minimization involving self-concordant-like cost functions},
  author={Tran-Dinh, Quoc and Li, Yen-Huan and Cevher, Volkan},
  booktitle={Modelling, Computation and Optimization in Information Systems and Management Sciences},
  pages={155--168},
  year={2015},
  publisher={Springer}
}
@article{teboulle1992entropic,
author = {Teboulle, Marc},
title = {Entropic Proximal Mappings with Applications to Nonlinear Programming},
journal = {Mathematics of Operations Research},
volume = {17},
number = {3},
pages = {670-690},
year = {1992},
doi = {10.1287/moor.17.3.670},
URL = {https://doi.org/10.1287/moor.17.3.670},
eprint = {https://doi.org/10.1287/moor.17.3.670},
abstract = { We introduce a family of new transforms based on imitating the
             proximal mapping of Moreau and the associated Moreau-Yosida
             proximal approximation of a function. The transforms are
             constructed in terms of the φ-divergence functional (a
             generalization of the relative entropy) and of Bregman's
             measure of distance. An analogue of Moreau's theorem
             associated with these entropy-like distances is proved. We
             show that the resulting Entropic Proximal Maps share
             properties similar to the proximal mapping and provide a
             fairly general framework for constructing approximation and
             smoothing schemes for optimization problems. Applications of
             the results to the construction of generalized augmented
             Lagrangians for nonlinear programs and the minimax problem are
             presented. }
}
% =========
% --- v ---
% =========
@article{van2020comparator,
  title     = {Comparator-adaptive Convex Bandits},
  author    = {Dirk van der Hoeven and
               Ashok Cutkosky and
               Haipeng Luo},
  journal   = {CoRR},
  volume    = {abs/2007.08448},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2007.08448},
  timestamp = {Wed, 22 Jul 2020 12:09:15 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{van2021distributed,
  title={Distributed Online Learning for Joint Regret with Communication Constraints},
  author={van der Hoeven, Dirk and Hadiji, H{\'e}di and van Erven, Tim},
  journal={arXiv preprint arXiv:2102.07521},
  year={2021}
}
@inproceedings{van2019user,
  title={User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning},
  author={van der Hoeven, Dirk},
  booktitle={NeurIPS},
  pages={14080--14089},
  year={2019}
}
@article{vanerven2016metagrad,
  author    = {Tim van Erven and
               Wouter M. Koolen},
  title     = {MetaGrad: Multiple Learning Rates in Online Learning},
  journal   = {CoRR},
  volume    = {abs/1604.08740},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1604.08740},
  timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% =========
% --- W ---
% =========
@misc{wagenmaker2021regret,
      title={Beyond No Regret: Instance-Dependent PAC Reinforcement Learning},
      author={Andrew Wagenmaker and Max Simchowitz and Kevin Jamieson},
      year={2021},
      eprint={2108.02717},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{waiss2021lastiterate,
  title = 	 {The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities},
  author =       {Azizian, Wa\"iss and Iutzeler, Franck and Malick, J\'er\^ome and Mertikopoulos, Panayotis},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {326--358},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  abstract = 	 {In this paper, we analyze the local convergence rate of
                  optimistic mirror descent methods in stochastic variational
                  inequalities, a class of optimization problems with important
                  applications to learning theory and machine learning. Our
                  analysis reveals an intricate relation between the algorithm’s
                  rate of convergence and the local geometry induced by the
                  method’s underlying Bregman function. We quantify this
                  relation by means of the Legendre exponent, a notion that we
                  introduce to measure the growth rate of the Bregman divergence
                  relative to the ambient norm near a solution. We show that
                  this exponent determines both the optimal step-size policy of
                  the algorithm and the optimal rates attained, explaining in
                  this way the differences observed for some popular Bregman
                  functions (Euclidean projection, negative entropy, fractional
                  power, etc.).}
}
@misc{wang2021noregret,
      title={No-Regret Dynamics in the Fenchel Game: A Unified Framework for Algorithmic Convex Optimization},
      author={Jun-Kun Wang and Jacob Abernethy and Kfir Y. Levy},
      year={2021},
      eprint={2111.11309},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@phdthesis{wang2021understanding,
  title={Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum},
  author={Wang, Jun-Kun},
  year={2021},
  school={Georgia Institute of Technology}
}
@misc{weisz2021queryefficient,
      title={On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function},
      author={Gellert Weisz and Philip Amortila and Barnabás Janzer and Yasin Abbasi-Yadkori and Nan Jiang and Csaba Szepesvári},
      year={2021},
      eprint={2102.02049},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{wang2020finite,
  title = 	 {Finite-sample Analysis of Greedy-GQ with Linear Function Approximation under Markovian Noise},
  author =       {Wang, Yue and Zou, Shaofeng},
  booktitle = 	 {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {11--20},
  year = 	 {2020},
  editor = 	 {Jonas Peters and David Sontag},
  volume = 	 {124},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--06 Aug},
  publisher =    {PMLR},
  abstract = {Greedy-GQ is an off-policy two timescale algorithm for optimal
                  control in reinforcement learning. This paper develops the
                  first finite-sample analysis for the Greedy-GQ algorithm with
                  linear function approximation under Markovian noise. Our
                  finite-sample analysis provides theoretical justification for
                  choosing stepsizes for this two timescale algorithm for faster
                  convergence in practice, and suggests a trade-off between the
                  convergence rate and the quality of the obtained policy. Our
                  paper extends the finite-sample analyses of two timescale
                  reinforcement learning algorithms from policy evaluation to
                  optimal control, which is of more practical interest.
                  Specifically, in contrast to existing finite-sample analyses
                  for two timescale methods, e.g., GTD, GTD2 and TDC, where
                  their objective functions are convex, the objective function
                  of the Greedy-GQ algorithm is non-convex. Moreover, the
                  Greedy-GQ algorithm is also not a linear two-timescale
                  stochastic approximation algorithm. Our techniques in this
                  paper provide a general framework for finite-sample analysis
                  of non-convex value-based reinforcement learning algorithms
                  for optimal control.}
}
@ARTICLE{wilson2019adaptive,
  title={Adaptive Sequential Stochastic Optimization},
  author={Wilson, Craig and Veeravalli, Venugopal V. and Nedić, Angelia},
  journal={IEEE Transactions on Automatic Control},
  year={2019},
  volume={64},
  number={2},
  pages={496-509},
  doi={10.1109/TAC.2018.2816168}
}
@article{wibisono2016variational,
  title={A variational perspective on accelerated methods in optimization},
  author={Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal={proceedings of the National Academy of Sciences},
  volume={113},
  number={47},
  pages={E7351--E7358},
  year={2016},
  publisher={National Acad Sciences}
}
@article{wilson2016lyapunov,
  title={A lyapunov analysis of momentum methods in optimization},
  author={Wilson, Ashia C and Recht, Benjamin and Jordan, Michael I},
  year={2016},
  journal={arXiv preprint arXiv:1611.02635},
}
@INPROCEEDINGS{whiteson2011protecting,
  title={Protecting against evaluation overfitting in empirical reinforcement learning},
  author={S. {Whiteson} and B. {Tanner} and M. E. {Taylor} and P. {Stone}},
  booktitle={2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  year={2011},
  volume={},
  number={},
  pages={120-127},
  abstract={Empirical evaluations play an important role in machine learning.
                  However, the usefulness of any evaluation depends on the
                  empirical methodology employed. Designing good empirical
                  methodologies is difficult in part because agents can overfit
                  test evaluations and thereby obtain misleadingly high scores.
                  We argue that reinforcement learning is particularly
                  vulnerable to environment overfitting and propose as a remedy
                  generalized methodologies, in which evaluations are based on
                  multiple environments sampled from a distribution. In
                  addition, we consider how to summarize performance when scores
                  from different environments may not have commensurate values.
                  Finally, we present proof-of-concept results demonstrating how
                  these methodologies can validate an intuitively useful
                  range-adaptive tile coding method.},
  doi={10.1109/ADPRL.2011.5967363},
  ISSN={2325-1867},
  month={April}
}
% ==========
% --- X ---
% ==========
@article{xiao2021optimality,
  title={On the Optimality of Batch Policy Optimization Algorithms},
  author={Xiao, Chenjun and Wu, Yifan and Lattimore, Tor and Dai, Bo and Mei, Jincheng and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2104.02293},
  year={2021}
}
@misc{xiong2020nonasymptotic,
      title={Non-asymptotic Convergence of Adam-type Reinforcement Learning Algorithms under Markovian Sampling},
      author={Huaqing Xiong and Tengyu Xu and Yingbin Liang and Wei Zhang},
      year={2020},
      eprint={2002.06286},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{xu2019twotimescale,
  author    = {Tengyu Xu and
               Shaofeng Zou and
               Yingbin Liang},
  title     = {Two Time-scale Off-Policy {TD} Learning: Non-asymptotic Analysis over Markovian Samples},
  journal   = {CoRR},
  volume    = {abs/1909.11907},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1909.11907},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% =========
% --- Y ---
% =========
@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}
@InProceedings{yaqi2020minimax,
  title = 	 {Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation},
  author =       {Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2701--2709},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = {This paper studies the statistical theory of off-policy evaluation
                  with function approximation in batch data reinforcement
                  learning problem. We consider a regression-based fitted
                  Q-iteration method, show that it is equivalent to a
                  model-based method that estimates a conditional mean embedding
                  of the transition operator, and prove that this method is
                  information-theoretically optimal and has nearly minimal
                  estimation error. In particular, by leveraging contraction
                  property of Markov processes and martingale concentration, we
                  establish a finite-sample instance-dependent error upper bound
                  and a nearly-matching minimax lower bound. The policy
                  evaluation error depends sharply on a restricted
                  $\chi^2$-divergence over the function class between the
                  long-term distribution of target policy and the distribution
                  of past data. This restricted $\chi^2$-divergence
                  characterizes the statistical limit of off-policy evaluation
                  and is both instance-dependent and function-class-dependent.
                  Further, we provide an easily computable confidence bound for
                  the policy evaluator, which may be useful for optimistic
                  planning and safe policy improvement.}
}
@article{yuan2019trading,
  author    = {Jianjun Yuan and
               Andrew G. Lamperski},
  title     = {Trading-Off Static and Dynamic Regret in Online Least-Squares and
               Beyond},
  journal   = {CoRR},
  volume    = {abs/1909.03118},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1909.03118},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% =========
% --- Z ---
% =========
@misc{zhang2022pdebased,
      title={PDE-Based Optimal Strategy for Unconstrained Online Learning},
      author={Zhiyu Zhang and Ashok Cutkosky and Ioannis Paschalidis},
      year={2022},
      eprint={2201.07877},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zaki2021improper,
      title={Improper Learning with Gradient-based Policy Optimization},
      author={Mohammadi Zaki and Avinash Mohan and Aditya Gopalan and Shie Mannor},
      year={2021},
      eprint={2102.08201},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zanette2021cautiously,
      title={Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation},
      author={Andrea Zanette and Ching-An Cheng and Alekh Agarwal},
      year={2021},
      eprint={2103.12923},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zanette2021exponential,
      title={Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL},
      author={Andrea Zanette},
      year={2021},
      eprint={2012.08005},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{zhang2021dual,
title={Dual Adaptivity: A Universal Algorithm for Minimizing the Adaptive Regret of Convex Functions},
author={Lijun Zhang and Guanghui Wang and Wei-Wei Tu and Wei Jiang and Zhi-hua Zhou},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
}
@misc{zhang2021strongly,
      title={Strongly Adaptive OCO with Memory},
      author={Zhiyu Zhang and Ashok Cutkosky and Ioannis Ch. Paschalidis},
      year={2021},
      eprint={2102.01623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{zanette2020learning,
  title = 	 {Learning Near Optimal Policies with Low Inherent {B}ellman Error},
  author =       {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10978--10989},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = 	 {We study the exploration problem with approximate linear
                  action-value functions in episodic reinforcement learning
                  under the notion of low inherent Bellman error, a condition
                  normally employed to show convergence of approximate value
                  iteration. First we relate this condition to other common
                  frameworks and show that it is strictly more general than the
                  low rank (or linear) MDP assumption of prior work. Second we
                  provide an algorithm with a high probability regret bound
                  $\widetilde O(\sum_{t=1}^H d_t \sqrt{K} + \sum_{t=1}^H
                  \sqrt{d_t} \IBE K)$ where $H$ is the horizon, $K$ is the
                  number of episodes, $\IBE$ is the value if the inherent
                  Bellman error and $d_t$ is the feature dimension at timestep
                  $t$. In addition, we show that the result is unimprovable
                  beyond constants and logs by showing a matching lower bound.
                  This has two important consequences: 1) it shows that
                  exploration is possible using only \emph{batch assumptions}
                  with an algorithm that achieves the optimal statistical rate
                  for the setting we consider, which is more general than prior
                  work on low-rank MDPs 2) the lack of closedness (measured by
                  the inherent Bellman error) is only amplified by $\sqrt{d_t}$
                  despite working in the online setting. Finally, the algorithm
                  reduces to the celebrated \textsc{LinUCB} when $H=1$ but with
                  a different choice of the exploration parameter that allows
                  handling misspecified contextual linear bandits. While
                  computational tractability questions remain open for the MDP
                  setting, this enriches the class of MDPs with a linear
                  representation for the action-value function where
                  statistically efficient reinforcement learning is possible.}
}
@misc{zhang2020stochastic,
      title={Stochastic Optimization with Non-stationary Noise},
      author={Jingzhao Zhang and Hongzhou Lin and Subhro Das and Suvrit Sra and Ali Jadbabaie},
      year={2020},
      eprint={2006.04429},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@InProceedings{zhang2020wasserstein,
  title = 	 {Wasserstein Control of Mirror Langevin Monte Carlo},
  author =       {Zhang, Kelvin Shuangjian and Peyr\'e, Gabriel and Fadili, Jalal and Pereyra, Marcelo},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3814--3841},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  abstract = 	 { Discretized Langevin diffusions are efficient Monte Carlo
                  methods for sampling from high dimensional target densities
                  that are log-Lipschitz-smooth and (strongly) log-concave. In
                  particular, the Euclidean Langevin Monte Carlo sampling
                  algorithm has received much attention lately, leading to a
                  detailed understanding of its non-asymptotic convergence
                  properties and of the role that smoothness and log-concavity
                  play in the convergence rate. Distributions that do not
                  possess these regularity properties can be addressed by
                  considering a Riemannian Langevin diffusion with a metric
                  capturing the local geometry of the log-density. However, the
                  Monte Carlo algorithms derived from discretizations of such
                  Riemannian Langevin diffusions are notoriously difficult to
                  analyze. In this paper, we consider Langevin diffusions on a
                  Hessian-type manifold and study a discretization that is
                  closely related to the mirror-descent scheme. We establish for
                  the first time a non-asymptotic upper-bound on the sampling
                  error of the resulting Hessian Riemannian Langevin Monte Carlo
                  algorithm. This bound is measured according to a Wasserstein
                  distance induced by a Riemannian metric ground cost capturing
                  the squared Hessian structure and closely related to a
                  self-concordance-like condition. The upper-bound implies, for
                  instance, that the iterates contract toward a Wasserstein ball
                  around the target density whose radius is made explicit. Our
                  theory recovers existing Euclidean results and can cope with a
                  wide variety of Hessian metrics related to highly non-flat
                  geometries. }
}
@inproceedings{zhao2020dynamic,
 author = {Zhao, Peng and Zhang, Yu-Jie and Zhang, Lijun and Zhou, Zhi-Hua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12510--12520},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic Regret of Convex and Smooth Functions},
 volume = {33},
 year = {2020}
}
@inproceedings{zhou2020regret,
 author = {Zhou, Yihan and Sanches Portella, Victor and Schmidt, Mark and Harvey, Nicholas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15823--15833},
 publisher = {Curran Associates, Inc.},
 title = {Regret Bounds without Lipschitz Continuity: Online Learning with Relative-Lipschitz Losses},
 volume = {33},
 year = {2020}
}
@InProceedings{zhuang2019surrogate,
  title = 	 {Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization},
  author =       {Zhuang, Zhenxun and Cutkosky, Ashok and Orabona, Francesco},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7664--7672},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {Stochastic Gradient Descent (SGD) has played a central role in
                  machine learning. However, it requires a carefully hand-picked
                  stepsize for fast convergence, which is notoriously tedious
                  and time-consuming to tune. Over the last several years, a
                  plethora of adaptive gradient-based algorithms have emerged to
                  ameliorate this problem. In this paper, we propose new
                  surrogate losses to cast the problem of learning the optimal
                  stepsizes for the stochastic optimization of a non-convex
                  smooth objective function onto an online convex optimization
                  problem. This allows the use of no-regret online algorithms to
                  compute optimal stepsizes on the fly. In turn, this results in
                  a SGD algorithm with self-tuned stepsizes that guarantees
                  convergence rates that are automatically adaptive to the level
                  of noise.}
}
@inproceedings{zhang2018adaptive,
  title={Adaptive online learning in dynamic environments},
  author={Zhang, Lijun and Lu, Shiyin and Zhou, Zhi-Hua},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={1330--1340},
  year={2018}
}
@misc{zhang2018direct,
      title={Direct Runge-Kutta Discretization Achieves Acceleration},
      author={Jingzhao Zhang and Aryan Mokhtari and Suvrit Sra and Ali Jadbabaie},
      year={2018},
      eprint={1805.00521},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@InProceedings{zhang2018dynamic,
  title = 	 {Dynamic Regret of Strongly Adaptive Methods},
  author =       {Zhang, Lijun and Yang, Tianbao and rong jin and Zhou, Zhi-Hua},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5882--5891},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  abstract = 	 {To cope with changing environments, recent developments in
                  online learning have introduced the concepts of adaptive
                  regret and dynamic regret independently. In this paper, we
                  illustrate an intrinsic connection between these two concepts
                  by showing that the dynamic regret can be expressed in terms
                  of the adaptive regret and the functional variation. This
                  observation implies that strongly adaptive algorithms can be
                  directly leveraged to minimize the dynamic regret. As a
                  result, we present a series of strongly adaptive algorithms
                  that have small dynamic regrets for convex functions,
                  exponentially concave functions, and strongly convex
                  functions, respectively. To the best of our knowledge, this is
                  the first time that exponential concavity is utilized to upper
                  bound the dynamic regret. Moreover, all of those adaptive
                  algorithms do not need any prior knowledge of the functional
                  variation, which is a significant advantage over previous
                  specialized methods for minimizing dynamic regret.}
}
@article{zhou2018fenchel,
  title={On the fenchel duality between strong convexity and lipschitz continuous gradient},
  author={Zhou, Xingyu},
  journal={arXiv preprint arXiv:1803.06573},
  year={2018}
}
@inproceedings{zhang2017improved,
 title = {Improved Dynamic Regret for Non-degenerate Functions},
 author = {Zhang, Lijun and Yang, Tianbao and Yi, Jinfeng and Jin, Rong and Zhou, Zhi-Hua},
 year = {2017},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 publisher = {Curran Associates, Inc.},
 volume = {30},
}
@article{zimmert2021tsallis,
  title   = {Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits},
  author  = {Julian Zimmert and Yevgeny Seldin},
  year    = {2021},
  journal = {Journal of Machine Learning Research},
  volume  = {22},
  number  = {28},
  pages   = {1-49},
}

@inproceedings{duchi10adagrad,
  author = {J. Duchi and E. Hazan and Y. Singer},
  booktitle = {Conference on Learning Theory},
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  year = {2010},
}

@inproceedings{mcmahan2010adaptive,
title = {Adaptive Bound Optimization for Online Convex Optimization},
author  = {H. Brendan McMahan and Matthew Streeter},
year  = 2010,
booktitle = {Conference on Learning Theory}
}
@incollection{foster2015adaptive,
title = {Adaptive Online Learning},
author = {Foster, Dylan J and Rakhlin, Alexander and Sridharan, Karthik},
booktitle = {Advances in Neural Information Processing Systems 28},
year = {2015},
}
@inproceedings{hazan2007adaptive,
 author = {Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Online Gradient Descent},
 volume = {20},
 year = {2007}
}

@phdthesis{shalev07online,
  author = {S. Shalev-Shwartz},
  school = {The Hebrew University of Jerusalem},
  title = {Online Learning: Theory, Algorithms, and Applications},
  year = {2007},
}

@article{shalev2011online,
  title={Online learning and online convex optimization},
  author={Shalev-Shwartz, Shai},
  journal={Foundations and Trends in Machine Learning},
  volume={4},
  number={2},
  year={2011}
}
@misc{mayo2022scalefree,
  title = {Scale-free Unconstrained Online Learning for Curved Losses},
  author = {Jack J. Mayo and Hédi Hadiji and Tim van Erven},
  year = {2022},
  eprint = {2202.05630},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
}
@inproceedings{zinkevich2003online,
  title={Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
  author={Zinkevich, Martin},
  booktitle={International Conference on Machine Learning},
  year={2003}
}
@misc{luo2022corralling,
  title = {Corralling a Larger Band of Bandits: A Case Study on Switching Regret for Linear Bandits},
  author = {Luo, Haipeng and Zhang, Mengxiao and Zhao, Peng and Zhou, Zhi-Hua},
  year = {2022},
  doi = {10.48550/ARXIV.2202.06151},
}
@article{cesa1996worst,
  title = {Worst-case quadratic loss bounds for prediction using linear functions and gradient descent},
  author = {Cesa-Bianchi, Nicolo and Long, Philip M and Warmuth, Manfred K},
  year = {1996},
  journal = {IEEE Transactions on Neural Networks},
  volume = {7},
  number = {3},
  pages = {604--619},
  publisher = {IEEE},
}
@article{kivinen1997exponentiated,
  title={Exponentiated gradient versus gradient descent for linear predictors},
  author={Kivinen, Jyrki and Warmuth, Manfred K},
  journal={information and computation},
  volume={132},
  number={1},
  pages={1--63},
  year={1997},
  publisher={Elsevier}
}
@misc{telgarsky2022stochastic,
  title = {Stochastic linear optimization never overfits with quadratically-bounded losses on general data},
  author = {Telgarsky, Matus},
  year = {2022},
  doi = {10.48550/ARXIV.2202.06915},
}
@misc{hall2016online,
  title = {Online Optimization in Dynamic Environments},
  author = {Eric C. Hall and Rebecca M. Willett},
  year = {2016},
  eprint = {1307.5944},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML},
}

@InProceedings{ibrahim2020linear,
  title = 	 {Linear Lower Bounds and Conditioning of Differentiable Games},
  author =       {Ibrahim, Adam and Azizian, Wa\"{\i}ss and Gidel, Gauthier and Mitliagkas, Ioannis},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4583--4593},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  abstract = 	 {Recent successes of game-theoretic formulations in ML have
                  caused a resurgence of research interest in differentiable
                  games. Overwhelmingly, that research focuses on methods and
                  upper bounds on their speed of convergence. In this work, we
                  approach the question of fundamental iteration complexity by
                  providing lower bounds to complement the linear (i.e.
                  geometric) upper bounds observed in the literature on a wide
                  class of problems. We cast saddle-point and min-max problems
                  as 2-player games. We leverage tools from single-objective
                  convex optimisation to propose new linear lower bounds for
                  convex-concave games. Notably, we give a linear lower bound
                  for $n$-player differentiable games, by using the spectral
                  properties of the update operator. We then propose a new
                  definition of the condition number arising from our lower
                  bound analysis. Unlike past definitions, our condition number
                  captures the fact that linear rates are possible in games,
                  even in the absence of strong convexity or strong concavity in
                  the variables.}
}
@inproceedings{azizian2020accelerating,
  title = {Accelerating Smooth Games by Manipulating Spectral Shapes},
  author = {Azizian, Wa\"iss and Scieur, Damien and Mitliagkas, Ioannis and Lacoste-Julien, Simon and Gidel, Gauthier},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = {1705--1715},
  year = {2020},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  volume = {108},
  series = {Proceedings of Machine Learning Research},
  month = {26--28 Aug},
  publisher = {PMLR},
  abstract = {We use matrix iteration theory to characterize acceleration in
                  smooth games. We define the spectral shape of a family of
                  games as the set containing all eigenvalues of the Jacobians
                  of standard gradient dynamics in the family. Shapes restricted
                  to the real line represent well-understood classes of
                  problems, like minimization. Shapes spanning the complex plane
                  capture the added numerical challenges in solving smooth
                  games. In this framework, we describe gradient-based methods,
                  such as extragradient, as transformations on the spectral
                  shape. Using this perspective, we propose an optimal algorithm
                  for bilinear games. For smooth and strongly monotone
                  operators, we identify a continuum between convex
                  minimization, where acceleration is possible using Polyak’s
                  momentum, and the worst case where gradient descent is
                  optimal. Finally, going beyond first-order methods, we propose
                  an accelerated version of consensus optimization. },
}
