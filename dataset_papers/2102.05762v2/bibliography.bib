@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{nilim2005robust,
  title={Robust control of {Markov} decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}

@article{ruszczynski2010risk,
	title={Risk-averse dynamic programming for {Markov} decision processes},
	author={Ruszczy{\'n}ski, Andrzej},
	journal={Mathematical programming},
	volume={125},
	number={2},
	pages={235--261},
	year={2010},
	publisher={Springer}
}

@article{micchelli2006universal,
	title={Universal Kernels.},
	author={Micchelli, Charles A and Xu, Yuesheng and Zhang, Haizhang},
	journal={Journal of Machine Learning Research},
	volume={7},
	number={12},
	year={2006}
}

@inproceedings{srinivas2010gaussian,
	author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
	title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
	year = {2010},
	isbn = {9781605589077},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	pages = {1015â€“1022},
	numpages = {8},
	location = {Haifa, Israel},
	series = {ICML'10}
}

@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@article{rigter2021lexicographic,
	title={Lexicographic Optimisation of Conditional Value at Risk and Expected Value for Risk-Averse Planning in {MDPs}}, 
	author={Marc Rigter and Paul Duckworth and Bruno Lacerda and Nick Hawes},
	year={2021},
	journal={arXiv preprint arXiv:2110.12746},
	primaryClass={cs.AI}
}

@article{clements2019estimating,
	title={Estimating risk and uncertainty in deep reinforcement learning},
	author={Clements, William R and Van Delft, Bastien and Robaglia, Beno{\^\i}t-Marie and Slaoui, Reda Bahi and Toth, S{\'e}bastien},
	journal={arXiv preprint arXiv:1905.09638},
	year={2019}
}

@article{mnih2015human,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}

@article{eriksson2021sentinel,
	title={SENTINEL: Taming Uncertainty with Ensemble-based Distributional Reinforcement Learning},
	author={Eriksson, Hannes and Basu, Debabrota and Alibeigi, Mina and Dimitrakakis, Christos},
	journal={arXiv preprint arXiv:2102.11075},
	year={2021}
}

@inproceedings{auger2013continuous,
	title={Continuous upper confidence trees with polynomial exploration--consistency},
	author={Auger, David and Couetoux, Adrien and Teytaud, Olivier},
	booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages={194--209},
	year={2013},
	organization={Springer}
}

@book{shapiro2014lectures,
	title={Lectures on stochastic programming: modeling and theory},
	author={Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'n}ski, Andrzej},
	year={2014},
	publisher={SIAM}
}

@inproceedings{couetoux2011continuous,
	title={Continuous upper confidence trees},
	author={Cou{\"e}toux, Adrien and Hoock, Jean-Baptiste and Sokolovska, Nataliya and Teytaud, Olivier and Bonnard, Nicolas},
	booktitle={International Conference on Learning and Intelligent Optimization},
	pages={433--445},
	year={2011},
	organization={Springer}
}

@article{guez2012efficient,
	title={Efficient {Bayes}-adaptive reinforcement learning using sample-based search},
	author={Guez, Arthur and Silver, David and Dayan, Peter},
	journal={Advances in neural information processing systems},
	volume={25},
	pages={1025--1033},
	year={2012}
}

@article{mern2020bayesian,
	title={Bayesian Optimized {Monte Carlo} Planning},
	author={Mern, John and Yildiz, Anil and Sunberg, Zachary and Mukerji, Tapan and Kochenderfer, Mykel J},
	journal={arXiv preprint arXiv:2010.03597},
	year={2020}
}

@article{morere2016bayesian,
	title={Bayesian optimisation for solving continuous state-action-observation POMDPs},
	author={Morere, Philippe and Marchant, Roman and Ramos, Fabio},
	journal={Advances in Neural Information Processing Systems (NIPS)},
	year={2016}
}

@article{gelly2011monte,
	title={{Monte-Carlo} tree search and rapid action value estimation in computer {Go}},
	author={Gelly, Sylvain and Silver, David},
	journal={Artificial Intelligence},
	volume={175},
	number={11},
	pages={1856--1875},
	year={2011},
	publisher={Elsevier}
}

@article{rigter2020minimax, title={Minimax Regret Optimisation for Robust Planning in Uncertain {Markov} Decision Processes}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17417}, number={13}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Rigter, Marc and Lacerda, Bruno and Hawes, Nick}, year={2021}, month={May}, pages={11930-11938} }

@article{silver2016mastering,
	title={Mastering the game of {Go} with deep neural networks and tree search},
	author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	journal={nature},
	volume={529},
	number={7587},
	pages={484--489},
	year={2016},
	publisher={Nature Publishing Group}
}

@article{pflug2016time,
	title={Time-consistent decisions and temporal decomposition of coherent risk functionals},
	author={Pflug, Georg Ch and Pichler, Alois},
	journal={Mathematics of Operations Research},
	volume={41},
	number={2},
	pages={682--699},
	year={2016},
	publisher={INFORMS}
}

@inproceedings{lee2018monte,
	title={Monte-Carlo Tree Search for Constrained POMDPs.},
	author={Lee, Jongmin and Kim, Geon-Hyeong and Poupart, Pascal and Kim, Kee-Eung},
	booktitle={NeurIPS},
	pages={7934--7943},
	year={2018}
}

@inproceedings{wolff2012robust,
  title={Robust control of uncertain {Markov} decision processes with temporal logic specifications},
  author={Wolff, Eric M and Topcu, Ufuk and Murray, Richard M},
  booktitle={2012 IEEE 51st IEEE Conference on Decision and Control (CDC)},
  pages={3372--3379},
  year={2012},
  organization={IEEE}
}

@book{knight1921risk,
  title={Risk, uncertainty and profit},
  author={Knight, Frank Hyneman},
  volume={31},
  year={1921},
  publisher={Houghton Mifflin}
}

@inproceedings{xu2010distributionally,
  title={Distributionally robust {Markov} decision processes},
  author={Xu, Huan and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2505--2513},
  year={2010}
}

@inproceedings{guez2014bayes,
	title={Bayes-Adaptive Simulation-based Search with Value Function Approximation.},
	author={Guez, Arthur and Heess, Nicolas and Silver, David and Dayan, Peter},
	booktitle={NIPS},
	volume={27},
	pages={451--459},
	year={2014}
}

@inproceedings{sharma2019robust,
  title={Robust and adaptive planning under model uncertainty},
  author={Sharma, Apoorva and Harrison, James and Tsao, Matthew and Pavone, Marco},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={29},
  number={1},
  pages={410--418},
  year={2019}
}

@inproceedings{chen2012tractable,
  title={Tractable objectives for robust policy optimization},
  author={Chen, Katherine and Bowling, Michael},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2069--2077},
  year={2012}
}

@article{delage2010percentile,
  title={Percentile optimization for {Markov} decision processes with parameter uncertainty},
  author={Delage, Erick and Mannor, Shie},
  journal={Operations research},
  volume={58},
  number={1},
  pages={203--213},
  year={2010},
  publisher={INFORMS}
}

@article{howard1972risk,
  title={Risk-sensitive {Markov} decision processes},
  author={Howard, Ronald A and Matheson, James E},
  journal={Management science},
  volume={18},
  number={7},
  pages={356--369},
  year={1972},
  publisher={INFORMS}
}

@article{sobel1982variance,
  title={The variance of discounted {Markov} decision processes},
  author={Sobel, Matthew J},
  journal={Journal of Applied Probability},
  volume={19},
  number={4},
  pages={794--802},
  year={1982},
  publisher={Cambridge University Press}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@article{santana2016rao,
  title={{RAO*}: An algorithm for chance-constrained {POMDPâ€™s}},
  author={Santana, Pedro and Thi{\'e}baux, Sylvie and Williams, Brian},
  year={2016},
  publisher={Association for the Advancement of Artificial Intelligence}
}

@inproceedings{lee2017constrained,
  title={Constrained {Bayesian} Reinforcement Learning via Approximate Linear Programming.},
  author={Lee, Jongmin and Jang, Youngsoo and Poupart, Pascal and Kim, Kee-Eung},
  booktitle={IJCAI},
  pages={2088--2095},
  year={2017}
}

@article{bauerle2011markov,
  title={Markov decision processes with average-value-at-risk criteria},
  author={B{\"a}uerle, Nicole and Ott, Jonathan},
  journal={Mathematical Methods of Operations Research},
  volume={74},
  number={3},
  pages={361--379},
  year={2011},
  publisher={Springer}
}

@inproceedings{mannor2004bias,
	title={Bias and variance in value function estimation},
	author={Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
	booktitle={Proceedings of the twenty-first international conference on Machine learning},
	pages={72},
	year={2004}
}

@article{sonnenburg2010shogun,
	title={The {SHOGUN} machine learning toolbox},
	author={Sonnenburg, S{\"o}ren and R{\"a}tsch, Gunnar and Henschel, Sebastian and Widmer, Christian and Behr, Jonas and Zien, Alexander and Bona, Fabio de and Binder, Alexander and Gehl, Christian and Franc, Vojt{\v{e}}ch},
	journal={The Journal of Machine Learning Research},
	volume={11},
	pages={1799--1802},
	year={2010},
	publisher={JMLR. org}
}

@article{rockafellar2000optimization,
	title={Optimization of conditional value-at-risk},
	author={Rockafellar, R Tyrrell and Uryasev, Stanislav and others},
	journal={Journal of risk},
	volume={2},
	pages={21--42},
	year={2000}
}

@article{boda2006time,
	title={Time consistent dynamic risk measures},
	author={Boda, Kang and Filar, Jerzy A},
	journal={Mathematical Methods of Operations Research},
	volume={63},
	number={1},
	pages={169--186},
	year={2006},
	publisher={Springer}
}

@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a {CVaR} optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1522--1530},
  year={2015}
}

@inproceedings{chow2014algorithms,
  title={Algorithms for {CVaR} optimization in MDPs},
  author={Chow, Yinlam and Ghavamzadeh, Mohammad},
  booktitle={Advances in neural information processing systems},
  pages={3509--3517},
  year={2014}
}

@inproceedings{tamar2015policy,
  title={Policy gradient for coherent risk measures},
  author={Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1468--1476},
  year={2015}
}

@inproceedings{derman2020bayesian,
  title={A bayesian approach to robust reinforcement learning},
  author={Derman, Esther and Mankowitz, Daniel and Mann, Timothy and Mannor, Shie},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={648--658},
  year={2020},
  organization={PMLR}
}

@InProceedings{pinto17robust,
title = {Robust Adversarial Reinforcement Learning},
author = {Lerrel Pinto and James Davidson and Rahul Sukthankar and Abhinav Gupta},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
pages = {2817--2826},
year = {2017},
volume = {70}} 

@inproceedings{tamar2014optimizing,
author = {Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
title = {Optimizing the {CVaR} via Sampling},
year = {2015},
publisher = {AAAI Press},
abstract = {Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2993â€“2999},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{pan2019risk,
  title={Risk averse robust adversarial reinforcement learning},
  author={Pan, Xinlei and Seita, Daniel and Gao, Yang and Canny, John},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8522--8528},
  year={2019},
  organization={IEEE}
}

@article{ghavamzadeh2015bayesian,
	year = {2015},
	volume = {8},
	journal = {Foundations and TrendsÂ® in Machine Learning},
	title = {Bayesian Reinforcement Learning: A Survey},
	doi = {10.1561/2200000049},
	issn = {1935-8237},
	number = {5-6},
	pages = {359-483},
	author = {Mohammad Ghavamzadeh and Shie Mannor and Joelle Pineau and Aviv Tamar}
}

@book{martin1967bayesian,
	title={Bayesian decision problems and {Markov} chains},
	author={Martin, James John},
	year={1967},
	publisher={Wiley}
}

@article{chow2017risk,
  title={Risk-constrained reinforcement learning with percentile risk criteria},
  author={Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6070--6120},
  year={2017},
  publisher={JMLR. org}
}

@article{tang2019worst,
  title={Worst cases policy gradients},
  author={Tang, Yichuan Charlie and Zhang, Jian and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1911.03618},
  year={2019}
}

@article{tamar2016sequential,
  title={Sequential decision making with coherent risk},
  author={Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
  journal={IEEE Transactions on Automatic Control},
  volume={62},
  number={7},
  pages={3323--3338},
  year={2016},
  publisher={IEEE}
}


@inproceedings{o2018uncertainty,
  title={The uncertainty {Bellman} equation and exploration},
  author={Oâ€™Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={3836--3845},
  year={2018}
}

@article{artzner1999coherent,
  title={Coherent measures of risk},
  author={Artzner, Philippe and Delbaen, Freddy and Eber, Jean-Marc and Heath, David},
  journal={Mathematical finance},
  volume={9},
  number={3},
  pages={203--228},
  year={1999},
  publisher={Wiley Online Library}
}

@article{duff2003optimal,
  title={Optimal learning: Computational procedures for {Bayes-adaptive Markov} decision processes.},
  author={Duff, Michael O},
  year={2003}
}

@inproceedings{keramati2020being,
  title={Being optimistic to be conservative: Quickly learning a {CVaR} policy},
  author={Keramati, Ramtin and Dann, Christoph and Tamkin, Alex and Brunskill, Emma},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={4436--4443},
  year={2020}
}

@inproceedings{borkar2010risk,
  title={Risk-constrained {Markov} decision processes},
  author={Borkar, Vivek and Jain, Rahul},
  booktitle={49th IEEE Conference on Decision and Control (CDC)},
  pages={2664--2669},
  year={2010},
  organization={IEEE}
}

@inproceedings{prashanth2014policy,
  title={Policy gradients for {CVaR-constrained MDPs}},
  author={Prashanth, LA},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={155--169},
  year={2014},
  organization={Springer}
}

@inproceedings{petrik2012an,
author = {Petrik, Marek and Subramanian, Dharmashankar},
title = {An Approximate Solution Method for Large Risk-Averse {Markov} Decision Processes},
year = {2012},
isbn = {9780974903989},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Stochastic domains often involve risk-averse decision makers. While recent work has focused on how to model risk in Markov decision processes using risk measures, it has not addressed the problem of solving large risk-averse formulations. In this paper, we propose and analyze a new method for solving large risk-averse MDPs with hybrid continuous-discrete state spaces and continuous action spaces. The proposed method iteratively improves a bound on the value function using a linearity structure of the MDP. We demonstrate the utility and properties of the method on a portfolio optimization problem.},
booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
pages = {805â€“814},
numpages = {10},
location = {Catalina Island, CA},
series = {UAI'12}
}

@article{osogami2012robustness,
	title={Robustness and risk-sensitivity in {Markov} decision processes},
	author={Osogami, Takayuki},
	journal={Advances in Neural Information Processing Systems},
	volume={25},
	pages={233--241},
	year={2012}
}
