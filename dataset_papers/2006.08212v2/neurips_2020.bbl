\begin{thebibliography}{10}

\bibitem{aldous2013interacting}
D.~Aldous.
\newblock Interacting particle systems as stochastic social dynamics.
\newblock {\em Bernoulli}, 19(4):1122--1149, 2013.

\bibitem{aldous2012lecture}
D.~Aldous and D.~Lanoue.
\newblock A lecture on the averaging process.
\newblock {\em Probability Surveys}, 9:90--102, 2012.

\bibitem{avrachenkov2019eigenvalues}
K.~Avrachenkov, L.~Cottatellucci, and M.~Hamidouche.
\newblock Eigenvalues and spectral dimension of random geometric graphs in
  thermodynamic regime.
\newblock In {\em International Conference on Complex Networks and Their
  Applications}, pages 965--975. Springer, 2019.

\bibitem{bach2013non}
F.~Bach and E.~Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate ${O}(1/n)$.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  773--781, 2013.

\bibitem{bauer2017nonparametric}
B.~Bauer, L.~Devroye, M.~Kohler, A.~Krzy{\.z}ak, and H.~Walk.
\newblock Nonparametric estimation of a function from noiseless observations at
  random points.
\newblock {\em Journal of Multivariate Analysis}, 160:93--104, 2017.

\bibitem{berthier2020accelerated}
R.~Berthier, F.~Bach, and P.~Gaillard.
\newblock Accelerated gossip in networks of given dimension using
  \uppercase{J}acobi polynomial iterations.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(1):24--47, 2020.

\bibitem{bottou2008tradeoffs}
L.~Bottou and O.~Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Advances in Neural Information Processing Systems 20}, pages
  161--168, 2008.

\bibitem{bottou2005line}
L.~Bottou and Y.~Le~Cun.
\newblock On-line learning for very large data sets.
\newblock {\em Applied Stochastic Models in Business and Industry},
  21(2):137--151, 2005.

\bibitem{caponnetto2007optimal}
A.~Caponnetto and E.~De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{cevher2019linear}
V.~Cevher and B.~C. V{\~u}.
\newblock On the linear convergence of the stochastic gradient method with
  constant step-size.
\newblock {\em Optimization Letters}, 13(5):1177--1187, 2019.

\bibitem{chung1997spectral}
F.~R. Chung and F.~C. Graham.
\newblock {\em Spectral Graph Theory}.
\newblock Number~92 in CBMS Regional Conference Series in Mathematics. American
  Mathematical Soc., 1997.

\bibitem{dieuleveut2016nonparametric}
A.~Dieuleveut and F.~Bach.
\newblock Nonparametric stochastic approximation with large step-sizes.
\newblock {\em The Annals of Statistics}, 44(4):1363--1399, 2016.

\bibitem{dieuleveut2017harder}
A.~Dieuleveut, N.~Flammarion, and F.~Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3520--3570,
  2017.

\bibitem{NIST:DLMF}
{\it NIST Digital Library of Mathematical Functions}.
\newblock http://dlmf.nist.gov/, Release 1.0.26 of 2020-03-15.

\bibitem{fischer2017sobolev}
S.~Fischer and I.~Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithm.
\newblock {\em arXiv preprint arXiv:1702.07254}, 2017.

\bibitem{gyorfi2006distribution}
L.~Gy{\"o}rfi, M.~Kohler, A.~Krzyzak, and H.~Walk.
\newblock {\em A distribution-free theory of nonparametric regression}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{hofmann2008kernel}
T.~Hofmann, B.~Sch{\"o}lkopf, and A.~J. Smola.
\newblock Kernel methods in machine learning.
\newblock {\em The Annals of Statistics}, pages 1171--1220, 2008.

\bibitem{jun2019kernel}
K.-S. Jun, A.~Cutkosky, and F.~Orabona.
\newblock Kernel truncated randomized ridge regression: Optimal rates and low
  noise acceleration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15332--15341, 2019.

\bibitem{kohler2013optimal}
M.~Kohler and A.~Krzy{\.z}ak.
\newblock Optimal global rates of convergence for interpolation problems with
  random design.
\newblock {\em Statistics \& Probability Letters}, 83(8):1871--1879, 2013.

\bibitem{lin2018optimal}
J.~Lin and V.~Cevher.
\newblock Optimal convergence for distributed learning with stochastic gradient
  methods and spectral-regularization algorithms.
\newblock {\em arXiv preprint arXiv:1801.07226}, 2018.

\bibitem{ma2017power}
S.~Ma, R.~Bassily, and M.~Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 3325--3334, 2018.

\bibitem{mathieu2004isoperimetry}
P.~Mathieu and E.~Remy.
\newblock Isoperimetry and heat kernel decay on percolation clusters.
\newblock {\em The Annals of Probability}, 32(1A):100--128, 2004.

\bibitem{mucke2019beating}
N.~M{\"u}cke, G.~Neu, and L.~Rosasco.
\newblock Beating \uppercase{SGD} saturation with tail-averaging and
  minibatching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12568--12577, 2019.

\bibitem{nedic2010constrained}
A.~Nedic, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Constrained consensus and optimization in multi-agent networks.
\newblock {\em IEEE Transactions on Automatic Control}, 55(4):922--938, 2010.

\bibitem{pillaud2018statistical}
L.~Pillaud-Vivien, A.~Rudi, and F.~Bach.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8114--8124, 2018.

\bibitem{rosasco2015learning}
L.~Rosasco and S.~Villa.
\newblock Learning with incremental iterative regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1630--1638, 2015.

\bibitem{schmidt2013fast}
M.~Schmidt and N.~Le~Roux.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock {\em arXiv preprint arXiv:1308.6370}, 2013.

\bibitem{shah2009gossip}
D.~Shah.
\newblock Gossip algorithms.
\newblock {\em Foundations and Trends{\textregistered} in Networking},
  3(1):1--125, 2009.

\bibitem{tarres2014online}
P.~Tarr\`es and Y.~Yao.
\newblock Online learning as stochastic approximation of regularization paths:
  Optimality and almost-sure convergence.
\newblock {\em IEEE Transactions on Information Theory}, 60(9):5716--5735,
  2014.

\bibitem{tsybakov2008introduction}
A.~B. Tsybakov.
\newblock {\em Introduction to Nonparametric Estimation}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{vaswani2018fast}
S.~Vaswani, F.~Bach, and M.~Schmidt.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In {\em Proceedings of Machine Learning Research}, pages 1195--1204,
  2019.

\bibitem{wahba1990spline}
G.~Wahba.
\newblock {\em Spline Models for Observational Data}.
\newblock Society for Industrial and Applied Mathematics, 1990.

\bibitem{wendland2004scattered}
H.~Wendland.
\newblock {\em Scattered Data Approximation}.
\newblock Cambridge Monographs on Applied and Computational Mathematics.
  Cambridge University Press, 2004.

\bibitem{ying2008online}
Y.~Ying and M.~Pontil.
\newblock Online gradient descent learning algorithms.
\newblock {\em Foundations of Computational Mathematics}, 8(5):561--596, 2008.

\end{thebibliography}
