\begin{thebibliography}{10}

\bibitem{ando2017deep}
Shin Ando and Chun~Yuan Huang.
\newblock Deep over-sampling framework for classifying imbalanced data.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 770--785. Springer, 2017.

\bibitem{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  233--242. PMLR, 2017.

\bibitem{bahri2020deep}
Dara Bahri, Heinrich Jiang, and Maya Gupta.
\newblock Deep k-nn for noisy labels.
\newblock In {\em International Conference on Machine Learning}, pages
  540--550. PMLR, 2020.

\bibitem{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5049--5059, 2019.

\bibitem{buda2018systematic}
Mateusz Buda, Atsuto Maki, and Maciej~A Mazurowski.
\newblock A systematic study of the class imbalance problem in convolutional
  neural networks.
\newblock {\em Neural Networks}, 106:249--259, 2018.

\bibitem{cao2020heteroskedastic}
Kaidi Cao, Yining Chen, Junwei Lu, Nikos Arechiga, Adrien Gaidon, and Tengyu
  Ma.
\newblock Heteroskedastic and imbalanced deep learning with adaptive
  regularization.
\newblock {\em arXiv preprint arXiv:2006.15766}, 2020.

\bibitem{chawla2002smote}
Nitesh~V Chawla, Kevin~W Bowyer, Lawrence~O Hall, and W~Philip Kegelmeyer.
\newblock Smote: synthetic minority over-sampling technique.
\newblock {\em Journal of artificial intelligence research}, 16:321--357, 2002.

\bibitem{cheng2020learning}
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach.
\newblock {\em arXiv preprint arXiv:2010.02347}, 2020.

\bibitem{corbiere2017leveraging}
Charles Corbiere, Hedi Ben-Younes, Alexandre Ram{\'e}, and Charles Ollion.
\newblock Leveraging weakly annotated data for fashion image retrieval and
  label prediction.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision Workshops}, pages 2268--2274, 2017.

\bibitem{cordeiro2021longremix}
Filipe~R Cordeiro, Ragav Sachdeva, Vasileios Belagiannis, Ian Reid, and Gustavo
  Carneiro.
\newblock Longremix: Robust learning with high confidence samples in a noisy
  label environment.
\newblock {\em arXiv preprint arXiv:2103.04173}, 2021.

\bibitem{Feng_2021}
Di~Feng, Christian Haase-Schutz, Lars Rosenbaum, Heinz Hertlein, Claudius
  Glaser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer.
\newblock Deep multi-modal object detection and semantic segmentation for
  autonomous driving: Datasets, methods, and challenges.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems},
  22(3):1341â€“1360, Mar 2021.

\bibitem{fisher1936use}
Ronald~A Fisher.
\newblock The use of multiple measurements in taxonomic problems.
\newblock {\em Annals of eugenics}, 7(2):179--188, 1936.

\bibitem{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and PS~Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In {\em AAAI}, 2017.

\bibitem{han2018coteaching}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In {\em Advances in neural information processing systems}, pages
  8527--8537, 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}, 2016.

\bibitem{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock {\em arXiv preprint arXiv:1802.05300}, 2018.

\bibitem{huang2019o2u}
Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao.
\newblock O2u-net: A simple noisy label detection approach for deep neural
  networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3326--3334, 2019.

\bibitem{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels, 2018.

\bibitem{kim2021comparing}
Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, and Se-Young Yun.
\newblock Comparing kullback-leibler divergence and mean squared error loss in
  knowledge distillation, 2021.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{le2020adversarial}
Ronan Le~Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew
  Peters, Ashish Sabharwal, and Yejin Choi.
\newblock Adversarial filters of dataset biases.
\newblock In {\em International Conference on Machine Learning}, pages
  1078--1088. PMLR, 2020.

\bibitem{lee2018simple}
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7167--7177, 2018.

\bibitem{lee2019robust}
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo~Li, and Jinwoo Shin.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock In {\em International Conference on Machine Learning}, pages
  3763--3772. PMLR, 2019.

\bibitem{Li2020DivideMix}
Junnan Li, Richard Socher, and Steven~C.H. Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2020gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4313--4324. PMLR, 2020.

\bibitem{li2019repair}
Yi~Li and Nuno Vasconcelos.
\newblock Repair: Removing representation bias by dataset resampling.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9572--9581, 2019.

\bibitem{liang2017enhancing}
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock {\em arXiv preprint arXiv:1706.02690}, 2017.

\bibitem{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock {\em arXiv preprint arXiv:2007.00151}, 2020.

\bibitem{lukasik2020does}
Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar.
\newblock Does label smoothing mitigate label noise?
\newblock In {\em International Conference on Machine Learning}, pages
  6448--6458. PMLR, 2020.

\bibitem{maennel2020neural}
Hartmut Maennel, Ibrahim Alabdulmohsin, Ilya Tolstikhin, Robert~JN Baldock,
  Olivier Bousquet, Sylvain Gelly, and Daniel Keysers.
\newblock What do neural networks learn when trained with random labels?
\newblock {\em arXiv preprint arXiv:2006.10455}, 2020.

\bibitem{mirzasoleiman2020coresets}
Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec.
\newblock Coresets for robust training of neural networks against noisy labels.
\newblock {\em arXiv preprint arXiv:2011.07451}, 2020.

\bibitem{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1944--1952, 2017.

\bibitem{pleiss2020identifying}
Geoff Pleiss, Tianyi Zhang, Ethan~R Elenberg, and Kilian~Q Weinberger.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock {\em arXiv preprint arXiv:2001.10528}, 2020.

\bibitem{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock {\em arXiv preprint arXiv:1412.6596}, 2014.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4334--4343. PMLR, 2018.

\bibitem{song2020prestopping}
Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee.
\newblock Prestopping: How does early stopping help generalization against
  label noise?, 2020.

\bibitem{van2015learning}
Brendan Van~Rooyen, Aditya~Krishna Menon, and Robert~C Williamson.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock {\em arXiv preprint arXiv:1505.07634}, 2015.

\bibitem{veit2017learning}
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge
  Belongie.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 839--847, 2017.

\bibitem{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem{wei2021dst}
Yi~Wei, Xue Mei, Xin Liu, and Pengxiang Xu.
\newblock Dst: Data selection and joint training for learning with noisy
  labels.
\newblock {\em arXiv preprint arXiv:2103.00813}, 2021.

\bibitem{wu2020topological}
Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen.
\newblock A topological filter for learning with label noise.
\newblock {\em arXiv preprint arXiv:2012.04835}, 2020.

\bibitem{xia2021robust}
Xiaobo Xia, Tongliang Liu, Bo~Han, Chen Gong, Nannan Wang, Zongyuan Ge, and
  Yi~Chang.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{xiao2015learning}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2691--2699, 2015.

\bibitem{xu2019l_dmi}
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In {\em NeurIPS}, pages 6222--6233, 2019.

\bibitem{yang2020robust}
Seunghan Yang, Hyoungseob Park, Junyoung Byun, and Changick Kim.
\newblock Robust federated learning with noisy labels, 2020.

\bibitem{yu2019does}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor~W Tsang, and Masashi
  Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock {\em arXiv preprint arXiv:1901.04215}, 2019.

\bibitem{yu2018learning}
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao.
\newblock Learning with biased complementary labels.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 68--83, 2018.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhang2018generalized}
Zhilu Zhang and Mert~R Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock {\em arXiv preprint arXiv:1805.07836}, 2018.

\bibitem{zhang2020distilling}
Zizhao Zhang, Han Zhang, Sercan~O Arik, Honglak Lee, and Tomas Pfister.
\newblock Distilling effective supervision from severe label noise.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9294--9303, 2020.

\bibitem{Zhang_2020_CVPR}
Zizhao Zhang, Han Zhang, Sercan~O. Arik, Honglak Lee, and Tomas Pfister.
\newblock Distilling effective supervision from severe label noise.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\end{thebibliography}
