\begin{thebibliography}{}

\bibitem[Ben-David and Urner, 2014]{ben2014sample}
Ben-David, S. and Urner, R. (2014).
\newblock The sample complexity of agnostic learning under deterministic
  labels.
\newblock In {\em Conference on Learning Theory}, pages 527--542. PMLR.

\bibitem[Bietti et~al., 2021]{bietti2021sample}
Bietti, A., Venturi, L., and Bruna, J. (2021).
\newblock On the sample complexity of learning under geometric stability.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Bloem-Reddy and Teh, 2020]{bloem2020probabilistic}
Bloem-Reddy, B. and Teh, Y.~W. (2020).
\newblock Probabilistic symmetries and invariant neural networks.
\newblock {\em J. Mach. Learn. Res.}, 21:90--1.

\bibitem[Blumer et~al., 1989]{blumer1989learnability}
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M.~K. (1989).
\newblock Learnability and the {Vapnik-Chervonenkis} dimension.
\newblock {\em Journal of the ACM (JACM)}, 36(4):929--965.

\bibitem[Chatzipantazis et~al., 2021]{chatzipantazis2021learning}
Chatzipantazis, E., Pertigkiozoglou, S., Dobriban, E., and Daniilidis, K.
  (2021).
\newblock Learning augmentation distributions using transformed risk
  minimization.
\newblock {\em arXiv preprint arXiv:2111.08190}.

\bibitem[Chen et~al., 2020]{Chen2020}
Chen, S., Dobriban, E., and Lee, J.~H. (2020).
\newblock A group-theoretic framework for data augmentation.
\newblock {\em Journal of Machine Learning Research}, 21:1--71.

\bibitem[Cohen and Welling, 2016]{cohen2016group}
Cohen, T. and Welling, M. (2016).
\newblock Group equivariant convolutional networks.
\newblock In {\em International conference on machine learning}, pages
  2990--2999. PMLR.

\bibitem[Cubuk et~al., 2018]{cubuk2018autoaugment}
Cubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V. (2018).
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}.

\bibitem[Daniely and Shalev-Shwartz, 2014]{daniely2014optimal}
Daniely, A. and Shalev-Shwartz, S. (2014).
\newblock Optimal learners for multiclass problems.
\newblock In {\em Conference on Learning Theory}, pages 287--316. PMLR.

\bibitem[Dao et~al., 2019]{Dao2019}
Dao, T., Gu, A., Ratner, A., Smith, V., De~Sa, C., and R{\'e}, C. (2019).
\newblock A kernel theory of modern data augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  1528--1537. PMLR.

\bibitem[David et~al., 2016]{david2016supervised}
David, O., Moran, S., and Yehudayoff, A. (2016).
\newblock Supervised learning through the lens of compression.
\newblock {\em Advances in Neural Information Processing Systems}, 29.

\bibitem[Dieleman et~al., 2016]{dieleman2016exploiting}
Dieleman, S., De~Fauw, J., and Kavukcuoglu, K. (2016).
\newblock Exploiting cyclic symmetry in convolutional neural networks.
\newblock In {\em International conference on machine learning}, pages
  1889--1898. PMLR.

\bibitem[Ehrenfeucht et~al., 1989]{ehrenfeucht1989general}
Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L. (1989).
\newblock A general lower bound on the number of examples needed for learning.
\newblock {\em Information and Computation}, 82(3):247--261.

\bibitem[Elesedy, 2021]{elesedy2021kernel}
Elesedy, B. (2021).
\newblock Provably strict generalisation benefit for invariance in kernel
  methods.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Elesedy, 2022]{elesedy2022group}
Elesedy, B. (2022).
\newblock Group symmetry in pac learning.
\newblock In {\em ICLR 2022 Workshop on Geometrical and Topological
  Representation Learning}.

\bibitem[Elesedy and Zaidi, 2021]{elesedy2021linear}
Elesedy, B. and Zaidi, S. (2021).
\newblock Provably strict generalisation benefit for equivariant models.
\newblock In {\em International Conference on Machine Learning}, pages
  2959--2969. PMLR.

\bibitem[Fawzi et~al., 2016]{fawzi2016adaptive}
Fawzi, A., Samulowitz, H., Turaga, D., and Frossard, P. (2016).
\newblock Adaptive data augmentation for image classification.
\newblock In {\em 2016 IEEE international conference on image processing
  (ICIP)}, pages 3688--3692. Ieee.

\bibitem[Fukushima and Miyake, 1982]{fukushima1982neocognitron}
Fukushima, K. and Miyake, S. (1982).
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In {\em Competition and cooperation in neural nets}, pages 267--285.
  Springer.

\bibitem[Gontijo-Lopes et~al., 2020]{gontijo2020affinity}
Gontijo-Lopes, R., Smullin, S.~J., Cubuk, E.~D., and Dyer, E. (2020).
\newblock Affinity and diversity: Quantifying mechanisms of data augmentation.
\newblock {\em arXiv preprint arXiv:2002.08973}.

\bibitem[Graepel et~al., 2005]{graepel2005pac}
Graepel, T., Herbrich, R., and Shawe-Taylor, J. (2005).
\newblock Pac-bayesian compression bounds on the prediction error of learning
  algorithms for classification.
\newblock {\em Machine Learning}, 59(1-2):55--76.

\bibitem[Haussler et~al., 1994]{haussler1994predicting}
Haussler, D., Littlestone, N., and Warmuth, M.~K. (1994).
\newblock Predicting $\{$0, 1$\}$-functions on randomly drawn points.
\newblock {\em Information and Computation}, 115(2):248--292.

\bibitem[Hopkins et~al., 2021]{hopkins2021realizable}
Hopkins, M., Kane, D., Lovett, S., and Mahajan, G. (2021).
\newblock Realizable learning is all you need.
\newblock {\em arXiv preprint arXiv:2111.04746}.

\bibitem[Kondor and Trivedi, 2018]{kondor2018generalization}
Kondor, R. and Trivedi, S. (2018).
\newblock On the generalization of equivariance and convolution in neural
  networks to the action of compact groups.
\newblock In {\em International Conference on Machine Learning}, pages
  2747--2755. PMLR.

\bibitem[Krizhevsky et~al., 2012]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E. (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25.

\bibitem[LeCun et~al., 1989]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D. (1989).
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551.

\bibitem[Littlestone and Warmuth, 1986]{littlestone1986relating}
Littlestone, N. and Warmuth, M. (1986).
\newblock Relating data compression and learnability.

\bibitem[Lyle et~al., 2020]{Lyle2020}
Lyle, C., van~der Wilk, M., Kwiatkowska, M., Gal, Y., and Bloem-Reddy, B.
  (2020).
\newblock On the benefits of invariance in neural networks.
\newblock {\em arXiv preprint arXiv:2005.00178}.

\bibitem[Mei et~al., 2021]{mei2021learning}
Mei, S., Misiakiewicz, T., and Montanari, A. (2021).
\newblock Learning with invariances in random features and kernel models.
\newblock In {\em Conference on Learning Theory}, pages 3351--3418. PMLR.

\bibitem[Nakkiran, 2019]{nakkiran2019adversarial}
Nakkiran, P. (2019).
\newblock Adversarial robustness may be at odds with simplicity.
\newblock {\em arXiv preprint arXiv:1901.00532}.

\bibitem[Raghunathan et~al., 2019]{raghunathan2019adversarial}
Raghunathan, A., Xie, S.~M., Yang, F., Duchi, J.~C., and Liang, P. (2019).
\newblock Adversarial training can hurt generalization.
\newblock {\em arXiv preprint arXiv:1906.06032}.

\bibitem[Ravanbakhsh et~al., 2017]{ravanbakhsh2017equivariance}
Ravanbakhsh, S., Schneider, J., and Poczos, B. (2017).
\newblock Equivariance through parameter-sharing.
\newblock In {\em International Conference on Machine Learning}, pages
  2892--2901. PMLR.

\bibitem[Schapire and Freund, 2012]{schapire2012boosting}
Schapire, R.~E. and Freund, Y. (2012).
\newblock {\em Boosting: Foundations and Algorithms}.
\newblock MIT Press.

\bibitem[Schmidt et~al., 2018]{schmidt2018adversarially}
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and M{\k{a}}dry, A.
  (2018).
\newblock Adversarially robust generalization requires more data.
\newblock {\em arXiv preprint arXiv:1804.11285}.

\bibitem[Shen et~al., 2022]{shen2022data}
Shen, R., Bubeck, S., and Gunasekar, S. (2022).
\newblock Data augmentation as feature manipulation: a story of desert cows and
  grass cows.
\newblock {\em arXiv preprint arXiv:2203.01572}.

\bibitem[Vapnik and Chervonenkis, 1974]{vapnik1974}
Vapnik, V. and Chervonenkis, A. (1974).
\newblock {\em Theory of Pattern Recognition}.
\newblock Nauka, Moscow.

\bibitem[Wang et~al., 2021]{wang2020tent}
Wang, D., Shelhamer, E., Liu, S., Olshausen, B.~A., and Darrell, T. (2021).
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Wood and Shawe-Taylor, 1996]{wood1996representation}
Wood, J. and Shawe-Taylor, J. (1996).
\newblock Representation theory and invariant neural networks.
\newblock {\em Discrete applied mathematics}, 69(1-2):33--60.

\bibitem[Worrall et~al., 2017]{worrall2017harmonic}
Worrall, D.~E., Garbin, S.~J., Turmukhambetov, D., and Brostow, G.~J. (2017).
\newblock Harmonic networks: Deep translation and rotation equivariance.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5028--5037.

\end{thebibliography}
