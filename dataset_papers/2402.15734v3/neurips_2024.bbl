\begin{thebibliography}{83}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Han et~al.(2018)Han, Jentzen, and E]{han2018solving}
Jiequn Han, Arnulf Jentzen, and Weinan E.
\newblock Solving high-dimensional partial differential equations using deep learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0 (34):\penalty0 8505--8510, 2018.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and Karniadakis]{raissi2019physics}
Maziar Raissi, Paris Perdikaris, and George~E Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.
\newblock \emph{Journal of Computational physics}, 378:\penalty0 686--707, 2019.

\bibitem[Kovachki et~al.(2023)Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar]{kovachki2023neural}
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Neural operator: Learning maps between function spaces with applications to {PDEs}.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (89):\penalty0 1--97, 2023.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Kovachki, Azizzadenesheli, liu, Bhattacharya, Stuart, and Anandkumar]{li2021fourier}
Zongyi Li, Nikola~Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Liu, Stuart, Bhattacharya, and Anandkumar]{li2020multipole}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar.
\newblock Multipole graph neural operator for parametric partial differential equations.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 6755--6766, 2020.

\bibitem[Lu et~al.(2019)Lu, Jin, and Karniadakis]{lu2019deeponet}
Lu~Lu, Pengzhan Jin, and George~Em Karniadakis.
\newblock Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.
\newblock \emph{arXiv preprint arXiv:1910.03193}, 2019.

\bibitem[Krishnapriyan et~al.(2023)Krishnapriyan, Queiruga, Erichson, and Mahoney]{continuousPhysics23_commph}
A.~S. Krishnapriyan, A.~F. Queiruga, N.~B. Erichson, and M.~W. Mahoney.
\newblock Learning continuous models for continuous physics.
\newblock \emph{Communications Physics}, 6:\penalty0 319, 2023.

\bibitem[Hansen et~al.(2024)Hansen, Maddix, Alizadeh, Gupta, and Mahoney]{hansen24_physicad}
D.~Hansen, D.~C. Maddix, S.~Alizadeh, G.~Gupta, and M.~W. Mahoney.
\newblock Learning physical models that can respect conservation laws.
\newblock \emph{Physica D: Nonlinear Phenomena}, 457:\penalty0 133952, 2024.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Zheng, Kovachki, Jin, Chen, Liu, Azizzadenesheli, and Anandkumar]{li2021physics}
Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock Physics-informed neural operator for learning partial differential equations.
\newblock \emph{arXiv preprint arXiv:2111.03794}, 2021{\natexlab{b}}.

\bibitem[Takamoto et~al.(2022)Takamoto, Praditia, Leiteritz, MacKinlay, Alesiani, Pfl{\"u}ger, and Niepert]{PDEBench2022}
Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pfl{\"u}ger, and Mathias Niepert.
\newblock Pdebench: An extensive benchmark for scientific machine learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1596--1611, 2022.

\bibitem[Subramanian et~al.(2023)Subramanian, Harrington, Keutzer, Bhimji, Morozov, Mahoney, and Gholami]{subramanian2023towards}
Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael~W Mahoney, and Amir Gholami.
\newblock Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior.
\newblock \emph{arXiv preprint arXiv:2306.00258}, 2023.

\bibitem[Sirignano et~al.(2020)Sirignano, MacArt, and Freund]{sirignano2020dpm}
Justin Sirignano, Jonathan~F MacArt, and Jonathan~B Freund.
\newblock Dpm: A deep learning pde augmentation method with application to large-eddy simulation.
\newblock \emph{Journal of Computational Physics}, 423:\penalty0 109811, 2020.

\bibitem[McCallen et~al.(2021)McCallen, Petersson, Rodgers, Pitarka, Miah, Petrone, Sjogreen, Abrahamson, and Tang]{mccallen2021eqsim_1}
David McCallen, Anders Petersson, Arthur Rodgers, Arben Pitarka, Mamun Miah, Floriana Petrone, Bjorn Sjogreen, Norman Abrahamson, and Houjun Tang.
\newblock Eqsimâ€”a multidisciplinary framework for fault-to-structure earthquake simulations on exascale computers part i: Computational models and workflow.
\newblock \emph{Earthquake Spectra}, 37\penalty0 (2):\penalty0 707--735, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and Hinton]{chen2020simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In \emph{International conference on machine learning}, pages 1597--1607. PMLR, 2020{\natexlab{a}}.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9729--9738, 2020.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and He]{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.

\bibitem[Noroozi and Favaro(2016)]{noroozi2016unsupervised}
Mehdi Noroozi and Paolo Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw puzzles.
\newblock In \emph{European conference on computer vision}, pages 69--84. Springer, 2016.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{gidaris2018unsupervised}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock \emph{arXiv preprint arXiv:1803.07728}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Logan~IV et~al.(2021)Logan~IV, Bala{\v{z}}evi{\'c}, Wallace, Petroni, Singh, and Riedel]{logan2021cutting}
Robert~L Logan~IV, Ivana Bala{\v{z}}evi{\'c}, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel.
\newblock Cutting down on prompts and parameters: Simple few-shot learning with language models.
\newblock \emph{arXiv preprint arXiv:2106.13353}, 2021.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Liu, Meng, and Osher]{yang2023context}
Liu Yang, Siting Liu, Tingwei Meng, and Stanley~J Osher.
\newblock In-context operator learning with data prompts for differential equation problems.
\newblock \emph{Proceedings of the National Academy of Sciences}, 120\penalty0 (39):\penalty0 e2310142120, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Meng, Liu, and Osher]{yang2023prompting}
Liu Yang, Tingwei Meng, Siting Liu, and Stanley~J Osher.
\newblock Prompting in-context operator learning with sensor data, equations, and natural language.
\newblock \emph{arXiv preprint arXiv:2308.05061}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Erichson, Bhatia, Mahoney, and Re]{liu2023does}
Jerry~Weihong Liu, N~Benjamin Erichson, Kush Bhatia, Michael~W Mahoney, and Christopher Re.
\newblock Does in-context operator learning generalize to domain-shifted settings?
\newblock In \emph{The Symbiosis of Deep Learning and Differential Equations III}, 2023.

\bibitem[Lagaris et~al.(1998)Lagaris, Likas, and Fotiadis]{lagaris1998artificial}
Isaac~E Lagaris, Aristidis Likas, and Dimitrios~I Fotiadis.
\newblock Artificial neural networks for solving ordinary and partial differential equations.
\newblock \emph{IEEE transactions on neural networks}, 9\penalty0 (5):\penalty0 987--1000, 1998.

\bibitem[Lagaris et~al.(2000)Lagaris, Likas, and Papageorgiou]{lagaris2000neural}
Isaac~E Lagaris, Aristidis~C Likas, and Dimitris~G Papageorgiou.
\newblock Neural-network methods for boundary value problems with irregular boundaries.
\newblock \emph{IEEE Transactions on Neural Networks}, 11\penalty0 (5):\penalty0 1041--1049, 2000.

\bibitem[Chen and Chen(1995{\natexlab{a}})]{chen1995universal}
Tianping Chen and Hong Chen.
\newblock Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems.
\newblock \emph{IEEE transactions on neural networks}, 6\penalty0 (4):\penalty0 911--917, 1995{\natexlab{a}}.

\bibitem[Chen and Chen(1995{\natexlab{b}})]{chen1995approximation}
Tianping Chen and Hong Chen.
\newblock Approximation capability to functions of several variables, nonlinear functionals, and operators by radial basis function neural networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 6\penalty0 (4):\penalty0 904--910, 1995{\natexlab{b}}.

\bibitem[Zhu et~al.(2019)Zhu, Zabaras, Koutsourelakis, and Perdikaris]{zhu2019physics}
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris.
\newblock Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data.
\newblock \emph{Journal of Computational Physics}, 394:\penalty0 56--81, 2019.

\bibitem[Geneva and Zabaras(2020)]{geneva2020modeling}
Nicholas Geneva and Nicholas Zabaras.
\newblock Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks.
\newblock \emph{Journal of Computational Physics}, 403:\penalty0 109056, 2020.

\bibitem[Gao et~al.(2021)Gao, Sun, and Wang]{gao2021phygeonet}
Han Gao, Luning Sun, and Jian-Xun Wang.
\newblock Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain.
\newblock \emph{Journal of Computational Physics}, 428:\penalty0 110079, 2021.

\bibitem[Ren et~al.(2022)Ren, Rao, Liu, Wang, and Sun]{ren2022phycrnet}
Pu~Ren, Chengping Rao, Yang Liu, Jian-Xun Wang, and Hao Sun.
\newblock Phycrnet: Physics-informed convolutional-recurrent network for solving spatiotemporal pdes.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 389:\penalty0 114399, 2022.

\bibitem[Krishnapriyan et~al.(2021)Krishnapriyan, Gholami, Zhe, Kirby, and Mahoney]{krishnapriyan2021characterizing}
Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael~W Mahoney.
\newblock Characterizing possible failure modes in physics-informed neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26548--26560, 2021.

\bibitem[Edwards(2022)]{EdwCACM22}
C.~Edwards.
\newblock Neural networks learn to speed up simulations.
\newblock \emph{Communications of the ACM}, 65\penalty0 (5):\penalty0 27--29, 2022.

\bibitem[Brandstetter et~al.(2022)Brandstetter, Welling, and Worrall]{brandstetter2022lie}
Johannes Brandstetter, Max Welling, and Daniel~E Worrall.
\newblock Lie point symmetry data augmentation for neural pde solvers.
\newblock In \emph{International Conference on Machine Learning}, pages 2241--2256. PMLR, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Helwig, Luo, Fu, Xie, Liu, Lin, Xu, Yan, et~al.]{zhang2023artificial}
Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et~al.
\newblock Artificial intelligence for science in quantum, atomistic, and continuum systems.
\newblock \emph{arXiv preprint arXiv:2307.08423}, 2023.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pages 2790--2799. PMLR, 2019.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Tian et~al.(2020)Tian, Krishnan, and Isola]{tian2020contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16}, pages 776--794. Springer, 2020.

\bibitem[Misra and Maaten(2020)]{misra2020self}
Ishan Misra and Laurens van~der Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6707--6717, 2020.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{OpenAI}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Mialon et~al.(2023)Mialon, Garrido, Lawrence, Rehman, LeCun, and Kiani]{mialon2023self}
Gr{\'e}goire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak~T Kiani.
\newblock Self-supervised learning with lie symmetries for partial differential equations.
\newblock \emph{arXiv preprint arXiv:2307.05432}, 2023.

\bibitem[Lanusse et~al.(2023)Lanusse, Parker, Golkar, Cranmer, Bietti, Eickenberg, Krawezik, McCabe, Ohana, Pettee, et~al.]{lanusse2023astroclip}
Francois Lanusse, Liam Parker, Siavash Golkar, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, et~al.
\newblock Astroclip: Cross-modal pre-training for astronomical foundation models.
\newblock \emph{arXiv preprint arXiv:2310.03024}, 2023.

\bibitem[McCabe et~al.(2023)McCabe, Blancard, Parker, Ohana, Cranmer, Bietti, Eickenberg, Golkar, Krawezik, Lanusse, et~al.]{mccabe2023multiple}
Michael McCabe, Bruno R{\'e}galdo-Saint Blancard, Liam~Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et~al.
\newblock Multiple physics pretraining for physical surrogate models.
\newblock \emph{arXiv preprint arXiv:2310.02994}, 2023.

\bibitem[Fei-Fei et~al.(2006)Fei-Fei, Fergus, and Perona]{fei2006one}
Li~Fei-Fei, Robert Fergus, and Pietro Perona.
\newblock One-shot learning of object categories.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 28\penalty0 (4):\penalty0 594--611, 2006.

\bibitem[Zhang et~al.(2020)Zhang, Wei, Yang, and Huang]{zhang2020sg}
Xiaolin Zhang, Yunchao Wei, Yi~Yang, and Thomas~S Huang.
\newblock Sg-one: Similarity guidance network for one-shot semantic segmentation.
\newblock \emph{IEEE transactions on cybernetics}, 50\penalty0 (9):\penalty0 3855--3865, 2020.

\bibitem[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba]{zhou2022large}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers.
\newblock \emph{arXiv preprint arXiv:2211.01910}, 2022.

\bibitem[Sordoni et~al.(2023)Sordoni, Yuan, C{\^o}t{\'e}, Pereira, Trischler, Xiao, Hosseini, Niedtner, and Roux]{sordoni2023deep}
Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C{\^o}t{\'e}, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas~Le Roux.
\newblock Deep language networks: Joint prompt training of stacked llms using variational inference.
\newblock \emph{arXiv preprint arXiv:2306.12509}, 2023.

\bibitem[Nandakumaran and Datti(2020)]{nandakumaran2020partial}
AK~Nandakumaran and PS~Datti.
\newblock \emph{Partial differential equations: classical theory with a modern touch}.
\newblock Cambridge University Press, 2020.

\bibitem[Hairer et~al.(1987)Hairer, N{\o}rsett, and Wanner]{HairerNorsettWanner1987}
E~Hairer, S~P N{\o}rsett, and G~Wanner.
\newblock \emph{{Solving ordinary differential equations I. nonstiff problems}}, volume~29.
\newblock Springer, 1987.
\newblock \doi{10.1016/0378-4754(87)90083-8}.

\bibitem[Hairer et~al.(2006)Hairer, Lubich, and Wanner]{HairerLubichWanner2006}
E.~Hairer, C.~Lubich, and G.~Wanner.
\newblock \emph{{Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations (Springer Series in Computational Mathematics)}}, volume~31.
\newblock Springer, 2006.

\bibitem[Brunton et~al.(2020)Brunton, Noack, and Koumoutsakos]{brunton2020machine}
Steven~L Brunton, Bernd~R Noack, and Petros Koumoutsakos.
\newblock Machine learning for fluid mechanics.
\newblock \emph{Annual Review of Fluid Mechanics}, 52:\penalty0 477--508, 2020.

\bibitem[Erichson et~al.(2020)Erichson, Mathelin, Yao, Brunton, Mahoney, and Kutz]{erichson2020shallow}
N~Benjamin Erichson, Lionel Mathelin, Zhewei Yao, Steven~L Brunton, Michael~W Mahoney, and J~Nathan Kutz.
\newblock Shallow neural networks for fluid flow reconstruction with limited sensors.
\newblock \emph{Proceedings of the Royal Society A}, 476\penalty0 (2238):\penalty0 20200097, 2020.

\bibitem[Jayaraman and Mamun(2020)]{jayaraman2020data}
Balaji Jayaraman and SM~Abdullah~Al Mamun.
\newblock On data-driven sparse sensing and linear estimation of fluid flows.
\newblock \emph{Sensors}, 20\penalty0 (13):\penalty0 3752, 2020.

\bibitem[Chung et~al.(2023)Chung, Akoush, Sharma, Tamkin, Jung, Chen, Guo, Brouzet, Talei, Savard, et~al.]{chung2023turbulence}
Wai~Tong Chung, Bassem Akoush, Pushan Sharma, Alex Tamkin, Ki~Sung Jung, Jacqueline Chen, Jack Guo, Davy Brouzet, Mohsen Talei, Bruno Savard, et~al.
\newblock Turbulence in focus: Benchmarking scaling behavior of 3d volumetric super-resolution with blastnet 2.0 data.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem[Ren et~al.(2023)Ren, Erichson, Subramanian, San, Lukic, and Mahoney]{ren2023superbench}
Pu~Ren, N~Benjamin Erichson, Shashank Subramanian, Omer San, Zarija Lukic, and Michael~W Mahoney.
\newblock Superbench: A super-resolution benchmark dataset for scientific machine learning.
\newblock \emph{arXiv preprint arXiv:2306.14070}, 2023.

\bibitem[Kochkov et~al.(2021)Kochkov, Smith, Alieva, Wang, Brenner, and Hoyer]{kochkov2021machine}
Dmitrii Kochkov, Jamie~A Smith, Ayya Alieva, Qing Wang, Michael~P Brenner, and Stephan Hoyer.
\newblock Machine learning--accelerated computational fluid dynamics.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0 (21):\penalty0 e2101784118, 2021.

\bibitem[Vinuesa and Brunton(2022)]{vinuesa2022enhancing}
Ricardo Vinuesa and Steven~L Brunton.
\newblock Enhancing computational fluid dynamics with machine learning.
\newblock \emph{Nature Computational Science}, 2\penalty0 (6):\penalty0 358--366, 2022.

\bibitem[Maulik et~al.(2019)Maulik, San, Rasheed, and Vedula]{maulik2019subgrid}
Romit Maulik, Omer San, Adil Rasheed, and Prakash Vedula.
\newblock Subgrid modelling for two-dimensional turbulence using neural networks.
\newblock \emph{Journal of Fluid Mechanics}, 858:\penalty0 122--144, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{tong2022videomae}
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
\newblock Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 10078--10093, 2022.

\bibitem[Zhang et~al.(2018)Zhang, Wei, Kang, Yang, and Huang]{zhang2018self}
Xiaolin Zhang, Yunchao Wei, Guoliang Kang, Yi~Yang, and Thomas Huang.
\newblock Self-produced guidance for weakly-supervised object localization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pages 597--613, 2018.

\bibitem[Min et~al.(2021)Min, Kang, and Cho]{min2021hypercorrelation}
Juhong Min, Dahyun Kang, and Minsu Cho.
\newblock Hypercorrelation squeeze for few-shot segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 6941--6952, 2021.

\bibitem[Shi et~al.(2022)Shi, Wei, Zhang, Lu, Ning, Chen, Ma, and Zheng]{shi2022dense}
Xinyu Shi, Dong Wei, Yu~Zhang, Donghuan Lu, Munan Ning, Jiashun Chen, Kai Ma, and Yefeng Zheng.
\newblock Dense cross-query-and-support attention weighted mask aggregation for few-shot segmentation.
\newblock In \emph{European Conference on Computer Vision}, pages 151--168. Springer, 2022.

\bibitem[Kang and Cho(2022)]{kang2022integrative}
Dahyun Kang and Minsu Cho.
\newblock Integrative few-shot learning for classification and segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9979--9990, 2022.

\bibitem[Liu et~al.(2022)Liu, Bao, Xie, Xiong, Sonke, and Gavves]{liu2022dynamic}
Jie Liu, Yanqi Bao, Guo-Sen Xie, Huan Xiong, Jan-Jakob Sonke, and Efstratios Gavves.
\newblock Dynamic prototype convolution network for few-shot semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11553--11562, 2022.

\bibitem[Peng et~al.(2023)Peng, Tian, Wu, Wang, Liu, Su, and Jia]{peng2023hierarchical}
Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang, Shu Liu, Jingyong Su, and Jiaya Jia.
\newblock Hierarchical dense correlation distillation for few-shot segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23641--23651, 2023.

\bibitem[Goyal et~al.(2017)Goyal, Ebrahimi~Kahou, Michalski, Materzynska, Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag, et~al.]{goyal2017something}
Raghav Goyal, Samira Ebrahimi~Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et~al.
\newblock The" something something" video database for learning and evaluating visual common sense.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 5842--5850, 2017.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Zhu et~al.(2024)Zhu, Fu, Zhou, and Lin]{zhu2024critical}
Xuekai Zhu, Yao Fu, Bowen Zhou, and Zhouhan Lin.
\newblock Critical data size of language models from a grokking perspective.
\newblock \emph{arXiv preprint arXiv:2401.10463}, 2024.

\bibitem[Thuerey et~al.(2020)Thuerey, Wei{\ss}enow, Prantl, and Hu]{thuerey2020deep}
Nils Thuerey, Konstantin Wei{\ss}enow, Lukas Prantl, and Xiangyu Hu.
\newblock Deep learning methods for reynolds-averaged navier--stokes simulations of airfoil flows.
\newblock \emph{AIAA Journal}, 58\penalty0 (1):\penalty0 25--36, 2020.

\bibitem[Pathak et~al.(2022)Pathak, Subramanian, Harrington, Raja, Chattopadhyay, Mardani, Kurth, Hall, Li, Azizzadenesheli, et~al.]{pathak2022fourcastnet}
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et~al.
\newblock Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators.
\newblock \emph{arXiv preprint arXiv:2202.11214}, 2022.

\bibitem[Poli et~al.(2022)Poli, Massaroli, Berto, Park, Dao, R{\'e}, and Ermon]{poli2022transform}
Michael Poli, Stefano Massaroli, Federico Berto, Jinkyoo Park, Tri Dao, Christopher R{\'e}, and Stefano Ermon.
\newblock Transform once: Efficient operator learning in frequency domain.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7947--7959, 2022.

\bibitem[Hersbach et~al.(2020)Hersbach, Bell, Berrisford, Hirahara, Hor{\'a}nyi, Mu{\~n}oz-Sabater, Nicolas, Peubey, Radu, Schepers, et~al.]{hersbach2020era5}
Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr{\'a}s Hor{\'a}nyi, Joaqu{\'\i}n Mu{\~n}oz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et~al.
\newblock The era5 global reanalysis.
\newblock \emph{Quarterly Journal of the Royal Meteorological Society}, 146\penalty0 (730):\penalty0 1999--2049, 2020.

\bibitem[Kalnay et~al.(2018)Kalnay, Kanamitsu, Kistler, Collins, Deaven, Gandin, Iredell, Saha, White, Woollen, et~al.]{kalnay2018ncep}
Eugenia Kalnay, Masao Kanamitsu, Robert Kistler, William Collins, Dennis Deaven, Lev Gandin, Mark Iredell, Suranjana Saha, Glenn White, John Woollen, et~al.
\newblock The ncep/ncar 40-year reanalysis project.
\newblock In \emph{Renewable energy}, pages Vol1\_146--Vol1\_194. Routledge, 2018.

\bibitem[Kalnay(2003)]{kalnay2003atmospheric}
Eugenia Kalnay.
\newblock \emph{Atmospheric modeling, data assimilation and predictability}.
\newblock Cambridge university press, 2003.

\bibitem[Eckert et~al.(2019)Eckert, Um, and Thuerey]{eckert2019scalarflow}
Marie-Lena Eckert, Kiwon Um, and Nils Thuerey.
\newblock Scalarflow: a large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 38\penalty0 (6):\penalty0 1--16, 2019.

\bibitem[Klaasen and Troy(1984)]{klaasen1984stationary}
Gene~A Klaasen and William~C Troy.
\newblock Stationary wave solutions of a system of reaction-diffusion equations derived from the fitzhugh--nagumo equations.
\newblock \emph{SIAM Journal on Applied Mathematics}, 44\penalty0 (1):\penalty0 96--110, 1984.

\bibitem[Defazio and Mishchenko(2023)]{defazio2023learning}
Aaron Defazio and Konstantin Mishchenko.
\newblock Learning-rate-free learning by d-adaptation.
\newblock \emph{arXiv preprint arXiv:2301.07733}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Hasani and Ward(2024)]{hasani2024generating}
Erisa Hasani and Rachel~A Ward.
\newblock Generating synthetic data for neural operators.
\newblock \emph{arXiv preprint arXiv:2401.02398}, 2024.

\bibitem[Hao et~al.(2024)Hao, Su, Liu, Berner, Ying, Su, Anandkumar, Song, and Zhu]{hao2024dpot}
Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, and Jun Zhu.
\newblock Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training.
\newblock \emph{arXiv preprint arXiv:2403.03542}, 2024.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition (CVPR)}, pages 248--255. IEEE, 2009.

\end{thebibliography}
