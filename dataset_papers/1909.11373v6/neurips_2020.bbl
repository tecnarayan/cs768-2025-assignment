\begin{thebibliography}{10}

\bibitem{SOP}
Che Wang, Yanqiu Wu, Quan Vuong, and Keith Ross.
\newblock Striving for simplicity and performance in off-policy drl: Output
  normalization and non-uniform sampling, 2020.

\bibitem{PPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em CoRR}, abs/1707.06347, 2017.

\bibitem{SAC}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em CoRR}, abs/1801.01290, 2018.

\bibitem{TRPO}
John Schulman, Sergey Levine, Philipp Moritz, Michael~I. Jordan, and Pieter
  Abbeel.
\newblock Trust region policy optimization.
\newblock {\em CoRR}, abs/1502.05477, 2015.

\bibitem{SPU}
Quan~Ho Vuong, Yiming Zhang, and Keith~W. Ross.
\newblock Supervised policy update.
\newblock {\em CoRR}, abs/1805.11706, 2018.

\bibitem{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2002.08396}, 2020.

\bibitem{agarwal2019optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1907.04543}, 2019.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11761--11771, 2019.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062, 2019.

\bibitem{chen2019bail}
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, Qing Deng, and Keith
  Ross.
\newblock Bail: Best-action imitation learning for batch deep reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1910.12179}, 2019.

\bibitem{rakelly2019efficient}
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock {\em arXiv preprint arXiv:1903.08254}, 2019.

\bibitem{chu2017cyclegan}
Casey Chu, Andrey Zhmoginov, and Mark Sandler.
\newblock Cyclegan, a master of steganography.
\newblock {\em arXiv preprint arXiv:1712.02950}, 2017.

\bibitem{zintgraf2020varibad}
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin
  Gal, Katja Hofmann, and Shimon Whiteson.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock In {\em International Conference on Learning Representation (ICLR)},
  2020.

\bibitem{rusu2015policy}
Andrei~A Rusu, Sergio~Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
  James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and
  Raia Hadsell.
\newblock Policy distillation.
\newblock {\em arXiv preprint arXiv:1511.06295}, 2015.

\bibitem{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4496--4506, 2017.

\bibitem{ghosh2017divide}
Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine.
\newblock Divide-and-conquer reinforcement learning.
\newblock {\em arXiv preprint arXiv:1711.09874}, 2017.

\bibitem{czarnecki2019distilling}
Wojciech~Marian Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant~M
  Jayakumar, Grzegorz Swirszcz, and Max Jaderberg.
\newblock Distilling policy distillation.
\newblock {\em arXiv preprint arXiv:1902.02186}, 2019.

\bibitem{ActorMimicParisotto2015}
Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock {\em CoRR}, abs/1511.06342, 2015.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{pearl_2009}
Judea Pearl.
\newblock {\em Causality}.
\newblock Cambridge University Press, 2009.

\bibitem{Peters2017}
J.~Peters, D.~Janzing, and B.~Sch\"olkopf.
\newblock {\em Elements of Causal Inference: Foundations and Learning
  Algorithms}.
\newblock MIT Press, Cambridge, MA, USA, 2017.

\bibitem{Bengio2020A}
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan~Rosemary Ke, Sebastien
  Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal.
\newblock A meta-transfer objective for learning to disentangle causal
  mechanisms.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{hermans2017defense}
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
\newblock In defense of the triplet loss for person re-identification.
\newblock {\em arXiv preprint arXiv:1703.07737}, 2017.

\bibitem{hessel2019multi}
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
  and Hado van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3796--3803, 2019.

\bibitem{impala2018}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem{fakoor2019meta}
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander~J Smola.
\newblock Meta-q-learning.
\newblock {\em arXiv preprint arXiv:1910.00125}, 2019.

\bibitem{de2019causal}
Pim de~Haan, Dinesh Jayaraman, and Sergey Levine.
\newblock Causal confusion in imitation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11693--11704, 2019.

\bibitem{xu2019can}
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon~S Du, Ken-ichi Kawarabayashi, and
  Stefanie Jegelka.
\newblock What can neural networks reason about?
\newblock {\em arXiv preprint arXiv:1905.13211}, 2019.

\bibitem{scarselli2008graph}
Franco Scarselli, Marco Gori, Ah~Chung Tsoi, Markus Hagenbuchner, and Gabriele
  Monfardini.
\newblock The graph neural network model.
\newblock {\em IEEE Transactions on Neural Networks}, 20(1):61--80, 2008.

\bibitem{wu2020comprehensive}
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S~Yu
  Philip.
\newblock A comprehensive survey on graph neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem{zhou2018graph}
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
  Changcheng Li, and Maosong Sun.
\newblock Graph neural networks: A review of methods and applications.
\newblock {\em arXiv preprint arXiv:1812.08434}, 2018.

\bibitem{pearl2010introduction}
Judea Pearl.
\newblock An introduction to causal inference.
\newblock {\em The international journal of biostatistics}, 6(2), 2010.

\bibitem{pearl2009causal}
Judea Pearl et~al.
\newblock Causal inference in statistics: An overview.
\newblock {\em Statistics surveys}, 3:96--146, 2009.

\bibitem{xie2019selftraining}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V. Le.
\newblock Self-training with noisy student improves imagenet classification,
  2019.

\bibitem{mobahi2020selfdistillation}
Hossein Mobahi, Mehrdad Farajtabar, and Peter~L. Bartlett.
\newblock Self-distillation amplifies regularization in hilbert space, 2020.

\bibitem{peng2019MCP}
Xue~Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine.
\newblock {MCP:} learning composable hierarchical control with multiplicative
  compositional policies.
\newblock {\em CoRR}, abs/1905.09808, 2019.

\bibitem{sharma2020emergent}
Archit Sharma, Michael Ahn, Sergey Levine, Vikash Kumar, Karol Hausman, and
  Shixiang Gu.
\newblock Emergent real-world robotic skills via unsupervised off-policy
  reinforcement learning, 2020.

\bibitem{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{metagenrl}
Louis Kirsch, Sjoerd van Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock Improving generalization in meta reinforcement learning using learned
  objectives.
\newblock {\em arXiv preprint arXiv:1910.04098}, 2019.

\bibitem{ddpg}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{TD3}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em CoRR}, abs/1802.09477, 2018.

\bibitem{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.04779}, 2020.

\bibitem{cabi2020ScalingDR}
Serkan Cabi, Sergio~G{\'o}mez Colmenarejo, Alexander Novikov, Ksenia
  Konyushkova, Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden,
  Mel Vecer{\'i}k, Oleg Sushkov, David J.~P. Barker, Jonathan Scholz, Misha
  Denil, Nando de~Freitas, and Ziyu Wang.
\newblock Scaling data-driven robotics with reward sketching and batch
  reinforcement learning.
\newblock {\em arXiv: Robotics}, 2020.

\bibitem{humplik2019meta}
Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro~A Ortega, Yee~Whye
  Teh, and Nicolas Heess.
\newblock Meta reinforcement learning as task inference.
\newblock {\em arXiv preprint arXiv:1905.06424}, 2019.

\bibitem{lan2019meta}
Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang.
\newblock Meta reinforcement learning with task embedding and shared policy.
\newblock {\em arXiv preprint arXiv:1905.06527}, 2019.

\bibitem{saemundsson2018meta}
Steind{\'o}r S{\ae}mundsson, Katja Hofmann, and Marc~Peter Deisenroth.
\newblock Meta reinforcement learning with latent variable gaussian processes.
\newblock {\em arXiv preprint arXiv:1803.07551}, 2018.

\bibitem{CAVIA}
Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon
  Whiteson.
\newblock Fast context adaptation via meta-learning.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  7693--7702, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem{roth2020revisiting}
Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjoern Ommer, and
  Joseph~Paul Cohen.
\newblock Revisiting training strategies and generalization performance in deep
  metric learning.
\newblock {\em arXiv preprint arXiv:2002.08473}, 2020.

\bibitem{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}, 2018.

\bibitem{yang2020multi}
Ruihan Yang, Huazhe Xu, Yi~Wu, and Xiaolong Wang.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock {\em arXiv preprint arXiv:2003.13661}, 2020.

\bibitem{yumulti}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Multi-task reinforcement learning without interference.

\bibitem{d2019sharing}
Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan
  Peters.
\newblock Sharing knowledge in multi-task deep reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{LearnToReinforceLearnWang2016}
Jane~X. Wang, Zeb Kurth{-}Nelson, Dhruva Tirumala, Hubert Soyer, Joel~Z. Leibo,
  R{\'{e}}mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick.
\newblock Learning to reinforcement learn.
\newblock {\em CoRR}, abs/1611.05763, 2016.

\bibitem{duan2016rl}
Yan Duan, John Schulman, Xi~Chen, Peter~L Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Rl{\textdollar}{\^{}}2{\textdollar}: Fast reinforcement learning via
  slow reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.02779}, 2016.

\bibitem{finn2017maml}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock {\em CoRR}, abs/1703.03400, 2017.

\bibitem{nichol2018Reptile}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock {\em CoRR}, abs/1803.02999, 2018.

\bibitem{houthooft2018evolvedpg}
Rein Houthooft, Richard~Y. Chen, Phillip Isola, Bradly~C. Stadie, Filip Wolski,
  Jonathan Ho, and Pieter Abbeel.
\newblock Evolved policy gradients.
\newblock {\em CoRR}, abs/1802.04821, 2018.

\bibitem{choi2019meta}
Kristy Choi, Mike Wu, Noah Goodman, and Stefano Ermon.
\newblock Meta-amortized variational inference and learning.
\newblock {\em arXiv preprint arXiv:1902.01950}, 2019.

\bibitem{geiger2012we}
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
\newblock Are we ready for autonomous driving? the kitti vision benchmark
  suite.
\newblock In {\em 2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 3354--3361. IEEE, 2012.

\bibitem{chua2018deep}
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4754--4765, 2018.

\bibitem{SpinningUp2018}
Joshua Achiam.
\newblock {Spinning Up in Deep Reinforcement Learning}.
\newblock 2018.

\end{thebibliography}
