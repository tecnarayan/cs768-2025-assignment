\begin{thebibliography}{10}

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em Advances in neural information processing systems}, pages
  6571--6583, 2018.

\bibitem{chen2019residual}
Tian~Qi Chen, Jens Behrmann, David~K Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9913--9923, 2019.

\bibitem{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock {\em arXiv preprint arXiv:1807.03819}, 2018.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{durkan2019neural}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7509--7520, 2019.

\bibitem{fakoor2020trade}
Rasool Fakoor, Pratik Chaudhari, Jonas Mueller, and Alexander~J Smola.
\newblock Trade: Transformers for density estimation.
\newblock {\em arXiv preprint arXiv:2004.02441}, 2020.

\bibitem{finlay2020train}
Chris Finlay, J{\"o}rn-Henrik Jacobsen, Levon Nurbekyan, and Adam~M Oberman.
\newblock How to train your neural ode: the world of jacobian and kinetic
  regularization.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{grathwohl2019ffjord}
Will Grathwohl, Ricky~TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David
  Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{ho2019flow++}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In {\em International Conference on Machine Learning}, pages
  2722--2730, 2019.

\bibitem{hoogeboom2019emerging}
Emiel Hoogeboom, Rianne Van Den~Berg, and Max Welling.
\newblock Emerging convolutions for generative normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  2771--2780, 2019.

\bibitem{ljspeech17}
Keith Ito.
\newblock The lj speech dataset.
\newblock \url{https://keithito.com/LJ-Speech-Dataset/}, 2017.

\bibitem{kim2019flowavenet}
Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon.
\newblock Flowavenet: A generative flow for raw audio.
\newblock In {\em International Conference on Machine Learning}, pages
  3370--3378, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2016improved}
Diederik~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4743--4751, 2016.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em International Conference on Learning Representations}, 2013.

\bibitem{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in neural information processing systems}, pages
  10215--10224, 2018.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{ma2019macow}
Xuezhe Ma, Xiang Kong, Shanghang Zhang, and Eduard Hovy.
\newblock Macow: Masked convolutional generative flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5891--5900, 2019.

\bibitem{oord2018parallel}
Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray
  Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg,
  et~al.
\newblock Parallel wavenet: Fast high-fidelity speech synthesis.
\newblock In {\em International Conference on Machine Learning}, pages
  3918--3926, 2018.

\bibitem{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2338--2347, 2017.

\bibitem{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In {\em International Conference on Machine Learning}, pages
  4055--4064, 2018.

\bibitem{ping2020waveflow}
Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song.
\newblock Waveflow: A compact flow-based model for raw audio.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{prenger2019waveglow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock Waveglow: A flow-based generative network for speech synthesis.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3617--3621. IEEE, 2019.

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  1530--1538, 2015.

\bibitem{song2019mintnet}
Yang Song, Chenlin Meng, and Stefano Ermon.
\newblock Mintnet: Building invertible neural networks with masked
  convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11004--11014, 2019.

\bibitem{van2016wavenet}
A{\"a}ron Van Den~Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew~W Senior, and Koray
  Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock In {\em SSW}, page 125, 2016.

\bibitem{van2016pixel}
Aaron Van~Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1747--1756, 2016.

\end{thebibliography}
