\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{http://tensorflow.org/}.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and Legenstein]{Bellec2017}
Bellec, G., Kappel, D., Maass, W., and Legenstein, R.~A.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Chen} et~al.(2019){Chen}, {Yang}, {Emer}, and
  {Sze}]{eyeriss_sparse_accelerator}
{Chen}, Y., {Yang}, T., {Emer}, J., and {Sze}, V.
\newblock Eyeriss v2: A flexible accelerator for emerging deep neural networks
  on mobile devices.
\newblock \emph{IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems}, 9\penalty0 (2):\penalty0 292--308, June 2019.
\newblock \doi{10.1109/JETCAS.2019.2910232}.

\bibitem[Cho et~al.(2014)Cho, {van Merrienboer}, Gulcehre, Bougares, Schwenk,
  and Bengio]{GRU}
Cho, K., {van Merrienboer}, B., Gulcehre, C., Bougares, F., Schwenk, H., and
  Bengio, Y.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2014)}, 2014.

\bibitem[Christos~Louizos(2018)]{Louizos2018}
Christos~Louizos, Max~Welling, D. P.~K.
\newblock Learning sparse neural networks through $l_0$ regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dai et~al.(2018)Dai, Zhu, Guo, and Wipf]{vib2018}
Dai, B., Zhu, C., Guo, B., and Wipf, D.
\newblock Compressing neural networks using the variational information
  bottleneck.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and Zettlemoyer]{dettmers2019}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.04840}.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{Draxler2018}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.~A.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International Conference on Machine Learning}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf}.

\bibitem[Elsen et~al.(2019)Elsen, Dukhan, Gale, and Simonyan]{elsen2019fast}
Elsen, E., Dukhan, M., Gale, T., and Simonyan, K.
\newblock Fast sparse convnets.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.09723}.

\bibitem[Evci(2018)]{evci2018}
Evci, U.
\newblock Detecting dead weights and units in neural networks.
\newblock \emph{ArXiv}, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.06068}.

\bibitem[Evci et~al.(2019)Evci, Pedregosa, Gomez, and Elsen]{Evci2019}
Evci, U., Pedregosa, F., Gomez, A.~N., and Elsen, E.
\newblock The difficulty of training sparse neural networks.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.10732}.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and Carbin]{frankle2019}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock The lottery ticket hypothesis at scale.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.01611}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.09574}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{Garipov2018}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{DynamicSurgery}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient {DNN}s.
\newblock \emph{ArXiv}, 2016.
\newblock URL \url{http://arxiv.org/abs/1608.04493}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Han et~al.(2016{\natexlab{a}})Han, Liu, Mao, Pu, Pedram, Horowitz, and
  Dally]{Han2016}
Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.~A., and Dally,
  W.~J.
\newblock {EIE}: Efficient {I}nference {E}ngine on compressed deep neural
  network.
\newblock In \emph{Proceedings of the 43rd International Symposium on Computer
  Architecture}, 2016{\natexlab{a}}.

\bibitem[Han et~al.(2016{\natexlab{b}})Han, Mao, and Dally]{deepcompression}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations},
  2016{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993}
Hassibi, B. and Stork, D.
\newblock {Second order derivatives for network pruning: Optimal Brain
  Surgeon}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1993.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the 2015 IEEE International Conference on
  Computer Vision (ICCV)}, 2015.

\bibitem[Hong et~al.(2019)Hong, Sukumaran-Rajam, Nisa, Singh, and
  Sadayappan]{AdaptiveSparseTilingGPU}
Hong, C., Sukumaran-Rajam, A., Nisa, I., Singh, K., and Sadayappan, P.
\newblock Adaptive sparse tiling for sparse matrix multiplication.
\newblock In \emph{Proceedings of the 24th Symposium on Principles and Practice
  of Parallel Programming}, 2019.
\newblock URL \url{http://doi.acm.org/10.1145/3293883.3295712}.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{mobilenetv1}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{ArXiv}, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.04861}.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, Oord, Dieleman, and
  Kavukcuoglu]{kalchbrenner2018}
Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart,
  E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K.
\newblock Efficient neural audio synthesis.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Krizhevsky(2009)]{cifar10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock In \emph{University of Toronto}, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{kusupati2020str}
Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.,
  and Farhadi, A.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock {Optimal Brain Damage}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{SNIP}
Lee, N., Ajanthan, T., and Torr, P. H.~S.
\newblock {SNIP:} {S}ingle-shot {N}etwork {P}runing based on {C}onnection
  {S}ensitivity.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu et~al.(2018)Liu, Bellec, Vogginger, Kappel, Partzsch, Neumaerker,
  H{\"o}ppner, Maass, Furber, Legenstein, and Mayr]{Liu2018MemoryEfficientDL}
Liu, C., Bellec, G., Vogginger, B., Kappel, D., Partzsch, J., Neumaerker, F.,
  H{\"o}ppner, S., Maass, W., Furber, S.~B., Legenstein, R.~A., and Mayr, C.
\newblock Memory-efficient deep learning on a spinnaker 2 prototype.
\newblock In \emph{Front. Neurosci.}, 2018.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{Liu2018}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{wikitext103}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{ArXiv}, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.07843}.

\bibitem[Merrill \& Garland(2016)Merrill and Garland]{Merrill_MergePath_SpMV}
Merrill, D. and Garland, M.
\newblock Merge-based sparse matrix-vector multiplication (spmv) using the csr
  storage format.
\newblock In \emph{Proceedings of the 21st ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, 2016.
\newblock URL \url{http://doi.acm.org/10.1145/2851141.2851190}.

\bibitem[Mike~Ashby(2019)]{myrtleunstructured}
Mike~Ashby, Christiaan~Baaij, P. B. M. B. O. B. A. C. C. C. L. C. S. D. N. v.
  D. J. F. G. H. B. H. D. P. J. S. S.~S.
\newblock Exploiting unstructured sparsity on next-generation datacenter
  hardware.
\newblock 2019.
\newblock URL
  \url{https://myrtle.ai/wp-content/uploads/2019/06/IEEEformatMyrtle.ai_.21.06.19_b.pdf}.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{Mocanu2018}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature Communications}, 2018.
\newblock URL \url{http://www.nature.com/articles/s41467-018-04316-3}.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{variational-dropout}
Molchanov, D., Ashukha, A., and Vetrov, D.~P.
\newblock Variational {D}ropout {S}parsifies {D}eep {N}eural {N}etworks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2498--2507, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{pruning-convnet-nvidia}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning {C}onvolutional {N}eural {N}etworks for {R}esource
  {E}fficient {T}ransfer {L}earning.
\newblock \emph{ArXiv}, 2016.
\newblock URL \url{https://arxiv.org/abs/1611.06440}.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{Mostafa2019}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/mostafa19a.html}.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989}
Mozer, M.~C. and Smolensky, P.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1989.

\bibitem[Narang et~al.(2017)Narang, Diamos, Sengupta, and
  Elsen]{exploring-sparsity-rnn}
Narang, S., Diamos, G., Sengupta, S., and Elsen, E.
\newblock Exploring sparsity in recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=BylSPv9gx}.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{sbp2017}
Neklyudov, K., Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Park et~al.(2016)Park, Li, Wen, Li, Chen, and
  Dubey]{SparseCNN_Intel_Park16}
Park, J., Li, S.~R., Wen, W., Li, H., Chen, Y., and Dubey, P.
\newblock Holistic {S}parse{CNN}: Forging the trident of accuracy, speed, and
  size.
\newblock \emph{ArXiv}, 2016.
\newblock URL \url{http://arxiv.org/abs/1608.01409}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 2015.

\bibitem[{Sandler} et~al.(2018){Sandler}, {Howard}, {Zhu}, {Zhmoginov}, and
  {Chen}]{mobilenetv2}
{Sandler}, M., {Howard}, A., {Zhu}, M., {Zhmoginov}, A., and {Chen}, L.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2018.

\bibitem[{Srinivas} et~al.(2017){Srinivas}, {Subramanya}, and
  {Babu}]{pruning_spike_slab_prior}
{Srinivas}, S., {Subramanya}, A., and {Babu}, R.~V.
\newblock Training sparse neural networks.
\newblock In \emph{2017 IEEE Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)}, 2017.

\bibitem[Str\"om(1997)]{sparse-connection-1997}
Str\"om, N.
\newblock Sparse {C}onnection and {P}runing in {L}arge {D}ynamic {A}rtificial
  {N}eural {N}etworks.
\newblock In \emph{EUROSPEECH}, 1997.

\bibitem[Sundar \& Dwaraknath(2021)Sundar and
  Dwaraknath]{sundar2021reproducibility}
Sundar, V. and Dwaraknath, R.~V.
\newblock [reproducibility report] rigging the lottery: Making all tickets
  winners.
\newblock \emph{arXiv}, 2021.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{labelsmooth}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition,}, 2016.
\newblock URL \url{http://arxiv.org/abs/1512.00567}.

\bibitem[Tartaglione et~al.(2018)Tartaglione, Leps{\o}y, Fiandrotti, and
  Francini]{SparsityDrivenRegularization}
Tartaglione, E., Leps{\o}y, S., Fiandrotti, A., and Francini, G.
\newblock Learning sparse neural networks via sensitivity-driven
  regularization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3327144.3327303}.

\bibitem[Thimm \& Fiesler(1995)Thimm and Fiesler]{Thimm95evaluatingpruning}
Thimm, G. and Fiesler, E.
\newblock Evaluating pruning methods.
\newblock In \emph{Proceedings of the International Symposium on Artificial
  Neural Networks}, 1995.

\bibitem[Wang et~al.(2018)Wang, Ji, Hong, Lyu, Wang, and
  Xie]{snnramsparsehardware}
Wang, P., Ji, Y., Hong, C., Lyu, Y., Wang, D., and Xie, Y.
\newblock Snrram: An efficient sparse neural network computation architecture
  based on resistive random-access memory.
\newblock In \emph{Proceedings of the 55th Annual Design Automation
  Conference}, 2018.
\newblock URL \url{http://doi.acm.org/10.1145/3195970.3196116}.

\bibitem[Wortsman et~al.(2019)Wortsman, Farhadi, and
  Rastegari]{wortsman2019dnw}
Wortsman, M., Farhadi, A., and Rastegari, M.
\newblock Discovering neural wirings.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.00586}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{wideresnet}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference}, 2016.
\newblock URL \url{http://www.bmva.org/bmvc/2016/papers/paper087/index.html}.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{deconstructing_lottery}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock \emph{ArXiv}, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.01067}.

\bibitem[Zhu \& Gupta(2018)Zhu and Gupta]{gupta2018}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression.
\newblock In \emph{International Conference on Learning Representations
  Workshop}, 2018.
\newblock URL \url{https://arxiv.org/abs/1710.01878}.

\end{thebibliography}
