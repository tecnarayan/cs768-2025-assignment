\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Adaptive Agent Team} et~al.(2023){Adaptive Agent Team}, Bauer,
  Baumli, Baveja, Behbahani, Bhoopchand, Bradley{-}Schmieg, Chang, Clay,
  Collister, Dasagi, Gonzalez, Gregor, Hughes, Kashem, Loks{-}Thompson,
  Openshaw, Parker{-}Holder, Pathak, Nieves, Rakicevic, Rockt{\"{a}}schel,
  Schroecker, Sygnowski, Tuyls, York, Zacherl, and Zhang]{bauer2023human}
{Adaptive Agent Team}, Bauer, J., Baumli, K., Baveja, S., Behbahani, F. M.~P.,
  Bhoopchand, A., Bradley{-}Schmieg, N., Chang, M., Clay, N., Collister, A.,
  Dasagi, V., Gonzalez, L., Gregor, K., Hughes, E., Kashem, S.,
  Loks{-}Thompson, M., Openshaw, H., Parker{-}Holder, J., Pathak, S., Nieves,
  N.~P., Rakicevic, N., Rockt{\"{a}}schel, T., Schroecker, Y., Sygnowski, J.,
  Tuyls, K., York, S., Zacherl, A., and Zhang, L.
\newblock Human-timescale adaptation in an open-ended task space.
\newblock \emph{CoRR}, abs/2301.07608, 2023.

\bibitem[Aji \& McEliece(2000)Aji and McEliece]{aji2000generalized}
Aji, S.~M. and McEliece, R.~J.
\newblock The generalized distributive law.
\newblock \emph{{IEEE} Trans. Inf. Theory}, 46\penalty0 (2):\penalty0 325--343,
  2000.

\bibitem[Bengio et~al.(1991)Bengio, Bengio, and Cloutier]{bengio1991learning}
Bengio, Y., Bengio, S., and Cloutier, J.
\newblock Learning a synaptic learning rule.
\newblock In \emph{IJCNN-91-Seattle International Joint Conference on Neural
  Networks}, 1991.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J.~G., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{{ACL} {(1)}}, pp.\  2978--2988. Association for
  Computational Linguistics, 2019.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Duan, Y., Schulman, J., Chen, X., Bartlett, P.~L., Sutskever, I., and Abbeel,
  P.
\newblock Rl{\textdollar}{\^{}}2{\textdollar}: Fast reinforcement learning via
  slow reinforcement learning.
\newblock \emph{CoRR}, abs/1611.02779, 2016.

\bibitem[Gershman \& Goodman(2014)Gershman and Goodman]{gershman2014amortized}
Gershman, S. and Goodman, N.~D.
\newblock Amortized inference in probabilistic reasoning.
\newblock In \emph{CogSci}. cognitivesciencesociety.org, 2014.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{{ICANN}}, volume 2130 of \emph{Lecture Notes in Computer
  Science}, pp.\  87--94. Springer, 2001.

\bibitem[Hutter(2005)]{hutter2005universal}
Hutter, M.
\newblock \emph{Universal Artificial Intelligence: Sequential Decisions Based
  on Algorithmic Probability}.
\newblock Springer, 2005.

\bibitem[Joulin \& Mikolov(2015)Joulin and Mikolov]{joulin2015inferring}
Joulin, A. and Mikolov, T.
\newblock Inferring algorithmic patterns with stack-augmented recurrent nets.
\newblock In \emph{{NIPS}}, pp.\  190--198, 2015.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl{-}Dickstein, and
  Metz]{kirsch2022general}
Kirsch, L., Harrison, J., Sohl{-}Dickstein, J., and Metz, L.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock \emph{CoRR}, abs/2212.04458, 2022.

\bibitem[Koolen \& de~Rooij(2008)Koolen and de~Rooij]{koolen2008combining}
Koolen, W.~M. and de~Rooij, S.
\newblock Combining expert advice efficiently.
\newblock In \emph{{COLT}}, pp.\  275--286. Omnipress, 2008.

\bibitem[Krichevsky \& Trofimov(1981)Krichevsky and
  Trofimov]{krichevsky1981performance}
Krichevsky, R.~E. and Trofimov, V.~K.
\newblock The performance of universal encoding.
\newblock \emph{{IEEE} Trans. Inf. Theory}, 27\penalty0 (2):\penalty0 199--206,
  1981.

\bibitem[Mikulik et~al.(2020)Mikulik, Del{\'{e}}tang, McGrath, Genewein,
  Martic, Legg, and Ortega]{mikulik2020meta}
Mikulik, V., Del{\'{e}}tang, G., McGrath, T., Genewein, T., Martic, M., Legg,
  S., and Ortega, P.~A.
\newblock Meta-trained agents implement bayes-optimal agents.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[M{\"{u}}ller et~al.(2022)M{\"{u}}ller, Hollmann, Pineda{-}Arango,
  Grabocka, and Hutter]{muller2021transformers}
M{\"{u}}ller, S., Hollmann, N., Pineda{-}Arango, S., Grabocka, J., and Hutter,
  F.
\newblock Transformers can do bayesian inference.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Ortega et~al.(2019)Ortega, Wang, Rowland, Genewein, Kurth{-}Nelson,
  Pascanu, Heess, Veness, Pritzel, Sprechmann, Jayakumar, McGrath, Miller,
  Azar, Osband, Rabinowitz, Gy{\"{o}}rgy, Chiappa, Osindero, Teh, van Hasselt,
  de~Freitas, Botvinick, and Legg]{ortega2019meta}
Ortega, P.~A., Wang, J.~X., Rowland, M., Genewein, T., Kurth{-}Nelson, Z.,
  Pascanu, R., Heess, N., Veness, J., Pritzel, A., Sprechmann, P., Jayakumar,
  S.~M., McGrath, T., Miller, K.~J., Azar, M.~G., Osband, I., Rabinowitz,
  N.~C., Gy{\"{o}}rgy, A., Chiappa, S., Osindero, S., Teh, Y.~W., van Hasselt,
  H., de~Freitas, N., Botvinick, M.~M., and Legg, S.
\newblock Meta-learning of sequential strategies.
\newblock \emph{CoRR}, abs/1905.03030, 2019.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{press2022train}
Press, O., Smith, N.~A., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth{-}Maron, Gimenez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi,
  Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and
  de~Freitas]{reed2022generalist}
Reed, S.~E., Zolna, K., Parisotto, E., Colmenarejo, S.~G., Novikov, A.,
  Barth{-}Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.~T.,
  Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell,
  R., Vinyals, O., Bordbar, M., and de~Freitas, N.
\newblock A generalist agent.
\newblock \emph{CoRR}, abs/2205.06175, 2022.

\bibitem[Ritchie et~al.(2016)Ritchie, Horsfall, and Goodman]{ritchie2016deep}
Ritchie, D., Horsfall, P., and Goodman, N.~D.
\newblock Deep amortized inference for probabilistic programs.
\newblock \emph{CoRR}, abs/1610.05735, 2016.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016meta}
Santoro, A., Bartunov, S., Botvinick, M.~M., Wierstra, D., and Lillicrap, T.~P.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{{ICML}}, volume~48 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pp.\  1842--1850. JMLR.org, 2016.

\bibitem[Schmidhuber et~al.(1996)Schmidhuber, Zhao, and
  Wiering]{schmidhuber1996simple}
Schmidhuber, J., Zhao, J., and Wiering, M.
\newblock Simple principles of metalearning.
\newblock Technical report, IDSIA, 1996.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun1998learning}
Thrun, S. and Pratt, L.~Y.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to Learn}, pp.\  3--17. Springer, 1998.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{{NIPS}}, pp.\  5998--6008, 2017.

\bibitem[Veness et~al.(2013)Veness, White, Bowling, and
  Gy{\"{o}}rgy]{veness2013partition}
Veness, J., White, M., Bowling, M., and Gy{\"{o}}rgy, A.
\newblock Partition tree weighting.
\newblock In \emph{{DCC}}, pp.\  321--330. {IEEE}, 2013.

\bibitem[Wang et~al.(2017)Wang, Kurth{-}Nelson, Soyer, Leibo, Tirumala, Munos,
  Blundell, Kumaran, and Botvinick]{wang2017learning}
Wang, J., Kurth{-}Nelson, Z., Soyer, H., Leibo, J.~Z., Tirumala, D., Munos, R.,
  Blundell, C., Kumaran, D., and Botvinick, M.~M.
\newblock Learning to reinforcement learn.
\newblock In \emph{CogSci}. cognitivesciencesociety.org, 2017.

\bibitem[Willems(1996)]{willems1996coding}
Willems, F. M.~J.
\newblock Coding for a binary independent piecewise-identically-distributed
  source.
\newblock \emph{{IEEE} Trans. Inf. Theory}, 42\penalty0 (6):\penalty0
  2210--2217, 1996.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022explanation}
Xie, S.~M., Raghunathan, A., Liang, P., and Ma, T.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2020varibad}
Zintgraf, L.~M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: {A} very good method for bayes-adaptive deep {RL} via
  meta-learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\end{thebibliography}
