@InProceedings{Defazio2021,
  author    = {Defazio, Aaron and Gower, Robert M.},
  booktitle = {Proceedings of The 13th Asian Conference on Machine Learning},
  title     = {The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization},
  year      = {2021},
  editor    = {Balasubramanian, Vineeth N. and Tsang, Ivor},
  month     = {17--19 Nov},
  pages     = {49--64},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {157},
  abstract  = {The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step-sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).},
  pdf       = {https://proceedings.mlr.press/v157/defazio21a/defazio21a.pdf},
  url       = {https://proceedings.mlr.press/v157/defazio21a.html},
}
@InProceedings{Chen2023,
  author    = {Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Hieu Pham and Xuanyi Dong and Thang Luong and Cho{-}Jui Hsieh and Yifeng Lu and Quoc V. Le},
  booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023},
  title     = {Symbolic Discovery of Optimization Algorithms},
  year      = {2023},
  editor    = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/ChenLHRW0DLHLL23.bib},
  timestamp = {Fri, 01 Mar 2024 16:26:20 +0100},
}
@InCollection{Ronneberger2015,
  author    = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  year      = {2015},
  pages     = {234--241},
  doi       = {10.1007/978-3-319-24574-4_28},
}
@InProceedings{Defazio2023,
  author    = {Defazio, Aaron and Mishchenko, Konstantin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Learning-Rate-Free Learning by {D}-Adaptation},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {7449--7479},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  pdf       = {https://proceedings.mlr.press/v202/defazio23a/defazio23a.pdf},
  url       = {https://proceedings.mlr.press/v202/defazio23a.html},
}
@article{Orabona2019,
  author       = {Francesco Orabona},
  title        = {A Modern Introduction to Online Learning},
  journal      = {CoRR},
  volume       = {abs/1912.13213},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.13213},
  eprinttype    = {arXiv},
  eprint       = {1912.13213},
}
@InProceedings{Dosovitskiy2021,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year      = {2021},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  groups    = {deep_learning},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
}
@Article{Chadha2021,
  author        = {Karan Chadha and Gary Cheng and John C. Duchi},
  title         = {Accelerated, Optimal, and Parallel: Some Results on Model-Based Stochastic Optimization},
  year          = {2021},
  month         = jan,
  archiveprefix = {arXiv},
  eprint        = {2101.02696},
  file          = {:http\://arxiv.org/pdf/2101.02696v1:PDF},
  keywords      = {math.OC, cs.LG, stat.ML},
  primaryclass  = {math.OC},
}

@InProceedings{Wang2023,
  author    = {Wang, Xiaoyu and Johansson, Mikael and Zhang, Tong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Generalized {P}olyak Step Size for First Order Optimization with Momentum},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {35836--35863},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {In machine learning applications, it is well known that carefully designed learning rate (step size) schedules can significantly improve the convergence of commonly used first-order optimization algorithms. Therefore how to set step size adaptively becomes an important research question. A popular and effective method is the Polyak step size, which sets step size adaptively for gradient descent or stochastic gradient descent without the need to estimate the smoothness parameter of the objective function. However, there has not been a principled way to generalize the Polyak step size for algorithms with momentum accelerations. This paper presents a general framework to set the learning rate adaptively for first-order optimization methods with momentum, motivated by the derivation of Polyak step size. It is shown that the resulting techniques are much less sensitive to the choice of momentum parameter and may avoid the oscillation of the heavy-ball method on ill-conditioned problems. These adaptive step sizes are further extended to the stochastic settings, which are attractive choices for stochastic gradient descent with momentum. Our methods are demonstrated to be more effective for stochastic gradient methods than prior adaptive step size algorithms in large-scale machine learning tasks.},
  pdf       = {https://proceedings.mlr.press/v202/wang23l/wang23l.pdf},
  url       = {https://proceedings.mlr.press/v202/wang23l.html},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L{}ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention is All you Need},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@Article{raffel2020exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {J Mach Learn Res},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year    = {2020},
  number  = {140},
  pages   = {1--67},
  volume  = {21},
  url     = {http://jmlr.org/papers/v21/20-074.html},
}


@misc{sharir2020cost,
      title={The Cost of Training {NLP} Models: A Concise Overview}, 
      author={Or Sharir and Barak Peleg and Yoav Shoham},
      year={2020},
      eprint={2004.08900},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@InProceedings{Ott2019,
  author    = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  year      = {2019},
}

@InProceedings{Schmidt2021,
  author    = {Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {9367--9376},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pdf       = {http://proceedings.mlr.press/v139/schmidt21a/schmidt21a.pdf},
  url       = {https://proceedings.mlr.press/v139/schmidt21a.html},
}

@Article{Sun2020,
  author    = {Ruo-Yu Sun},
  journal   = {J. Oper. Res. Soc. China},
  title     = {Optimization for Deep Learning: An Overview},
  year      = {2020},
  month     = {jun},
  number    = {2},
  pages     = {249--294},
  volume    = {8},
  doi       = {10.1007/s40305-020-00309-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{Polyak1987,
  author    = {Polyak, Boris T.},
  publisher = {Optimization Software, Inc., Publications Division, New York},
  title     = {Introduction to optimization},
  year      = {1987},
  isbn      = {0-911575-14-6},
  note      = {Translated from the Russian, With a foreword by Dimitri P. Bertsekas},
  series    = {Translations Series in Mathematics and Engineering},
  mrclass   = {49-01 (65Kxx 90Cxx)},
  mrnumber  = {1099605},
  pages     = {xxvii+438},
}

@misc{criteo-display-ad-challenge,
    author    = {Jean-Baptiste Tien and Olivier Chapelle},
    title = {Display Advertising Challenge},
    publisher = {Kaggle},
    year = {2014},
    url = {https://kaggle.com/competitions/criteo-display-ad-challenge}
}

@InProceedings{Zhang2019,
  author    = {Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger B. Grosse},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
  title     = {Three Mechanisms of Weight Decay Regularization},
  year      = {2019}
}

@inproceedings{Luo2019,
  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations},
  month = {May},
  year = {2019}
}



@InProceedings{Zhuang2020,
  author    = {Juntang Zhuang and Tommy Tang and Yifan Ding and Sekhar Tatikonda and Nicha C. Dvornek and Xenophon Papademetris and James S. Duncan},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
  title     = {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},
  year      = {2020},
  editor    = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/ZhuangTDTDPD20.bib},
  timestamp = {Mon, 02 May 2022 12:54:37 +0200},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html},
}



@misc{handbookgrad,
      title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods}, 
      author={Guillaume Garrigos and Robert M. Gower},
      year={2023},
      eprint={2301.11235},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@InProceedings{defazio2023learningratefree,
  author    = {Defazio, Aaron and Mishchenko, Konstantin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Learning-Rate-Free Learning by {D}-{A}daptation},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {7449--7479},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202}
}

@InProceedings{Chen2022,
  author    = {Chen, Keyi and Cutkosky, Ashok and Orabona, Francesco},
  booktitle = {Proceedings of The 33rd International Conference on Algorithmic Learning Theory},
  title     = {Implicit Parameter-free Online Learning with Truncated Linear Models},
  year      = {2022},
  editor    = {Dasgupta, Sanjoy and Haghtalab, Nika},
  month     = {29 Mar--01 Apr},
  pages     = {148--175},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {167},
  abstract  = {Parameter-free algorithms are online learning algorithms that do not require setting learning rates. They achieve optimal regret with respect to the distance between the initial point and any competitor. Yet, parameter-free algorithms do not take into account the geometry of the losses. Recently, in the stochastic optimization literature, it has been proposed to instead use truncated linear lower bounds, which produce better performance by more closely modeling the losses. In particular, truncated linear models greatly reduce the problem of overshooting the minimum of the loss function. Unfortunately, truncated linear models cannot be used with parameter-free algorithms because the updates become very expensive to compute. In this paper, we propose new parameter-free algorithms that can take advantage of truncated linear models through a new update that has an “implicit” flavor. Based on a novel decomposition of the regret, the new update is efficient, requires only one gradient at each step, never overshoots the minimum of the truncated model, and retains the favorable parameter-free properties. We also conduct an empirical study demonstrating the practical utility of our algorithms.},
  pdf       = {https://proceedings.mlr.press/v167/chen22a/chen22a.pdf},
  url       = {https://proceedings.mlr.press/v167/chen22a.html},
}


@InProceedings{Vaswani2019,
  author    = {Sharan Vaswani and Aaron Mishkin and Issam H. Laradji and Mark Schmidt and Gauthier Gidel and Simon Lacoste{-}Julien},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  title     = {Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates},
  year      = {2019},
  pages     = {3727--3740},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/VaswaniMLSGL19.bib},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html},
}


@InProceedings{Berrada2019,
  title = 	 {Training Neural Networks for and by Interpolation},
  author =       {Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {799--809},
  year = 	 {2020},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR}
}


@Article{Duchi2011,
  author   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal  = {J. Mach. Learn. Res.},
  title    = {Adaptive subgradient methods for online learning and stochastic optimization},
  year     = {2011},
  issn     = {1532-4435},
  pages    = {2121--2159},
  volume   = {12},
  fjournal = {Journal of Machine Learning Research (JMLR)},
  groups   = {stochastic_methods},
  mrclass  = {68T05 (62G08 90C15 90C25)},
  mrnumber = {2825422},
}

@Article{Paren2022,
  author  = {Alasdair Paren and Leonard Berrada and Rudra P. K. Poudel and M. Pawan Kumar},
  journal = {J Mach Learn Res},
  title   = {A Stochastic Bundle Method for Interpolation},
  year    = {2022},
  number  = {15},
  pages   = {1--57},
  volume  = {23},
  url     = {http://jmlr.org/papers/v23/20-1248.html},
}

@Article{Polyak1964,
  author   = {Boris T. Polyak},
  journal  = {USSR Computational Mathematics and Mathematical Physics},
  title    = {Some methods of speeding up the convergence of iteration methods},
  year     = {1964},
  issn     = {0041-5553},
  number   = {5},
  pages    = {1-17},
  volume   = {4}
}

@InProceedings{Loizou2021,
  author    = {Loizou, Nicolas and Vaswani, Sharan and Hadj Laradji, Issam and Lacoste-Julien, Simon},
  booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  title     = {Stochastic {P}olyak Step-size for {SGD}: An Adaptive Learning Rate for Fast Convergence},
  year      = {2021},
  editor    = {Banerjee, Arindam and Fukumizu, Kenji},
  month     = {13--15 Apr},
  pages     = {1306--1314},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  pdf       = {http://proceedings.mlr.press/v130/loizou21a/loizou21a.pdf},
  url       = {https://proceedings.mlr.press/v130/loizou21a.html},
}

@article{castera2ndorder2022,
  author    = {Camille Castera and
               J{\'{e}}r{\^{o}}me Bolte and
               C{\'{e}}dric F{\'{e}}votte and
               Edouard Pauwels},
  title     = {Second-order step-size tuning of {SGD} for non-convex optimization},
  journal   = {CoRR},
  volume    = {abs/2103.03570},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03570},
  eprinttype = {arXiv},
  eprint    = {2103.03570}
}

@incollection{combettes,
  TITLE = {{Proximal Splitting Methods in Signal Processing}},
  AUTHOR = {Combettes, Patrick Louis and Pesquet, Jean-Christophe},
  BOOKTITLE = {{Fixed-Point Algorithms for Inverse Problems in Science and Engineering}},
  EDITOR = {Bauschke and H.H.; Burachik and R.S.; Combettes and P.L.; Elser and V.; Luke and D.R.; Wolkowicz and H. (Eds.)},
  PUBLISHER = {{Springer}},
  PAGES = {185-212},
  YEAR = {2011}
}

@Article{Schaipp2023,
  author  = {Fabian Schaipp and Robert M. Gower and Michael Ulbrich},
  journal = {Transactions on Machine Learning Research},
  title   = {A Stochastic Proximal {P}olyak Step Size},
  year    = {2023},
  issn    = {2835-8856},
  note    = {Reproducibility Certification},
  groups  = {Own},
  url     = {https://openreview.net/forum?id=jWr41htaB3},
}

@Article{Asi2019,
  author     = {Asi, Hilal and Duchi, John C.},
  journal    = {SIAM J. Optim.},
  title      = {Stochastic (approximate) proximal point methods: convergence, optimality, and adaptivity},
  year       = {2019},
  issn       = {1052-6234},
  number     = {3},
  pages      = {2257--2290},
  volume     = {29},
  doi        = {10.1137/18M1230323},
  fjournal   = {SIAM Journal on Optimization},
  groups     = {prox_point},
  mrclass    = {90C15 (65K10 90C06 90C25)},
  mrnumber   = {4000228},
  mrreviewer = {Wim van Ackooij},
}

@Article{Davis2019,
  author     = {Davis, Damek and Drusvyatskiy, Dmitriy},
  journal    = {SIAM J. Optim.},
  title      = {Stochastic model-based minimization of weakly convex functions},
  year       = {2019},
  issn       = {1052-6234},
  number     = {1},
  pages      = {207--239},
  volume     = {29},
  doi        = {10.1137/18M1178244},
  fjournal   = {SIAM Journal on Optimization},
  groups     = {prox_point},
  mrclass    = {90C25 (65K05 65K10 90C15)},
  mrnumber   = {3902455},
  mrreviewer = {Wim van Ackooij},
}

@article{scalinglaws,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {CoRR},
  keywords = {llms lstm transformer},
  title = {Scaling Laws for Neural Language Models.},
  url = {https://arxiv.org/pdf/2001.08361.pdf},
  volume = {abs/2001.08361},
  year = 2020
}


@InProceedings{meng2022a,
  author    = {Meng, Si Yi and Gower, Robert M.},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {A Model-Based Method for Minimizing {CV}a{R} and Beyond},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {24436--24456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {We develop a variant of the stochastic prox-linear method for minimizing the Conditional Value-at-Risk (CVaR) objective. CVaR is a risk measure focused on minimizing worst-case performance, defined as the average of the top quantile of the losses. In machine learning, such a risk measure is useful to train more robust models. Although the stochastic subgradient method (SGM) is a natural choice for minimizing the CVaR objective, we show that our stochastic prox-linear (SPL+) algorithm can better exploit the structure of the objective, while still providing a convenient closed form update. Our SPL+ method also adapts to the scaling of the loss function, which allows for easier tuning. We then specialize a general convergence theorem for SPL+ to our setting, and show that it allows for a wider selection of step sizes compared to SGM. We support this theoretical finding experimentally.},
  pdf       = {https://proceedings.mlr.press/v202/meng23a/meng23a.pdf},
  url       = {https://proceedings.mlr.press/v202/meng23a.html},
}



@Article{Asi2019a,
  author     = {Asi, Hilal and Duchi, John C.},
  journal    = {Proc. Natl. Acad. Sci. USA},
  title      = {The importance of better models in stochastic optimization},
  year       = {2019},
  issn       = {0027-8424},
  number     = {46},
  pages      = {22924--22930},
  volume     = {116},
  doi        = {10.1073/pnas.1908018116},
  fjournal   = {Proceedings of the National Academy of Sciences of the United States of America},
  groups     = {prox_point},
  mrclass    = {90C15 (62G35)},
  mrnumber   = {4036122},
  mrreviewer = {Wim van Ackooij},
}

@Book{Beck2017,
  author     = {Beck, Amir},
  publisher  = {Society for Industrial and Applied Mathematics (SIAM), Philadelphia},
  title      = {First-order methods in optimization},
  year       = {2017},
  isbn       = {978-1-611974-98-0},
  series     = {MOS-SIAM Series on Optimization},
  volume     = {25},
  doi        = {10.1137/1.9781611974997.ch1},
  pages      = {xii+475},
}

@InProceedings{Krogh1991,
  author    = {Krogh, Anders and Hertz, John},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A Simple Weight Decay Can Improve Generalization},
  year      = {1991},
  editor    = {J. Moody and S. Hanson and R.P. Lippmann},
  publisher = {Morgan-Kaufmann},
  volume    = {4},
  url       = {https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},
}

@inproceedings{coin-betting,
 author = {Orabona, Francesco and Tommasi, Tatiana},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Training Deep Networks without Learning Rates Through Coin Betting},
 volume = {30},
 year = {2017}
}



@Article{LeCun2010,
  author       = {LeCun, Yann and Cortes, Corinna},
  title        = {{MNIST} handwritten digit database},
  year         = {2010},
  biburl       = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups       = {public, software_datasets},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  timestamp    = {2016-07-12T19:25:30.000+0200},
}

@InProceedings{Kingma2015,
  author    = {Diederik P. Kingma and Jimmy Ba},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  title     = {Adam: {A} Method for Stochastic Optimization},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  groups    = {stochastic_methods},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
}

@Article{Zhuang2022,
  author  = {Zhenxun Zhuang and Mingrui Liu and Ashok Cutkosky and Francesco Orabona},
  journal = {Transactions on Machine Learning Research},
  title   = {Understanding {A}dam{W} through Proximal Methods and Scale-Freeness},
  year    = {2022},
  url     = {https://openreview.net/forum?id=IKhEPWGdwK},
}

@InProceedings{Loshchilov2019,
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
  title     = {Decoupled Weight Decay Regularization},
  year      = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  url       = {https://openreview.net/forum?id=Bkg6RiCqY7},
}

@InProceedings{Bottou2010,
  author    = {Bottou, L\'{e}on},
  booktitle = {Proceedings of {COMPSTAT}'2010},
  title     = {Large-scale machine learning with stochastic gradient descent},
  year      = {2010},
  pages     = {177--186},
  publisher = {Physica-Verlag/Springer, Heidelberg},
  mrclass   = {68T05 (62L20)},
  mrnumber  = {3362066},
}

@Article{Robbins1951,
  author     = {Robbins, Herbert and Monro, Sutton},
  journal    = {Ann. Math. Statistics},
  title      = {A stochastic approximation method},
  year       = {1951},
  issn       = {0003-4851},
  pages      = {400--407},
  volume     = {22},
  doi        = {10.1214/aoms/1177729586},
  fjournal   = {Annals of Mathematical Statistics},
  groups     = {stochastic_methods},
  mrclass    = {62.0X},
  mrnumber   = {42668},
  mrreviewer = {R. P. Peterson},
}

@InProceedings{Ma2018,
  author    = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {The Power of Interpolation: Understanding the Effectiveness of {SGD} in Modern Over-parametrized Learning},
  year      = {2018},
  month     = {10--15 Jul},
  editor    = {Dy, Jennifer and Krause, Andreas},
  pages     = {3325--3334},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pdf       = {http://proceedings.mlr.press/v80/ma18a/ma18a.pdf},
  url       = {https://proceedings.mlr.press/v80/ma18a.html},
}

@InProceedings{Gower2021,
  author    = {Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  title     = {{SGD} for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation},
  year      = {2021},
  editor    = {Banerjee, Arindam and Fukumizu, Kenji},
  month     = {13--15 Apr},
  pages     = {1315--1323},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  pdf       = {http://proceedings.mlr.press/v130/gower21a/gower21a.pdf},
  url       = {https://proceedings.mlr.press/v130/gower21a.html},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90},
}

@InProceedings{Simonyan2015,
  author    = {Karen Simonyan and Andrew Zisserman},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  url       = {http://arxiv.org/abs/1409.1556},
}

@InProceedings{Ioffe2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Bach, Francis and Blei, David},
  month     = {07--09 Jul},
  pages     = {448--456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {https://proceedings.mlr.press/v37/ioffe15.html},
}

@InProceedings{Orvieto2022,
  author    = {Antonio Orvieto and Simon Lacoste{-}Julien and Nicolas Loizou},
  booktitle = {NeurIPS},
  title     = {Dynamics of {SGD} with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution},
  year      = {2022},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/OrvietoLL22.bib},
  timestamp = {Thu, 11 May 2023 17:08:21 +0200},
}
  %url       = {http://papers.nips.cc/paper\_files/paper/2022/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html},

@InProceedings{Sebbouh2021,
  author    = {Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
  booktitle = {Proceedings of Thirty Fourth Conference on Learning Theory},
  title     = {Almost sure convergence rates for Stochastic Gradient Descent and Stochastic Heavy Ball},
  year      = {2021},
  editor    = {Belkin, Mikhail and Kpotufe, Samory},
  month     = {15--19 Aug},
  pages     = {3935--3971},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {134},
  abstract  = {We study stochastic gradient descent (SGD) and the stochastic heavy ball method (SHB, otherwise known as the momentum method) for the general stochastic approximation problem.  For SGD, in the convex and smooth setting, we provide the first \emph{almost sure} asymptotic convergence \emph{rates} for a weighted average of the iterates . More precisely, we show that the convergence rate of the function values is arbitrarily close to $o(1/\sqrt{k})$, and is exactly $o(1/k)$ in the so-called overparametrized case. We show that these results still hold when using a decreasing step size version of stochastic line search and stochastic Polyak stepsizes, thereby giving the first proof of convergence of these methods in the non-overparametrized regime.  Using a substantially different analysis, we show that these rates hold for SHB as well, but at the last iterate. This distinction is important because it is the last iterate of SGD and SHB which is used in practice. We also show that the last iterate of SHB converges to a minimizer \emph{almost surely}. Additionally, we prove that the function values of the deterministic HB converge at a $o(1/k)$ rate, which is faster than the previously known $O(1/k)$.  Finally, in the nonconvex setting, we prove similar rates on the lowest gradient norm along the trajectory of SGD.},
  pdf       = {http://proceedings.mlr.press/v134/sebbouh21a/sebbouh21a.pdf},
  url       = {https://proceedings.mlr.press/v134/sebbouh21a.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
