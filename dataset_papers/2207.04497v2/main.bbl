\begin{thebibliography}{10}

\bibitem{barni2019clean}
Mauro Barni, Kassem Kallas, and Benedetta Tondi.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In {\em 2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 101--105. IEEE, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{chen2022efficient}
Jinghui Chen, Yu~Cheng, Zhe Gan, Quanquan Gu, and Jingjing Liu.
\newblock Efficient robust training via backward smoothing.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence
  (AAAI)}, 2022.

\bibitem{chen2020rays}
Jinghui Chen and Quanquan Gu.
\newblock Rays: A ray searching method for hard-label adversarial attack.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1739--1747, 2020.

\bibitem{chen2020frank}
Jinghui Chen, Dongruo Zhou, Jinfeng Yi, and Quanquan Gu.
\newblock A frank-wolfe framework for efficient and effective adversarial
  attacks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem{chen2017blend}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock {\em arXiv preprint arXiv:1712.05526}, 2017.

\bibitem{chen2021refit}
Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo~Li, and
  Dawn Song.
\newblock Refit: a unified watermark removal framework for deep learning
  systems with limited data.
\newblock In {\em Proceedings of the 2021 ACM Asia Conference on Computer and
  Communications Security}, pages 321--335, 2021.

\bibitem{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In {\em International conference on machine learning}, pages
  2206--2216. PMLR, 2020.

\bibitem{devlin2018APPbert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{diakonikolas2019sever}
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and
  Alistair Stewart.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1596--1606. PMLR, 2019.

\bibitem{feng2019learning}
Ji~Feng, Qi-Zhi Cai, and Zhi-Hua Zhou.
\newblock Learning to confuse: generating training time adversarial data with
  auto-encoder.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{gao2020generative}
Chao Gao, Yuan Yao, and Weizhi Zhu.
\newblock Generative adversarial nets for robust scatter estimation: A proper
  scoring rule perspective.
\newblock {\em J. Mach. Learn. Res.}, 21:160--1, 2020.

\bibitem{goodfellow2014explainingAE}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{gou2021knowledgedistill}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock {\em arXiv preprint arXiv:1708.06733}, 2017.

\bibitem{guo2019tabor}
Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song.
\newblock Tabor: A highly accurate approach to inspecting and restoring trojan
  backdoors in ai systems.
\newblock {\em arXiv preprint arXiv:1908.01763}, 2019.

\bibitem{he2016APPrecog}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hinton2012speech}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em IEEE Signal processing magazine}, 29(6):82--97, 2012.

\bibitem{Houben2013GTSRB}
Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and
  Christian Igel.
\newblock Detection of traffic signs in real-world images: The german traffic
  sign detection benchmark.
\newblock In {\em The 2013 international joint conference on neural networks
  (IJCNN)}, pages 1--8. Ieee, 2013.

\bibitem{huang2020metapoison}
W~Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.
\newblock Metapoison: Practical general-purpose clean-label data poisoning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12080--12091, 2020.

\bibitem{ilyas2018black}
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In {\em International Conference on Machine Learning}, pages
  2137--2146. PMLR, 2018.

\bibitem{ilyas2019featuresnotbugs}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem{jia2020certified}
Jinyuan Jia, Xiaoyu Cao, and Neil~Zhenqiang Gong.
\newblock Certified robustness of nearest neighbors against data poisoning
  attacks.
\newblock {\em arXiv preprint arXiv:2012.03765}, 2020.

\bibitem{krizhevsky2009cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{li2019L0L2inv}
Shaofeng Li, Benjamin Zi~Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, and
  Haojin Zhu.
\newblock Invisible backdoor attacks against deep neural networks.
\newblock {\em arXiv preprint arXiv:1909.02742}, 2019.

\bibitem{li2020NAD}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16463--16472, 2021.

\bibitem{lin2020composite}
Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang.
\newblock Composite backdoor attack for deep neural network by mixing existing
  benign features.
\newblock In {\em Proceedings of the 2020 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 113--131, 2020.

\bibitem{liu2018finepruning}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock In {\em International Symposium on Research in Attacks, Intrusions,
  and Defenses}, pages 273--294. Springer, 2018.

\bibitem{liu2017trojanatk}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock In {\em 25th Annual Network and Distributed System Security
  Symposium, {NDSS} 2018, San Diego, California, USA, February 18-221, 2018}.
  The Internet Society, 2018.

\bibitem{liu2020naturalatk}
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In {\em European Conference on Computer Vision}, pages 182--199.
  Springer, 2020.

\bibitem{ma2018shufflenetv2}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{ma2019nic}
Shiqing Ma and Yingqi Liu.
\newblock Nic: Detecting adversarial samples with neural network invariant
  checking.
\newblock In {\em Proceedings of the 26th network and distributed system
  security symposium (NDSS 2019)}, 2019.

\bibitem{madry2018resistAdAtk}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{nguyen2020dynamicatk}
Tuan~Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock {\em Advances in Neural Information Processing Systems},
  33:3454--3464, 2020.

\bibitem{nguyen2021wanet}
Tuan~Anh Nguyen and Anh~Tuan Tran.
\newblock Wanet - imperceptible warping-based backdoor attack.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{peng2022learnability}
Weiqi Peng and Jinghui Chen.
\newblock Learnability lock: Authorized learnability control through
  adversarial invertible transformations.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{qiu2021deepsweep}
Han Qiu, Yi~Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani
  Thuraisingham.
\newblock Deepsweep: An evaluation framework for mitigating dnn backdoor
  attacks using data augmentation.
\newblock In {\em Proceedings of the 2021 ACM Asia Conference on Computer and
  Communications Security}, pages 363--377, 2021.

\bibitem{howard2018mobilnetsv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem{shafahi2018clean}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
  Tudor Dumitras, and Tom Goldstein.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Mohammad~Amin Ghiasi, Zheng Xu, John Dickerson,
  Christoph Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{shokri2020bypassing}
Reza Shokri et~al.
\newblock Bypassing backdoor detection algorithms in deep learning.
\newblock In {\em 2020 IEEE European Symposium on Security and Privacy
  (EuroS\&P)}, pages 175--183. IEEE, 2020.

\bibitem{simonyan2014vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{steinhardt2017certified}
Jacob Steinhardt, Pang Wei~W Koh, and Percy~S Liang.
\newblock Certified defenses for data poisoning attacks.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{szegedy2013APPobject}
Christian Szegedy, Alexander Toshev, and Dumitru Erhan.
\newblock Deep neural networks for object detection.
\newblock {\em Advances in neural information processing systems}, 26, 2013.

\bibitem{tao2022better_sparse}
Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma,
  Pan Li, and Xiangyu Zhang.
\newblock Better trigger inversion optimization in backdoor scanning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13368--13378, 2022.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{turner2019clb}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock {\em arXiv preprint arXiv:1912.02771}, 2019.

\bibitem{wang2019NC}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723. IEEE, 2019.

\bibitem{wang2020practical}
Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang.
\newblock Practical detection of trojan neural networks: Data-limited and
  data-free cases.
\newblock In {\em European Conference on Computer Vision}, pages 222--238.
  Springer, 2020.

\bibitem{weber2020rab}
Maurice Weber, Xiaojun Xu, Bojan Karla{\v{s}}, Ce~Zhang, and Bo~Li.
\newblock Rab: Provable robustness against backdoor attacks.
\newblock {\em arXiv preprint arXiv:2003.08904}, 2020.

\bibitem{Wong2020Fast}
Eric Wong, Leslie Rice, and J.~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wu2021wider}
Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu.
\newblock Do wider neural networks really help adversarial robustness?
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{wu2021ANP}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{xu2020defending}
Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu~Zhao, and Xue Lin.
\newblock Defending against backdoor attack on deep neural networks.
\newblock {\em arXiv preprint arXiv:2002.12162}, 2020.

\bibitem{xu2021detecting}
Xiaojun Xu, Qi~Wang, Huichen Li, Nikita Borisov, Carl~A Gunter, and Bo~Li.
\newblock Detecting ai trojans using meta neural analysis.
\newblock In {\em 2021 IEEE Symposium on Security and Privacy (SP)}, pages
  103--120. IEEE, 2021.

\bibitem{zeng2021IBAU}
Yi~Zeng, Si~Chen, Won Park, Z~Morley Mao, Ming Jin, and Ruoxi Jia.
\newblock Adversarial unlearning of backdoors via implicit hypergradient.
\newblock {\em arXiv preprint arXiv:2110.03735}, 2021.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em International conference on machine learning}, pages
  7472--7482. PMLR, 2019.

\bibitem{zhang2020notkill}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In {\em International conference on machine learning}, pages
  11278--11287. PMLR, 2020.

\bibitem{zhao2019object}
Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu.
\newblock Object detection with deep learning: A review.
\newblock {\em IEEE transactions on neural networks and learning systems},
  30(11):3212--3232, 2019.

\end{thebibliography}
