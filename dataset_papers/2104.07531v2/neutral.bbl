\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ambrosio et~al.(2008)Ambrosio, Gigli, and
  Savar\'e]{ambrosio2008gradient}
L.~Ambrosio, N.~Gigli, and G.~Savar\'e.
\newblock \emph{Gradient Flows In Metric Spaces and in the Space of Probability
  Measures}.
\newblock Birkhäuser Basel, 2008.

\bibitem[Atkinson and Han(2012)]{atkinson2012spherical}
K.~Atkinson and W.~Han.
\newblock \emph{Spherical Harmonics and Approximations on the Unit Sphere: An
  Introduction}, volume 2044.
\newblock Springer, 01 2012.

\bibitem[Bach(2017{\natexlab{a}})]{bach2017breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (19):\penalty0 1--53, 2017{\natexlab{a}}.

\bibitem[Bach(2017{\natexlab{b}})]{bach2017onthe}
F.~Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (1):\penalty0 714–751,
  Jan. 2017{\natexlab{b}}.
\newblock ISSN 1532-4435.

\bibitem[Bakry et~al.(2014)Bakry, Gentil, and Ledoux]{bakry2014analysis}
D.~Bakry, I.~Gentil, and M.~Ledoux.
\newblock \emph{Analysis and Geometry of Markov Diffusion Operators}.
\newblock Grundlehren der mathematischen Wissenschaften. Springer International
  Publishing, 2014.
\newblock ISBN 978-3-319-00227-9.

\bibitem[Barp et~al.(2019)Barp, Briol, Duncan, Girolami, and
  Mackey]{barp2019minimum}
A.~Barp, F.-X. Briol, A.~Duncan, M.~Girolami, and L.~Mackey.
\newblock Minimum stein discrepancy estimators.
\newblock In \emph{Advances in Neural Information Processing Systems}. Curran
  Associates, Inc., 2019.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{The Journal of Machine Learning Research}, 3:\penalty0
  463--482, 2002.

\bibitem[Block et~al.(2020)Block, Mroueh, and Rakhlin]{block2020generative}
A.~Block, Y.~Mroueh, and A.~Rakhlin.
\newblock Generative modeling with denoising auto-encoders and langevin
  sampling.
\newblock \emph{arXiv preprint arXiv:2002.00107}, 2020.

\bibitem[Bortoli et~al.(2020)Bortoli, Durmus, Pereyra, and
  Vidal]{debortoli2020efficient}
V.~D. Bortoli, A.~Durmus, M.~Pereyra, and A.~F. Vidal.
\newblock Efficient stochastic optimisation by unadjusted langevin monte carlo.
  application to maximum marginal likelihood and empirical bayesian estimation,
  2020.

\bibitem[Borwein and Zhu(2005)]{borwein2005techniques}
J.~Borwein and Q.~Zhu.
\newblock \emph{Techniques of Variational Analysis}.
\newblock CMS Books in Mathematics. Springer-Verlag New York, 2005.

\bibitem[Bourgain and Lindenstrauss(1988)]{bourgain1988projection}
J.~Bourgain and J.~Lindenstrauss.
\newblock Projection bodies.
\newblock In \emph{Geometric Aspects of Functional Analysis}, pages 250--270.
  Springer, 1988.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{Advances in neural information processing systems}, pages
  3036--3046, 2018.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
L.~Chizat and F.~Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Y.~Cho and L.~K. Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 22}, pages
  342--350. Curran Associates, Inc., 2009.

\bibitem[Chwialkowski et~al.(2016)Chwialkowski, Strathmann, and
  Gretton]{chwialkowski2016gretton}
K.~Chwialkowski, H.~Strathmann, and A.~Gretton.
\newblock A kernel test of goodness of fit.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pages 2606--2615. PMLR,
  2016.

\bibitem[{Della Pietra} et~al.(1997){Della Pietra}, {Della Pietra}, and
  {Lafferty}]{pietra1997inducing}
S.~{Della Pietra}, V.~{Della Pietra}, and J.~{Lafferty}.
\newblock Inducing features of random fields.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 19\penalty0 (4):\penalty0 380--393, 1997.
\newblock \doi{10.1109/34.588021}.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Y.~Du and I.~Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Gabrié et~al.(2021)Gabrié, Vanden-Eijnden, and Rotskoff]{marylou}
M.~Gabrié, E.~Vanden-Eijnden, and G.~Rotskoff.
\newblock Adaptive monte carlo augmented with normalizing flows.
\newblock \emph{In preparation}, 2021.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2020neural}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock When do neural networks outperform kernel methods?, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2672--2680, 2014.

\bibitem[Gorham and Mackey(2015)]{gorham2015measuring}
J.~Gorham and L.~Mackey.
\newblock Measuring sample quality with stein's method.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~28, pages 226--234. Curran Associates, Inc., 2015.

\bibitem[Gorham and Mackey(2017)]{gorham2017measuring}
J.~Gorham and L.~Mackey.
\newblock Measuring sample quality with kernels.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pages 1292--1301. PMLR, 2017.

\bibitem[Grathwohl et~al.(2020)Grathwohl, Wang, Jacobsen, Duvenaud, and
  Zemel]{grathwohl2020learning}
W.~Grathwohl, K.-C. Wang, J.-H. Jacobsen, D.~Duvenaud, and R.~Zemel.
\newblock Learning the stein discrepancy for training and evaluating
  energy-based models without sampling.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 3732--3747, 2020.

\bibitem[Hyv{{\"a}}rinen(2005)]{hyvarinen05estimation}
A.~Hyv{{\"a}}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (24):\penalty0 695--709, 2005.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, pages 8571--8580. Curran Associates, Inc., 2018.

\bibitem[Kakade et~al.(2009)Kakade, Sridharan, and Tewari]{kakade2009onthe}
S.~M. Kakade, K.~Sridharan, and A.~Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~21, pages 793--800. Curran Associates, Inc., 2009.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kneser(1952)]{kneser1952surun}
H.~Kneser.
\newblock Sur un theoreme fondamentale de la theorie des jeux.
\newblock \emph{C. R. Acad. Sci. Paris}, 234:\penalty0 2418--2420, 1952.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
Y.~LeCun, S.~Chopra, R.~Hadsell, M.~Ranzato, and F.~Huang.
\newblock A tutorial on energy-based learning.
\newblock 2006.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, pages 2378--2386. Curran Associates, Inc., 2016.

\bibitem[Liu et~al.(2016)Liu, Lee, and Jordan]{liu2016akernelized}
Q.~Liu, J.~Lee, and M.~Jordan.
\newblock A kernelized stein discrepancy for goodness-of-fit tests.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48, pages 276--284, New York, New York, USA, 20--22 Jun
  2016. PMLR.

\bibitem[Malach et~al.(2021)Malach, Kamath, Abbe, and
  Srebro]{malach2021quantifying}
E.~Malach, P.~Kamath, E.~Abbe, and N.~Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent
  kernels.
\newblock \emph{arXiv preprint arXiv:2103.01210}, 2021.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
S.~Mei, A.~Montanari, and P.-M. Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2012foundations}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock The MIT Press, 2012.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015insearch}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Ongie et~al.(2019)Ongie, Willett, Soudry, and
  Srebro]{ongie2019function}
G.~Ongie, R.~Willett, D.~Soudry, and N.~Srebro.
\newblock A function space view of bounded norm infinite width relu nets: The
  multivariate case.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2020)}, 2019.

\bibitem[Posner(1975)]{posner_random}
E.~C. Posner.
\newblock Random coding strategies for minimum entropy.
\newblock \emph{IEEE Transations on Information Theory}, 21\penalty0
  (4):\penalty0 388--391, 1975.

\bibitem[Rahimi and Recht(2008)]{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In J.~C. Platt, D.~Koller, Y.~Singer, and S.~T. Roweis, editors,
  \emph{Advances in Neural Information Processing Systems 20}, pages
  1177--1184. Curran Associates, Inc., 2008.

\bibitem[Ranzato et~al.(2007)Ranzato, Poultney, Chopra,
  et~al.]{ranzato2007efficient}
M.~Ranzato, C.~Poultney, S.~Chopra, et~al.
\newblock Efficient learning of sparse representations with an energy-based
  model.
\newblock 2007.

\bibitem[Rotskoff and Vanden-Eijnden(2018)]{rotskoff2018neural}
G.~M. Rotskoff and E.~Vanden-Eijnden.
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock \emph{arXiv preprint arXiv:1805.00915}, 2018.

\bibitem[Ruelle(1969)]{ruelle1969statistical}
D.~Ruelle.
\newblock \emph{Statistical mechanics: Rigorous results}.
\newblock W.A. Benjamin, 1969.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{savarese2019infinite}
P.~Savarese, I.~Evron, D.~Soudry, and N.~Srebro.
\newblock How do infinite width bounded norm networks look in function space?
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Serfling(2009)]{serfling2009approximation}
R.~Serfling.
\newblock \emph{Approximation Theorems of Mathematical Statistics}, volume 162.
\newblock John Wiley \& Sons, 2009.

\bibitem[Singh et~al.(2018)Singh, Uppal, Li, Li, Zaheer, and
  Póczos]{singh2018nonparametric}
S.~Singh, A.~Uppal, B.~Li, C.-L. Li, M.~Zaheer, and B.~Póczos.
\newblock Nonparametric density estimation under adversarial losses, 2018.

\bibitem[Sirignano and Spiliopoulos(2019)]{sirignano2019mean}
J.~Sirignano and K.~Spiliopoulos.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock \emph{Stochastic Processes and their Applications}, 2019.

\bibitem[Song and Ermon(2019)]{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{arXiv preprint arXiv:1907.05600}, 2019.

\bibitem[Song and Kingma(2021)]{song2021train}
Y.~Song and D.~P. Kingma.
\newblock How to train your energy-based models, 2021.

\bibitem[Stein(1972)]{stein1972abound}
C.~Stein.
\newblock A bound for the error in the normal approximation to the distribution
  of a sum of dependent random variables.
\newblock In \emph{Proceedings of the Sixth Berkeley Symposium on Mathematical
  Statistics and Probability, Volume 2: Probability Theory}, pages 583--602,
  1972.

\bibitem[Swendsen and Wang(1986)]{PhysRevLett.57.2607}
R.~H. Swendsen and J.-S. Wang.
\newblock Replica monte carlo simulation of spin-glasses.
\newblock \emph{Phys. Rev. Lett.}, 57:\penalty0 2607--2609, Nov 1986.
\newblock \doi{10.1103/PhysRevLett.57.2607}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.57.2607}.

\bibitem[Tsybakov(2008)]{tsybakov2008introduction}
A.~B. Tsybakov.
\newblock \emph{Introduction to nonparametric estimation}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[von Luxburg and Bousquet(2004)]{von2004distance}
U.~von Luxburg and O.~Bousquet.
\newblock Distance-based classification with lipschitz functions.
\newblock \emph{J. Mach. Learn. Res.}, 5:\penalty0 669--695, 2004.

\bibitem[Wainwright and Jordan(2008)]{wainwright2008graphical}
M.~Wainwright and M.~Jordan.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Foundations and Trends in Machine Learning}, 1:\penalty0
  1--305, 01 2008.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
C.~Wei, J.~D. Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wei et~al.(2020)Wei, Lee, Liu, and Ma]{wei2020regularization}
C.~Wei, J.~D. Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel, 2020.

\bibitem[Williams et~al.(2019)Williams, Trager, Silva, Panozzo, Zorin, and
  Bruna]{williams2019gradient}
F.~Williams, M.~Trager, C.~Silva, D.~Panozzo, D.~Zorin, and J.~Bruna.
\newblock Gradient dynamics of shallow univariate relu networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
B.~Woodworth, S.~Gunasekar, J.~D. Lee, E.~Moroshko, P.~Savarese, I.~Golan,
  D.~Soudry, and N.~Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, 2020.

\end{thebibliography}
