\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GWB{\etalchar{+}}17}

\bibitem[ACH18]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In {\em 35th International Conference on Machine Learning, ICML
  2018}, pages 372--389. International Machine Learning Society (IMLS), 2018.

\bibitem[ADH{\etalchar{+}}19]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[AGCH19]{arora2019convergence}
Sanjeev Arora, Noah Golowich, Nadav Cohen, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In {\em 7th International Conference on Learning Representations,
  ICLR 2019}, 2019.

\bibitem[AZLL19]{NEURIPS2019_62dad6e2}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[AZLS19]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[BDR21]{bhargava2021generalization}
Prajjwal Bhargava, Aleksandr Drozd, and Anna Rogers.
\newblock Generalization in nli: Ways (not) to go beyond simple heuristics,
  2021.

\bibitem[BMR{\etalchar{+}}20]{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem[CGMR20]{chou2020gradient}
Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut.
\newblock Gradient descent for deep matrix factorization: Dynamics and implicit
  bias towards low rank.
\newblock {\em arXiv preprint arXiv:2011.13772}, 2020.

\bibitem[CKL{\etalchar{+}}21]{DBLP:conf/iclr/CohenKLKT21}
Jeremy~M. Cohen, Simran Kaur, Yuanzhi Li, J.~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[CZT{\etalchar{+}}20]{DBLP:conf/ijcai/ChenZTYCG20}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In Christian Bessiere, editor, {\em Proceedings of the Twenty-Ninth
  International Joint Conference on Artificial Intelligence, {IJCAI} 2020},
  pages 3267--3275. ijcai.org, 2020.

\bibitem[DBBU20]{defossez2020simple}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of adam and adagrad.
\newblock {\em arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[DCLT19]{DBLP:conf/naacl/DevlinCLT19}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pages 4171--4186. Association for Computational
  Linguistics, 2019.

\bibitem[DHS11]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem[DZPS18]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[EFSS16]{W16-3210}
Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia.
\newblock Multi30k: Multilingual english-german image descriptions.
\newblock In {\em Proceedings of the 5th Workshop on Vision and Language},
  pages 70--74. Association for Computational Linguistics, 2016.

\bibitem[ENV21]{DBLP:conf/aaai/EneNV21}
Alina Ene, Huy~L. Nguyen, and Adrian Vladu.
\newblock Adaptive gradient methods for constrained convex optimization and
  variational inequalities.
\newblock In {\em Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial
  Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in
  Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021},
  pages 7314--7321. {AAAI} Press, 2021.

\bibitem[FKMN21]{DBLP:conf/iclr/ForetKMN21}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[GWB{\etalchar{+}}17]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem[JT20]{DBLP:conf/iclr/JiT20}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Kaw16]{NIPS2016_f2fc9902}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem[KB15]{DBLP:journals/corr/KingmaB14}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem[LBD{\etalchar{+}}20]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock {\em arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Ler19]{lerasle2019lecture}
Matthieu Lerasle.
\newblock Lecture notes: Selected topics on robust statistical learning theory.
\newblock {\em arXiv preprint arXiv:1908.10761}, 2019.

\bibitem[LMZ18]{pmlr-v75-li18a}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In SÃ©bastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75 of {\em Proceedings of Machine Learning Research}, pages 2--47.
  PMLR, 06--09 Jul 2018.

\bibitem[LZB22]{liu2022loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock {\em Applied and Computational Harmonic Analysis}, 2022.

\bibitem[MDP{\etalchar{+}}11]{maas-EtAl:2011:ACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

\bibitem[MXBS17]{DBLP:conf/iclr/MerityX0S17}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[RKK18]{DBLP:conf/iclr/ReddiKK18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem[TCG21]{pmlr-v139-tian21a}
Yuandong Tian, Xinlei Chen, and Surya Ganguli.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 10268--10278. PMLR, 18--24 Jul 2021.

\bibitem[TCLT19]{turc2019}
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Well-read students learn better: On the importance of pre-training
  compact models.
\newblock {\em arXiv preprint arXiv:1908.08962v2}, 2019.

\bibitem[TH{\etalchar{+}}12]{tieleman2012lecture}
Tijmen Tieleman, Geoffrey Hinton, et~al.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2):26--31,
  2012.

\bibitem[VSP{\etalchar{+}}17]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem[WWB20]{ward2020adagrad}
Rachel Ward, Xiaoxia Wu, and L{\'e}on Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock {\em Journal of Machine Learning Research}, 21(219):1--30, 2020.

\end{thebibliography}
