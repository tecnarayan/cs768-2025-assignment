\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2017)Bahdanau, Brakel, Xu, Goyal, Lowe, Pineau,
  Courville, and Bengio]{bahdanau2016actor}
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville,
  A., and Bengio, Y.
\newblock An actor-critic algorithm for sequence prediction.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Cohen \& Beck(2019)Cohen and Beck]{cohen2019empirical}
Cohen, E. and Beck, C.
\newblock Empirical analysis of beam search performance degradation in neural
  sequence models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1290--1299, 2019.

\bibitem[Covington et~al.(2016)Covington, Adams, and Sargin]{covington2016deep}
Covington, P., Adams, J., and Sargin, E.
\newblock Deep neural networks for youtube recommendations.
\newblock In \emph{Proceedings of the 10th ACM conference on recommender
  systems}, pp.\  191--198, 2016.

\bibitem[Daum{\'e}~III \& Marcu(2005)Daum{\'e}~III and
  Marcu]{daume2005learning}
Daum{\'e}~III, H. and Marcu, D.
\newblock Learning as search optimization: Approximate large margin methods for
  structured prediction.
\newblock In \emph{International Conference on Machine learning}, pp.\
  169--176. ACM, 2005.

\bibitem[Goyal et~al.(2018)Goyal, Neubig, Dyer, and
  Berg-Kirkpatrick]{goyal2018continuous}
Goyal, K., Neubig, G., Dyer, C., and Berg-Kirkpatrick, T.
\newblock A continuous relaxation of beam search for end-to-end training of
  neural sequence models.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[He \& McAuley(2016)He and McAuley]{he2016ups}
He, R. and McAuley, J.
\newblock Ups and downs: Modeling the visual evolution of fashion trends with
  one-class collaborative filtering.
\newblock In \emph{proceedings of the 25th international conference on world
  wide web}, pp.\  507--517, 2016.

\bibitem[Jain et~al.(2016)Jain, Prabhu, and Varma]{jain2016extreme}
Jain, H., Prabhu, Y., and Varma, M.
\newblock Extreme multi-label loss functions for recommendation, tagging,
  ranking \& other missing label applications.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  935--944, 2016.

\bibitem[Jasinska et~al.(2016)Jasinska, Dembczynski, Busa-Fekete, Pfannschmidt,
  Klerx, and Hullermeier]{jasinska2016extreme}
Jasinska, K., Dembczynski, K., Busa-Fekete, R., Pfannschmidt, K., Klerx, T.,
  and Hullermeier, E.
\newblock Extreme f-measure maximization using sparse probability estimates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1435--1444, 2016.

\bibitem[Khandagale et~al.(2019)Khandagale, Xiao, and
  Babbar]{khandagale2019bonsai}
Khandagale, S., Xiao, H., and Babbar, R.
\newblock Bonsai-diverse and shallow trees for extreme multi-label
  classification.
\newblock \emph{arXiv preprint arXiv:1904.08249}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kool et~al.(2019)Kool, Van~Hoof, and Welling]{kool2019stochastic}
Kool, W., Van~Hoof, H., and Welling, M.
\newblock Stochastic beams and where to find them: The gumbel-top-k trick for
  sampling sequences without replacement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3499--3508, 2019.

\bibitem[Lapin et~al.(2017)Lapin, Hein, and Schiele]{lapin2017analysis}
Lapin, M., Hein, M., and Schiele, B.
\newblock Analysis and optimization of loss functions for multiclass, top-k,
  and multilabel classification.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (7):\penalty0 1533--1554, 2017.

\bibitem[McAuley et~al.(2015)McAuley, Targett, Shi, and Van
  Den~Hengel]{mcauley2015image}
McAuley, J., Targett, C., Shi, Q., and Van Den~Hengel, A.
\newblock Image-based recommendations on styles and substitutes.
\newblock In \emph{Proceedings of the 38th International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pp.\  43--52, 2015.

\bibitem[Menon et~al.(2019)Menon, Rawat, Reddi, and Kumar]{menon2019multilabel}
Menon, A.~K., Rawat, A.~S., Reddi, S., and Kumar, S.
\newblock Multilabel reductions: what is my loss optimising?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10599--10610, 2019.

\bibitem[Morin \& Bengio(2005)Morin and Bengio]{morin2005hierarchical}
Morin, F. and Bengio, Y.
\newblock Hierarchical probabilistic neural network language model.
\newblock In \emph{Proceedings of the eighth international conference on
  artificial intelligence and statistics}, volume~5, pp.\  246--252. Citeseer,
  2005.

\bibitem[Negrinho et~al.(2018)Negrinho, Gormley, and
  Gordon]{negrinho2018learning}
Negrinho, R., Gormley, M., and Gordon, G.~J.
\newblock Learning beam search policies via imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10652--10661, 2018.

\bibitem[Prabhu \& Varma(2014)Prabhu and Varma]{prabhu2014fastxml}
Prabhu, Y. and Varma, M.
\newblock Fastxml: A fast, accurate and stable tree-classifier for extreme
  multi-label learning.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  263--272, 2014.

\bibitem[Prabhu et~al.(2018)Prabhu, Kag, Harsola, Agrawal, and
  Varma]{prabhu2018parabel}
Prabhu, Y., Kag, A., Harsola, S., Agrawal, R., and Varma, M.
\newblock Parabel: Partitioned label trees for extreme classification with
  application to dynamic search advertising.
\newblock In \emph{Proceedings of the 2018 World Wide Web Conference}, pp.\
  993--1002. International World Wide Web Conferences Steering Committee, 2018.

\bibitem[Ranzato et~al.(2016)Ranzato, Chopra, Auli, and
  Zaremba]{ranzato2015sequence}
Ranzato, M., Chopra, S., Auli, M., and Zaremba, W.
\newblock Sequence level training with recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  627--635, 2011.

\bibitem[Sarwar et~al.(2001)Sarwar, Karypis, Konstan, and
  Riedl]{sarwar2001item}
Sarwar, B., Karypis, G., Konstan, J., and Riedl, J.
\newblock Item-based collaborative filtering recommendation algorithms.
\newblock In \emph{Proceedings of the 10th international conference on World
  Wide Web}, pp.\  285--295, 2001.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Wiseman \& Rush(2016)Wiseman and Rush]{wiseman2016sequence}
Wiseman, S. and Rush, A.~M.
\newblock Sequence-to-sequence learning as beam-search optimization.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1296--1306, 2016.

\bibitem[Wu \& Zhou(2017)Wu and Zhou]{wu2017unified}
Wu, X.-Z. and Zhou, Z.-H.
\newblock A unified view of multi-label performance measures.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3780--3788. JMLR. org, 2017.

\bibitem[Wydmuch et~al.(2018)Wydmuch, Jasinska, Kuznetsov, Busa-Fekete, and
  Dembczynski]{wydmuch2018no}
Wydmuch, M., Jasinska, K., Kuznetsov, M., Busa-Fekete, R., and Dembczynski, K.
\newblock A no-regret generalization of hierarchical softmax to extreme
  multi-label classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6355--6366, 2018.

\bibitem[Xu \& Fern(2007)Xu and Fern]{xu2007learning}
Xu, Y. and Fern, A.
\newblock On learning linear ranking functions for beam search.
\newblock In \emph{International Conference on Machine learning}, pp.\
  1047--1054, 2007.

\bibitem[Yang \& Koyejo(2019)Yang and Koyejo]{yang2019consistency}
Yang, F. and Koyejo, S.
\newblock On the consistency of top-k surrogate losses.
\newblock \emph{arXiv preprint arXiv:1901.11141}, 2019.

\bibitem[You et~al.(2019)You, Zhang, Wang, Dai, Mamitsuka, and
  Zhu]{you2019attentionxml}
You, R., Zhang, Z., Wang, Z., Dai, S., Mamitsuka, H., and Zhu, S.
\newblock Attentionxml: Label tree-based attention-aware deep model for
  high-performance extreme multi-label text classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5812--5822, 2019.

\bibitem[Zhu et~al.(2018)Zhu, Li, Zhang, Li, He, Li, and Gai]{zhu2018learning}
Zhu, H., Li, X., Zhang, P., Li, G., He, J., Li, H., and Gai, K.
\newblock Learning tree-based deep model for recommender systems.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1079--1088. ACM, 2018.

\bibitem[Zhu et~al.(2019)Zhu, Chang, Xu, Zhang, Li, He, Li, Xu, and
  Gai]{zhu2019joint}
Zhu, H., Chang, D., Xu, Z., Zhang, P., Li, X., He, J., Li, H., Xu, J., and Gai,
  K.
\newblock Joint optimization of tree-based index and deep model for recommender
  systems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3973--3982, 2019.

\end{thebibliography}
