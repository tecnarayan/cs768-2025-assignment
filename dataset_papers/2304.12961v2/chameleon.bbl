\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreina et~al.(2021)Andreina, Marson, M{\"o}llering, and
  Karame]{andreina2021baffle}
Andreina, S., Marson, G.~A., M{\"o}llering, H., and Karame, G.
\newblock Baffle: Backdoor detection via feedback-based federated learning.
\newblock In \emph{2021 IEEE 41st International Conference on Distributed
  Computing Systems (ICDCS)}, pp.\  852--863. IEEE, 2021.

\bibitem[Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov]{bagdasaryan01}
Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., and Shmatikov, V.
\newblock How to backdoor federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2938--2948. PMLR, 2020.

\bibitem[Bhagoji et~al.(2019)Bhagoji, Chakraborty, Mittal, and
  Calo]{bhagoji2019analyzing}
Bhagoji, A.~N., Chakraborty, S., Mittal, P., and Calo, S.
\newblock Analyzing federated learning through an adversarial lens.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  634--643. PMLR, 2019.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{blanchard2017machine}
Blanchard, P., El~Mhamdi, E.~M., Guerraoui, R., and Stainer, J.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Chen et~al.(2017{\natexlab{a}})Chen, Liu, Li, Lu, and
  Song]{chen2017targeted}
Chen, X., Liu, C., Li, B., Lu, K., and Song, D.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017{\natexlab{a}}.

\bibitem[Chen et~al.(2017{\natexlab{b}})Chen, Su, and Xu]{chen2017distributed}
Chen, Y., Su, L., and Xu, J.
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 1\penalty0 (2):\penalty0 1--25, 2017{\natexlab{b}}.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Cohen, G., Afshar, S., Tapson, J., and Van~Schaik, A.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In \emph{2017 international joint conference on neural networks
  (IJCNN)}, pp.\  2921--2926. IEEE, 2017.

\bibitem[Fang et~al.(2020)Fang, Cao, Jia, and Gong]{fang2020local}
Fang, M., Cao, X., Jia, J., and Gong, N.~Z.
\newblock Local model poisoning attacks to byzantine-robust federated learning.
\newblock In \emph{Proceedings of the 29th USENIX Conference on Security
  Symposium}, pp.\  1623--1640, 2020.

\bibitem[Fung et~al.(2020)Fung, Yoon, and Beschastnikh]{fung2020limitations}
Fung, C., Yoon, C.~J., and Beschastnikh, I.
\newblock The limitations of federated learning in sybil settings.
\newblock In \emph{RAID}, pp.\  301--316, 2020.

\bibitem[Geiping et~al.(2020)Geiping, Bauermeister, Dr{\"o}ge, and
  Moeller]{geiping2020inverting}
Geiping, J., Bauermeister, H., Dr{\"o}ge, H., and Moeller, M.
\newblock Inverting gradients-how easy is it to break privacy in federated
  learning?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16937--16947, 2020.

\bibitem[Gu et~al.(2019)Gu, Liu, Dolan-Gavitt, and Garg]{gu2019badnets}
Gu, T., Liu, K., Dolan-Gavitt, B., and Garg, S.
\newblock Badnets: Evaluating backdooring attacks on deep neural networks.
\newblock \emph{IEEE Access}, 7:\penalty0 47230--47244, 2019.

\bibitem[Guerraoui et~al.(2018)Guerraoui, Rouault, et~al.]{guerraoui2018hidden}
Guerraoui, R., Rouault, S., et~al.
\newblock The hidden vulnerability of distributed learning in byzantium.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3521--3530. PMLR, 2018.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  9729--9738, 2020.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
  A., Liu, C., and Krishnan, D.
\newblock Supervised contrastive learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18661--18673, 2020.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.]{james01}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020.

\bibitem[Liu et~al.(2018)Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2017trojaning}
Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., and Zhang, X.
\newblock Trojaning attack on neural networks.
\newblock In \emph{25th Annual Network And Distributed System Security
  Symposium (NDSS 2018)}. Internet Soc, 2018.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{liu2020reflection}
Liu, Y., Ma, X., Bailey, J., and Lu, F.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part X 16}, pp.\  182--199.
  Springer, 2020.

\bibitem[Lyu et~al.(2022)Lyu, Yu, Ma, Chen, Sun, Zhao, Yang, and
  Philip]{lyu2022privacy}
Lyu, L., Yu, H., Ma, X., Chen, C., Sun, L., Zhao, J., Yang, Q., and Philip,
  S.~Y.
\newblock Privacy and robustness in federated learning: Attacks and defenses.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  2022.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Naseri et~al.(2022)Naseri, Hayes, and De~Cristofaro]{naseri2022local}
Naseri, M., Hayes, J., and De~Cristofaro, E.
\newblock Local and central differential privacy for robustness and privacy in
  federated learning.
\newblock In \emph{NDSS}, 2022.

\bibitem[Nasr et~al.(2019)Nasr, Shokri, and Houmansadr]{nasr2019comprehensive}
Nasr, M., Shokri, R., and Houmansadr, A.
\newblock Comprehensive privacy analysis of deep learning: Passive and active
  white-box inference attacks against centralized and federated learning.
\newblock In \emph{2019 IEEE symposium on security and privacy (SP)}, pp.\
  739--753. IEEE, 2019.

\bibitem[Nguyen et~al.(2022)Nguyen, Rieger, Chen, Yalame, M{\"o}llering,
  Fereidooni, Marchal, Miettinen, Mirhoseini, Zeitouni,
  et~al.]{nguyen2022flame}
Nguyen, T.~D., Rieger, P., Chen, H., Yalame, H., M{\"o}llering, H., Fereidooni,
  H., Marchal, S., Miettinen, M., Mirhoseini, A., Zeitouni, S., et~al.
\newblock $\{$FLAME$\}$: Taming backdoors in federated learning.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pp.\
  1415--1432, 2022.

\bibitem[Rieger et~al.(2022{\natexlab{a}})Rieger, Krau{\ss}, Miettinen,
  Dmitrienko, and Sadeghi]{rieger2022close}
Rieger, P., Krau{\ss}, T., Miettinen, M., Dmitrienko, A., and Sadeghi, A.-R.
\newblock Close the gate: Detecting backdoored models in federated learning
  based on client-side deep layer output analysis.
\newblock \emph{arXiv preprint arXiv:2210.07714}, 2022{\natexlab{a}}.

\bibitem[Rieger et~al.(2022{\natexlab{b}})Rieger, Nguyen, Miettinen, and
  Sadeghi]{rieger2022deepsight}
Rieger, P., Nguyen, T.~D., Miettinen, M., and Sadeghi, A.-R.
\newblock Deepsight: Mitigating backdoor attacks in federated learning through
  deep model inspection.
\newblock In \emph{NDSS}, 2022{\natexlab{b}}.

\bibitem[Saha et~al.(2020)Saha, Subramanya, and Pirsiavash]{saha2020hidden}
Saha, A., Subramanya, A., and Pirsiavash, H.
\newblock Hidden trigger backdoor attacks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  11957--11965, 2020.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{shafahi2018poison}
Shafahi, A., Huang, W.~R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and
  Goldstein, T.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Shejwalkar \& Houmansadr(2021)Shejwalkar and
  Houmansadr]{shejwalkar2021manipulating}
Shejwalkar, V. and Houmansadr, A.
\newblock Manipulating the byzantine: Optimizing model poisoning attacks and
  defenses for federated learning.
\newblock In \emph{NDSS}, 2021.

\bibitem[Shejwalkar et~al.(2022)Shejwalkar, Houmansadr, Kairouz, and
  Ramage]{shejwalkar2022back}
Shejwalkar, V., Houmansadr, A., Kairouz, P., and Ramage, D.
\newblock Back to the drawing board: A critical evaluation of poisoning attacks
  on production federated learning.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pp.\
  1354--1371. IEEE, 2022.

\bibitem[Shen et~al.(2016)Shen, Tople, and Saxena]{shen2016auror}
Shen, S., Tople, S., and Saxena, P.
\newblock Auror: Defending against poisoning attacks in collaborative deep
  learning systems.
\newblock In \emph{Proceedings of the 32nd Annual Conference on Computer
  Security Applications}, pp.\  508--519, 2016.

\bibitem[Sun et~al.(2019)Sun, Kairouz, Suresh, and McMahan]{zitengsun01}
Sun, Z., Kairouz, P., Suresh, A.~T., and McMahan, H.~B.
\newblock Can you really backdoor federated learning?
\newblock \emph{arXiv preprint arXiv:1911.07963}, 2019.

\bibitem[Tang et~al.(2021)Tang, Wang, Tang, and Zhang]{tang2021demon}
Tang, D., Wang, X., Tang, H., and Zhang, K.
\newblock Demon in the variant: Statistical analysis of dnns for robust
  backdoor contamination detection.
\newblock In \emph{USENIX Security Symposium}, pp.\  1541--1558, 2021.

\bibitem[Van~der Maaten \& Hinton(2008)Van~der Maaten and
  Hinton]{van2008visualizing}
Van~der Maaten, L. and Hinton, G.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Wang et~al.(2020)Wang, Sreenivasan, Rajput, Vishwakarma, Agarwal,
  Sohn, Lee, and Papailiopoulos]{Hongyi01}
Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn,
  J.-y., Lee, K., and Papailiopoulos, D.
\newblock Attack of the tails: Yes, you really can backdoor federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16070--16084, 2020.

\bibitem[Wen et~al.(2022)Wen, Geiping, Fowl, Souri, Chellappa, Goldblum, and
  Goldstein]{Yuxin01}
Wen, Y., Geiping, J., Fowl, L., Souri, H., Chellappa, R., Goldblum, M., and
  Goldstein, T.
\newblock Thinking two moves ahead: Anticipating other users improves backdoor
  attacks in federated learning.
\newblock \emph{arXiv preprint arXiv:2210.09305}, 2022.

\bibitem[Xie et~al.(2019)Xie, Huang, Chen, and Li]{xie2019dba}
Xie, C., Huang, K., Chen, P.-Y., and Li, B.
\newblock Dba: Distributed backdoor attacks against federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xie et~al.(2021)Xie, Chen, Chen, and Li]{xie2021crfl}
Xie, C., Chen, M., Chen, P.-Y., and Li, B.
\newblock Crfl: Certifiably robust federated learning against backdoor attacks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11372--11382. PMLR, 2021.

\bibitem[Yin et~al.(2018)Yin, Chen, Kannan, and Bartlett]{yin2018byzantine}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5650--5659. PMLR, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Panda, Song, Yang, Mahoney, Mittal, Kannan,
  and Gonzalez]{Zhengming01}
Zhang, Z., Panda, A., Song, L., Yang, Y., Mahoney, M., Mittal, P., Kannan, R.,
  and Gonzalez, J.
\newblock Neurotoxin: Durable backdoors in federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  26429--26446. PMLR, 2022.

\bibitem[Zhao et~al.(2020)Zhao, Hu, Wang, Jiang, Shen, Luo, and
  Hu]{zhao2020shielding}
Zhao, L., Hu, S., Wang, Q., Jiang, J., Shen, C., Luo, X., and Hu, P.
\newblock Shielding collaborative learning: Mitigating poisoning attacks
  through client-side detection.
\newblock \emph{IEEE Transactions on Dependable and Secure Computing},
  18\penalty0 (5):\penalty0 2029--2041, 2020.

\end{thebibliography}
