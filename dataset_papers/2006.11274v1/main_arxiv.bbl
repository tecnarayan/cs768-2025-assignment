\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2012)Abbasi-Yadkori, Pal, and
  Szepesvari]{abbasi2012online}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Online-to-confidence-set conversions and application to sparse
  stochastic bandits.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1--9, 2012.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 22--31. JMLR. org, 2017.

\bibitem[Agarwal et~al.(2019)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2019optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock \emph{arXiv preprint arXiv:1908.00261}, 2019.

\bibitem[Altman(1999)]{altman1999constrained}
Eitan Altman.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Csaba, Wang, and Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Szepesvari Csaba, Mengdi Wang, and Lin~F. Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \emph{arXiv preprint arXiv:2006.01107}, 2020.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in neural information processing systems}, pages
  1471--1479, 2016.

\bibitem[Bertsekas and Tsitsiklis(1996)]{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}, volume~5.
\newblock Athena Scientific Belmont, MA, 1996.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2019provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.00360}, 2019.

\bibitem[Co-Reyes et~al.(2018)Co-Reyes, Liu, Gupta, Eysenbach, Abbeel, and
  Levine]{co2018self}
John~D Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel,
  and Sergey Levine.
\newblock Self-consistent trajectory autoencoder: Hierarchical reinforcement
  learning with trajectory embeddings.
\newblock \emph{arXiv preprint arXiv:1806.02813}, 2018.

\bibitem[Colas et~al.(2018)Colas, Fournier, Sigaud, and
  Oudeyer]{colas2018curious}
C{\'e}dric Colas, Pierre Fournier, Olivier Sigaud, and Pierre-Yves Oudeyer.
\newblock Curious: Intrinsically motivated multi-task multi-goal reinforcement
  learning.
\newblock 2018.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Krishnamurthy, Jiang, Agarwal,
  Dud{\'\i}k, and Langford]{du2019decoding}
Simon~S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav
  Dud{\'\i}k, and John Langford.
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock \emph{arXiv preprint arXiv:1901.09018}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Luo, Wang, and Zhang]{du2019provably}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8058--8068, 2019{\natexlab{b}}.

\bibitem[Du et~al.(2020{\natexlab{a}})Du, Kakade, Wang, and Yang]{Du2020Is}
Simon~S. Du, Sham~M. Kakade, Ruosong Wang, and Lin~F. Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=r1genAVKPB}.

\bibitem[Du et~al.(2020{\natexlab{b}})Du, Lee, Mahajan, and
  Wang]{du2020agnostic}
Simon~S Du, Jason~D Lee, Gaurav Mahajan, and Ruosong Wang.
\newblock Agnostic q-learning with function approximation in deterministic
  systems: Tight bounds on approximation error and sample complexity.
\newblock \emph{arXiv preprint arXiv:2002.07125}, 2020{\natexlab{b}}.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Florensa et~al.(2017)Florensa, Held, Geng, and
  Abbeel]{florensa2017automatic}
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock \emph{arXiv preprint arXiv:1705.06366}, 2017.

\bibitem[Hazan et~al.(2018)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2018provably}
Elad Hazan, Sham~M Kakade, Karan Singh, and Abby Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock \emph{arXiv preprint arXiv:1812.02690}, 2018.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
Rein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter
  Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1109--1117, 2016.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2019provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Johnson and Lindenstrauss(1984)]{johnson1984extensions}
William~B Johnson and Joram Lindenstrauss.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock \emph{Contemporary mathematics}, 26\penalty0 (189-206):\penalty0 1,
  1984.

\bibitem[Miryoosefi et~al.(2019)Miryoosefi, Brantley, Daume~III, Dudik, and
  Schapire]{miryoosefi2019reinforcement}
Sobhan Miryoosefi, Kiant{\'e} Brantley, Hal Daume~III, Miro Dudik, and Robert~E
  Schapire.
\newblock Reinforcement learning with convex constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14070--14079, 2019.

\bibitem[Misra et~al.(2019)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2019kinematic}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.05815}, 2019.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018visual}
Ashvin~V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and
  Sergey Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9191--9200, 2018.

\bibitem[Oudeyer et~al.(2007)Oudeyer, Kaplan, and Hafner]{oudeyer2007intrinsic}
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena~V Hafner.
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock \emph{IEEE transactions on evolutionary computation}, 11\penalty0
  (2):\penalty0 265--286, 2007.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 16--17, 2017.

\bibitem[Pong et~al.(2019)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2019skew}
Vitchyr~H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and
  Sergey Levine.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.03698}, 2019.

\bibitem[Schmidhuber(2010)]{schmidhuber2010formal}
J{\"u}rgen Schmidhuber.
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990--2010).
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2\penalty0
  (3):\penalty0 230--247, 2010.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  DeTurck, and Abbeel]{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI~Xi Chen, Yan
  Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  2753--2762, 2017.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1805.11074}, 2018.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020provably}
Ruosong Wang, Ruslan Salakhutdinov, and Lin~F Yang.
\newblock Provably efficient reinforcement learning with general value function
  approximation.
\newblock \emph{arXiv preprint arXiv:2005.10804}, 2020.

\bibitem[Wen and Van~Roy(2013)]{wen2013efficient}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3021--3029, 2013.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin~F. Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004, 2019.

\bibitem[Yao(1977)]{yao1977probabilistic}
Andrew Chi-Chin Yao.
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \emph{18th Annual Symposium on Foundations of Computer Science
  (sfcs 1977)}, pages 222--227. IEEE, 1977.

\bibitem[Zanette et~al.(2019)Zanette, Brandfonbrener, Pirotta, and
  Lazaric]{zanette2019frequentist}
Andrea Zanette, David Brandfonbrener, Matteo Pirotta, and Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock \emph{arXiv preprint arXiv:1911.00567}, 2019.

\end{thebibliography}
