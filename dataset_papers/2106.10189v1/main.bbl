\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balaji et~al.(2019)Balaji, Goldstein, and Hoffman]{balaji2019instance}
Yogesh Balaji, Tom Goldstein, and Judy Hoffman.
\newblock Instance adaptive adversarial training: Improved accuracy tradeoffs
  in neural nets.
\newblock \emph{arXiv preprint arXiv:1910.08051}, 2019.

\bibitem[Baraniuk(2007)]{baraniuk2007compressive}
Richard~G Baraniuk.
\newblock Compressive sensing [lecture notes].
\newblock \emph{IEEE signal processing magazine}, 24\penalty0 (4):\penalty0
  118--121, 2007.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Bayati and Montanari(2011)]{bayati2011lasso}
Mohsen Bayati and Andrea Montanari.
\newblock The lasso risk for gaussian matrices.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (4):\penalty0 1997--2017, 2011.

\bibitem[Ben-David and Schuller(2003)]{ben2003exploiting}
Shai Ben-David and Reba Schuller.
\newblock Exploiting task relatedness for multiple task learning.
\newblock In \emph{Learning theory and kernel machines}, pages 567--580.
  Springer, 2003.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Biggio and Roli(2018)]{biggio2018wild}
Battista Biggio and Fabio Roli.
\newblock Wild patterns: Ten years after the rise of adversarial machine
  learning.
\newblock \emph{Pattern Recognition}, 84:\penalty0 317--331, 2018.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and
  Van~Gool]{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In \emph{European conference on computer vision}, pages 446--461.
  Springer, 2014.

\bibitem[Cand{\`e}s et~al.(2006)]{candes2006compressive}
Emmanuel~J Cand{\`e}s et~al.
\newblock Compressive sampling.
\newblock In \emph{Proceedings of the international congress of
  mathematicians}, volume~3, pages 1433--1452. Madrid, Spain, 2006.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and
  Liang]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11190--11201, 2019.

\bibitem[Chapelle et~al.(2009)Chapelle, Scholkopf, and Zien]{chapelle2009semi}
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien.
\newblock Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book
  reviews].
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (3):\penalty0 542--542, 2009.

\bibitem[Chiang et~al.(2020)Chiang, Ni, Abdelkader, Zhu, Studor, and
  Goldstein]{chiang2020certified}
Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studor, and
  Tom Goldstein.
\newblock Certified defenses for adversarial patches.
\newblock \emph{arXiv preprint arXiv:2003.06693}, 2020.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Jeremy~M Cohen, Elan Rosenfeld, and J~Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock \emph{arXiv preprint arXiv:1902.02918}, 2019.

\bibitem[Conneau and Kiela(2018)]{conneau2018senteval}
Alexis Conneau and Douwe Kiela.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock \emph{arXiv preprint arXiv:1803.05449}, 2018.

\bibitem[Dalvi et~al.(2004)Dalvi, Domingos, Sanghai, and
  Verma]{dalvi2004adversarial}
Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, and Deepak Verma.
\newblock Adversarial classification.
\newblock In \emph{Proceedings of the tenth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 99--108, 2004.

\bibitem[Deng et~al.(2020{\natexlab{a}})Deng, Dwork, Wang, and
  Zhang]{deng2020interpreting}
Zhun Deng, Cynthia Dwork, Jialiang Wang, and Linjun Zhang.
\newblock Interpreting robust optimization via adversarial influence functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  2464--2473. PMLR, 2020{\natexlab{a}}.

\bibitem[Deng et~al.(2020{\natexlab{b}})Deng, He, Huang, and
  Su]{deng2020towards}
Zhun Deng, Hangfeng He, Jiaoyang Huang, and Weijie Su.
\newblock Towards understanding the dynamics of the first-order adversaries.
\newblock In \emph{International Conference on Machine Learning}, pages
  2484--2493. PMLR, 2020{\natexlab{b}}.

\bibitem[Deng et~al.(2021)Deng, Zhang, Ghorbani, and Zou]{deng2020improving}
Zhun Deng, Linjun Zhang, Amirata Ghorbani, and James Zou.
\newblock Improving adversarial robustness via unlabeled out-of-domain data.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[Donahue et~al.(2014)Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and
  Darrell]{donahue2014decaf}
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric
  Tzeng, and Trevor Darrell.
\newblock Decaf: A deep convolutional activation feature for generic visual
  recognition.
\newblock In \emph{International conference on machine learning}, pages
  647--655. PMLR, 2014.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Salman, Santurkar, and
  Tsipras]{robustness}
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris
  Tsipras.
\newblock Robustness (python library), 2019.
\newblock URL \url{https://github.com/MadryLab/robustness}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock \emph{arXiv preprint arXiv:1901.09960}, 2019.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pages
  2790--2799. PMLR, 2019.

\bibitem[Huh et~al.(2016)Huh, Agrawal, and Efros]{huh2016makes}
Minyoung Huh, Pulkit Agrawal, and Alexei~A Efros.
\newblock What makes imagenet good for transfer learning?
\newblock \emph{arXiv preprint arXiv:1608.08614}, 2016.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2019big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 6\penalty0 (2):\penalty0 8,
  2019.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem[Lecuyer et~al.(2019)Lecuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{lecuyer2019certified}
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
  Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  656--672. IEEE, 2019.

\bibitem[Lim(2012)]{lim2012transfer}
Joseph~Jaewhan Lim.
\newblock \emph{Transfer learning by borrowing examples for multiclass object
  detection}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2012.

\bibitem[Liu et~al.(2020)Liu, Feng, Wang, and Dong]{liu2020enhancing}
Chizhou Liu, Yunzhen Feng, Ranran Wang, and Bin Dong.
\newblock Enhancing certified robustness of smoothed classifiers via weighted
  model ensembling.
\newblock \emph{arXiv preprint arXiv:2005.09363}, 2020.

\bibitem[Lowd and Meek(2005)]{lowd2005adversarial}
Daniel Lowd and Christopher Meek.
\newblock Adversarial learning.
\newblock In \emph{Proceedings of the eleventh ACM SIGKDD international
  conference on Knowledge discovery in data mining}, pages 641--647, 2005.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and
  Vedaldi]{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and
  Romera-Paredes]{maurer2016benefit}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (81):\penalty0 1--32, 2016.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Mokrii et~al.(2021)Mokrii, Boytsov, and
  Braslavski]{mokrii2021systematic}
Iurii Mokrii, Leonid Boytsov, and Pavel Braslavski.
\newblock A systematic evaluation of transfer learning and pseudo-labeling with
  bert-based ranking models.
\newblock \emph{arXiv preprint arXiv:2103.03335}, 2021.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 427--436, 2015.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE, 2008.

\bibitem[Pajor(1998)]{pajor1998metric}
Alain Pajor.
\newblock Metric entropy of the grassmann manifold.
\newblock \emph{Convex Geometric Analysis}, 34:\penalty0 181--188, 1998.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Raghu et~al.(2019)Raghu, Zhang, Kleinberg, and
  Bengio]{raghu2019transfusion}
Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio.
\newblock Transfusion: Understanding transfer learning for medical imaging.
\newblock \emph{arXiv preprint arXiv:1902.07208}, 2019.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
\newblock Certified defenses against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.09344}, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{arXiv preprint arXiv:2007.08489}, 2020.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  M{\k{a}}dry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander M{\k{a}}dry.
\newblock Adversarially robust generalization requires more data.
\newblock \emph{arXiv preprint arXiv:1804.11285}, 2018.

\bibitem[Sharif~Razavian et~al.(2014)Sharif~Razavian, Azizpour, Sullivan, and
  Carlsson]{sharif2014cnn}
Ali Sharif~Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pages 806--813, 2014.

\bibitem[Shin et~al.(2016)Shin, Roth, Gao, Lu, Xu, Nogues, Yao, Mollura, and
  Summers]{shin2016deep}
Hoo-Chang Shin, Holger~R Roth, Mingchen Gao, Le~Lu, Ziyue Xu, Isabella Nogues,
  Jianhua Yao, Daniel Mollura, and Ronald~M Summers.
\newblock Deep convolutional neural networks for computer-aided detection: Cnn
  architectures, dataset characteristics and transfer learning.
\newblock \emph{IEEE transactions on medical imaging}, 35\penalty0
  (5):\penalty0 1285--1298, 2016.

\bibitem[Stanforth et~al.(2019)Stanforth, Fawzi, Kohli,
  et~al.]{stanforth2019labels}
Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et~al.
\newblock Are labels required for improving adversarial robustness?
\newblock \emph{arXiv preprint arXiv:1905.13725}, 2019.

\bibitem[Su et~al.(2017)Su, Bogdan, Candes, et~al.]{su2017false}
Weijie Su, Ma{\l}gorzata Bogdan, Emmanuel Candes, et~al.
\newblock False discoveries occur early on the lasso path.
\newblock \emph{Annals of Statistics}, 45\penalty0 (5):\penalty0 2133--2150,
  2017.

\bibitem[Tripuraneni et~al.(2020{\natexlab{a}})Tripuraneni, Jin, and
  Jordan]{tripuraneni2020provable}
Nilesh Tripuraneni, Chi Jin, and Michael~I Jordan.
\newblock Provable meta-learning of linear representations.
\newblock \emph{arXiv preprint arXiv:2002.11684}, 2020{\natexlab{a}}.

\bibitem[Tripuraneni et~al.(2020{\natexlab{b}})Tripuraneni, Jordan, and
  Jin]{tripuraneni2020theory}
Nilesh Tripuraneni, Michael~I Jordan, and Chi Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock \emph{arXiv preprint arXiv:2006.11650}, 2020{\natexlab{b}}.

\bibitem[Tsybakov(2008)]{tsybakov2008introduction}
Alexandre~B Tsybakov.
\newblock \emph{Introduction to nonparametric estimation}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Utrera et~al.()Utrera, Kravitz, Erichson, Khanna, and
  Mahoney]{utreraadversarially}
Francisco Utrera, Evan Kravitz, N~Benjamin Erichson, Rajiv Khanna, and
  Michael~W Mahoney.
\newblock Adversarially-trained deep nets transfer better: Illustration on
  image classification.

\bibitem[Utrera et~al.(2020)Utrera, Kravitz, Erichson, Khanna, and
  Mahoney]{utrera2020adversarially}
Francisco Utrera, Evan Kravitz, N~Benjamin Erichson, Rajiv Khanna, and
  Michael~W Mahoney.
\newblock Adversarially-trained deep nets transfer better.
\newblock \emph{arXiv preprint arXiv:2007.05869}, 2020.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Yu et~al.(2015)Yu, Wang, and Samworth]{yu2015useful}
Yi~Yu, Tengyao Wang, and Richard~J Samworth.
\newblock A useful variant of the davis--kahan theorem for statisticians.
\newblock \emph{Biometrika}, 102\penalty0 (2):\penalty0 315--323, 2015.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7472--7482. PMLR, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Deng, Kawaguchi, Ghorbani, and
  Zou]{zhang2020does}
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
\newblock How does mixup help with robustness and generalization?
\newblock \emph{arXiv preprint arXiv:2010.04819}, 2020.

\bibitem[Zhong et~al.(2020)Zhong, LeBien, Campos-Cerqueira, Dodhia, Ferres,
  Velev, and Aide]{zhong2020multispecies}
Ming Zhong, Jack LeBien, Marconi Campos-Cerqueira, Rahul Dodhia, Juan~Lavista
  Ferres, Julian~P Velev, and T~Mitchell Aide.
\newblock Multispecies bioacoustic classification using transfer learning of
  deep convolutional neural networks with pseudo-labeling.
\newblock \emph{Applied Acoustics}, 166:\penalty0 107375, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Oliver, Wu, and Zheng]{zhou2018semi}
Hong-Yu Zhou, Avital Oliver, Jianxin Wu, and Yefeng Zheng.
\newblock When semi-supervised learning meets transfer learning: Training
  strategies, models and datasets.
\newblock \emph{arXiv preprint arXiv:1812.05313}, 2018.

\bibitem[Zhu and Goldberg(2009)]{zhu2009introduction}
Xiaojin Zhu and Andrew~B Goldberg.
\newblock Introduction to semi-supervised learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 3\penalty0 (1):\penalty0 1--130, 2009.

\bibitem[Zhu et~al.(2003)Zhu, Ghahramani, and Lafferty]{zhu2003semi}
Xiaojin Zhu, Zoubin Ghahramani, and John~D Lafferty.
\newblock Semi-supervised learning using gaussian fields and harmonic
  functions.
\newblock In \emph{Proceedings of the 20th International conference on Machine
  learning (ICML-03)}, pages 912--919, 2003.

\end{thebibliography}
