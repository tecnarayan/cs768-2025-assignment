\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Absil et~al.(2005)Absil, Mahony, and Andrews]{Absil2005Convergence}
Absil, P.~A., Mahony, R., and Andrews, B.
\newblock Convergence of the {{Iterates}} of {{Descent Methods}} for {{Analytic
  Cost Functions}}.
\newblock \emph{SIAM Journal on Optimization}, 16\penalty0 (2):\penalty0
  531--547, 2005.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{Arjovsky2017Wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein {{Generative Adversarial Networks}}.
\newblock In \emph{Proceedings of the 34th {{International Conference}} on
  {{Machine Learning}}}, pp.\  214--223. {PMLR}, 2017.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{Arora2018Optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the {{Optimization}} of {{Deep Networks}}: {{Implicit
  Acceleration}} by {{Overparameterization}}.
\newblock In \emph{Proceedings of the 35th {{International Conference}} on
  {{Machine Learning}}}, pp.\  244--253, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{Arora2019Convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A {{Convergence Analysis}} of {{Gradient Descent}} for {{Deep Linear
  Neural Networks}}.
\newblock In \emph{International {{Conference}} on {{Learning
  Representations}}}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Cohen, Hu, and
  Luo]{Arora2019Implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit {{Regularization}} in {{Deep Matrix Factorization}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~32, 2019{\natexlab{b}}.

\bibitem[Bah et~al.(2021)Bah, Rauhut, Terstiege, and
  Westdickenberg]{Bah2021Learning}
Bah, B., Rauhut, H., Terstiege, U., and Westdickenberg, M.
\newblock Learning deep linear neural networks: {{Riemannian}} gradient flows
  and convergence to global minimizers.
\newblock \emph{Information and Inference: A Journal of the IMA}, 11\penalty0
  (1):\penalty0 307--353, 2021.

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{Baldi1989Neural}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: {{Learning}} from
  examples without local minima.
\newblock \emph{Neural Networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Bhatia et~al.(2019)Bhatia, Jain, and Lim]{bhatia2019bures}
Bhatia, R., Jain, T., and Lim, Y.
\newblock On the {{Bures}}\textendash{{Wasserstein}} distance between positive
  definite matrices.
\newblock \emph{Expositiones Mathematicae}, 37\penalty0 (2):\penalty0 165--191,
  2019.

\bibitem[Boumal(2022)]{boumal2022intromanifolds}
Boumal, N.
\newblock An introduction to optimization on smooth manifolds.
\newblock To appear with Cambridge University Press, 2022.
\newblock URL \url{http://www.nicolasboumal.net/book}.

\bibitem[Bures(1969)]{Bures1969extension}
Bures, D.
\newblock An extension of {{Kakutani}}'s theorem on infinite product measures
  to the tensor product of semifinite {$w$}*-algebras.
\newblock \emph{Transactions of the American Mathematical Society},
  135:\penalty0 199--212, 1969.

\bibitem[Chewi et~al.(2020)Chewi, Maunu, Rigollet, and
  Stromme]{chewi2020gradient}
Chewi, S., Maunu, T., Rigollet, P., and Stromme, A.
\newblock Gradient descent algorithms for {Bures-Wasserstein} barycenters.
\newblock In \emph{Conference on Learning Theory}, pp.\  1276--1304, 2020.

\bibitem[Chitour et~al.(2022)Chitour, Liao, and Couillet]{Chitour2022Geometric}
Chitour, Y., Liao, Z., and Couillet, R.
\newblock A {{Geometric Approach}} of {{Gradient Descent Algorithms}} in
  {{Linear Neural Networks}}.
\newblock \emph{Mathematical Control and Related Fields}, 2022.

\bibitem[Chulhee et~al.(2018)Chulhee, Suvrit, and Ali]{yun2018global}
Chulhee, Y., Suvrit, S., and Ali, J.
\newblock Global optimality conditions for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJk7Gf-CZ}.

\bibitem[De~Meulemeester et~al.(2021)De~Meulemeester, Schreurs, Fanuel,
  De~Moor, and Suykens]{de2021bures}
De~Meulemeester, H., Schreurs, J., Fanuel, M., De~Moor, B., and Suykens, J.~A.
\newblock The bures metric for generative adversarial networks.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases.
  Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September
  13--17, 2021, Proceedings, Part II 21}, pp.\  52--66. Springer, 2021.

\bibitem[Dowson \& Landau(1982)Dowson and Landau]{Dowson1982Frechet}
Dowson, D.~C. and Landau, B.~V.
\newblock The {{Fr\'echet}} distance between multivariate normal distributions.
\newblock \emph{Journal of Multivariate Analysis}, 12\penalty0 (3):\penalty0
  450--455, 1982.

\bibitem[Dryden et~al.(2009)Dryden, Koloydenko, and Zhou]{dryden2009non}
Dryden, I.~L., Koloydenko, A., and Zhou, D.
\newblock Non-{E}uclidean statistics for covariance matrices with applications
  to diffusion tensor imaging.
\newblock \emph{The Annals of Applied Statistics}, 3\penalty0 (3):\penalty0
  1102--1123, 2009.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{Du2018Algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic {{Regularization}} in {{Learning Deep Homogeneous
  Models}}: {{Layers}} are {{Automatically Balanced}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~31, 2018.

\bibitem[Dutta \& Li(2017)Dutta and Li]{dutta2017problem}
Dutta, A. and Li, X.
\newblock On a problem of weighted low-rank approximation of matrices.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 38\penalty0
  (2):\penalty0 530--553, 2017.

\bibitem[Eckart \& Young(1936)Eckart and Young]{Eckart1936approximation}
Eckart, C. and Young, G.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Feizi et~al.(2020)Feizi, Farnia, Ginart, and
  Tse]{Feizi2020understanding}
Feizi, S., Farnia, F., Ginart, T., and Tse, D.
\newblock Understanding {GAN}s in the {LQG} setting: Formulation,
  generalization and stability.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 304--311, 2020.

\bibitem[Fukumizu(1998)]{Fukumizu1998Effect}
Fukumizu, K.
\newblock Effect of batch learning in multilayer neural networks.
\newblock In \emph{In {{Proceedings}} of {{ICONIP}}'98}, 1998.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and {Lacoste-Julien}]{GBL2019Implicit}
Gidel, G., Bach, F., and {Lacoste-Julien}, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Gillis \& Shitov(2019)Gillis and Shitov]{gillis2017low}
Gillis, N. and Shitov, Y.
\newblock Low-rank matrix approximation in the infinity norm.
\newblock \emph{Linear Algebra and its Applications}, 581:\penalty0 367--382,
  2019.

\bibitem[Gillis \& Vavasis(2018)Gillis and Vavasis]{gillis2018complexity}
Gillis, N. and Vavasis, S.~A.
\newblock {On the Complexity of Robust PCA and {$\ell$}1-Norm Low-Rank Matrix
  Approximation}.
\newblock \emph{Mathematics of Operations Research}, 43\penalty0 (4):\penalty0
  1072--1084, 2018.

\bibitem[Hardt \& Ma(2017)Hardt and Ma]{hardt2017identity}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=ryxB0Rtxx}.

\bibitem[Helmke \& Shayman(1995)Helmke and Shayman]{HS1995Critical}
Helmke, U. and Shayman, M.~A.
\newblock Critical points of matrix least squares distance functions.
\newblock \emph{Linear Algebra and its Applications}, 1995.

\bibitem[Kantorovitch(1958)]{10.2307/2626967}
Kantorovitch, L.
\newblock On the translocation of masses.
\newblock \emph{Management Science}, 5\penalty0 (1):\penalty0 1--4, 1958.
\newblock URL \url{http://www.jstor.org/stable/2626967}.

\bibitem[Kawaguchi(2016)]{Kawaguchi2016Deep}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29. Curran Associates, Inc., 2016.
\newblock URL \url{https://proceedings.neurips.cc/paper/2016/file/
  f2fc990265c712c49d51a18a32b39f0c-Paper.pdf}.

\bibitem[Kohn et~al.(2022)Kohn, Merkh, Mont\'{u}far, and
  Trager]{Kohn2021GeometryOL}
Kohn, K., Merkh, T., Mont\'{u}far, G., and Trager, M.
\newblock Geometry of linear convolutional networks.
\newblock \emph{SIAM Journal on Applied Algebra and Geometry}, 6\penalty0
  (3):\penalty0 368--406, 2022.
\newblock URL \url{https://doi.org/10.1137/21M1441183}.

\bibitem[Kohn et~al.(2023)Kohn, Montúfar, Shahverdi, and
  Trager]{kohn2023function}
Kohn, K., Montúfar, G., Shahverdi, V., and Trager, M.
\newblock Function space and critical points of linear convolutional networks,
  2023.

\bibitem[Kroshnin et~al.(2021)Kroshnin, Spokoiny, and
  Suvorikova]{Kroshnin2021Statistical}
Kroshnin, A., Spokoiny, V., and Suvorikova, A.
\newblock {Statistical Inference for {{Bures}}\textendash{{Wasserstein}}
  Barycenters}.
\newblock \emph{The Annals of Applied Probability}, 31\penalty0 (3), 2021.

\bibitem[Laurent \& Brecht(2018)Laurent and Brecht]{Laurent2018Deep}
Laurent, T. and Brecht, J.
\newblock Deep {{Linear Networks}} with {{Arbitrary Loss}}: {{All Local Minima
  Are Global}}.
\newblock In \emph{Proceedings of the 35th {{International Conference}} on
  {{Machine Learning}}}, pp.\  2902--2907. PMLR, 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/laurent18a.html}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural {n}etworks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022.

\bibitem[Lu \& Kawaguchi(2017)Lu and Kawaguchi]{Lu2017Depth}
Lu, H. and Kawaguchi, K.
\newblock Depth {{Creates No Bad Local Minima}}.
\newblock \emph{arXiv.1702.08580}, 2017.

\bibitem[Magnus \& Neudecker(2019)Magnus and Neudecker]{Magnus2019Matrix}
Magnus, J.~R. and Neudecker, H.
\newblock \emph{Matrix Differential Calculus with Applications in Statistics
  and Econometrics}.
\newblock Wiley Series in Probability and Statistics. {Wiley}, third edition
  edition, 2019.

\bibitem[Mallasto et~al.(2022)Mallasto, Gerolin, and Minh]{mallasto2022entropy}
Mallasto, A., Gerolin, A., and Minh, Q.
\newblock Entropy-regularized 2-{W}asserstein distance between {G}aussian
  measures.
\newblock \emph{Information Geometry}, 5\penalty0 (1):\penalty0 289--323, 2022.

\bibitem[Masarotto et~al.(2019)Masarotto, Panaretos, and
  Zemel]{masarotto2019procrustes}
Masarotto, V., Panaretos, V., and Zemel, Y.
\newblock {Procrustes metrics on Covariance operators and optimal
  transportation of {G}aussian processes}.
\newblock \emph{Sankhya A}, 81\penalty0 (1):\penalty0 172--213, 2019.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.
\newblock {A mean field view of the landscape of two-layer neural networks}.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Min et~al.(2021)Min, Tarmoun, Vidal, and Mallada]{Min2021Explicit}
Min, H., Tarmoun, S., Vidal, R., and Mallada, E.
\newblock On the explicit role of initialization on the convergence and
  implicit bias of overparametrized linear networks.
\newblock In \emph{Proceedings of the 38th {{International Conference}} on
  {{Machine Learning}}}, pp.\  7760--7768, 2021.

\bibitem[Mirsky(1960)]{mirsky1960symmetric}
Mirsky, L.
\newblock Symmetric {{Gauge Functions}} and {{Unitary Invariant Norms}}.
\newblock \emph{The Quarterly Journal of Mathematics}, 11\penalty0
  (1):\penalty0 50--59, 1960.

\bibitem[Muzellec \& Cuturi(2018)Muzellec and Cuturi]{NEURIPS2018_b613e70f}
Muzellec, B. and Cuturi, M.
\newblock Generalizing {P}oint {E}mbeddings using the {W}asserstein {S}pace of
  {E}lliptical {D}istributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Nguegnang et~al.(2021)Nguegnang, Rauhut, and
  Terstiege]{nguegnang2021convergence}
Nguegnang, G.~M., Rauhut, H., and Terstiege, U.
\newblock Convergence of gradient descent for learning linear neural networks,
  2021.
\newblock URL \url{https://arxiv.org/abs/2108.02040}.

\bibitem[Pele \& Werman(2009)Pele and Werman]{Pele2009Fast}
Pele, O. and Werman, M.
\newblock Fast and robust earth mover's distances.
\newblock In \emph{2009 IEEE 12th International Conference on Computer Vision},
  pp.\  460--467, 2009.

\bibitem[Ruben \& Zamir(1979)Ruben and Zamir]{ruben1979lower}
Ruben, G. and Zamir, S.
\newblock Lower rank approximation of matrices by least squares with any choice
  of weights.
\newblock \emph{Technometrics}, 21\penalty0 (4):\penalty0 489--498, 1979.
\newblock URL \url{http://www.jstor.org/stable/1268288}.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and
  Ganguli]{DBLP:journals/corr/SaxeMG13}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In \emph{2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6120}.

\bibitem[Schmitt(1992)]{Sch1992Perturbation}
Schmitt, B.~A.
\newblock Perturbation bounds for matrix square roots and pythagorean sums.
\newblock \emph{Linear Algebra and its Applications}, 1992.

\bibitem[Song et~al.(2017)Song, Woodruff, and Zhong]{zhao2017lowl1}
Song, Z., Woodruff, D.~P., and Zhong, P.
\newblock Low {R}ank {A}pproximation with {E}ntrywise {L}1-{N}orm {E}rror.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, STOC 2017, pp.\  688–701, New York, NY, USA, 2017.
  Association for Computing Machinery.

\bibitem[Tarmoun et~al.(2021)Tarmoun, Franca, Haeffele, and
  Vidal]{Tarmoun2021Understanding}
Tarmoun, S., Franca, G., Haeffele, B.~D., and Vidal, R.
\newblock Understanding the {{Dynamics}} of {{Gradient Flow}} in
  {{Overparameterized Linear}} models.
\newblock In \emph{Proceedings of the 38th {{International Conference}} on
  {{Machine Learning}}}, pp.\  10153--10161. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/tarmoun21a.html}.

\bibitem[Trager et~al.(2020)Trager, Kohn, and Bruna]{Trager2020Pure}
Trager, M., Kohn, K., and Bruna, J.
\newblock Pure and spurious critical points: a geometric study of linear
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgOlCVYvB}.

\bibitem[Villani(2003)]{villani2003topics}
Villani, C.
\newblock \emph{Topics in Optimal Transportation}.
\newblock Graduate studies in mathematics. American Mathematical Society, 2003.
\newblock URL \url{https://books.google.com/books?id=idyFAwAAQBAJ}.

\bibitem[Villani(2008)]{villani2008optimal}
Villani, C.
\newblock \emph{{Optimal Transport: Old and New}}.
\newblock Grundlehren der mathematischen Wissenschaften. Springer Berlin
  Heidelberg, 2008.
\newblock URL \url{https://books.google.com/books?id=hV8o5R7\_5tkC}.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{Yun2021unifying}
Yun, C., Krishnan, S., and Mobahi, H.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ZsZM-4iMQkH}.

\end{thebibliography}
