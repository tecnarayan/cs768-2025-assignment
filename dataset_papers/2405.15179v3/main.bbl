\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2020)Aghajanyan, Zettlemoyer, and Gupta]{aghajanyan2020intrinsic}
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
\newblock Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
\newblock \emph{arXiv preprint arXiv:2012.13255}, 2020.

\bibitem[Arora et~al.(2013)Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, and Zhu]{arora2013practical}
Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu.
\newblock A practical algorithm for topic modeling with provable guarantees.
\newblock In \emph{International Conference on Machine Learning}, pages 280--288. PMLR, 2013.

\bibitem[Ba{\l}azy et~al.(2024)Ba{\l}azy, Banaei, Aberer, and Tabor]{balazy2024lora}
Klaudia Ba{\l}azy, Mohammadreza Banaei, Karl Aberer, and Jacek Tabor.
\newblock Lo{R}{A}-{X}{S}: Low-rank adaptation with extremely small number of parameters.
\newblock \emph{arXiv preprint arXiv:2405.17604}, 2024.

\bibitem[Ben~Zaken et~al.(2022)Ben~Zaken, Goldberg, and Ravfogel]{ben-zaken-etal-2022-bitfit}
Elad Ben~Zaken, Yoav Goldberg, and Shauli Ravfogel.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 1--9, Dublin, Ireland, May 2022.

\bibitem[Borzunov et~al.(2024)Borzunov, Ryabinin, Chumachenko, Baranchuk, Dettmers, Belkada, Samygin, and Raffel]{borzunov2024distributed}
Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin~A Raffel.
\newblock Distributed inference and fine-tuning of large language models over the internet.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Caccia et~al.(2023)Caccia, Ponti, Su, Pereira, Le~Roux, and Sordoni]{caccia2023multi}
Lucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le~Roux, and Alessandro Sordoni.
\newblock Multi-head adapter routing for cross-task generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and Specia]{cer-etal-2017-semeval}
Daniel Cer, Mona Diab, Eneko Agirre, I{\~n}igo Lopez-Gazpio, and Lucia Specia.
\newblock {S}em{E}val-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation.
\newblock In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif~M. Mohammad, Daniel Cer, and David Jurgens, editors, \emph{Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)}, pages 1--14, Vancouver, Canada, August 2017.

\bibitem[Chan et~al.(2009)Chan, Chi, Huang, and Ma]{chan2009convex}
Tsung-Han Chan, Chong-Yung Chi, Yu-Min Huang, and Wing-Kin Ma.
\newblock A convex analysis-based minimum-volume enclosing simplex algorithm for hyperspectral unmixing.
\newblock \emph{IEEE Transactions on Signal Processing}, 57\penalty0 (11):\penalty0 4418--4432, 2009.

\bibitem[Cichocki(2014)]{cichocki2014era}
Andrzej Cichocki.
\newblock Era of big data processing: A new approach via tensor networks and tensor decompositions.
\newblock \emph{arXiv preprint arXiv:1403.2048}, 2014.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{Tim-2023-qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock {Q}{L}o{R}{A}: Efficient finetuning of quantized {L}{L}{M}s.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 10088--10115, 2023.

\bibitem[Ding et~al.(2008)Ding, Li, and Jordan]{ding2008convex}
Chris~HQ Ding, Tao Li, and Michael~I Jordan.
\newblock Convex and semi-nonnegative matrix factorizations.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 32\penalty0 (1):\penalty0 45--55, 2008.

\bibitem[Ding et~al.(2023)Ding, Qin, Yang, Wei, Yang, Su, Hu, Chen, Chan, Chen, et~al.]{ding2023parameter}
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et~al.
\newblock Parameter-efficient fine-tuning of large-scale pre-trained language models.
\newblock \emph{Nature Machine Intelligence}, 5\penalty0 (3):\penalty0 220--235, 2023.

\bibitem[Dolan and Brockett(2005)]{dolan-brockett-2005-automatically}
William~B. Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)}, 2005.

\bibitem[Fu et~al.(2015)Fu, Ma, Huang, and Sidiropoulos]{fu2015blind}
Xiao Fu, Wing-Kin Ma, Kejun Huang, and Nicholas~D Sidiropoulos.
\newblock Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0 (9):\penalty0 2306--2320, 2015.

\bibitem[He et~al.(2021)He, Zhou, Ma, Berg-Kirkpatrick, and Neubig]{he2021towards}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycksmath2021}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {N}{L}{P}.
\newblock In \emph{International Conference on Machine Learning}, pages 2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.]{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lo{R}{A}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Inouye et~al.(2014)Inouye, Ravikumar, and Dhillon]{inouye2014admixture}
David Inouye, Pradeep Ravikumar, and Inderjit Dhillon.
\newblock Admixture of {P}oisson {M}{R}{F}s: A topic model with word dependencies.
\newblock In \emph{International Conference on Machine Learning}, pages 683--691. PMLR, 2014.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparametrization with {G}umble-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Jie and Deng(2023)]{jie2023fact}
Shibo Jie and Zhi-Hong Deng.
\newblock Fact: Factor-tuning for lightweight adaptation on vision transformer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 1060--1068, 2023.

\bibitem[Karimi~Mahabadi et~al.(2021)Karimi~Mahabadi, Henderson, and Ruder]{karimi2021compacter}
Rabeeh Karimi~Mahabadi, James Henderson, and Sebastian Ruder.
\newblock Compacter: Efficient low-rank hypercomplex adapter layers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1022--1035, 2021.

\bibitem[Khoromskij(2011)]{khoromskij2011d}
Boris~N Khoromskij.
\newblock O (d log n)-quantics approximation of n-d tensors in high-dimensional numerical modeling.
\newblock \emph{Constructive Approximation}, 34:\penalty0 257--280, 2011.

\bibitem[Kolda and Bader(2009)]{kolda2009tensor}
Tamara~G Kolda and Brett~W Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Kopiczko et~al.(2024)Kopiczko, Blankevoort, and Asano]{kopiczko2023vera}
Dawid~Jan Kopiczko, Tijmen Blankevoort, and Yuki~M Asano.
\newblock Ve{RA}: Vector-based random matrix adaptation.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, Smola, et~al.]{le2013fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, Alex Smola, et~al.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In \emph{International Conference on Machine Learning}, volume~85, 2013.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 2021.

\bibitem[Li and Bioucas-Dias(2008)]{li2008minimum}
Jun Li and Jos{\'e}~M Bioucas-Dias.
\newblock Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data.
\newblock In \emph{IGARSS 2008-2008 IEEE International Geoscience and Remote Sensing Symposium}, volume~3, pages III--250. IEEE, 2008.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, 2021.

\bibitem[Lin et~al.(2015)Lin, Ma, Li, Chi, and Ambikapathi]{lin2015identifiability}
Chia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, and ArulMurugan Ambikapathi.
\newblock Identifiability of the simplex volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case.
\newblock \emph{IEEE Transactions on Geoscience and Remote Sensing}, 53\penalty0 (10):\penalty0 5530--5546, 2015.

\bibitem[Liu et~al.(2022)Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and Raffel]{liu2022few}
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin~A Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1950--1965, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Ro{B}{E}{R}{T}a: A robustly optimized {B}{E}{R}{T} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random variables.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Mahabadi et~al.(2021)Mahabadi, Ruder, Dehghani, and Henderson]{mahabadi2021parameter}
Rabeeh~Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.
\newblock Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 565--576, 2021.

\bibitem[Mazzawi et~al.(2024)Mazzawi, Gonzalvo, Wunder, Jerome, and Dherin]{mazzawideep}
Hanna Mazzawi, Javier Gonzalvo, Michael Wunder, Sammy Jerome, and Benoit Dherin.
\newblock Deep fusion: Efficient network training via pre-trained initializations.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Novikova et~al.(2017)Novikova, Du{\v{s}}ek, and Rieser]{novikova-etal-2017-e2e}
Jekaterina Novikova, Ond{\v{r}}ej Du{\v{s}}ek, and Verena Rieser.
\newblock The {E}2{E} dataset: New challenges for end-to-end generation.
\newblock In Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louis, editors, \emph{Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue}, pages 201--206, Saarbr{\"u}cken, Germany, August 2017.

\bibitem[Ortiz et~al.(2024)Ortiz, Guttag, and Dalca]{ortiz2024magnitude}
Jose Javier~Gonzalez Ortiz, John Guttag, and Adrian~V Dalca.
\newblock Magnitude invariant parametrizations improve hypernetwork learning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Oseledets(2010)]{oseledets2010approximation}
Ivan~V Oseledets.
\newblock Approximation of $2^{}d\times2^{}d$ matrices using tensor decomposition.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 31\penalty0 (4):\penalty0 2130--2145, 2010.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Ponti et~al.(2022)Ponti, Sordoni, Bengio, and Reddy]{ponti2022combining}
Edoardo~M Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy.
\newblock Combining modular skills in multitask learning.
\newblock \emph{arXiv preprint arXiv:2202.13914}, 2022.

\bibitem[Pritchard et~al.(2000)Pritchard, Stephens, and Donnelly]{pritchard2000inference}
Jonathan~K Pritchard, Matthew Stephens, and Peter Donnelly.
\newblock Inference of population structure using multilocus genotype data.
\newblock \emph{Genetics}, 155\penalty0 (2):\penalty0 945--959, 2000.

\bibitem[Qin et~al.(2021)Qin, Wang, Su, Lin, Ding, Yi, Chen, Liu, Li, Hou, et~al.]{qin2021exploring}
Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, et~al.
\newblock Exploring universal intrinsic task subspace via prompt tuning.
\newblock \emph{arXiv preprint arXiv:2110.07867}, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar-etal-2018-know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In Iryna Gurevych and Yusuke Miyao, editors, \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 784--789, Melbourne, Australia, July 2018.

\bibitem[Reisinger et~al.(2010)Reisinger, Waters, Silverthorn, and Mooney]{reisinger2010spherical}
Joseph Reisinger, Austin Waters, Bryan Silverthorn, and Raymond~J Mooney.
\newblock Spherical topic models.
\newblock In \emph{International Conference on Machine Learning}, pages 903--910. Citeseer, 2010.

\bibitem[Renduchintala et~al.(2024)Renduchintala, Konuk, and Kuchaiev]{renduchintala2024tied}
Adithya Renduchintala, Tugrul Konuk, and Oleksii Kuchaiev.
\newblock Tied-{L}o{R}{A}: Enhancing parameter efficiency of {L}o{R}{A} with weight tying.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 8686--8697, 2024.

\bibitem[Shazeer et~al.(2016)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2016outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Sheng et~al.(2023)Sheng, Cao, Li, Hooper, Lee, Yang, Chou, Zhu, Zheng, Keutzer, et~al.]{sheng2023s}
Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et~al.
\newblock S-{L}o{R}{A}: Serving thousands of concurrent {L}o{R}{A} adapters.
\newblock \emph{arXiv preprint arXiv:2311.03285}, 2023.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, \emph{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA, October 2013.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023Llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[van~der Maaten and Hinton(2008)]{tsne}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-{S}{N}{E}.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (86):\penalty0 2579--2605, 2008.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang-etal-2018-glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In Tal Linzen, Grzegorz Chrupa{\l}a, and Afra Alishahi, editors, \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355, Brussels, Belgium, November 2018.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt-etal-2019-neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 625--641, 2019.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Lu, Deng, Zheng, Wang, Chowdhury, Yun, Cui, Xuchao, Zhao, et~al.]{zhao2023domain}
Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Li~Yun, Hejie Cui, Zhang Xuchao, Tianjiao Zhao, et~al.
\newblock Domain specialization as the key to make large language models disruptive: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2305.18703}, 2023.

\bibitem[Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging {L}{L}{M}-as-a-judge with {M}{T}-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
