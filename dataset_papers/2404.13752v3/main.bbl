\begin{thebibliography}{10}

\bibitem{allen2022feature}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep learning.
\newblock In {\em 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)}, pages 977--988. IEEE, 2022.

\bibitem{azaria2023internal}
Amos Azaria and Tom Mitchell.
\newblock The internal state of an llm knows when its lying.
\newblock {\em arXiv preprint arXiv:2304.13734}, 2023.

\bibitem{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback, 2022.

\bibitem{chen2024can}
Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, et~al.
\newblock Can editing llms inject harm?
\newblock {\em arXiv preprint arXiv:2407.20224}, 2024.

\bibitem{chen2023rethinking}
Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, and Jun Zhu.
\newblock Rethinking model ensemble in transfer-based adversarial attacks.
\newblock {\em arXiv preprint arXiv:2303.09105}, 2023.

\bibitem{guanaco}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock {\em arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{dong2023robust}
Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu~Tian, Hang Su, and Jun Zhu.
\newblock How robust is google's bard to adversarial image attacks?
\newblock {\em arXiv preprint arXiv:2309.11751}, 2023.

\bibitem{enguehard2023sequential}
Joseph Enguehard.
\newblock Sequential integrated gradients: a simple but effective method for explaining language models, 2023.

\bibitem{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran{\c{c}}ois Laviolette, Mario March, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock {\em Journal of machine learning research}, 17(59):1--35, 2016.

\bibitem{goodfellow2014generative}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem{grabinski2022robust}
Julia Grabinski, Paul Gavrikov, Janis Keuper, and Margret Keuper.
\newblock Robust models are less over-confident.
\newblock {\em Advances in Neural Information Processing Systems}, 35:39059--39075, 2022.

\bibitem{gurnee2023finding}
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.
\newblock Finding neurons in a haystack: Case studies with sparse probing.
\newblock {\em arXiv preprint arXiv:2305.01610}, 2023.

\bibitem{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In {\em International conference on machine learning}, pages 2790--2799. PMLR, 2019.

\bibitem{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem{hu2024gradient}
Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho.
\newblock Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes, 2024.

\bibitem{huang2024can}
Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, and Kai Shu.
\newblock Can knowledge editing really correct hallucinations?
\newblock {\em arXiv preprint arXiv:2410.16251}, 2024.

\bibitem{huang2023can}
Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani~H Gilpin.
\newblock Can large language models explain themselves? a study of llm-generated self-explanations.
\newblock {\em arXiv preprint arXiv:2310.11207}, 2023.

\bibitem{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features, 2019.

\bibitem{jia2024improved}
Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, and Min Lin.
\newblock Improved techniques for optimization-based jailbreaking on large language models.
\newblock {\em arXiv preprint arXiv:2405.21018}, 2024.

\bibitem{jiang2020robust}
Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang.
\newblock Robust pre-training by adversarial contrastive learning.
\newblock {\em Advances in neural information processing systems}, 33:16199--16210, 2020.

\bibitem{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem{li2023inferencetime}
Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg.
\newblock Inference-time intervention: Eliciting truthful answers from a language model, 2023.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{li2024deepinception}
Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo~Han.
\newblock Deepinception: Hypnotize large language model to be jailbreaker, 2024.

\bibitem{li2023reinforcement}
Zihao Li, Zhuoran Yang, and Mengdi Wang.
\newblock Reinforcement learning with human feedback: Learning dynamic choices via pessimism, 2023.

\bibitem{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2022.

\bibitem{lin2021straight}
Xiang Lin, Simeng Han, and Shafiq Joty.
\newblock Straight to the gradient: Learning to use novel tokens for neural text generation, 2021.

\bibitem{liu2024dora}
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang~Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.
\newblock Dora: Weight-decomposed low-rank adaptation.
\newblock {\em arXiv preprint arXiv:2402.09353}, 2024.

\bibitem{autodan}
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.
\newblock Autodan: Generating stealthy jailbreak prompts on aligned large language models.
\newblock {\em CoRR}, abs/2310.04451, 2023.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{mitchell2022fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D. Manning.
\newblock Fast model editing at scale, 2022.

\bibitem{mo2024fight}
Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang.
\newblock Fight back against jailbreaking via prompt adversarial tuning.
\newblock {\em NeurIPS 2024}, 2024.

\bibitem{mu2020compositional}
Jesse Mu and Jacob Andreas.
\newblock Compositional explanations of neurons.
\newblock {\em Advances in Neural Information Processing Systems}, 33:17153--17163, 2020.

\bibitem{openai2024gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.

\bibitem{pfeiffer2020mad}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock Mad-x: An adapter-based framework for multi-task cross-lingual transfer.
\newblock {\em arXiv preprint arXiv:2005.00052}, 2020.

\bibitem{justinphan3110_harmful_harmless_instructions}
Justin Phan.
\newblock Harmful harmless instructions dataset.
\newblock \url{https://huggingface.co/datasets/justinphan3110/harmful_harmless_instructions}, 2023.
\newblock Accessed: 2024-03-30.

\bibitem{qi2023finetuning}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!, 2023.

\bibitem{santurkar2019image}
Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
\newblock Image synthesis with a single (robust) classifier.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert L. Logan~IV au2, Eric Wallace, and Sameer Singh.
\newblock Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020.

\bibitem{sikdar2021integrated}
Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese.
\newblock Integrated directional gradients: Feature interaction attribution for neural nlp models.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 865--878, 2021.

\bibitem{singh2024rethinking}
Chandan Singh, Jeevana~Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao.
\newblock Rethinking interpretability in the era of large language models.
\newblock {\em arXiv preprint arXiv:2402.01761}, 2024.

\bibitem{srinivas2024models}
Suraj Srinivas, Sebastian Bordt, and Himabindu Lakkaraju.
\newblock Which models have perceptually-aligned gradients? an explanation via off-manifold robustness.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{sun2018domain}
Sining Sun, Ching-Feng Yeh, Mei-Yuh Hwang, Mari Ostendorf, and Lei Xie.
\newblock Domain adversarial training for accented speech recognition.
\newblock In {\em 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, pages 4854--4858. IEEE, 2018.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{llama2}
Hugo Touvron et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em CoRR}, abs/2307.09288, 2023.

\bibitem{wang2024theoretical}
Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang.
\newblock A theoretical understanding of self-correction through in-context alignment.
\newblock In {\em NeurIPS 2024}, 2024.

\bibitem{wei2024assessing}
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson.
\newblock Assessing the brittleness of safety alignment via pruning and low-rank modifications.
\newblock {\em arXiv preprint arXiv:2402.05162}, 2024.

\bibitem{wei2023cfa}
Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock Cfa: Class-wise calibrated fair adversarial training.
\newblock In {\em CVPR}, 2023.

\bibitem{wei2023jailbreak}
Zeming Wei, Yifei Wang, and Yisen Wang.
\newblock Jailbreak and guard aligned language models with only few in-context demonstrations.
\newblock {\em arXiv preprint arXiv:2310.06387}, 2023.

\bibitem{welleck2019neural}
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.
\newblock Neural text generation with unlikelihood training, 2019.

\bibitem{selfreminder}
Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie.
\newblock Defending chatgpt against jailbreak attack via self-reminder.
\newblock 04 2023.

\bibitem{wu2024reftrepresentationfinetuninglanguage}
Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher~D. Manning, and Christopher Potts.
\newblock Reft: Representation finetuning for language models, 2024.

\bibitem{xu2022learning}
Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li.
\newblock Learning to break the loop: Analyzing and mitigating repetitions for neural text generation, 2022.

\bibitem{xu2023parameterefficient}
Lingling Xu, Haoran Xie, Si-Zhao~Joe Qin, Xiaohui Tao, and Fu~Lee Wang.
\newblock Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023.

\bibitem{yang2023shadow}
Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin.
\newblock Shadow alignment: The ease of subverting safely-aligned language models, 2023.

\bibitem{yao2023editinglargelanguagemodels}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.
\newblock Editing large language models: Problems, methods, and opportunities, 2023.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em ICML}, 2019.

\bibitem{Zhang2023EvaluatingTP}
Xiaotian Zhang, Chunyang Li, Yi~Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.
\newblock Evaluating the performance of large language models on gaokao benchmark.
\newblock 2023.

\bibitem{zhang2024duality}
Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, and Zeming Wei.
\newblock On the duality between sharpness-aware minimization and adversarial training.
\newblock {\em arXiv preprint arXiv:2402.15152}, 2024.

\bibitem{zhang2024boosting}
Yihao Zhang and Zeming Wei.
\newblock Boosting jailbreak attack with momentum.
\newblock In {\em ICLR 2024 Workshop on Reliable and Responsible Foundation Models}.

\bibitem{zhang2024alleviating}
Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi.
\newblock Alleviating hallucinations of large language models through induced hallucinations, 2024.

\bibitem{zhang2023sirens}
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, Longyue Wang, Anh~Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.
\newblock Siren's song in the ai ocean: A survey on hallucination in large language models, 2023.

\bibitem{zhao2024explainability}
Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du.
\newblock Explainability for large language models: A survey.
\newblock {\em ACM Transactions on Intelligent Systems and Technology}, 15(2):1--38, 2024.

\bibitem{zhao2023arcl}
Xuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, and Weiran Huang.
\newblock Arcl: enhancing contrastive learning with augmentation-robust representations.
\newblock {\em arXiv preprint arXiv:2303.01092}, 2023.

\bibitem{zheng2024prompt}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock Prompt-driven llm safeguarding via directed representation optimization.
\newblock {\em arXiv preprint arXiv:2401.18018}, 2024.

\bibitem{vicuna}
Lianmin Zheng, Wei{-}Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric~P. Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em CoRR}, abs/2306.05685, 2023.

\bibitem{zhu2018texygen}
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Texygen: A benchmarking platform for text generation models, 2018.

\bibitem{zou2023representation}
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael~J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J.~Zico Kolter, and Dan Hendrycks.
\newblock Representation engineering: A top-down approach to ai transparency, 2023.

\bibitem{zou2023universal}
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models, 2023.

\bibitem{gcg}
Andy Zou, Zifan Wang, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock {\em CoRR}, abs/2307.15043, 2023.

\end{thebibliography}
