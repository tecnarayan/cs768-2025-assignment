\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Dud{\'\i}k, Kale, Langford, and
  Schapire]{agarwal2012contextual}
Alekh Agarwal, Miroslav Dud{\'\i}k, Satyen Kale, John Langford, and Robert~E
  Schapire.
\newblock Contextual bandit learning with predictable rewards.
\newblock In \emph{AISTATS}, 2012.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{MLJ}, 2008.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SICOMP}, 2002.

\bibitem[Azizzadenesheli et~al.(2016)Azizzadenesheli, Lazaric, and
  Anandkumar]{azizzadenesheli2016reinforcement}
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar.
\newblock Reinforcement learning of {POMDP}s using spectral methods.
\newblock In \emph{COLT}, 2016.

\bibitem[Baird(1995)]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{ICML}, 1995.

\bibitem[Brafman and Tennenholtz(2003)]{brafman2003r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max -- a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{JMLR}, 2003.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{NIPS}, 2015.

\bibitem[Dudik et~al.(2011)Dudik, Hsu, Kale, Karampatziakis, Langford, Reyzin,
  and Zhang]{dudik2011efficient}
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford,
  Lev Reyzin, and Tong Zhang.
\newblock Efficient optimal learning for contextual bandits.
\newblock In \emph{UAI}, 2011.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, and Singh]{jiang2015abstraction}
Nan Jiang, Alex Kulesza, and Satinder Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In \emph{ICML}, 2015.

\bibitem[Jong and Stone(2007)]{jong2007model}
Nicholas~K Jong and Peter Stone.
\newblock Model-based exploration in continuous state spaces.
\newblock In \emph{Abstraction, Reformulation, and Approximation}, 2007.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, 2002.

\bibitem[Kakade et~al.(2003)Kakade, Kearns, and
  Langford]{kakade2003exploration}
Sham Kakade, Michael~J Kearns, and John Langford.
\newblock Exploration in metric state spaces.
\newblock In \emph{ICML}, 2003.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{MLJ}, 2002.

\bibitem[Kearns et~al.(1999)Kearns, Mansour, and Ng]{kearns1999approximate}
Michael~J Kearns, Yishay Mansour, and Andrew~Y Ng.
\newblock Approximate planning in large {POMDP}s via reusable trajectories.
\newblock In \emph{NIPS}, 1999.

\bibitem[Kearns et~al.(2002)Kearns, Mansour, and Ng]{kearns2002sparse}
Michael~J. Kearns, Yishay Mansour, and Andrew~Y. Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large markov
  decision processes.
\newblock \emph{MLJ}, 2002.

\bibitem[Langford and Zhang(2008)]{langford2008epoch}
John Langford and Tong Zhang.
\newblock The epoch-greedy algorithm for multi-armed bandits with side
  information.
\newblock In \emph{NIPS}, 2008.

\bibitem[Li and Littman(2010)]{li2010reducing}
Lihong Li and Michael~L Littman.
\newblock Reducing reinforcement learning to {KWIK} online regression.
\newblock \emph{Ann. Math AI}, 2010.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Lihong Li, Thomas~J Walsh, and Michael~L Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock In \emph{ISAIM}, 2006.

\bibitem[Mansour(1999)]{mansour1999reinforcement}
Yishay Mansour.
\newblock Reinforcement learning and mistake bounded algorithms.
\newblock In \emph{COLT}, 1999.

\bibitem[Meuleau et~al.(1999)Meuleau, Peshkin, Kim, and
  Kaelbling]{meuleau1999learning}
Nicolas Meuleau, Leonid Peshkin, Kee-Eung Kim, and Leslie~Pack Kaelbling.
\newblock Learning finite-state controllers for partially observable
  environments.
\newblock In \emph{UAI}, 1999.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Charles, Amir,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, Stig Petersen, Beattie Charles, Sadik Amir, Ioannis Antonoglou,
  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Nguyen et~al.(2013)Nguyen, Maillard, Ryabko, and
  Ortner]{nguyen2013competing}
Phuong Nguyen, Odalric-Ambrym Maillard, Daniil Ryabko, and Ronald Ortner.
\newblock Competing with an infinite set of models in reinforcement learning.
\newblock In \emph{AISTATS}, 2013.

\bibitem[Pazis and Parr(2016)]{pazis2016efficient}
Jason Pazis and Ronald Parr.
\newblock Efficient {PAC}-optimal exploration in concurrent, continuous state
  {MDP}s with delayed updates.
\newblock In \emph{AAAI}, 2016.

\bibitem[Perkins and Precup(2002)]{perkins2002convergent}
Theodore~J Perkins and Doina Precup.
\newblock A convergent form of approximate policy iteration.
\newblock In \emph{NIPS}, 2002.

\bibitem[Reveliotis and Bountourelis(2007)]{reveliotis2007efficient}
Spyros Reveliotis and Theologos Bountourelis.
\newblock Efficient {PAC} learning for episodic tasks with acyclic state
  spaces.
\newblock \emph{DEDS}, 2007.

\bibitem[Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman]{strehl2006pac}
Alexander~L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L
  Littman.
\newblock {PAC} model-free reinforcement learning.
\newblock In \emph{ICML}, 2006.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{NIPS}, 1999.

\bibitem[Tsitsiklis and Van~Roy(1997)]{tsitsiklis1997analysis}
John~N Tsitsiklis and Benjamin Van~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE TAC}, 1997.

\end{thebibliography}
