
@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9712--9724},
  year={2019}
}

@article{zielinski2020explaining,
  title={Explaining Memorization and Generalization: A Large-Scale Study with Coherent Gradients},
  author={Zielinski, Piotr and Krishnan, Shankar and Chatterjee, Satrajit},
  journal={arXiv preprint arXiv:2003.07422},
  year={2020}
}

@article{shah2018minimum,
  title={Minimum weight norm models do not always generalize well for over-parameterized problems},
  author={Shah, Vatsal and Kyrillidis, Anastasios and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:1811.07055},
  year={2018}
}

@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4148--4158},
  year={2017}
}

@article{chatterjee2020coherent,
  title={Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization},
  author={Chatterjee, Satrajit},
  journal={ICLR},
  year={2020}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@article{li2019exponential,
  title={An exponential learning rate schedule for deep learning},
  author={Li, Zhiyuan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1910.07454},
  year={2019}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={stat},
  volume={1050},
  pages={11},
  year={2018}
}

@article{ma2017power,
	title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
	author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
	journal={arXiv preprint arXiv:1712.06559},
	year={2017}
}

@article{poggio2017theory,
	title={Theory of Deep Learning III: explaining the non-overfitting puzzle},
	author={Poggio, Tomaso and Kawaguchi, Kenji and Liao, Qianli and Miranda, Brando and Rosasco, Lorenzo and Boix, Xavier and Hidary, Jack and Mhaskar, Hrushikesh},
	journal={arXiv preprint arXiv:1801.00173},
	year={2017}
}

@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@inproceedings{neyshabur2017exploring,
	title={Exploring generalization in deep learning},
	author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5947--5956},
	year={2017}
}

@article{novak2018sensitivity,
	title={Sensitivity and generalization in neural networks: an empirical study},
	author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	journal={arXiv preprint arXiv:1802.08760},
	year={2018}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={AT\&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  volume={2},
  year={2010}
}

@misc{noauthor_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	year = {1998},
	urldate = {2018-04-27},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:files/73/mnist.html:text/html}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}



@article{goodfellow6572explaining,
  title={Explaining and harnessing adversarial examples (2014)},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={Citeseer}
}

@article{neyshabur2017implicit,
  title={Implicit regularization in deep learning},
  author={Neyshabur, Behnam},
  journal={arXiv preprint arXiv:1709.01953},
  year={2017}
}

@article{brutzkus2017sgd,
	title={Sgd learns over-parameterized networks that provably generalize on linearly separable data},
	author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
	journal={arXiv preprint arXiv:1710.10174},
	year={2017}
}

@article{belkin2018understand,
	title={To understand deep learning we need to understand kernel learning},
	author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
	journal={arXiv preprint arXiv:1802.01396},
	year={2018}
}


@article{krizhevsky2014cifar,
  title={The CIFAR-10 dataset},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  journal={online: http://www. cs. toronto. edu/kriz/cifar. html},
  year={2014}
}

@article{storkey2018cinic,
  title={Cinic-10 is not imagenet or cifar-10},
  author={Darlow, Luke N and Crowley, Elliot J and Antoniou, Antreas and Storkey, Amos J},
  journal={arXiv preprint arXiv:1810.03505},
  year={2018}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã— deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
	month = jun,
	year = {2016},
	keywords = {CIFAR-10, COCO object detection dataset, COCO segmentation, Complexity theory, deep residual learning, deep residual nets, deeper neural network training, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, image classification, image recognition, Image recognition, Image segmentation, ImageNet dataset, ImageNet localization, ImageNet test set, learning (artificial intelligence), neural nets, Neural networks, object detection, residual function learning, residual nets, Training, VGG nets, visual recognition tasks, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Abstract Record:files/56/7780459.html:text/html}
}


@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}


@inproceedings{gunasekar_implicit_nodate,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}

@article{neyshabur_geometry_2017,
	title = {Geometry of {Optimization} and {Implicit} {Regularization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1705.03071},
	abstract = {We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.},
	urldate = {2018-04-26},
	journal = {arXiv:1705.03071 [cs]},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
	month = may,
	year = {2017},
	note = {arXiv: 1705.03071},
	keywords = {Computer Science - Learning},
	annote = {Comment: This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) "Why \& When Deep Learning works -- looking inside Deep Learning" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap with arXiv:1506.02617},
	file = {arXiv\:1705.03071 PDF:files/6/Neyshabur et al. - 2017 - Geometry of Optimization and Implicit Regularizati.pdf:application/pdf;arXiv.org Snapshot:files/7/1705.html:text/html}
}


@inproceedings{lin_generalization_nodate,
  title={Generalization properties and implicit regularization for multiple passes sgm},
  author={Lin, Junhong and Camoriano, Raffaello and Rosasco, Lorenzo},
  booktitle={International Conference on Machine Learning},
  pages={2340--2348},
  year={2016},
  organization={PMLR}
}

@article{neyshabur_implicit_2017,
	title = {Implicit {Regularization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1709.01953},
	abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
	urldate = {2018-04-26},
	journal = {arXiv:1709.01953 [cs]},
	author = {Neyshabur, Behnam},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01953},
	keywords = {Computer Science - Learning},
	annote = {Comment: PhD Thesis},
	file = {arXiv\:1709.01953 PDF:files/22/Neyshabur - 2017 - Implicit Regularization in Deep Learning.pdf:application/pdf;arXiv.org Snapshot:files/23/1709.html:text/html}
}

@article{mhamdi_robustness_2017,
	title = {On {The} {Robustness} of a {Neural} {Network}},
	url = {http://arxiv.org/abs/1707.08167},
	abstract = {With the development of neural networks based machine learning and their usage in mission critical applications, voices are rising against the {\textbackslash}textit\{black box\} aspect of neural networks as it becomes crucial to understand their limits and capabilities. With the rise of neuromorphic hardware, it is even more critical to understand how a neural network, as a distributed system, tolerates the failures of its computing nodes, neurons, and its communication channels, synapses. Experimentally assessing the robustness of neural networks involves the quixotic venture of testing all the possible failures, on all the possible inputs, which ultimately hits a combinatorial explosion for the first, and the impossibility to gather all the possible inputs for the second. In this paper, we prove an upper bound on the expected error of the output when a subset of neurons crashes. This bound involves dependencies on the network parameters that can be seen as being too pessimistic in the average case. It involves a polynomial dependency on the Lipschitz coefficient of the neurons activation function, and an exponential dependency on the depth of the layer where a failure occurs. We back up our theoretical results with experiments illustrating the extent to which our prediction matches the dependencies between the network parameters and robustness. Our results show that the robustness of neural networks to the average crash can be estimated without the need to neither test the network on all failure configurations, nor access the training set used to train the network, both of which are practically impossible requirements.},
	urldate = {2018-04-26},
	journal = {arXiv:1707.08167 [cs, stat]},
	author = {Mhamdi, El Mahdi El and Guerraoui, Rachid and Rouault, Sebastien},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.08167},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: 36th IEEE International Symposium on Reliable Distributed Systems 26 - 29 September 2017. Hong Kong, China},
	file = {arXiv\:1707.08167 PDF:files/28/Mhamdi et al. - 2017 - On The Robustness of a Neural Network.pdf:application/pdf;arXiv.org Snapshot:files/29/1707.html:text/html}
}

@article{neyshabur_search_2014,
	title = {In {Search} of the {Real} {Inductive} {Bias}: {On} the {Role} of {Implicit} {Regularization} in {Deep} {Learning}},
	shorttitle = {In {Search} of the {Real} {Inductive} {Bias}},
	url = {http://arxiv.org/abs/1412.6614},
	abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
	urldate = {2018-04-26},
	journal = {arXiv:1412.6614 [cs, stat]},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6614},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning}
}

@article{soudry2018implicit,
	title={The implicit bias of gradient descent on separable data},
	author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={2822--2878},
	year={2018},
	publisher={JMLR. org}
}

@incollection{NIPS2019_8609,
title = {SGD on Neural Networks Learns Functions of Increasing Complexity},
author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {3491--3501},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@ARTICLE{2014arXiv1412.6572G,
       author = {{Goodfellow}, Ian J. and {Shlens}, Jonathon and {Szegedy}, Christian},
        title = "{Explaining and Harnessing Adversarial Examples}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2014",
        month = "Dec",
          eid = {arXiv:1412.6572},
        pages = {arXiv:1412.6572},
archivePrefix = {arXiv},
       eprint = {1412.6572},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6572G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{poggio2020theoretical,
  title={Theoretical issues in deep networks},
  author={Poggio, Tomaso and Banburski, Andrzej and Liao, Qianli},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}

@techreport{liao2018classical,
  title={Classical generalization bounds are surprisingly tight for deep networks},
  author={Liao, Qianli and Miranda, Brando and Hidary, Jack and Poggio, Tomaso},
  year={2018},
  institution={Center for Brains, Minds and Machines (CBMM)}
}
