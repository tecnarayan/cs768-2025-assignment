\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{http://tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{achille2018emergence}
Achille, A. and Soatto, S.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1947--1980, 2018.

\bibitem[Achille et~al.(2019)Achille, Paolini, Mbeng, and
  Soatto]{achille2019information}
Achille, A., Paolini, G., Mbeng, G., and Soatto, S.
\newblock The information complexity of learning tasks, their structure and
  their distance.
\newblock \emph{arXiv preprint arXiv:1904.03292}, 2019.

\bibitem[Ambroladze et~al.(2007)Ambroladze, Parrado-Hern{\'a}ndez, and
  Shawe-taylor]{ambroladze2007tighter}
Ambroladze, A., Parrado-Hern{\'a}ndez, E., and Shawe-taylor, J.~S.
\newblock Tighter pac-bayes bounds.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  9--16, 2007.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6240--6249, 2017.

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.~M.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Blier \& Ollivier(2018)Blier and Ollivier]{blier2018description}
Blier, L. and Ollivier, Y.
\newblock The description length of deep learning models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2216--2226, 2018.

\bibitem[Catoni(2003)]{catoni2003pac}
Catoni, O.
\newblock A pac-bayesian approach to adaptive classification.
\newblock \emph{preprint}, 840, 2003.

\bibitem[Catoni(2007)]{catoni2007pac}
Catoni, O.
\newblock Pac-bayesian supervised classification: the thermodynamics of
  statistical learning.
\newblock \emph{arXiv preprint arXiv:0712.0248}, 2007.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
Chollet, F. et~al.
\newblock Keras.
\newblock \url{https://keras.io}, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Dillon et~al.(2017)Dillon, Langmore, Tran, Brevdo, Vasudevan, Moore,
  Patton, Alemi, Hoffman, and Saurous]{dillon2017tensorflow}
Dillon, J.~V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
  Patton, B., Alemi, A., Hoffman, M., and Saurous, R.~A.
\newblock Tensorflow distributions.
\newblock \emph{arXiv preprint arXiv:1711.10604}, 2017.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1019--1028. JMLR. org, 2017.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Dong, X., Chen, S., and Pan, S.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4857--4867, 2017.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Dziugaite \& Roy(2018)Dziugaite and Roy]{dziugaite2018data}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Data-dependent pac-bayes priors via differential privacy.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8430--8441, 2018.

\bibitem[Germain et~al.(2016)Germain, Bach, Lacoste, and
  Lacoste-Julien]{germain2016pac}
Germain, P., Bach, F., Lacoste, A., and Lacoste-Julien, S.
\newblock Pac-bayesian theory meets bayesian inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1884--1892, 2016.

\bibitem[Golowich et~al.(2017)Golowich, Rakhlin, and Shamir]{golowich2017size}
Golowich, N., Rakhlin, A., and Shamir, O.
\newblock Size-independent sample complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1712.06541}, 2017.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, B. and Stork, D.~G.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  164--171, 1993.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Huang et~al.(2019)Huang, Touati, Vincent, Dziugaite, Lacoste, and
  Courville]{huang2019stochastic}
Huang, C.-W., Touati, A., Vincent, P., Dziugaite, G.~K., Lacoste, A., and
  Courville, A.
\newblock Stochastic neural network with kronecker flow.
\newblock \emph{arXiv preprint arXiv:1906.04282}, 2019.

\bibitem[Jia \& Su(2019)Jia and Su]{jia2019information}
Jia, Z. and Su, H.
\newblock Information-theoretic local minima characterization and
  regularization.
\newblock \emph{arXiv preprint arXiv:1911.08192}, 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2575--2583, 2015.

\bibitem[Krizhevsky \& Hinton(2010)Krizhevsky and
  Hinton]{krizhevsky2010convolutional}
Krizhevsky, A. and Hinton, G.
\newblock Convolutional deep belief networks on cifar-10.
\newblock \emph{Unpublished manuscript}, 40\penalty0 (7):\penalty0 1--9, 2010.

\bibitem[Kunstner et~al.(2019)Kunstner, Balles, and
  Hennig]{kunstner2019limitations}
Kunstner, F., Balles, L., and Hennig, P.
\newblock Limitations of the empirical fisher approximation.
\newblock \emph{arXiv preprint arXiv:1905.12558}, 2019.

\bibitem[Langford \& Caruana(2002)Langford and Caruana]{langford2002not}
Langford, J. and Caruana, R.
\newblock (not) bounding the true error.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  809--816, 2002.

\bibitem[LeCun \& Cortes(2010)LeCun and
  Cortes]{lecun-mnisthandwrittendigit-2010}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Ledent et~al.(2019)Ledent, Lei, and Kloft]{ledent2019improved}
Ledent, A., Lei, Y., and Kloft, M.
\newblock Improved generalisation bounds for deep learning through covering
  numbers.
\newblock \emph{arXiv preprint arXiv:1905.12430}, 2019.

\bibitem[Li et~al.(2019)Li, Gu, Zhou, Chen, and Banerjee]{li2019hessian}
Li, X., Gu, Q., Zhou, Y., Chen, T., and Banerjee, A.
\newblock Hessian based analysis of sgd for deep nets: Dynamics and
  generalization.
\newblock \emph{arXiv preprint arXiv:1907.10732}, 2019.

\bibitem[Liang et~al.(2017)Liang, Poggio, Rakhlin, and Stokes]{liang2017fisher}
Liang, T., Poggio, T., Rakhlin, A., and Stokes, J.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.01530}, 2017.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[McAllester(1999)]{mcallester1999some}
McAllester, D.~A.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, 1999.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{mishkin2018slang}
Mishkin, A., Kunstner, F., Nielsen, D., Schmidt, M., and Khan, M.~E.
\newblock Slang: Fast structured covariance approximations for bayesian deep
  learning with natural gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6245--6255, 2018.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{nagarajan2019uniform}
Nagarajan, V. and Kolter, J.~Z.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock \emph{arXiv preprint arXiv:1902.04742}, 2019.

\bibitem[Negrea et~al.(2019)Negrea, Dziugaite, and Roy]{negrea2019defense}
Negrea, J., Dziugaite, G.~K., and Roy, D.~M.
\newblock In defense of uniform convergence: Generalization via derandomization
  with an application to interpolating predictors.
\newblock \emph{arXiv preprint arXiv:1912.04265}, 2019.

\bibitem[Parrado-Hern{\'a}ndez et~al.(2012)Parrado-Hern{\'a}ndez, Ambroladze,
  Shawe-Taylor, and Sun]{parrado2012pac}
Parrado-Hern{\'a}ndez, E., Ambroladze, A., Shawe-Taylor, J., and Sun, S.
\newblock Pac-bayes bounds with data dependent priors.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Dec):\penalty0 3507--3531, 2012.

\bibitem[Peng et~al.(2019)Peng, Wu, Chen, and Huang]{peng2019collaborative}
Peng, H., Wu, J., Chen, S., and Huang, J.
\newblock Collaborative channel pruning for deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5113--5122, 2019.

\bibitem[Pitas et~al.(2019)Pitas, Loukas, Davies, and
  Vandergheynst]{pitas2019some}
Pitas, K., Loukas, A., Davies, M., and Vandergheynst, P.
\newblock Some limitations of norm based generalization bounds in deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.09677}, 2019.

\bibitem[Rangamani et~al.(2019)Rangamani, Nguyen, Kumar, Phan, Chin, and
  Tran]{rangamani2019scale}
Rangamani, A., Nguyen, N.~H., Kumar, A., Phan, D., Chin, S.~H., and Tran, T.~D.
\newblock A scale invariant flatness measure for deep network minima.
\newblock \emph{arXiv preprint arXiv:1902.02434}, 2019.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
Rezende, D.~J. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock \emph{arXiv preprint arXiv:1505.05770}, 2015.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem[Suzuki(2019)]{suzuki2019compression}
Suzuki, T.
\newblock Compression based bound for non-compressed network: unified
  generalization error analysis of large compressible deep neural network.
\newblock \emph{arXiv preprint arXiv:1909.11274}, 2019.

\bibitem[Tsuzuku et~al.(2019)Tsuzuku, Sato, and
  Sugiyama]{tsuzuku2019normalized}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock Normalized flat minima: Exploring scale invariant definition of flat
  minima for neural networks using pac-bayesian analysis.
\newblock \emph{arXiv preprint arXiv:1901.04653}, 2019.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wang et~al.(2019)Wang, Grosse, Fidler, and Zhang]{wang2019eigendamage}
Wang, C., Grosse, R., Fidler, S., and Zhang, G.
\newblock Eigendamage: Structured pruning in the kronecker-factored eigenbasis.
\newblock \emph{arXiv preprint arXiv:1905.05934}, 2019.

\bibitem[Wang et~al.(2018)Wang, Keskar, Xiong, and Socher]{wang2018identifying}
Wang, H., Keskar, N.~S., Xiong, C., and Socher, R.
\newblock Identifying generalization properties in neural networks.
\newblock \emph{arXiv preprint arXiv:1809.07402}, 2018.

\bibitem[Wei \& Ma(2019)Wei and Ma]{wei2019data}
Wei, C. and Ma, T.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock \emph{arXiv preprint arXiv:1905.03684}, 2019.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Wen, Y., Vicol, P., Ba, J., Tran, D., and Grosse, R.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock \emph{arXiv preprint arXiv:1803.04386}, 2018.

\bibitem[Wu et~al.(2018)Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and
  Gaunt]{wu2018deterministic}
Wu, A., Nowozin, S., Meeds, E., Turner, R.~E., Hern{\'a}ndez-Lobato, J.~M., and
  Gaunt, A.~L.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.03958}, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\end{thebibliography}
