\begin{thebibliography}{10}

\bibitem{sr}
Miles Cranmer.
\newblock Pysr: Fast \& parallelized symbolic regression in python/julia,
  september 2020.
\newblock {\em URL https://doi. org/10.5281/zenodo}, 4052869.

\bibitem{nofreelunch}
David~H Wolpert and William~G Macready.
\newblock No free lunch theorems for optimization.
\newblock {\em IEEE transactions on evolutionary computation}, 1(1):67--82,
  1997.

\bibitem{citeseer}
C~Lee Giles, Kurt~D Bollacker, and Steve Lawrence.
\newblock Citeseer: An automatic citation indexing system.
\newblock In {\em Proceedings of the third ACM conference on Digital
  libraries}, pages 89--98, 1998.

\bibitem{cora}
Andrew~Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
\newblock Automating the construction of internet portals with machine
  learning.
\newblock {\em Information Retrieval}, 3(2):127--163, 2000.

\bibitem{mrpc}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In {\em Third International Workshop on Paraphrasing (IWP2005)},
  2005.

\bibitem{pubmed}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock {\em AI magazine}, 29(3):93--93, 2008.

\bibitem{rte}
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In {\em TAC}, 2009.

\bibitem{cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{l2lgd2}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{l2o}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock {\em arXiv preprint arXiv:1606.01885}, 2016.

\bibitem{nsp}
Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong
  Zhou, and Pushmeet Kohli.
\newblock Neuro-symbolic program synthesis.
\newblock {\em arXiv preprint arXiv:1611.01855}, 2016.

\bibitem{wideresnet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{nos}
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc~V Le.
\newblock Neural optimizer search with reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  459--468. PMLR, 2017.

\bibitem{sts}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock {\em arXiv preprint arXiv:1708.00055}, 2017.

\bibitem{l2lngd}
Yutian Chen, Matthew~W Hoffman, Sergio~G{\'o}mez Colmenarejo, Misha Denil,
  Timothy~P Lillicrap, Matt Botvinick, and Nando Freitas.
\newblock Learning to learn without gradient descent by gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  748--756. PMLR, 2017.

\bibitem{ppi}
Will Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{pgd}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{gat}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{nas}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em ICLR}, 2017.

\bibitem{houdini}
Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat
  Chaudhuri.
\newblock Houdini: Lifelong learning as program synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{snas}
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
\newblock Snas: stochastic neural architecture search.
\newblock {\em arXiv preprint arXiv:1812.09926}, 2018.

\bibitem{carmon2019}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{clustergcn}
Wei-Lin Chiang, Xuanqing Liu, Si~Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
\newblock Cluster-gcn: An efficient algorithm for training deep and large graph
  convolutional networks.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 257--266, 2019.

\bibitem{gdas}
Xuanyi Dong and Yi~Yang.
\newblock Searching for a robust neural architecture in four gpu hours.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1761--1770, 2019.

\bibitem{repl}
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando
  Solar-Lezama.
\newblock Write, execute, assess: Program synthesis with a repl.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{engstrom2019}
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris
  Tsipras.
\newblock Robustness (python library), 2019.

\bibitem{nasdetect}
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc~V Le.
\newblock Nas-fpn: Learning scalable feature pyramid architecture for object
  detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7036--7045, 2019.

\bibitem{nasnlp}
Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo Zhu.
\newblock Improved differentiable architecture search for language modeling and
  named entity recognition.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 3585--3590, 2019.

\bibitem{bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{symbolic-math}
Guillaume Lample and Fran{\c{c}}ois Charton.
\newblock Deep learning for symbolic mathematics.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{autoloss}
Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli
  Ouyang.
\newblock Am-lfs: Automl for loss function search.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8410--8419, 2019.

\bibitem{fastautoaug}
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim.
\newblock Fast autoaugment.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{autodeeplab}
Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan~L
  Yuille, and Li~Fei-Fei.
\newblock Auto-deeplab: Hierarchical neural architecture search for semantic
  image segmentation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 82--92, 2019.

\bibitem{autoaug}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 2019.

\bibitem{darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{metz2019understanding}
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha
  Sohl-Dickstein.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In {\em International Conference on Machine Learning}, pages
  4556--4565. PMLR, 2019.

\bibitem{wang2020}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:625--641, 2019.

\bibitem{adabert}
Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo
  Deng, Jun Huang, Wei Lin, and Jingren Zhou.
\newblock Adabert: Task-adaptive bert compression with differentiable neural
  architecture search.
\newblock {\em arXiv preprint arXiv:2001.04246}, 2020.

\bibitem{robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock {\em arXiv preprint arXiv:2010.09670}, 2020.

\bibitem{autoattack}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In {\em ICML}, 2020.

\bibitem{gowal2020}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock {\em arXiv preprint arXiv:2010.03593}, 2020.

\bibitem{ogbn}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em Advances in neural information processing systems},
  33:22118--22133, 2020.

\bibitem{nasspeech}
Abhinav Mehrotra, Alberto Gil~CP Ramos, Sourav Bhattacharya, {\L}ukasz Dudziak,
  Ravichander Vipperla, Thomas Chau, Mohamed~S Abdelfattah, Samin Ishtiaq, and
  Nicholas~Donald Lane.
\newblock Nas-bench-asr: Reproducible neural architecture search for speech
  recognition.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{automl-zero}
Esteban Real, Chen Liang, David So, and Quoc Le.
\newblock Automl-zero: Evolving machine learning algorithms from scratch.
\newblock In {\em International Conference on Machine Learning}, pages
  8007--8019. PMLR, 2020.

\bibitem{sehwag2020}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:19655--19666, 2020.

\bibitem{near}
Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, and Swarat
  Chaudhuri.
\newblock Learning differentiable programs with admissible neural heuristics.
\newblock {\em Advances in neural information processing systems},
  33:4940--4952, 2020.

\bibitem{feynman}
Silviu-Marian Udrescu and Max Tegmark.
\newblock Ai feynman: A physics-inspired method for symbolic regression.
\newblock {\em Science Advances}, 6(16):eaay2631, 2020.

\bibitem{dartspt}
Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh.
\newblock Rethinking architecture selection in differentiable nas.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem{wong2020}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock {\em arXiv preprint arXiv:2001.03994}, 2020.

\bibitem{wu2020}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In {\em NeurIPS}, 2020.

\bibitem{l2obench}
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang
  Wang, and Wotao Yin.
\newblock Learning to optimize: A primer and a benchmark.
\newblock {\em arXiv preprint arXiv:2103.12828}, 2021.

\bibitem{dpads}
Guofeng Cui and He~Zhu.
\newblock Differentiable synthesis of program architectures.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{gmnas}
Shoukang Hu, Ruochen Wang, HONG Lanqing, Zhenguo Li, Cho-Jui Hsieh, and Jiashi
  Feng.
\newblock Generalizing few-shot nas with gradient matching.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{symbolicRL}
Mikel Landajuela, Brenden~K Petersen, Sookyung Kim, Claudio~P Santiago, Ruben
  Glatt, Nathan Mundhenk, Jacob~F Pettit, and Daniel Faissol.
\newblock Discovering symbolic policies with deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5979--5989. PMLR, 2021.

\bibitem{deepsr}
Brenden~K Petersen, Mikel Landajuela, T~Nathan Mundhenk, Claudio~P Santiago,
  Soo~K Kim, and Joanne~T Kim.
\newblock Deep symbolic regression: Recovering mathematical expressions from
  data via risk-seeking policy gradients.
\newblock In {\em Proc. of the International Conference on Learning
  Representations}, 2021.

\bibitem{xd}
Nicholas Roberts, Mikhail Khodak, Tri Dao, Liam Li, Christopher R{\'e}, and
  Ameet Talwalkar.
\newblock Rethinking neural operations for diverse tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{vicol2021unbiased}
Paul Vicol, Luke Metz, and Jascha Sohl-Dickstein.
\newblock Unbiased gradient estimation in unrolled computation graphs with
  persistent evolution strategies.
\newblock In {\em International Conference on Machine Learning}, pages
  10553--10563. PMLR, 2021.

\bibitem{symbolicl2o}
Wenqing Zheng, Tianlong Chen, Ting-Kuei Hu, and Zhangyang Wang.
\newblock Symbolic learning to optimize: Towards interpretability and
  scalability.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{automl-opt}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Yao Liu, Kaiyuan Wang,
  Cho-Jui Hsieh, Yifeng Lu, and Quoc~V Le.
\newblock Evolved optimizer for visio.
\newblock In {\em AutoML 2022 Workshop}, 2022.

\bibitem{mc}
Yu~A Shreider.
\newblock {\em The Monte Carlo method: the method of statistical trials},
  volume~87.
\newblock Elsevier, 2014.

\end{thebibliography}
