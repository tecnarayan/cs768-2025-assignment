\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Catfolis(1993)]{catfolis1993method}
T.~Catfolis.
\newblock A method for improving the real-time recurrent learning algorithm.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 807--821, 1993.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Czarnecki, Osindero, Vinyals, Graves,
  Silver, and Kavukcuoglu]{jaderberg2016decoupled}
M.~Jaderberg, W.~M. Czarnecki, S.~Osindero, O.~Vinyals, A.~Graves, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock \emph{arXiv preprint arXiv:1608.05343}, 2016.

\bibitem[Jaeger(2001)]{jaeger2001echo}
H.~Jaeger.
\newblock The “echo state” approach to analysing and training recurrent
  neural networks-with an erratum note.
\newblock \emph{Bonn, Germany: German National Research Center for Information
  Technology GMD Technical Report}, 148\penalty0 (34):\penalty0 13, 2001.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Luko{\v{s}}evi{\v{c}}ius and
  Jaeger(2009)]{lukovsevivcius2009reservoir}
M.~Luko{\v{s}}evi{\v{c}}ius and H.~Jaeger.
\newblock Reservoir computing approaches to recurrent neural network training.
\newblock \emph{Computer Science Review}, 3\penalty0 (3):\penalty0 127--149,
  2009.

\bibitem[Maass et~al.(2002)Maass, Natschl{\"a}ger, and Markram]{maass2002real}
W.~Maass, T.~Natschl{\"a}ger, and H.~Markram.
\newblock Real-time computing without stable states: A new framework for neural
  computation based on perturbations.
\newblock \emph{Neural computation}, 14\penalty0 (11):\penalty0 2531--2560,
  2002.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
M.~P. Marcus, M.~A. Marcinkiewicz, and B.~Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock \emph{Computational linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
G.~Melis, C.~Dyer, and P.~Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018analysis}
S.~Merity, N.~S. Keskar, and R.~Socher.
\newblock An analysis of neural language modeling at multiple scales.
\newblock \emph{arXiv preprint arXiv:1803.08240}, 2018.

\bibitem[Mikolov et~al.(2012)Mikolov, Sutskever, Deoras, Le, Kombrink, and
  Cernocky]{mikolov2012subword}
T.~Mikolov, I.~Sutskever, A.~Deoras, H.-S. Le, S.~Kombrink, and J.~Cernocky.
\newblock Subword language modeling with neural networks.
\newblock \emph{preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char.
  pdf)}, 2012.

\bibitem[Ollivier et~al.(2015)Ollivier, Tallec, and
  Charpiat]{ollivier2015training}
Y.~Ollivier, C.~Tallec, and G.~Charpiat.
\newblock Training recurrent networks online without backtracking.
\newblock \emph{arXiv preprint arXiv:1507.07680}, 2015.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533, 1986.

\bibitem[Tallec and Ollivier(2017{\natexlab{a}})]{tallec2017unbiased}
C.~Tallec and Y.~Ollivier.
\newblock Unbiased online recurrent optimization.
\newblock \emph{arXiv preprint arXiv:1702.05043}, 2017{\natexlab{a}}.

\bibitem[Tallec and Ollivier(2017{\natexlab{b}})]{tallec2017unbiasing}
C.~Tallec and Y.~Ollivier.
\newblock Unbiasing truncated backpropagation through time.
\newblock \emph{arXiv preprint arXiv:1705.08209}, 2017{\natexlab{b}}.

\bibitem[Williams and Peng(1990)]{Williams90anefficient}
R.~J. Williams and J.~Peng.
\newblock An efficient gradient-based algorithm for on-line training of
  recurrent network trajectories.
\newblock \emph{Neural Computation}, 2:\penalty0 490--501, 1990.

\bibitem[Williams and Zipser(1989)]{williams1989learning}
R.~J. Williams and D.~Zipser.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1\penalty0 (2):\penalty0 270--280, 1989.

\bibitem[Zilly et~al.(2016)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2016recurrent}
J.~G. Zilly, R.~K. Srivastava, J.~Koutn{\'\i}k, and J.~Schmidhuber.
\newblock Recurrent highway networks.
\newblock \emph{arXiv preprint arXiv:1607.03474}, 2016.

\end{thebibliography}
