\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ambrosio et~al.(2008)Ambrosio, Gigli, and
  Savar{\'e}]{ambrosio2008gradient}
Ambrosio, L., Gigli, N., and Savar{\'e}, G.
\newblock \emph{Gradient flows: in metric spaces and in the space of
  probability measures}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Arbel et~al.(2019)Arbel, Korba, Salim, and Gretton]{arbel2019maximum}
Arbel, M., Korba, A., Salim, A., and Gretton, A.
\newblock Maximum mean discrepancy gradient flow.
\newblock \emph{Advances in neural information processing systems}, 2019.

\bibitem[Arenz et~al.(2018)Arenz, Neumann, and Zhong]{arenz2018efficient}
Arenz, O., Neumann, G., and Zhong, M.
\newblock Efficient gradient-free variational inference using policy search.
\newblock In \emph{International conference on machine learning}, pp.\
  234--243. PMLR, 2018.

\bibitem[Aubin-Frankowski et~al.(2022)Aubin-Frankowski, Korba, and
  L{\'e}ger]{aubin2022mirror}
Aubin-Frankowski, P.-C., Korba, A., and L{\'e}ger, F.
\newblock Mirror descent with relative smoothness in measure spaces, with
  application to sinkhorn and em.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17263--17275, 2022.

\bibitem[Balasubramanian et~al.(2022)Balasubramanian, Chewi, Erdogdu, Salim,
  and Zhang]{balasubramanian2022towards}
Balasubramanian, K., Chewi, S., Erdogdu, M.~A., Salim, A., and Zhang, S.
\newblock Towards a theory of non-log-concave sampling: first-order
  stationarity guarantees for langevin monte carlo.
\newblock In \emph{Conference on Learning Theory}, pp.\  2896--2923. PMLR,
  2022.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Carrillo et~al.(2019)Carrillo, Craig, and
  Patacchini]{carrillo2019blob}
Carrillo, J.~A., Craig, K., and Patacchini, F.~S.
\newblock A blob method for diffusion.
\newblock \emph{Calculus of Variations and Partial Differential Equations},
  58\penalty0 (2):\penalty0 1--53, 2019.

\bibitem[Carrillo et~al.(2024)Carrillo, Esposito, and Wu]{carrillo2023nonlocal}
Carrillo, J.~A., Esposito, A., and Wu, J. S.-H.
\newblock Nonlocal approximation of nonlinear diffusion equations.
\newblock \emph{Calculus of Variations and Partial Differential Equations},
  63\penalty0 (4):\penalty0 100, 2024.

\bibitem[Chopin et~al.(2024)Chopin, Crucinio, and Korba]{chopin2023connection}
Chopin, N., Crucinio, F.~R., and Korba, A.
\newblock A connection between tempering and entropic mirror descent.
\newblock \emph{International Conference of Machine Learning}, 2024.

\bibitem[Craig et~al.(2023{\natexlab{a}})Craig, Elamvazhuthi, Haberland, and
  Turanova]{craig2023blob}
Craig, K., Elamvazhuthi, K., Haberland, M., and Turanova, O.
\newblock A blob method for inhomogeneous diffusion with applications to
  multi-agent control and sampling.
\newblock \emph{Mathematics of Computation}, 2023{\natexlab{a}}.

\bibitem[Craig et~al.(2023{\natexlab{b}})Craig, Jacobs, and
  Turanova]{craig2023nonlocal}
Craig, K., Jacobs, M., and Turanova, O.
\newblock Nonlocal approximation of slow and fast diffusion.
\newblock \emph{arXiv preprint arXiv:2312.11438}, 2023{\natexlab{b}}.

\bibitem[Delon \& Desolneux(2020)Delon and Desolneux]{delon2020wasserstein}
Delon, J. and Desolneux, A.
\newblock A {W}asserstein-type distance in the space of gaussian mixture
  models.
\newblock \emph{SIAM Journal on Imaging Sciences}, 13\penalty0 (2):\penalty0
  936--970, 2020.

\bibitem[Diao et~al.(2023)Diao, Balasubramanian, Chewi, and
  Salim]{diao2023forward}
Diao, M.~Z., Balasubramanian, K., Chewi, S., and Salim, A.
\newblock Forward-backward {G}aussian variational inference via {JKO} in the
  {B}ures-{W}asserstein space.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7960--7991. PMLR, 2023.

\bibitem[Domingo-Enrich \& Pooladian(2023)Domingo-Enrich and
  Pooladian]{domingo2023explicit}
Domingo-Enrich, C. and Pooladian, A.-A.
\newblock An explicit expansion of the {K}ullback-{L}eibler divergence along
  its {F}isher-{R}ao gradient flow.
\newblock \emph{Transactions of Machine Learning Research}, 2023.

\bibitem[Domke et~al.(2023)Domke, Garrigos, and Gower]{domke2023provable}
Domke, J., Garrigos, G., and Gower, R.
\newblock Provable convergence guarantees for black-box variational inference.
\newblock \emph{Advances in neural information processing systems}, 2023.

\bibitem[Duncan et~al.(2023)Duncan, N{\"u}sken, and
  Szpruch]{duncan2019geometry}
Duncan, A., N{\"u}sken, N., and Szpruch, L.
\newblock On the geometry of {Stein} variational gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 2023.

\bibitem[Friedrichs(1944)]{friedrichs1944identity}
Friedrichs, K.~O.
\newblock The identity of weak and strong extensions of differential operators.
\newblock \emph{Transactions of the American Mathematical Society},
  55:\penalty0 132--151, 1944.

\bibitem[Garrigos \& Gower(2023)Garrigos and Gower]{garrigos2023handbook}
Garrigos, G. and Gower, R.~M.
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock \emph{arXiv preprint arXiv:2301.11235}, 2023.

\bibitem[Gershman et~al.(2012)Gershman, Hoffman, and
  Blei]{gershman2012nonparametric}
Gershman, S., Hoffman, M., and Blei, D.
\newblock Nonparametric variational inference.
\newblock \emph{International Conference of Machine Learning}, 2012.

\bibitem[Helin \& Kretschmann(2022)Helin and Kretschmann]{helin2022non}
Helin, T. and Kretschmann, R.
\newblock Non-asymptotic error estimates for the laplace approximation in
  bayesian inverse problems.
\newblock \emph{Numerische Mathematik}, 150\penalty0 (2):\penalty0 521--549,
  2022.

\bibitem[Jiang et~al.(2023)Jiang, Chewi, and Pooladian]{jiang2023algorithms}
Jiang, Y., Chewi, S., and Pooladian, A.-A.
\newblock Algorithms for mean-field variational inference via polyhedral
  optimization in the {W}asserstein space.
\newblock \emph{arXiv preprint arXiv:2312.02849}, 2023.

\bibitem[Katsevich \& Rigollet(2023)Katsevich and
  Rigollet]{katsevich2023approximation}
Katsevich, A. and Rigollet, P.
\newblock On the approximation accuracy of gaussian variational inference.
\newblock \emph{arXiv preprint arXiv:2301.02168}, 2023.

\bibitem[Korba et~al.(2020)Korba, Salim, Arbel, Luise, and
  Gretton]{korba2020non}
Korba, A., Salim, A., Arbel, M., Luise, G., and Gretton, A.
\newblock A non-asymptotic analysis for {S}tein variational gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4672--4682, 2020.

\bibitem[Korba et~al.(2021)Korba, Aubin-Frankowski, Majewski, and
  Ablin]{korba2021kernel}
Korba, A., Aubin-Frankowski, P.-C., Majewski, S., and Ablin, P.
\newblock Kernel {S}tein discrepancy descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5719--5730. PMLR, 2021.

\bibitem[Kunstner et~al.(2021)Kunstner, Kumar, and
  Schmidt]{kunstner2021homeomorphic}
Kunstner, F., Kumar, R., and Schmidt, M.
\newblock Homeomorphic-invariance of {EM}: Non-asymptotic convergence in {KL}
  divergence for exponential families via mirror descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3295--3303. PMLR, 2021.

\bibitem[Lacker(2023)]{lacker2023independent}
Lacker, D.
\newblock Independent projections of diffusions: Gradient flows for variational
  inference and optimal mean field approximations.
\newblock \emph{arXiv preprint arXiv:2309.13332}, 2023.

\bibitem[Lambert et~al.(2022)Lambert, Chewi, Bach, Bonnabel, and
  Rigollet]{lambert2022variational}
Lambert, M., Chewi, S., Bach, F., Bonnabel, S., and Rigollet, P.
\newblock Variational inference via {W}asserstein gradient flows.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 14434--14447, 2022.

\bibitem[{\L}atuszy{\'n}ski et~al.(2013){\L}atuszy{\'n}ski, Miasojedow, and
  Niemiro]{latuszynski2013nonasymptotic}
{\L}atuszy{\'n}ski, K., Miasojedow, B., and Niemiro, W.
\newblock Nonasymptotic bounds on the estimation error of mcmc algorithms.
\newblock \emph{Bernoulli}, 19\penalty0 (5A):\penalty0 2033--2066, 2013.

\bibitem[Li \& Barron(1999)Li and Barron]{li1999mixture}
Li, J. and Barron, A.
\newblock Mixture density estimation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Li et~al.(2022)Li, Liu, Korba, Yurochkin, and Solomon]{li2022sampling}
Li, L., Liu, Q., Korba, A., Yurochkin, M., and Solomon, J.
\newblock Sampling with mollified interaction energy descent.
\newblock \emph{arXiv preprint arXiv:2210.13400}, 2022.

\bibitem[Lin et~al.(2019)Lin, Khan, and Schmidt]{lin2019fast}
Lin, W., Khan, M.~E., and Schmidt, M.
\newblock Fast and simple natural-gradient variational inference with mixture
  of exponential-family approximations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3992--4002. PMLR, 2019.

\bibitem[Moins et~al.(2023)Moins, Arbel, Dutfoy, and Girard]{moins2023use}
Moins, T., Arbel, J., Dutfoy, A., and Girard, S.
\newblock On the use of a local $\hat{R}$ to improve {MCMC} convergence
  diagnostic.
\newblock \emph{Bayesian Analysis}, 1\penalty0 (1):\penalty0 1--26, 2023.

\bibitem[Moulines \& Bach(2011)Moulines and Bach]{moulines2011non}
Moulines, E. and Bach, F.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Otto(2001)]{otto2001geometry}
Otto, F.
\newblock The geometry of dissipative evolution equations: the porous medium
  equation.
\newblock \emph{Taylor \& Francis}, 2001.

\bibitem[Pavliotis(2014)]{pavliotis2014stochastic}
Pavliotis, G.~A.
\newblock \emph{Stochastic processes and applications: diffusion processes, the
  Fokker-Planck and Langevin equations}, volume~60.
\newblock Springer, 2014.

\bibitem[Raskutti \& Mukherjee(2015)Raskutti and
  Mukherjee]{raskutti2015information}
Raskutti, G. and Mukherjee, S.
\newblock The information geometry of mirror descent.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (3):\penalty0 1451--1457, 2015.

\bibitem[Roberts \& Rosenthal(2004)Roberts and Rosenthal]{roberts2004general}
Roberts, G.~O. and Rosenthal, J.~S.
\newblock General state space {M}arkov chains and mcmc algorithms.
\newblock \emph{Probability surveys}, 1:\penalty0 20--71, 2004.

\bibitem[Salim et~al.(2020)Salim, Korba, and Luise]{salim2020wasserstein}
Salim, A., Korba, A., and Luise, G.
\newblock The {W}asserstein proximal gradient algorithm.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12356--12366, 2020.

\bibitem[Santambrogio(2017)]{santambrogio2017euclidean}
Santambrogio, F.
\newblock $\{$Euclidean, metric, and Wasserstein$\}$ gradient flows: an
  overview.
\newblock \emph{Bulletin of Mathematical Sciences}, 7:\penalty0 87--154, 2017.

\bibitem[Tops{\o}e(2007)]{topsoe12007some}
Tops{\o}e, F.
\newblock Some bounds for the logarithmic function.
\newblock \emph{Inequality theory and applications}, 4:\penalty0 137, 2007.

\bibitem[Villani(2009)]{villani2009optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Villani(2021)]{villani2021topics}
Villani, C.
\newblock \emph{Topics in optimal transportation}, volume~58.
\newblock American Mathematical Soc., 2021.

\bibitem[Wibisono(2018)]{wibisono2018sampling}
Wibisono, A.
\newblock Sampling as optimization in the space of measures: The {L}angevin
  dynamics as a composite optimization problem.
\newblock In \emph{Conference on Learning Theory}, pp.\  2093--3027. PMLR,
  2018.

\bibitem[Yao \& Yang(2022)Yao and Yang]{yao2022mean}
Yao, R. and Yang, Y.
\newblock Mean field variational inference via {W}asserstein gradient flow.
\newblock \emph{arXiv preprint arXiv:2207.08074}, 2022.

\bibitem[Yi \& Liu(2023)Yi and Liu]{yi2023bridging}
Yi, M. and Liu, S.
\newblock Bridging the gap between variational inference and wasserstein
  gradient flows.
\newblock \emph{arXiv preprint arXiv:2310.20090}, 2023.

\bibitem[Zhang et~al.(2018)Zhang, B{\"u}tepage, Kjellstr{\"o}m, and
  Mandt]{zhang2018advances}
Zhang, C., B{\"u}tepage, J., Kjellstr{\"o}m, H., and Mandt, S.
\newblock Advances in variational inference.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 2008--2026, 2018.

\end{thebibliography}
