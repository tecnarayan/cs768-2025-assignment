\begin{thebibliography}{10}

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock {\em URL https://s3-us-west-2. amazonaws.
  com/openai-assets/researchcovers/languageunsupervised/language understanding
  paper. pdf}, 2018.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, pages 4171--4186, 2019.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8), 2019.

\bibitem{song19MASS}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
\newblock {MASS}: Masked sequence to sequence pre-training for language
  generation.
\newblock In {\em ICML}, pages 5926--5936, 2019.

\bibitem{Yang2019XLNet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In {\em NIPS}, pages 5754--5764, 2019.

\bibitem{Dong2019Unilm}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock In {\em NIPS}, pages 13042--13054, 2019.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{joshi2019spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock {\em arXiv preprint arXiv:1907.10529}, 2019.

\bibitem{clark2020electra}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock {\em arXiv preprint arXiv:2003.10555}, 2020.

\bibitem{Vaswani2017transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, pages 5998--6008, 2017.

\bibitem{Yiming2019WWM}
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and
  Guoping Hu.
\newblock Pre-training with whole word masking for chinese {BERT}.
\newblock {\em CoRR}, abs/1906.08101, 2019.

\bibitem{shaw-etal-2018-self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In {\em NAACL}, pages 464--468, June 2018.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{Zhu_2015_ICCV}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In {\em ICCV}, pages 19--27, December 2015.

\bibitem{radford2019GPT2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem{Yinhan2019Roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR}, abs/1907.11692, 2019.

\bibitem{Trieu2018Stories}
Trieu~H. Trinh and Quoc~V. Le.
\newblock A simple method for commonsense reasoning.
\newblock {\em CoRR}, abs/1806.02847, 2018.

\bibitem{kingma2014method}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em ICLR}, 2019.

\bibitem{Alex2018CoLA}
Alex Warstadt, Amanpreet Singh, and Samuel~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock {\em CoRR}, abs/1805.12471, 2018.

\bibitem{socher2013sst}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em EMNLP}, pages 1631--1642, October 2013.

\bibitem{dolan2005mrpc}
William~B. Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In {\em Proceedings of the Third International Workshop on
  Paraphrasing ({IWP}2005)}, 2005.

\bibitem{cer2017stsb}
Daniel Cer, Mona Diab, Eneko Agirre, I{\~n}igo Lopez-Gazpio, and Lucia Specia.
\newblock {S}em{E}val-2017 task 1: Semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock In {\em Proceedings of the 11th International Workshop on Semantic
  Evaluation ({S}em{E}val-2017)}, pages 1--14, August 2017.

\bibitem{williams2018mnli}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em NAACL}, pages 1112--1122, June 2018.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In {\em EMNLP}, pages 2383--2392, 2016.

\bibitem{dagan2006rte}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In {\em Machine Learning Challenges. Evaluating Predictive
  Uncertainty, Visual Object Classification, and Recognising Tectual
  Entailment}, pages 177--190, 2006.

\bibitem{winograd2012wnli}
Hector~J. Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The {Winograd} {Schema} {Challenge}.
\newblock In {\em Proceedings of the {Thirteenth} {International} {Conference}
  on {Principles} of {Knowledge} {Representation} and {Reasoning}}, pages
  552--561. Rome, Italy, 2012.

\bibitem{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In {\em ACL}, pages 784--789, 2018.

\bibitem{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock {RACE}: Large-scale {R}e{A}ding comprehension dataset from
  examinations.
\newblock In {\em EMNLP}, pages 785--794, 2017.

\bibitem{maas-EtAl2011imdb}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics: Human Language Technologies}, pages 142--150, June
  2011.

\bibitem{Chi2019IMDB}
Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.
\newblock How to fine-tune {BERT} for text classification?
\newblock {\em CoRR}, abs/1905.05583, 2019.

\end{thebibliography}
