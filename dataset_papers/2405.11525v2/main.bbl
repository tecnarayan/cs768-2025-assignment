\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar, and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K., and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer and communications security}, pp.\  308--318, 2016.

\bibitem[Albuquerque et~al.(2019)Albuquerque, Monteiro, Darvishi, Falk, and Mitliagkas]{albuquerque2019generalizing}
Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T.~H., and Mitliagkas, I.
\newblock Generalizing to unseen domains via distribution matching.
\newblock \emph{arXiv preprint arXiv:1911.00804}, 2019.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and Rabbat]{assran2019stochastic}
Assran, M., Loizou, N., Ballas, N., and Rabbat, M.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  344--353. PMLR, 2019.

\bibitem[Beltr{\'a}n et~al.(2023)Beltr{\'a}n, P{\'e}rez, S{\'a}nchez, Bernal, Bovet, P{\'e}rez, P{\'e}rez, and Celdr{\'a}n]{beltran2023decentralized}
Beltr{\'a}n, E. T.~M., P{\'e}rez, M.~Q., S{\'a}nchez, P. M.~S., Bernal, S.~L., Bovet, G., P{\'e}rez, M.~G., P{\'e}rez, G.~M., and Celdr{\'a}n, A.~H.
\newblock Decentralized federated learning: Fundamentals, state of the art, frameworks, trends, and challenges.
\newblock \emph{IEEE Communications Surveys \& Tutorials}, 2023.

\bibitem[Ben-David et~al.(2006)Ben-David, Blitzer, Crammer, and Pereira]{ben2006analysis}
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira, and Vaughan]{ben2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79:\penalty0 151--175, 2010.

\bibitem[Carlini et~al.(2022{\natexlab{a}})Carlini, Chien, Nasr, Song, Terzis, and Tramer]{carlini2022membership}
Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F.
\newblock Membership inference attacks from first principles.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pp.\  1897--1914. IEEE, 2022{\natexlab{a}}.

\bibitem[Carlini et~al.(2022{\natexlab{b}})Carlini, Feldman, and Nasr]{carlini2022no}
Carlini, N., Feldman, V., and Nasr, M.
\newblock No free lunch in" privacy for free: How does dataset condensation help privacy".
\newblock \emph{arXiv preprint arXiv:2209.14987}, 2022{\natexlab{b}}.

\bibitem[Cazenavette et~al.(2022)Cazenavette, Wang, Torralba, Efros, and Zhu]{cazenavette2022dataset}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.-Y.
\newblock Dataset distillation by matching training trajectories.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  4750--4759, 2022.

\bibitem[Chan \& Ngai(2021)Chan and Ngai]{chan2021fedhe}
Chan, Y.~H. and Ngai, E.~C.
\newblock Fedhe: Heterogeneous models and communication-efficient federated learning.
\newblock In \emph{2021 17th International Conference on Mobility, Sensing and Networking (MSN)}, pp.\  207--214. IEEE, 2021.

\bibitem[Chang et~al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen, Rubin, and Kalpathy-Cramer]{chang2018distributed}
Chang, K., Balachandar, N., Lam, C., Yi, D., Brown, J., Beers, A., Rosen, B., Rubin, D.~L., and Kalpathy-Cramer, J.
\newblock Distributed deep learning networks among institutions for medical imaging.
\newblock \emph{Journal of the American Medical Informatics Association}, 25\penalty0 (8):\penalty0 945--954, 2018.

\bibitem[Chuang \& Mroueh(2021)Chuang and Mroueh]{chuang2021fair}
Chuang, C.-Y. and Mroueh, Y.
\newblock Fair mixup: Fairness via interpolation.
\newblock \emph{arXiv preprint arXiv:2103.06503}, 2021.

\bibitem[Crammer et~al.(2008)Crammer, Kearns, and Wortman]{crammer2008learning}
Crammer, K., Kearns, M., and Wortman, J.
\newblock Learning from multiple sources.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (8), 2008.

\bibitem[Cui et~al.(2023)Cui, Wang, Si, and Hsieh]{cui2023scaling}
Cui, J., Wang, R., Si, S., and Hsieh, C.-J.
\newblock Scaling up dataset distillation to imagenet-1k with constant memory.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6565--6590. PMLR, 2023.

\bibitem[Donahue \& Kleinberg(2021)Donahue and Kleinberg]{donahue2021model}
Donahue, K. and Kleinberg, J.
\newblock Model-sharing games: Analyzing federated learning under voluntary participation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  5303--5311, 2021.

\bibitem[Dong et~al.(2022)Dong, Zhao, and Lyu]{dong2022privacy}
Dong, T., Zhao, B., and Lyu, L.
\newblock Privacy for free: How does dataset condensation help privacy?
\newblock \emph{arXiv preprint arXiv:2206.00240}, 2022.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and Ozdaglar]{fallah2020personalized}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3557--3568, 2020.

\bibitem[Feng et~al.(2021)Feng, You, Chen, Zhang, Zhu, Wu, Wu, and Chen]{feng2021kd3a}
Feng, H., You, Z., Chen, M., Zhang, T., Zhu, M., Wu, F., Wu, C., and Chen, W.
\newblock Kd3a: Unsupervised multi-source decentralized domain adaptation via knowledge distillation.
\newblock In \emph{ICML}, pp.\  3274--3283, 2021.

\bibitem[Ganin \& Lempitsky(2015)Ganin and Lempitsky]{ganin2015unsupervised}
Ganin, Y. and Lempitsky, V.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International conference on machine learning}, pp.\  1180--1189. PMLR, 2015.

\bibitem[Gao et~al.(2022)Gao, Yao, and Yang]{gao2022survey}
Gao, D., Yao, X., and Yang, Q.
\newblock A survey on heterogeneous federated learning.
\newblock \emph{arXiv preprint arXiv:2210.04505}, 2022.

\bibitem[Ghosh et~al.(2022)Ghosh, Chung, Yin, and Ramchandran]{ghosh2022efficient}
Ghosh, A., Chung, J., Yin, D., and Ramchandran, K.
\newblock An efficient framework for clustered federated learning.
\newblock \emph{IEEE Transactions on Information Theory}, 68\penalty0 (12):\penalty0 8076--8091, 2022.

\bibitem[Gong et~al.(2022)Gong, Sharma, Karanam, Wu, Chen, Doermann, and Innanje]{gong2022preserving}
Gong, X., Sharma, A., Karanam, S., Wu, Z., Chen, T., Doermann, D., and Innanje, A.
\newblock Preserving privacy in federated learning with ensemble cross-domain knowledge distillation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  11891--11899, 2022.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and Smola]{gretton2012kernel}
Gretton, A., Borgwardt, K.~M., Rasch, M.~J., Sch{\"o}lkopf, B., and Smola, A.
\newblock A kernel two-sample test.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0 (1):\penalty0 723--773, 2012.

\bibitem[Griffin et~al.(2007)Griffin, Holub, and Perona]{griffin2007caltech}
Griffin, G., Holub, A., and Perona, P.
\newblock Caltech-256 object category dataset.
\newblock 2007.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Huang et~al.(2022)Huang, Ye, and Du]{huang2022learn}
Huang, W., Ye, M., and Du, B.
\newblock Learn from others and be yourself in heterogeneous federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10143--10153, 2022.

\bibitem[Huang et~al.(2021{\natexlab{a}})Huang, Chu, Zhou, Wang, Liu, Pei, and Zhang]{huang2021personalized}
Huang, Y., Chu, L., Zhou, Z., Wang, L., Liu, J., Pei, J., and Zhang, Y.
\newblock Personalized cross-silo federated learning on non-iid data.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  7865--7873, 2021{\natexlab{a}}.

\bibitem[Huang et~al.(2021{\natexlab{b}})Huang, Gupta, Song, Li, and Arora]{huang2021evaluating}
Huang, Y., Gupta, S., Song, Z., Li, K., and Arora, S.
\newblock Evaluating gradient inversion attacks and defenses in federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 7232--7241, 2021{\natexlab{b}}.

\bibitem[Hull(1994)]{hull1994database}
Hull, J.~J.
\newblock A database for handwritten text recognition research.
\newblock \emph{IEEE Transactions on pattern analysis and machine intelligence}, 16\penalty0 (5):\penalty0 550--554, 1994.

\bibitem[Jeong et~al.(2018)Jeong, Oh, Kim, Park, Bennis, and Kim]{jeong2018federated}
Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., and Kim, S.
\newblock Federated distillation and augmentation under non-iid private data.
\newblock \emph{NIPS Wksp. MLPCD}, 2018.

\bibitem[Jiang et~al.(2023)Jiang, Yang, Cheng, and Dou]{jiang2023iop}
Jiang, M., Yang, H., Cheng, C., and Dou, Q.
\newblock Iop-fl: Inside-outside personalization for federated medical image segmentation.
\newblock \emph{IEEE Transactions on Medical Imaging}, 2023.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5132--5143. PMLR, 2020.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Karras, T., Laine, S., and Aila, T.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  4401--4410, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lee et~al.(2022)Lee, Chun, Jung, Yun, and Yoon]{lee2022dataset}
Lee, S., Chun, S., Jung, S., Yun, S., and Yoon, S.
\newblock Dataset condensation with contrastive signals.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12352--12364. PMLR, 2022.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Li, and Varshney]{li2021decentralized}
Li, C., Li, G., and Varshney, P.~K.
\newblock Decentralized federated learning via mutual knowledge transfer.
\newblock \emph{IEEE Internet of Things Journal}, 9\penalty0 (2):\penalty0 1136--1147, 2021{\natexlab{a}}.

\bibitem[Li \& Wang(2019)Li and Wang]{li2019fedmd}
Li, D. and Wang, J.
\newblock Fedmd: Heterogenous federated learning via model distillation.
\newblock \emph{arXiv preprint arXiv:1910.03581}, 2019.

\bibitem[Li et~al.(2022)Li, Togo, Ogawa, and Haseyama]{li2022dataset}
Li, G., Togo, R., Ogawa, T., and Haseyama, M.
\newblock Dataset distillation for medical dataset sharing.
\newblock \emph{arXiv preprint arXiv:2209.14603}, 2022.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, He, and Song]{li2021model}
Li, Q., He, B., and Song, D.
\newblock Model-contrastive federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10713--10722, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0 429--450, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock \emph{International Conference on Learning Representations}, 2020{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2351--2363, 2020.

\bibitem[Luo \& Ye(2022)Luo and Ye]{luo2022decentralized}
Luo, L. and Ye, H.
\newblock Decentralized stochastic variance reduced extragradient method.
\newblock \emph{arXiv preprint arXiv:2202.00509}, 2022.

\bibitem[Marfoq et~al.(2021)Marfoq, Neglia, Bellet, Kameni, and Vidal]{marfoq2021federated}
Marfoq, O., Neglia, G., Bellet, A., Kameni, L., and Vidal, R.
\newblock Federated multi-task learning under a mixture of distributions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 15434--15447, 2021.

\bibitem[Matsuda et~al.(2022)Matsuda, Sasaki, Xiao, and Onizuka]{matsuda2022fedme}
Matsuda, K., Sasaki, Y., Xiao, C., and Onizuka, M.
\newblock Fedme: Federated learning via model exchange.
\newblock In \emph{Proceedings of the 2022 SIAM international conference on data mining (SDM)}, pp.\  459--467. SIAM, 2022.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282. PMLR, 2017.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Pappas et~al.(2021)Pappas, Chatzopoulos, Lalis, and Vavalis]{pappas2021ipls}
Pappas, C., Chatzopoulos, D., Lalis, S., and Vavalis, M.
\newblock Ipls: A framework for decentralized federated learning.
\newblock In \emph{2021 IFIP Networking Conference (IFIP Networking)}, pp.\  1--6. IEEE, 2021.

\bibitem[Rostami(2021)]{rostami2021lifelong}
Rostami, M.
\newblock Lifelong domain adaptation via consolidated internal distribution.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 11172--11183, 2021.

\bibitem[Roy et~al.(2019)Roy, Siddiqui, P{\"o}lsterl, Navab, and Wachinger]{roy2019braintorrent}
Roy, A.~G., Siddiqui, S., P{\"o}lsterl, S., Navab, N., and Wachinger, C.
\newblock Braintorrent: A peer-to-peer environment for decentralized federated learning.
\newblock \emph{arXiv preprint arXiv:1905.06731}, 2019.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and Darrell]{saenko2010adapting}
Saenko, K., Kulis, B., Fritz, M., and Darrell, T.
\newblock Adapting visual category models to new domains.
\newblock In \emph{Computer Vision--ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11}, pp.\  213--226. Springer, 2010.

\bibitem[Sheller et~al.(2019)Sheller, Reina, Edwards, Martin, and Bakas]{sheller2019multi}
Sheller, M.~J., Reina, G.~A., Edwards, B., Martin, J., and Bakas, S.
\newblock Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation.
\newblock In \emph{Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4}, pp.\  92--104. Springer, 2019.

\bibitem[Sheller et~al.(2020)Sheller, Edwards, Reina, Martin, Pati, Kotrotsou, Milchenko, Xu, Marcus, Colen, et~al.]{sheller2020federated}
Sheller, M.~J., Edwards, B., Reina, G.~A., Martin, J., Pati, S., Kotrotsou, A., Milchenko, M., Xu, W., Marcus, D., Colen, R.~R., et~al.
\newblock Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data.
\newblock \emph{Scientific reports}, 10\penalty0 (1):\penalty0 12598, 2020.

\bibitem[Shen et~al.(2023)Shen, Zhang, Jia, Zhang, Lv, Kuang, Wu, and Wu]{shen2023federated}
Shen, T., Zhang, J., Jia, X., Zhang, F., Lv, Z., Kuang, K., Wu, C., and Wu, F.
\newblock Federated mutual learning: a collaborative machine learning method for heterogeneous data, models, and objectives.
\newblock \emph{Frontiers of Information Technology \& Electronic Engineering}, 24\penalty0 (10):\penalty0 1390--1402, 2023.

\bibitem[Shokri et~al.(2017)Shokri, Stronati, Song, and Shmatikov]{shokri2017membership}
Shokri, R., Stronati, M., Song, C., and Shmatikov, V.
\newblock Membership inference attacks against machine learning models.
\newblock In \emph{2017 IEEE symposium on security and privacy (SP)}, pp.\  3--18. IEEE, 2017.

\bibitem[Song et~al.(2023)Song, Liu, Chen, Festag, Trinitis, Schulz, and Knoll]{song2023federated}
Song, R., Liu, D., Chen, D.~Z., Festag, A., Trinitis, C., Schulz, M., and Knoll, A.
\newblock Federated learning via decentralized dataset distillation in resource-constrained edge environments.
\newblock In \emph{2023 International Joint Conference on Neural Networks (IJCNN)}, pp.\  1--10. IEEE, 2023.

\bibitem[Tan et~al.(2022)Tan, Long, Liu, Zhou, Lu, Jiang, and Zhang]{tan2022fedproto}
Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., and Zhang, C.
\newblock Fedproto: Federated prototype learning across heterogeneous clients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  8432--8440, 2022.

\bibitem[Tang et~al.(2022)Tang, Zhang, Shi, He, Han, and Chu]{tang2022virtual}
Tang, Z., Zhang, Y., Shi, S., He, X., Han, B., and Chu, X.
\newblock Virtual homogeneity learning: Defending against data heterogeneity in federated learning.
\newblock \emph{arXiv preprint arXiv:2206.02465}, 2022.

\bibitem[Wang et~al.(2023)Wang, Chen, Kerkouche, and Fritz]{wang2023fed}
Wang, H.-P., Chen, D., Kerkouche, R., and Fritz, M.
\newblock Fed-gloss-dp: Federated, global learning using synthetic sets with record level differential privacy.
\newblock \emph{arXiv preprint arXiv:2302.01068}, 2023.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7611--7623, 2020.

\bibitem[Wang et~al.(2022)Wang, Zhao, Peng, Zhu, Yang, Wang, Huang, Bilen, Wang, and You]{wang2022cafe}
Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., Huang, G., Bilen, H., Wang, X., and You, Y.
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  12196--12205, 2022.

\bibitem[Wu et~al.(2022)Wu, Wu, Lyu, Huang, and Xie]{wu2022communication}
Wu, C., Wu, F., Lyu, L., Huang, Y., and Xie, X.
\newblock Communication-efficient federated learning via knowledge distillation.
\newblock \emph{Nature communications}, 13\penalty0 (1):\penalty0 2032, 2022.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xiong et~al.(2023)Xiong, Wang, Cheng, Yu, and Hsieh]{xiong2023feddm}
Xiong, Y., Wang, R., Cheng, M., Yu, F., and Hsieh, C.-J.
\newblock Feddm: Iterative distribution matching for communication-efficient federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  16323--16332, 2023.

\bibitem[Yang et~al.(2021)Yang, Tian, and Zhang]{yang2021regularized}
Yang, R., Tian, J., and Zhang, Y.
\newblock Regularized mutual learning for personalized federated learning.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  1521--1536. PMLR, 2021.

\bibitem[Ye et~al.(2020)Ye, Zhou, Luo, and Zhang]{ye2020decentralized}
Ye, H., Zhou, Z., Luo, L., and Zhang, T.
\newblock Decentralized accelerated proximal gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 18308--18317, 2020.

\bibitem[Ye et~al.(2022)Ye, Ni, Xu, Wang, Chen, and Eldar]{ye2022fedfm}
Ye, R., Ni, Z., Xu, C., Wang, J., Chen, S., and Eldar, Y.~C.
\newblock Fedfm: Anchor-based feature matching for data heterogeneity in federated learning.
\newblock \emph{arXiv preprint arXiv:2210.07615}, 2022.

\bibitem[Yuan et~al.(2023{\natexlab{a}})Yuan, Ma, Su, and Wang]{yuan2023peer}
Yuan, L., Ma, Y., Su, L., and Wang, Z.
\newblock Peer-to-peer federated continual learning for naturalistic driving action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  5249--5258, 2023{\natexlab{a}}.

\bibitem[Yuan et~al.(2023{\natexlab{b}})Yuan, Sun, Yu, and Wang]{yuan2023decentralized}
Yuan, L., Sun, L., Yu, P.~S., and Wang, Z.
\newblock Decentralized federated learning: A survey and perspective.
\newblock \emph{arXiv preprint arXiv:2306.01603}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Chen, Li, Lyu, Wu, Ding, Shen, and Wu]{zhang2022dense}
Zhang, J., Chen, C., Li, B., Lyu, L., Wu, S., Ding, S., Shen, C., and Wu, C.
\newblock Dense: Data-free one-shot federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21414--21428, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Shen, Ding, Tao, and Duan]{zhang2022fine}
Zhang, L., Shen, L., Ding, L., Tao, D., and Duan, L.-Y.
\newblock Fine-tuning global model via data-free knowledge distillation for non-iid federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10174--10183, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2018)Zhang, Xiang, Hospedales, and Lu]{zhang2018deep}
Zhang, Y., Xiang, T., Hospedales, T.~M., and Lu, H.
\newblock Deep mutual learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  4320--4328, 2018.

\bibitem[Zhao \& Bilen(2021)Zhao and Bilen]{zhao2021dataset}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12674--12685. PMLR, 2021.

\bibitem[Zhao \& Bilen(2022)Zhao and Bilen]{zhao2022synthesizing}
Zhao, B. and Bilen, H.
\newblock Synthesizing informative training samples with gan.
\newblock \emph{arXiv preprint arXiv:2204.07513}, 2022.

\bibitem[Zhao \& Bilen(2023)Zhao and Bilen]{zhao2023dataset}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with distribution matching.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  6514--6523, 2023.

\bibitem[Zhao et~al.(2020)Zhao, Mopuri, and Bilen]{zhao2020dataset}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhou et~al.(2022)Zhou, Zhang, and Tsang]{zhou2022fedfa}
Zhou, T., Zhang, J., and Tsang, D.
\newblock Fedfa: Federated learning with feature anchors to align feature and classifier for heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2211.09299}, 2022.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{zhu2021data}
Zhu, Z., Hong, J., and Zhou, J.
\newblock Data-free knowledge distillation for heterogeneous federated learning.
\newblock In \emph{International conference on machine learning}, pp.\  12878--12889. PMLR, 2021.

\end{thebibliography}
