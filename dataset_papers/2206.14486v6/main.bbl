\begin{thebibliography}{10}

\bibitem{Hestness2017-yq}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{Kaplan2020-ti}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{Henighan2020-jf}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
  Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock {\em arXiv preprint arXiv:2010.14701}, 2020.

\bibitem{rosenfeld2020a}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock {\em International Conference on Learning Representations}, 2020.

\bibitem{Gordon2021-az}
Mitchell~A Gordon, Kevin Duh, and Jared Kaplan.
\newblock Data and parameter scaling laws for neural machine translation.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5915--5922, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{Hernandez2021-ix}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock {\em arXiv preprint arXiv:2102.01293}, 2021.

\bibitem{Zhai2021-dl}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock {\em arXiv preprint arXiv:2106.04560}, 2021.

\bibitem{Hoffmann2022-gw}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{Toneva2019-hj}
Mariya Toneva, Alessandro Sordoni, Remi~Tachet des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In {\em {ICLR}}, 2019.

\bibitem{Paul2021-ci}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock {\em Adv. Neural Inf. Process. Syst.}, 34, December 2021.

\bibitem{Chitta2021-se}
Kashyap Chitta, Jos{\'e}~M {\'A}lvarez, Elmar Haussmann, and Cl{\'e}ment
  Farabet.
\newblock Training data subset search with ensemble active learning.
\newblock {\em IEEE Trans. Intell. Transp. Syst.}, pages 1--12, 2021.

\bibitem{Bommasani2021-mu}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{Feldman2020-yv}
Vitaly Feldman and Chiyuan Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via
  influence estimation.
\newblock {\em Adv. Neural Inf. Process. Syst.}, 33:2881--2891, 2020.

\bibitem{harutyunyan2021estimating}
Hrayr Harutyunyan, Alessandro Achille, Giovanni Paolini, Orchid Majumder,
  Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto.
\newblock Estimating informativeness of samples with smooth unique information.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Settles2009-mo}
Burr Settles.
\newblock Active learning literature survey.
\newblock {\em Technical Report}, 2009.

\bibitem{Bordes2005-nb}
Antoine Bordes, Seyda Ertekin, Jason Weston, L{\'e}on Botton, and Nello
  Cristianini.
\newblock Fast kernel classifiers with online and active learning.
\newblock {\em J. Mach. Learn. Res.}, 6(9), 2005.

\bibitem{Emam2021-wa}
Zeyad Ali~Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard
  Leapman, Micah Goldblum, and Tom Goldstein.
\newblock Active learning at the {ImageNet} scale.
\newblock {\em arXiv preprint arXiv:2111.12880}, 2021.

\bibitem{Sener2017-on}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock {\em arXiv preprint arXiv:1708.00489}, 2017.

\bibitem{Karamcheti2021-bs}
Siddharth Karamcheti, Ranjay Krishna, Li~Fei-Fei, and Christopher~D Manning.
\newblock Mind your outliers! {Investigating} the negative impact of outliers
  on active learning for visual question answering.
\newblock {\em arXiv preprint arXiv:2107.02331}, 2021.

\bibitem{Mirzasoleiman2020-fy}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In Hal~Daum{\'e} Iii and Aarti Singh, editors, {\em Proceedings of
  the 37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 6950--6960. PMLR, 2020.

\bibitem{Birodkar2019-on}
V~Birodkar, H~Mobahi, and S~Bengio.
\newblock Semantic redundancies in {Image-Classification} datasets: The 10\%
  you don't need.
\newblock {\em arXiv preprint arXiv:1901.11409}, 2019.

\bibitem{meding2022trivial}
Kristof Meding, Luca M.~Schulze Buschoff, Robert Geirhos, and Felix~A.
  Wichmann.
\newblock Trivial or impossible---dichotomous data difficulty masks model
  differences (on {ImageNet} and beyond).
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{JMLR:v23:20-1111}
Utkarsh Sharma and Jared Kaplan.
\newblock Scaling laws from the data manifold dimension.
\newblock {\em Journal of Machine Learning Research}, 23(9):1--34, 2022.

\bibitem{DBLP:journals/corr/abs-2102-06701}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock {\em CoRR}, abs/2102.06701, 2021.

\bibitem{DBLP:phd/us/Rosenfeld21}
Jonathan~S. Rosenfeld.
\newblock {\em Scaling Laws for Deep Learning}.
\newblock PhD thesis, Massachusetts Institute of Technology, {USA}, 2021.

\bibitem{Engel2001-sp}
A~Engel and C~V den Broeck.
\newblock {\em Statistical Mechanics of Learning}.
\newblock Cambridge Univ. Press, 2001.

\bibitem{Advani2013-en}
Madhu Advani, Subhaneil Lahiri, and Surya Ganguli.
\newblock Statistical mechanics of complex neural systems and high dimensional
  data.
\newblock {\em J. Stat. Mech: Theory Exp.}, 2013(03):P03014, 2013.

\bibitem{Bahri2020-mi}
Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam~S Schoenholz, Jascha
  Sohl-Dickstein, and Surya Ganguli.
\newblock Statistical mechanics of deep learning.
\newblock {\em Annual Review of Condensed Matter Physics}, March 2020.

\bibitem{Zdeborova2016-vk}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: thresholds and algorithms.
\newblock {\em Adv. Phys.}, 65(5):453--552, September 2016.

\bibitem{Gardner1988-wr}
E~Gardner.
\newblock The space of interactions in neural network models.
\newblock {\em J. of Physics A}, 21:257--270, 1988.

\bibitem{Seung1992-ob}
H~S Seung, H~Sompolinsky, and N~Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock {\em Phys. Rev. A}, 45(8):6056, 1992.

\bibitem{Freund1992-uv}
Yoav Freund, H~Sebastian Seung, Eli Shamir, and Naftali Tishby.
\newblock Information, prediction, and query by committee.
\newblock {\em Adv. Neural Inf. Process. Syst.}, 5, 1992.

\bibitem{Zhou2019-ha}
Hai-Jun Zhou.
\newblock Active online learning in the binary perceptron problem.
\newblock {\em Commun. Theor. Phys.}, 71(2):243, February 2019.

\bibitem{Cui2021-kp}
Hugo Cui, Luca Saglietti, and Lenka Zdeborov{\`a}.
\newblock Large deviations in the perceptron model and consequences for active
  learning, 2021.

\bibitem{Mezard1987-pc}
M~Mezard, G~Parisi, and M~A Virasoro.
\newblock {\em Spin glass theory and beyond}.
\newblock World scientific Singapore, 1987.

\bibitem{caron_swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 9912--9924. Curran Associates, Inc., 2020.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{Mahajan2018-qf}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In {\em Proceedings of the European conference on computer vision
  ({ECCV})}, pages 181--196, 2018.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{goyal2021vissl}
Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu,
  Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr
  Bojanowski, Armand Joulin, and Ishan Misra.
\newblock {VISSL}.
\newblock \url{https://github.com/facebookresearch/vissl}, 2021.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock {ImageNet} large scale visual recognition challenge.
\newblock {\em {International Journal of Computer Vision}}, 115(3):211--252,
  2015.

\bibitem{yang2020towards}
Kaiyu Yang, Klint Qinami, Li~Fei-Fei, Jia Deng, and Olga Russakovsky.
\newblock Towards fairer datasets: Filtering and balancing the distribution of
  the people subtree in the {ImageNet} hierarchy.
\newblock In {\em Proceedings of the 2020 Conference on Fairness,
  Accountability, and Transparency}, pages 547--558, 2020.

\bibitem{asano2021pass}
Yuki~M Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi.
\newblock {PASS}: An {ImageNet} replacement for self-supervised pretraining
  without humans.
\newblock {\em arXiv preprint arXiv:2109.13228}, 2021.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{johnson2019survey}
Justin~M Johnson and Taghi~M Khoshgoftaar.
\newblock Survey on deep learning with class imbalance.
\newblock {\em Journal of Big Data}, 6(1):1--54, 2019.

\bibitem{geirhos2021partial}
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer,
  Matthias Bethge, Felix~A Wichmann, and Wieland Brendel.
\newblock Partial success in closing the gap between human and machine vision.
\newblock {\em Advances in Neural Information Processing Systems},
  34:23885--23899, 2021.

\bibitem{wichmann2017methods}
Felix~A Wichmann, David~HJ Janssen, Robert Geirhos, Guillermo Aguilar, Heiko~H
  Sch{\"u}tt, Marianne Maertens, and Matthias Bethge.
\newblock Methods and measurements to compare men against machines.
\newblock {\em Electronic Imaging, Human Vision and Electronic Imaging},
  2017(14):36--45, 2017.

\bibitem{geirhos2018generalisation}
Robert Geirhos, Carlos~RM Temme, Jonas Rauber, Heiko~H Sch{\"u}tt, Matthias
  Bethge, and Felix~A Wichmann.
\newblock Generalisation in humans and deep neural networks.
\newblock In {\em {Advances in Neural Information Processing Systems}}, 2018.

\bibitem{geirhos2019imagenettrained}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A.
  Wichmann, and Wieland Brendel.
\newblock {ImageNet}-trained {CNN}s are biased towards texture; increasing
  shape bias improves accuracy and robustness.
\newblock In {\em {International Conference on Learning Representations}},
  2019.

\bibitem{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{geirhos2020beyond}
Robert Geirhos, Kristof Meding, and Felix~A Wichmann.
\newblock Beyond accuracy: quantifying trial-by-trial behaviour of {CNNs} and
  humans by measuring error consistency.
\newblock {\em {Advances in Neural Information Processing Systems}}, 33, 2020.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {ImageNet} classification.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1026--1034, 2015.

\bibitem{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In {\em International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\end{thebibliography}
