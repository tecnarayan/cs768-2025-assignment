\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and Vinyals]{Zhang16}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Alquier et~al.(2016)Alquier, Ridgway, and Chopin]{Alquier16}
Pierre Alquier, James Ridgway, and Nicolas Chopin.
\newblock On the properties of variational approximations of gibbs posteriors.
\newblock \emph{JMLR}, 2016.

\bibitem[Alquier et~al.(2019)Alquier, Cottet, and Lecu{\'e}]{Alquier19}
Pierre Alquier, Vincent Cottet, and Guillaume Lecu{\'e}.
\newblock Estimation bounds and sharp oracle inequalities of regularized
  procedures with lipschitz loss functions.
\newblock \emph{The Annals of Statistics}, 2019.

\bibitem[B{\'e}gin et~al.(2016)B{\'e}gin, Germain, Laviolette, and
  Roy]{Begin16}
Luc B{\'e}gin, Pascal Germain, Fran{\c{c}}ois Laviolette, and Jean-Francis Roy.
\newblock Pac-bayesian bounds based on the r{\'e}nyi divergence.
\newblock In \emph{Artificial Intelligence and Statistics}. PMLR, 2016.

\bibitem[Alquier and Guedj(2018)]{alquier2018simpler}
Pierre Alquier and Benjamin Guedj.
\newblock Simpler pac-bayesian bounds for hostile data.
\newblock \emph{Machine Learning}, 2018.

\bibitem[Germain et~al.(2016)Germain, Bach, Lacoste, and
  Lacoste-Julien]{Germain16}
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien.
\newblock Pac-bayesian theory meets bayesian inference.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Haddouche et~al.(2021)Haddouche, Guedj, Rivasplata, and
  Shawe-Taylor]{haddouche2021pac}
Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor.
\newblock Pac-bayes unleashed: generalisation bounds with unbounded losses.
\newblock \emph{Entropy}, 2021.

\bibitem[van Handel(2016)]{VanHendel}
Ramon van Handel.
\newblock Probability in high dimension, December 2016.

\bibitem[Bartlett et~al.(2017{\natexlab{a}})Bartlett, Foster, and
  Telgarsky]{bartlett17}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{NeurIPS}, 2017{\natexlab{a}}.

\bibitem[Gross(1975)]{gross1975logarithmic}
Leonard Gross.
\newblock Logarithmic sobolev inequalities.
\newblock \emph{American Journal of Mathematics}, 1975.

\bibitem[Titterington et~al.(1985)Titterington, Titterington, M, Smith, Makov,
  and Sons]{titterington1985statistical}
D.M. Titterington, P.S.D.M. Titterington, S.A.F. M, A.F.M. Smith, U.E. Makov,
  and John Wiley~\& Sons.
\newblock \emph{Statistical Analysis of Finite Mixture Distributions}.
\newblock Wiley, 1985.

\bibitem[Scott(1992)]{scott2015multivariate}
David~W Scott.
\newblock \emph{Multivariate density estimation: theory, practice, and
  visualization}.
\newblock John Wiley \& Sons, 1992.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Virmaux and Scaman(2018)]{virmaux2018lipschitz}
Aladin Virmaux and Kevin Scaman.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Combettes and Pesquet(2019)]{combettes2019lipschitz}
Patrick~L Combettes and Jean-Christophe Pesquet.
\newblock Lipschitz certificates for neural network structures driven by
  averaged activation operators.
\newblock \emph{arXiv preprint arXiv:1903.01014}, 2019.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{ICML}, 2019.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock 1998.

\bibitem[Dziugaite and Roy(2017)]{Dziugaite17}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Bartlett et~al.(2017{\natexlab{b}})Bartlett, Harvey, Liaw, and
  Mehrabian]{Bartlett17b}
Peter~L Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{arXiv preprint arXiv:1703.02930}, 2017{\natexlab{b}}.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{Bartlett19}
Peter~L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{JMLR}, 2019.

\bibitem[Bartlett and Mendelson(2002)]{Bartlett02}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{JMLR}, 2002.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{Neyshabur15}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[Golowich et~al.(2017)Golowich, Rakhlin, and Shamir]{Golowich17}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1712.06541}, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{Neyshabur18}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Wei and Ma(2019)]{Wei19}
Colin Wei and Tengyu Ma.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Kakade et~al.(2009)Kakade, Sridharan, and Tewari]{Kakade09}
Sham~M Kakade, Karthik Sridharan, and Ambuj Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In \emph{NeurIPS}, 2009.

\bibitem[Yang et~al.(2019)Yang, Sun, and Roy]{Yang19}
Jun Yang, Shengyang Sun, and Daniel~M Roy.
\newblock Fast-rate pac-bayes generalization bounds via shifted rademacher
  processes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bousquet and Elisseeff(2002)]{Bousquet02}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{JMLR}, 2002.

\bibitem[Rakhlin et~al.(2005)Rakhlin, Mukherjee, and Poggio]{Rakhlin05}
Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio.
\newblock Stability results in learning theory.
\newblock \emph{Analysis and Applications}, 2005.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{Shalev09}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, 2009.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{Hardt15}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1509.01240}, 2015.

\bibitem[Kuzborskij and Lampert(2017)]{Kuzborskij17}
Ilja Kuzborskij and Christoph~H Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1703.01678}, 2017.

\bibitem[London(2017)]{London17}
Ben London.
\newblock A pac-bayesian analysis of randomized learning with application to
  stochastic gradient descent.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Li et~al.(2019)Li, Luo, and Qiao]{li2019generalization}
Jian Li, Xuanyuan Luo, and Mingda Qiao.
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock \emph{arXiv preprint arXiv:1902.00621}, 2019.

\bibitem[Holland(2019)]{holland2019pac}
Matthew Holland.
\newblock Pac-bayes under potentially heavy tails.
\newblock \emph{NeurIPS}, 2019.

\bibitem[McAllester(2013)]{McAllester13}
David McAllester.
\newblock A pac-bayesian tutorial with a dropout bound.
\newblock \emph{arXiv preprint arXiv:1307.2118}, 2013.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{Neyshabur17}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017.

\bibitem[P{\'e}rez-Ortiz et~al.(2021)P{\'e}rez-Ortiz, Rivasplata, Shawe-Taylor,
  and Szepesv{\'a}ri]{perez2021tighter}
Mar{\i}a P{\'e}rez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba
  Szepesv{\'a}ri.
\newblock Tighter risk certificates for neural networks.
\newblock \emph{JMLR}, 2021.

\bibitem[Dziugaite et~al.(2021)Dziugaite, Hsu, Gharbieh, Arpino, and
  Roy]{dziugaite2020role}
Gintare~Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and
  Daniel~M Roy.
\newblock On the role of data in pac-bayes bounds.
\newblock \emph{AISTATS}, 2021.

\bibitem[Foong et~al.(2021)Foong, Bruinsma, Burt, and Turner]{foong2021tight}
Andrew~YK Foong, Wessel~P Bruinsma, David~R Burt, and Richard~E Turner.
\newblock How tight can pac-bayes be in the small data regime?
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Alquier et~al.(2012)Alquier, Wintenberger, et~al.]{Alquier12}
Pierre Alquier, Olivier Wintenberger, et~al.
\newblock Model selection for weakly dependent time series forecasting.
\newblock \emph{Bernoulli}, 2012.

\bibitem[Takimoto and Warmuth(2000)]{Takimoto00}
Eiji Takimoto and Manfred~K Warmuth.
\newblock The last-step minimax algorithm.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 279--290. Springer, 2000.

\bibitem[Banerjee(2006)]{Banerjee06}
Arindam Banerjee.
\newblock On bayesian bounds.
\newblock In \emph{ICML}, 2006.

\bibitem[Bartlett et~al.(2013)Bartlett, Grunwald, Harremo{\"e}s, Hedayati, and
  Kotlowski]{Bartlett13}
Peter Bartlett, Peter Grunwald, Peter Harremo{\"e}s, Fares Hedayati, and
  Wojciech Kotlowski.
\newblock Horizon-independent optimal prediction with log-loss in exponential
  families.
\newblock \emph{arXiv preprint arXiv:1305.4324}, 2013.

\bibitem[Gr{\"u}nwald and Mehta(2017)]{Grunwald17}
Peter~D Gr{\"u}nwald and Nishant~A Mehta.
\newblock A tight excess risk bound via a unified
  pac-bayesian-rademacher-shtarkov-mdl complexity.
\newblock \emph{arXiv preprint arXiv:1710.07732}, 2017.

\bibitem[Rothfuss et~al.(2020)Rothfuss, Fortuin, and Krause]{rothfuss2020pacoh}
Jonas Rothfuss, Vincent Fortuin, and Andreas Krause.
\newblock Pacoh: Bayes-optimal meta-learning with pac-guarantees.
\newblock \emph{arXiv preprint arXiv:2002.05551}, 2020.

\bibitem[Amit and Meir(2018)]{amit2017meta}
Ron Amit and Ron Meir.
\newblock Meta-learning by adjusting priors based on extended pac-bayes theory.
\newblock \emph{ICML}, 2018.

\bibitem[Farid and Majumdar(2021)]{Farid2021GeneralizationBF}
Alec Farid and Anirudha Majumdar.
\newblock Generalization bounds for meta-learning via pac-bayes and uniform
  stability.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ding et~al.(2021)Ding, Chen, Levinboim, Goodman, and
  Soricut]{ding2021bridging}
Nan Ding, Xi~Chen, Tomer Levinboim, Sebastian Goodman, and Radu Soricut.
\newblock Bridging the gap between practice and pac-bayes theory in few-shot
  meta-learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Gal and Ghahramani(2015)]{Gal15}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock \emph{arXiv preprint arXiv:1506.02142}, 2015.

\bibitem[Gal(2016)]{gal2016uncertainty}
Yarin Gal.
\newblock \emph{Uncertainty in deep learning}.
\newblock PhD thesis, PhD thesis, University of Cambridge, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava14}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{Kingma15}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{Blundell15}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Louizos and Welling(2016)]{Louizos16}
Christos Louizos and Max Welling.
\newblock Structured and efficient variational deep learning with matrix
  gaussian posteriors.
\newblock In \emph{ICLR}, 2016.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and Loh]{pensia2018generalization}
Ankit Pensia, Varun Jog, and Po-Ling Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In \emph{ISIT}, 2018.

\bibitem[Mou et~al.(2018)Mou, Wang, Zhai, and Zheng]{mou2018generalization}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In \emph{Conference on Learning Theory}, 2018.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{negrea2019information}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{haghifam2020sharpened}
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel~M Roy, and
  Gintare~Karolina Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Lecu{\'e} and Mendelson(2017)]{lecue2017regularization}
Guillaume Lecu{\'e} and Shahar Mendelson.
\newblock Regularization and the small-ball method ii: complexity dependent
  error rates.
\newblock \emph{The Journal of Machine Learning Research}, 2017.

\bibitem[Mendelson(2014)]{mendelson2014learning}
Shahar Mendelson.
\newblock Learning without concentration.
\newblock In \emph{Conference on Learning Theory}, 2014.

\bibitem[O'Donnell(2014)]{o2014analysis}
Ryan O'Donnell.
\newblock \emph{Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and Massart]{boucheron}
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart.
\newblock \emph{Concentration Inequalities: A Nonasymptotic Theory of
  Independence}.
\newblock OUP, 2013.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In \emph{NeurIPS}, 2018.

\end{thebibliography}
