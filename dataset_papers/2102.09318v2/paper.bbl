\begin{thebibliography}{79}
% BibTex style file: imsart-number.bst, 2017-11-03
% Default style options (sort=1,type=number).
% Used options (sort=1,type=number).

\bibitem{agarwal2019theory}
\begin{barticle}[author]
\bauthor{\bsnm{Agarwal},~\bfnm{Alekh}\binits{A.}},
  \bauthor{\bsnm{Kakade},~\bfnm{Sham~M}\binits{S.~M.}},
  \bauthor{\bsnm{Lee},~\bfnm{Jason~D}\binits{J.~D.}} \AND
  \bauthor{\bsnm{Mahajan},~\bfnm{Gaurav}\binits{G.}}
(\byear{2019}).
\btitle{On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift}.
\bjournal{Preprint arXiv:1908.00261}.
\end{barticle}
\endbibitem

\bibitem{azar2012dynamic}
\begin{barticle}[author]
\bauthor{\bsnm{Azar},~\bfnm{Mohammad~Gheshlaghi}\binits{M.~G.}},
  \bauthor{\bsnm{G{\'o}mez},~\bfnm{Vicen{\c{c}}}\binits{V.}} \AND
  \bauthor{\bsnm{Kappen},~\bfnm{Hilbert~J}\binits{H.~J.}}
(\byear{2012}).
\btitle{Dynamic policy programming}.
\bjournal{The Journal of Machine Learning Research}
\bvolume{13}
\bpages{3207--3245}.
\end{barticle}
\endbibitem

\bibitem{bahdanau2016actor}
\begin{barticle}[author]
\bauthor{\bsnm{Bahdanau},~\bfnm{Dzmitry}\binits{D.}},
  \bauthor{\bsnm{Brakel},~\bfnm{Philemon}\binits{P.}},
  \bauthor{\bsnm{Xu},~\bfnm{Kelvin}\binits{K.}},
  \bauthor{\bsnm{Goyal},~\bfnm{Anirudh}\binits{A.}},
  \bauthor{\bsnm{Lowe},~\bfnm{Ryan}\binits{R.}},
  \bauthor{\bsnm{Pineau},~\bfnm{Joelle}\binits{J.}},
  \bauthor{\bsnm{Courville},~\bfnm{Aaron}\binits{A.}} \AND
  \bauthor{\bsnm{Bengio},~\bfnm{Yoshua}\binits{Y.}}
(\byear{2016}).
\btitle{An actor-critic algorithm for sequence prediction}.
\bjournal{Preprint arXiv:1607.07086}.
\end{barticle}
\endbibitem

\bibitem{banach1922operations}
\begin{barticle}[author]
\bauthor{\bsnm{Banach},~\bfnm{Stefan}\binits{S.}}
(\byear{1922}).
\btitle{Sur les op{\'e}rations dans les ensembles abstraits et leur application
  aux {\'e}quations int{\'e}grales}.
\bjournal{Fund. math}
\bvolume{3}
\bpages{133--181}.
\end{barticle}
\endbibitem

\bibitem{barto1983neuronlike}
\begin{barticle}[author]
\bauthor{\bsnm{{Barto}},~\bfnm{A.~G.}\binits{A.~G.}},
  \bauthor{\bsnm{{Sutton}},~\bfnm{R.~S.}\binits{R.~S.}} \AND
  \bauthor{\bsnm{{Anderson}},~\bfnm{C.~W.}\binits{C.~W.}}
(\byear{1983}).
\btitle{Neuronlike adaptive elements that can solve difficult learning control
  problems}.
\bjournal{IEEE Transactions on Systems, Man, and Cybernetics}
\bvolume{SMC-13}
\bpages{834-846}.
\bdoi{10.1109/TSMC.1983.6313077}
\end{barticle}
\endbibitem

\bibitem{baxter2001infinite}
\begin{barticle}[author]
\bauthor{\bsnm{Baxter},~\bfnm{Jonathan}\binits{J.}} \AND
  \bauthor{\bsnm{Bartlett},~\bfnm{Peter~L}\binits{P.~L.}}
(\byear{2001}).
\btitle{Infinite-horizon policy-gradient estimation}.
\bjournal{Journal of Artificial Intelligence Research}
\bvolume{15}
\bpages{319--350}.
\end{barticle}
\endbibitem

\bibitem{beck2012error}
\begin{barticle}[author]
\bauthor{\bsnm{Beck},~\bfnm{Carolyn~L}\binits{C.~L.}} \AND
  \bauthor{\bsnm{Srikant},~\bfnm{Rayadurgam}\binits{R.}}
(\byear{2012}).
\btitle{Error bounds for constant step-size {$Q$}-learning}.
\bjournal{Systems \& control letters}
\bvolume{61}
\bpages{1203--1208}.
\end{barticle}
\endbibitem

\bibitem{beck2013improved}
\begin{binproceedings}[author]
\bauthor{\bsnm{Beck},~\bfnm{Carolyn~L}\binits{C.~L.}} \AND
  \bauthor{\bsnm{Srikant},~\bfnm{Rayadurgam}\binits{R.}}
(\byear{2013}).
\btitle{Improved upper bounds on the expected error in constant step-size
  {$Q$}-learning}.
In \bbooktitle{2013 American Control Conference}
\bpages{1926--1931}.
\bpublisher{IEEE}.
\end{binproceedings}
\endbibitem

\bibitem{bertsekas1996neuro}
\begin{bbook}[author]
\bauthor{\bsnm{Bertsekas},~\bfnm{Dimitri~P}\binits{D.~P.}} \AND
  \bauthor{\bsnm{Tsitsiklis},~\bfnm{John~N}\binits{J.~N.}}
(\byear{1996}).
\btitle{Neuro-dynamic programming}.
\bpublisher{Athena Scientific}.
\end{bbook}
\endbibitem

\bibitem{bhandari2020note}
\begin{barticle}[author]
\bauthor{\bsnm{Bhandari},~\bfnm{Jalaj}\binits{J.}} \AND
  \bauthor{\bsnm{Russo},~\bfnm{Daniel}\binits{D.}}
(\byear{2020}).
\btitle{A note on the linear convergence of policy gradient methods}.
\bjournal{Preprint arXiv:2007.11120}.
\end{barticle}
\endbibitem

\bibitem{bhandari2018finite}
\begin{binproceedings}[author]
\bauthor{\bsnm{Bhandari},~\bfnm{Jalaj}\binits{J.}},
  \bauthor{\bsnm{Russo},~\bfnm{Daniel}\binits{D.}} \AND
  \bauthor{\bsnm{Singal},~\bfnm{Raghav}\binits{R.}}
(\byear{2018}).
\btitle{{A Finite Time Analysis of Temporal Difference Learning With Linear
  Function Approximation}}.
In \bbooktitle{Conference On Learning Theory}
\bpages{1691--1692}.
\end{binproceedings}
\endbibitem

\bibitem{bhatnagar2009natural}
\begin{barticle}[author]
\bauthor{\bsnm{Bhatnagar},~\bfnm{Shalabh}\binits{S.}},
  \bauthor{\bsnm{Sutton},~\bfnm{Richard~S}\binits{R.~S.}},
  \bauthor{\bsnm{Ghavamzadeh},~\bfnm{Mohammad}\binits{M.}} \AND
  \bauthor{\bsnm{Lee},~\bfnm{Mark}\binits{M.}}
(\byear{2009}).
\btitle{Natural actor--critic algorithms}.
\bjournal{Automatica}
\bvolume{45}
\bpages{2471--2482}.
\end{barticle}
\endbibitem

\bibitem{borkar2009stochastic}
\begin{bbook}[author]
\bauthor{\bsnm{Borkar},~\bfnm{Vivek~S}\binits{V.~S.}}
(\byear{2009}).
\btitle{Stochastic approximation: a dynamical systems viewpoint}
\bvolume{48}.
\bpublisher{Springer}.
\end{bbook}
\endbibitem

\bibitem{borkar1997actor}
\begin{barticle}[author]
\bauthor{\bsnm{Borkar},~\bfnm{Vivek~S}\binits{V.~S.}} \AND
  \bauthor{\bsnm{Konda},~\bfnm{Vijaymohan~R}\binits{V.~R.}}
(\byear{1997}).
\btitle{The actor-critic algorithm as multi-time-scale stochastic
  approximation}.
\bjournal{Sadhana}
\bvolume{22}
\bpages{525--543}.
\end{barticle}
\endbibitem

\bibitem{borkar2000ode}
\begin{barticle}[author]
\bauthor{\bsnm{Borkar},~\bfnm{Vivek~S}\binits{V.~S.}} \AND
  \bauthor{\bsnm{Meyn},~\bfnm{Sean~P}\binits{S.~P.}}
(\byear{2000}).
\btitle{The {ODE} method for convergence of stochastic approximation and
  reinforcement learning}.
\bjournal{SIAM Journal on Control and Optimization}
\bvolume{38}
\bpages{447--469}.
\end{barticle}
\endbibitem

\bibitem{bottou2018optimization}
\begin{barticle}[author]
\bauthor{\bsnm{Bottou},~\bfnm{L{\'e}on}\binits{L.}},
  \bauthor{\bsnm{Curtis},~\bfnm{Frank~E}\binits{F.~E.}} \AND
  \bauthor{\bsnm{Nocedal},~\bfnm{Jorge}\binits{J.}}
(\byear{2018}).
\btitle{Optimization methods for large-scale machine learning}.
\bjournal{Siam Review}
\bvolume{60}
\bpages{223--311}.
\end{barticle}
\endbibitem

\bibitem{cen2020fast}
\begin{barticle}[author]
\bauthor{\bsnm{Cen},~\bfnm{Shicong}\binits{S.}},
  \bauthor{\bsnm{Cheng},~\bfnm{Chen}\binits{C.}},
  \bauthor{\bsnm{Chen},~\bfnm{Yuxin}\binits{Y.}},
  \bauthor{\bsnm{Wei},~\bfnm{Yuting}\binits{Y.}} \AND
  \bauthor{\bsnm{Chi},~\bfnm{Yuejie}\binits{Y.}}
(\byear{2020}).
\btitle{Fast global convergence of natural policy gradient methods with entropy
  regularization}.
\bjournal{Preprint arXiv:2007.06558}.
\end{barticle}
\endbibitem

\bibitem{chen2020finite}
\begin{barticle}[author]
\bauthor{\bsnm{Chen},~\bfnm{Zaiwei}\binits{Z.}},
  \bauthor{\bsnm{Maguluri},~\bfnm{Siva~Theja}\binits{S.~T.}},
  \bauthor{\bsnm{Shakkottai},~\bfnm{Sanjay}\binits{S.}} \AND
  \bauthor{\bsnm{Shanmugam},~\bfnm{Karthikeyan}\binits{K.}}
(\byear{2020}).
\btitle{{Finite-Sample Analysis of Contractive Stochastic Approximation Using
  Smooth Convex Envelopes}}.
\bjournal{Advances in Neural Information Processing Systems}
\bvolume{33}.
\end{barticle}
\endbibitem

\bibitem{chen2021finite}
\begin{barticle}[author]
\bauthor{\bsnm{Chen},~\bfnm{Zaiwei}\binits{Z.}},
  \bauthor{\bsnm{Maguluri},~\bfnm{Siva~Theja}\binits{S.~T.}},
  \bauthor{\bsnm{Shakkottai},~\bfnm{Sanjay}\binits{S.}} \AND
  \bauthor{\bsnm{Shanmugam},~\bfnm{Karthikeyan}\binits{K.}}
(\byear{2021}).
\btitle{{A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous
  $Q$-Learning and TD-Learning Variants}}.
\bjournal{Preprint arXiv:2102.01567}.
\end{barticle}
\endbibitem

\bibitem{cover1999elements}
\begin{bbook}[author]
\bauthor{\bsnm{Cover},~\bfnm{Thomas~M}\binits{T.~M.}}
(\byear{1999}).
\btitle{Elements of information theory}.
\bpublisher{John Wiley \& Sons}.
\end{bbook}
\endbibitem

\bibitem{dalal2018finite}
\begin{binproceedings}[author]
\bauthor{\bsnm{Dalal},~\bfnm{Gal}\binits{G.}},
  \bauthor{\bsnm{Sz{\"o}r{\'e}nyi},~\bfnm{Bal{\'a}zs}\binits{B.}},
  \bauthor{\bsnm{Thoppe},~\bfnm{Gugan}\binits{G.}} \AND
  \bauthor{\bsnm{Mannor},~\bfnm{Shie}\binits{S.}}
(\byear{2018}).
\btitle{{Finite sample analysis for TD$(0)$ with function approximation}}.
In \bbooktitle{Thirty-Second AAAI Conference on Artificial Intelligence}.
\end{binproceedings}
\endbibitem

\bibitem{dann2019policy}
\begin{binproceedings}[author]
\bauthor{\bsnm{Dann},~\bfnm{Christoph}\binits{C.}},
  \bauthor{\bsnm{Li},~\bfnm{Lihong}\binits{L.}},
  \bauthor{\bsnm{Wei},~\bfnm{Wei}\binits{W.}} \AND
  \bauthor{\bsnm{Brunskill},~\bfnm{Emma}\binits{E.}}
(\byear{2019}).
\btitle{{Policy certificates: Towards accountable reinforcement learning}}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{1507--1516}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{degris2012off}
\begin{binproceedings}[author]
\bauthor{\bsnm{Degris},~\bfnm{Thomas}\binits{T.}},
  \bauthor{\bsnm{White},~\bfnm{Martha}\binits{M.}} \AND
  \bauthor{\bsnm{Sutton},~\bfnm{Richard}\binits{R.}}
(\byear{2012}).
\btitle{{Off-Policy Actor-Critic}}.
In \bbooktitle{International Conference on Machine Learning}.
\end{binproceedings}
\endbibitem

\bibitem{espeholt2018impala}
\begin{binproceedings}[author]
\bauthor{\bsnm{Espeholt},~\bfnm{Lasse}\binits{L.}},
  \bauthor{\bsnm{Soyer},~\bfnm{Hubert}\binits{H.}},
  \bauthor{\bsnm{Munos},~\bfnm{Remi}\binits{R.}},
  \bauthor{\bsnm{Simonyan},~\bfnm{Karen}\binits{K.}},
  \bauthor{\bsnm{Mnih},~\bfnm{Vlad}\binits{V.}},
  \bauthor{\bsnm{Ward},~\bfnm{Tom}\binits{T.}},
  \bauthor{\bsnm{Doron},~\bfnm{Yotam}\binits{Y.}},
  \bauthor{\bsnm{Firoiu},~\bfnm{Vlad}\binits{V.}},
  \bauthor{\bsnm{Harley},~\bfnm{Tim}\binits{T.}},
  \bauthor{\bsnm{Dunning},~\bfnm{Iain}\binits{I.}} \betal{et~al.}
(\byear{2018}).
\btitle{{IMPALA}: Scalable Distributed Deep-RL with Importance Weighted
  Actor-Learner Architectures}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{1407--1416}.
\end{binproceedings}
\endbibitem

\bibitem{even2009online}
\begin{barticle}[author]
\bauthor{\bsnm{Even-Dar},~\bfnm{Eyal}\binits{E.}},
  \bauthor{\bsnm{Kakade},~\bfnm{Sham~M}\binits{S.~M.}} \AND
  \bauthor{\bsnm{Mansour},~\bfnm{Yishay}\binits{Y.}}
(\byear{2009}).
\btitle{Online Markov decision processes}.
\bjournal{Mathematics of Operations Research}
\bvolume{34}
\bpages{726--736}.
\end{barticle}
\endbibitem

\bibitem{even2003learning}
\begin{barticle}[author]
\bauthor{\bsnm{Even-Dar},~\bfnm{Eyal}\binits{E.}} \AND
  \bauthor{\bsnm{Mansour},~\bfnm{Yishay}\binits{Y.}}
(\byear{2003}).
\btitle{Learning rates for {$Q$}-learning}.
\bjournal{Journal of Machine Learning Research}
\bvolume{5}
\bpages{1--25}.
\end{barticle}
\endbibitem

\bibitem{geist2019theory}
\begin{binproceedings}[author]
\bauthor{\bsnm{Geist},~\bfnm{Matthieu}\binits{M.}},
  \bauthor{\bsnm{Scherrer},~\bfnm{Bruno}\binits{B.}} \AND
  \bauthor{\bsnm{Pietquin},~\bfnm{Olivier}\binits{O.}}
(\byear{2019}).
\btitle{A theory of regularized markov decision processes}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{2160--2169}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{geweke1989bayesian}
\begin{barticle}[author]
\bauthor{\bsnm{Geweke},~\bfnm{John}\binits{J.}}
(\byear{1989}).
\btitle{Bayesian inference in econometric models using Monte Carlo
  integration}.
\bjournal{Econometrica: Journal of the Econometric Society}
\bpages{1317--1339}.
\end{barticle}
\endbibitem

\bibitem{glynn1989importance}
\begin{barticle}[author]
\bauthor{\bsnm{Glynn},~\bfnm{Peter~W}\binits{P.~W.}} \AND
  \bauthor{\bsnm{Iglehart},~\bfnm{Donald~L}\binits{D.~L.}}
(\byear{1989}).
\btitle{Importance sampling for stochastic simulations}.
\bjournal{Management science}
\bvolume{35}
\bpages{1367--1392}.
\end{barticle}
\endbibitem

\bibitem{gottesman2020interpretable}
\begin{binproceedings}[author]
\bauthor{\bsnm{Gottesman},~\bfnm{Omer}\binits{O.}},
  \bauthor{\bsnm{Futoma},~\bfnm{Joseph}\binits{J.}},
  \bauthor{\bsnm{Liu},~\bfnm{Yao}\binits{Y.}},
  \bauthor{\bsnm{Parbhoo},~\bfnm{Sonali}\binits{S.}},
  \bauthor{\bsnm{Celi},~\bfnm{Leo}\binits{L.}},
  \bauthor{\bsnm{Brunskill},~\bfnm{Emma}\binits{E.}} \AND
  \bauthor{\bsnm{Doshi-Velez},~\bfnm{Finale}\binits{F.}}
(\byear{2020}).
\btitle{Interpretable off-policy evaluation in reinforcement learning by
  highlighting influential transitions}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{3658--3667}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{gu2017deep}
\begin{binproceedings}[author]
\bauthor{\bsnm{Gu},~\bfnm{Shixiang}\binits{S.}},
  \bauthor{\bsnm{Holly},~\bfnm{Ethan}\binits{E.}},
  \bauthor{\bsnm{Lillicrap},~\bfnm{Timothy}\binits{T.}} \AND
  \bauthor{\bsnm{Levine},~\bfnm{Sergey}\binits{S.}}
(\byear{2017}).
\btitle{Deep reinforcement learning for robotic manipulation with asynchronous
  off-policy updates}.
In \bbooktitle{2017 IEEE international conference on robotics and automation
  (ICRA)}
\bpages{3389--3396}.
\bpublisher{IEEE}.
\end{binproceedings}
\endbibitem

\bibitem{haarnoja2017reinforcement}
\begin{binproceedings}[author]
\bauthor{\bsnm{Haarnoja},~\bfnm{Tuomas}\binits{T.}},
  \bauthor{\bsnm{Tang},~\bfnm{Haoran}\binits{H.}},
  \bauthor{\bsnm{Abbeel},~\bfnm{Pieter}\binits{P.}} \AND
  \bauthor{\bsnm{Levine},~\bfnm{Sergey}\binits{S.}}
(\byear{2017}).
\btitle{Reinforcement learning with deep energy-based policies}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{1352--1361}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{ionides2008truncated}
\begin{barticle}[author]
\bauthor{\bsnm{Ionides},~\bfnm{Edward~L}\binits{E.~L.}}
(\byear{2008}).
\btitle{Truncated importance sampling}.
\bjournal{Journal of Computational and Graphical Statistics}
\bvolume{17}
\bpages{295--311}.
\end{barticle}
\endbibitem

\bibitem{jaakkola1994convergence}
\begin{binproceedings}[author]
\bauthor{\bsnm{Jaakkola},~\bfnm{Tommi}\binits{T.}},
  \bauthor{\bsnm{Jordan},~\bfnm{Michael~I}\binits{M.~I.}} \AND
  \bauthor{\bsnm{Singh},~\bfnm{Satinder~P}\binits{S.~P.}}
(\byear{1994}).
\btitle{Convergence of stochastic iterative dynamic programming algorithms}.
In \bbooktitle{Advances in neural information processing systems}
\bpages{703--710}.
\end{binproceedings}
\endbibitem

\bibitem{kakade2001natural}
\begin{barticle}[author]
\bauthor{\bsnm{Kakade},~\bfnm{Sham~M}\binits{S.~M.}}
(\byear{2001}).
\btitle{A natural policy gradient}.
\bjournal{Advances in neural information processing systems}
\bvolume{14}.
\end{barticle}
\endbibitem

\bibitem{khodadadian2021finite}
\begin{barticle}[author]
\bauthor{\bsnm{Khodadadian},~\bfnm{Sajad}\binits{S.}},
  \bauthor{\bsnm{Doan},~\bfnm{Thinh~T.}\binits{T.~T.}},
  \bauthor{\bsnm{Maguluri},~\bfnm{Siva~Theja}\binits{S.~T.}} \AND
  \bauthor{\bsnm{Romberg},~\bfnm{Justin}\binits{J.}}
(\byear{2021}).
\btitle{{Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic
  Algorithm}}.
\bjournal{Preprint arXiv:2101.10506}.
\end{barticle}
\endbibitem

\bibitem{konda2000actor}
\begin{binproceedings}[author]
\bauthor{\bsnm{Konda},~\bfnm{Vijay~R}\binits{V.~R.}} \AND
  \bauthor{\bsnm{Tsitsiklis},~\bfnm{John~N}\binits{J.~N.}}
(\byear{2000}).
\btitle{Actor-critic algorithms}.
In \bbooktitle{Advances in neural information processing systems}
\bpages{1008--1014}.
\bpublisher{Citeseer}.
\end{binproceedings}
\endbibitem

\bibitem{kumar2019sample}
\begin{barticle}[author]
\bauthor{\bsnm{Kumar},~\bfnm{Harshat}\binits{H.}},
  \bauthor{\bsnm{Koppel},~\bfnm{Alec}\binits{A.}} \AND
  \bauthor{\bsnm{Ribeiro},~\bfnm{Alejandro}\binits{A.}}
(\byear{2019}).
\btitle{{On the Sample Complexity of Actor-Critic Method for Reinforcement
  Learning with Function Approximation}}.
\bjournal{Preprint arXiv:1910.08412}.
\end{barticle}
\endbibitem

\bibitem{lakshminarayanan2018linear}
\begin{binproceedings}[author]
\bauthor{\bsnm{Lakshminarayanan},~\bfnm{Chandrashekar}\binits{C.}} \AND
  \bauthor{\bsnm{Szepesvari},~\bfnm{Csaba}\binits{C.}}
(\byear{2018}).
\btitle{{Linear Stochastic Approximation: How Far Does Constant Step-Size and
  Iterate Averaging Go?}}
In \bbooktitle{International Conference on Artificial Intelligence and
  Statistics}
\bpages{1347--1355}.
\end{binproceedings}
\endbibitem

\bibitem{lan2020first}
\begin{bbook}[author]
\bauthor{\bsnm{Lan},~\bfnm{Guanghui}\binits{G.}}
(\byear{2020}).
\btitle{{First-order and Stochastic Optimization Methods for Machine
  Learning}}.
\bpublisher{Springer}.
\end{bbook}
\endbibitem

\bibitem{lan2021policy}
\begin{barticle}[author]
\bauthor{\bsnm{Lan},~\bfnm{Guanghui}\binits{G.}}
(\byear{2021}).
\btitle{Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes}.
\bjournal{arXiv preprint arXiv:2102.00135}.
\end{barticle}
\endbibitem

\bibitem{levin2017markov}
\begin{bbook}[author]
\bauthor{\bsnm{Levin},~\bfnm{David~A}\binits{D.~A.}} \AND
  \bauthor{\bsnm{Peres},~\bfnm{Yuval}\binits{Y.}}
(\byear{2017}).
\btitle{Markov chains and mixing times}
\bvolume{107}.
\bpublisher{American Mathematical Soc.}
\end{bbook}
\endbibitem

\bibitem{levine2020offline}
\begin{barticle}[author]
\bauthor{\bsnm{Levine},~\bfnm{Sergey}\binits{S.}},
  \bauthor{\bsnm{Kumar},~\bfnm{Aviral}\binits{A.}},
  \bauthor{\bsnm{Tucker},~\bfnm{George}\binits{G.}} \AND
  \bauthor{\bsnm{Fu},~\bfnm{Justin}\binits{J.}}
(\byear{2020}).
\btitle{Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems}.
\bjournal{Preprint arXiv:2005.01643}.
\end{barticle}
\endbibitem

\bibitem{li2020sample}
\begin{barticle}[author]
\bauthor{\bsnm{Li},~\bfnm{Gen}\binits{G.}},
  \bauthor{\bsnm{Wei},~\bfnm{Yuting}\binits{Y.}},
  \bauthor{\bsnm{Chi},~\bfnm{Yuejie}\binits{Y.}},
  \bauthor{\bsnm{Gu},~\bfnm{Yuantao}\binits{Y.}} \AND
  \bauthor{\bsnm{Chen},~\bfnm{Yuxin}\binits{Y.}}
(\byear{2020}).
\btitle{{Sample Complexity of Asynchronous {$Q$}-Learning: Sharper Analysis and
  Variance Reduction}}.
\bjournal{Preprint arXiv:2006.03041}.
\end{barticle}
\endbibitem

\bibitem{lillicrap2015continuous}
\begin{barticle}[author]
\bauthor{\bsnm{Lillicrap},~\bfnm{Timothy~P}\binits{T.~P.}},
  \bauthor{\bsnm{Hunt},~\bfnm{Jonathan~J}\binits{J.~J.}},
  \bauthor{\bsnm{Pritzel},~\bfnm{Alexander}\binits{A.}},
  \bauthor{\bsnm{Heess},~\bfnm{Nicolas}\binits{N.}},
  \bauthor{\bsnm{Erez},~\bfnm{Tom}\binits{T.}},
  \bauthor{\bsnm{Tassa},~\bfnm{Yuval}\binits{Y.}},
  \bauthor{\bsnm{Silver},~\bfnm{David}\binits{D.}} \AND
  \bauthor{\bsnm{Wierstra},~\bfnm{Daan}\binits{D.}}
(\byear{2015}).
\btitle{Continuous control with deep reinforcement learning}.
\bjournal{Preprint arXiv:1509.02971}.
\end{barticle}
\endbibitem

\bibitem{liu2019neural}
\begin{barticle}[author]
\bauthor{\bsnm{Liu},~\bfnm{Boyi}\binits{B.}},
  \bauthor{\bsnm{Cai},~\bfnm{Qi}\binits{Q.}},
  \bauthor{\bsnm{Yang},~\bfnm{Zhuoran}\binits{Z.}} \AND
  \bauthor{\bsnm{Wang},~\bfnm{Zhaoran}\binits{Z.}}
(\byear{2019}).
\btitle{Neural proximal/trust region policy optimization attains globally
  optimal policy}.
\bjournal{Preprint arXiv:1906.10306}.
\end{barticle}
\endbibitem

\bibitem{liu2018representation}
\begin{barticle}[author]
\bauthor{\bsnm{Liu},~\bfnm{Yao}\binits{Y.}},
  \bauthor{\bsnm{Gottesman},~\bfnm{Omer}\binits{O.}},
  \bauthor{\bsnm{Raghu},~\bfnm{Aniruddh}\binits{A.}},
  \bauthor{\bsnm{Komorowski},~\bfnm{Matthieu}\binits{M.}},
  \bauthor{\bsnm{Faisal},~\bfnm{Aldo~A}\binits{A.~A.}},
  \bauthor{\bsnm{Doshi-Velez},~\bfnm{Finale}\binits{F.}} \AND
  \bauthor{\bsnm{Brunskill},~\bfnm{Emma}\binits{E.}}
(\byear{2018}).
\btitle{{Representation Balancing MDPs for Off-policy Policy Evaluation}}.
\bjournal{Advances in Neural Information Processing Systems}
\bvolume{31}
\bpages{2644--2653}.
\end{barticle}
\endbibitem

\bibitem{maei2018convergent}
\begin{barticle}[author]
\bauthor{\bsnm{Maei},~\bfnm{Hamid~Reza}\binits{H.~R.}}
(\byear{2018}).
\btitle{Convergent actor-critic algorithms under off-policy training and
  function approximation}.
\bjournal{arXiv preprint arXiv:1802.07842}.
\end{barticle}
\endbibitem

\bibitem{mandel2014offline}
\begin{binproceedings}[author]
\bauthor{\bsnm{Mandel},~\bfnm{Travis}\binits{T.}},
  \bauthor{\bsnm{Liu},~\bfnm{Yun-En}\binits{Y.-E.}},
  \bauthor{\bsnm{Levine},~\bfnm{Sergey}\binits{S.}},
  \bauthor{\bsnm{Brunskill},~\bfnm{Emma}\binits{E.}} \AND
  \bauthor{\bsnm{Popovic},~\bfnm{Zoran}\binits{Z.}}
(\byear{2014}).
\btitle{Offline policy evaluation across representations with applications to
  educational games.}
In \bbooktitle{AAMAS}
\bpages{1077--1084}.
\end{binproceedings}
\endbibitem

\bibitem{mei2020global}
\begin{binproceedings}[author]
\bauthor{\bsnm{Mei},~\bfnm{Jincheng}\binits{J.}},
  \bauthor{\bsnm{Xiao},~\bfnm{Chenjun}\binits{C.}},
  \bauthor{\bsnm{Szepesvari},~\bfnm{Csaba}\binits{C.}} \AND
  \bauthor{\bsnm{Schuurmans},~\bfnm{Dale}\binits{D.}}
(\byear{2020}).
\btitle{On the global convergence rates of softmax policy gradient methods}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{6820--6829}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{NIPS2009_3767}
\begin{binproceedings}[author]
\bauthor{\bsnm{Morimura},~\bfnm{Tetsuro}\binits{T.}},
  \bauthor{\bsnm{Uchibe},~\bfnm{Eiji}\binits{E.}},
  \bauthor{\bsnm{Yoshimoto},~\bfnm{Junichiro}\binits{J.}} \AND
  \bauthor{\bsnm{Doya},~\bfnm{Kenji}\binits{K.}}
(\byear{2009}).
\btitle{A generalized natural actor-critic algorithm}.
In \bbooktitle{Advances in neural information processing systems}
\bpages{1312--1320}.
\end{binproceedings}
\endbibitem

\bibitem{munos2016safe}
\begin{binproceedings}[author]
\bauthor{\bsnm{Munos},~\bfnm{R{\'e}mi}\binits{R.}},
  \bauthor{\bsnm{Stepleton},~\bfnm{Thomas}\binits{T.}},
  \bauthor{\bsnm{Harutyunyan},~\bfnm{Anna}\binits{A.}} \AND
  \bauthor{\bsnm{Bellemare},~\bfnm{Marc~G}\binits{M.~G.}}
(\byear{2016}).
\btitle{Safe and efficient off-policy reinforcement learning}.
In \bbooktitle{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}
\bpages{1054--1062}.
\end{binproceedings}
\endbibitem

\bibitem{nemirovskij1983problem}
\begin{barticle}[author]
\bauthor{\bsnm{Nemirovskij},~\bfnm{Arkadij~Semenovi{\v{c}}}\binits{A.~S.}} \AND
  \bauthor{\bsnm{Yudin},~\bfnm{David~Borisovich}\binits{D.~B.}}
(\byear{1983}).
\btitle{Problem complexity and method efficiency in optimization}.
\bjournal{Chichester: John Wiley}.
\end{barticle}
\endbibitem

\bibitem{peters2008natural}
\begin{barticle}[author]
\bauthor{\bsnm{Peters},~\bfnm{Jan}\binits{J.}} \AND
  \bauthor{\bsnm{Schaal},~\bfnm{Stefan}\binits{S.}}
(\byear{2008}).
\btitle{Natural actor-critic}.
\bjournal{Neurocomputing}
\bvolume{71}
\bpages{1180--1190}.
\end{barticle}
\endbibitem

\bibitem{pirotta2015policy}
\begin{barticle}[author]
\bauthor{\bsnm{Pirotta},~\bfnm{Matteo}\binits{M.}},
  \bauthor{\bsnm{Restelli},~\bfnm{Marcello}\binits{M.}} \AND
  \bauthor{\bsnm{Bascetta},~\bfnm{Luca}\binits{L.}}
(\byear{2015}).
\btitle{Policy gradient in lipschitz markov decision processes}.
\bjournal{Machine Learning}
\bvolume{100}
\bpages{255--283}.
\end{barticle}
\endbibitem

\bibitem{precup2000eligibility}
\begin{barticle}[author]
\bauthor{\bsnm{Precup},~\bfnm{Doina}\binits{D.}}
(\byear{2000}).
\btitle{Eligibility traces for off-policy policy evaluation}.
\bjournal{Computer Science Department Faculty Publication Series}
\bpages{80}.
\end{barticle}
\endbibitem

\bibitem{puterman1995markov}
\begin{barticle}[author]
\bauthor{\bsnm{Puterman},~\bfnm{Martin~L}\binits{M.~L.}}
(\byear{1995}).
\btitle{{Markov decision processes: Discrete stochastic dynamic programming}}.
\bjournal{Journal of the Operational Research Society}
\bvolume{46}
\bpages{792--792}.
\end{barticle}
\endbibitem

\bibitem{qiu2019finite}
\begin{binproceedings}[author]
\bauthor{\bsnm{Qiu},~\bfnm{Shuang}\binits{S.}},
  \bauthor{\bsnm{Yang},~\bfnm{Zhuoran}\binits{Z.}},
  \bauthor{\bsnm{Ye},~\bfnm{Jieping}\binits{J.}} \AND
  \bauthor{\bsnm{Wang},~\bfnm{Zhaoran}\binits{Z.}}
(\byear{2019}).
\btitle{On the finite-time convergence of actor-critic algorithm}.
In \bbooktitle{Optimization Foundations for Reinforcement Learning Workshop at
  Advances in Neural Information Processing Systems (NeurIPS)}.
\end{binproceedings}
\endbibitem

\bibitem{qu2020finite}
\begin{binproceedings}[author]
\bauthor{\bsnm{Qu},~\bfnm{Guannan}\binits{G.}} \AND
  \bauthor{\bsnm{Wierman},~\bfnm{Adam}\binits{A.}}
(\byear{2020}).
\btitle{{Finite-Time Analysis of Asynchronous Stochastic Approximation and
  $Q$-Learning}}.
In \bbooktitle{Conference on Learning Theory}
\bpages{3185--3205}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\bibitem{shani2020adaptive}
\begin{binproceedings}[author]
\bauthor{\bsnm{Shani},~\bfnm{Lior}\binits{L.}},
  \bauthor{\bsnm{Efroni},~\bfnm{Yonathan}\binits{Y.}} \AND
  \bauthor{\bsnm{Mannor},~\bfnm{Shie}\binits{S.}}
(\byear{2020}).
\btitle{{Adaptive Trust Region Policy Optimization: Global Convergence and
  Faster Rates for Regularized MDPs}}.
In \bbooktitle{Proceedings of the AAAI Conference on Artificial Intelligence}
\bvolume{34}
\bpages{5668--5675}.
\end{binproceedings}
\endbibitem

\bibitem{silver2016mastering}
\begin{barticle}[author]
\bauthor{\bsnm{Silver},~\bfnm{David}\binits{D.}},
  \bauthor{\bsnm{Huang},~\bfnm{Aja}\binits{A.}},
  \bauthor{\bsnm{Maddison},~\bfnm{Chris~J}\binits{C.~J.}},
  \bauthor{\bsnm{Guez},~\bfnm{Arthur}\binits{A.}},
  \bauthor{\bsnm{Sifre},~\bfnm{Laurent}\binits{L.}}, \bauthor{\bsnm{Van
  Den~Driessche},~\bfnm{George}\binits{G.}},
  \bauthor{\bsnm{Schrittwieser},~\bfnm{Julian}\binits{J.}},
  \bauthor{\bsnm{Antonoglou},~\bfnm{Ioannis}\binits{I.}},
  \bauthor{\bsnm{Panneershelvam},~\bfnm{Veda}\binits{V.}},
  \bauthor{\bsnm{Lanctot},~\bfnm{Marc}\binits{M.}} \betal{et~al.}
(\byear{2016}).
\btitle{Mastering the game of Go with deep neural networks and tree search}.
\bjournal{nature}
\bvolume{529}
\bpages{484}.
\end{barticle}
\endbibitem

\bibitem{srikant2019finite}
\begin{binproceedings}[author]
\bauthor{\bsnm{Srikant},~\bfnm{R}\binits{R.}} \AND
  \bauthor{\bsnm{Ying},~\bfnm{Lei}\binits{L.}}
(\byear{2019}).
\btitle{{Finite-Time Error Bounds For Linear Stochastic Approximation and TD
  Learning}}.
In \bbooktitle{Conference on Learning Theory}
\bpages{2803--2830}.
\end{binproceedings}
\endbibitem

\bibitem{sutton1988learning}
\begin{barticle}[author]
\bauthor{\bsnm{Sutton},~\bfnm{Richard~S}\binits{R.~S.}}
(\byear{1988}).
\btitle{Learning to predict by the methods of temporal differences}.
\bjournal{Machine learning}
\bvolume{3}
\bpages{9--44}.
\end{barticle}
\endbibitem

\bibitem{sutton2018reinforcement}
\begin{bbook}[author]
\bauthor{\bsnm{Sutton},~\bfnm{Richard~S}\binits{R.~S.}} \AND
  \bauthor{\bsnm{Barto},~\bfnm{Andrew~G}\binits{A.~G.}}
(\byear{2018}).
\btitle{{Reinforcement learning: An introduction}}.
\bpublisher{MIT press}.
\end{bbook}
\endbibitem

\bibitem{sutton1999policy}
\begin{binproceedings}[author]
\bauthor{\bsnm{Sutton},~\bfnm{Richard~S}\binits{R.~S.}},
  \bauthor{\bsnm{McAllester},~\bfnm{David~A}\binits{D.~A.}},
  \bauthor{\bsnm{Singh},~\bfnm{Satinder~P}\binits{S.~P.}},
  \bauthor{\bsnm{Mansour},~\bfnm{Yishay}\binits{Y.}} \betal{et~al.}
(\byear{1999}).
\btitle{Policy gradient methods for reinforcement learning with function
  approximation.}
In \bbooktitle{NIPs}
\bvolume{99}
\bpages{1057--1063}.
\bpublisher{Citeseer}.
\end{binproceedings}
\endbibitem

\bibitem{NIPS2013_5184}
\begin{binproceedings}[author]
\bauthor{\bsnm{Thomas},~\bfnm{Philip~S}\binits{P.~S.}},
  \bauthor{\bsnm{Dabney},~\bfnm{William}\binits{W.}},
  \bauthor{\bsnm{Mahadevan},~\bfnm{Sridhar}\binits{S.}} \AND
  \bauthor{\bsnm{Giguere},~\bfnm{Stephen}\binits{S.}}
(\byear{2013}).
\btitle{{Projected natural actor-critic}}.
In \bbooktitle{Proceedings of the 26th International Conference on Neural
  Information Processing Systems-Volume 2}
\bpages{2337--2345}.
\end{binproceedings}
\endbibitem

\bibitem{tsitsiklis1994asynchronous}
\begin{barticle}[author]
\bauthor{\bsnm{Tsitsiklis},~\bfnm{John~N}\binits{J.~N.}}
(\byear{1994}).
\btitle{Asynchronous stochastic approximation and {$Q$}-learning}.
\bjournal{Machine learning}
\bvolume{16}
\bpages{185--202}.
\end{barticle}
\endbibitem

\bibitem{tsitsiklis1997analysis}
\begin{binproceedings}[author]
\bauthor{\bsnm{Tsitsiklis},~\bfnm{John~N}\binits{J.~N.}} \AND
  \bauthor{\bsnm{Van~Roy},~\bfnm{Benjamin}\binits{B.}}
(\byear{1997}).
\btitle{Analysis of temporal-difference learning with function approximation}.
In \bbooktitle{Advances in neural information processing systems}
\bpages{1075--1081}.
\end{binproceedings}
\endbibitem

\bibitem{tsitsiklis1999average}
\begin{barticle}[author]
\bauthor{\bsnm{Tsitsiklis},~\bfnm{John~N}\binits{J.~N.}} \AND
  \bauthor{\bsnm{Van~Roy},~\bfnm{Benjamin}\binits{B.}}
(\byear{1999}).
\btitle{Average cost temporal-difference learning}.
\bjournal{Automatica}
\bvolume{35}
\bpages{1799--1808}.
\end{barticle}
\endbibitem

\bibitem{wainwright2019stochastic}
\begin{barticle}[author]
\bauthor{\bsnm{Wainwright},~\bfnm{Martin~J}\binits{M.~J.}}
(\byear{2019}).
\btitle{Stochastic approximation with cone-contractive operators: Sharp
  $\ell_\infty$-bounds for ${Q}$-learning}.
\bjournal{Preprint arXiv:1905.06265}.
\end{barticle}
\endbibitem

\bibitem{wang2019neural}
\begin{barticle}[author]
\bauthor{\bsnm{Wang},~\bfnm{Lingxiao}\binits{L.}},
  \bauthor{\bsnm{Cai},~\bfnm{Qi}\binits{Q.}},
  \bauthor{\bsnm{Yang},~\bfnm{Zhuoran}\binits{Z.}} \AND
  \bauthor{\bsnm{Wang},~\bfnm{Zhaoran}\binits{Z.}}
(\byear{2019}).
\btitle{{Neural policy gradient methods: Global optimality and rates of
  convergence}}.
\bjournal{Preprint arXiv:1909.01150}.
\end{barticle}
\endbibitem

\bibitem{wang2016sample}
\begin{barticle}[author]
\bauthor{\bsnm{Wang},~\bfnm{Ziyu}\binits{Z.}},
  \bauthor{\bsnm{Bapst},~\bfnm{Victor}\binits{V.}},
  \bauthor{\bsnm{Heess},~\bfnm{Nicolas}\binits{N.}},
  \bauthor{\bsnm{Mnih},~\bfnm{Volodymyr}\binits{V.}},
  \bauthor{\bsnm{Munos},~\bfnm{Remi}\binits{R.}},
  \bauthor{\bsnm{Kavukcuoglu},~\bfnm{Koray}\binits{K.}} \AND
  \bauthor{\bparticle{de} \bsnm{Freitas},~\bfnm{Nando}\binits{N.}}
(\byear{2016}).
\btitle{Sample efficient actor-critic with experience replay}.
\bjournal{Preprint arXiv:1611.01224}.
\end{barticle}
\endbibitem

\bibitem{watkins1992q}
\begin{barticle}[author]
\bauthor{\bsnm{Watkins},~\bfnm{Christopher~JCH}\binits{C.~J.}} \AND
  \bauthor{\bsnm{Dayan},~\bfnm{Peter}\binits{P.}}
(\byear{1992}).
\btitle{{$Q$}-learning}.
\bjournal{Machine learning}
\bvolume{8}
\bpages{279--292}.
\end{barticle}
\endbibitem

\bibitem{williams1990mathematical}
\begin{binproceedings}[author]
\bauthor{\bsnm{Williams},~\bfnm{Ronald~J}\binits{R.~J.}} \AND
  \bauthor{\bsnm{Baird},~\bfnm{Leemon~C}\binits{L.~C.}}
(\byear{1990}).
\btitle{A mathematical analysis of actor-critic architectures for learning
  optimal controls through incremental dynamic programming}.
In \bbooktitle{Proceedings of the Sixth Yale Workshop on Adaptive and Learning
  Systems}
\bpages{96--101}.
\bpublisher{Citeseer}.
\end{binproceedings}
\endbibitem

\bibitem{wu2020finite}
\begin{barticle}[author]
\bauthor{\bsnm{Wu},~\bfnm{Yue}\binits{Y.}},
  \bauthor{\bsnm{Zhang},~\bfnm{Weitong}\binits{W.}},
  \bauthor{\bsnm{Xu},~\bfnm{Pan}\binits{P.}} \AND
  \bauthor{\bsnm{Gu},~\bfnm{Quanquan}\binits{Q.}}
(\byear{2020}).
\btitle{{A Finite Time Analysis of Two Time-Scale Actor Critic Methods}}.
\bjournal{Preprint arXiv:2005.01350}.
\end{barticle}
\endbibitem

\bibitem{xu2020improving}
\begin{barticle}[author]
\bauthor{\bsnm{Xu},~\bfnm{Tengyu}\binits{T.}},
  \bauthor{\bsnm{Wang},~\bfnm{Zhe}\binits{Z.}} \AND
  \bauthor{\bsnm{Liang},~\bfnm{Yingbin}\binits{Y.}}
(\byear{2020}).
\btitle{Improving sample complexity bounds for (natural) actor-critic
  algorithms}.
\bjournal{Advances in Neural Information Processing Systems}
\bvolume{33}.
\end{barticle}
\endbibitem

\bibitem{xu2020non}
\begin{barticle}[author]
\bauthor{\bsnm{Xu},~\bfnm{Tengyu}\binits{T.}},
  \bauthor{\bsnm{Wang},~\bfnm{Zhe}\binits{Z.}} \AND
  \bauthor{\bsnm{Liang},~\bfnm{Yingbin}\binits{Y.}}
(\byear{2020}).
\btitle{{Non-asymptotic Convergence Analysis of Two Time-scale (Natural)
  Actor-Critic Algorithms}}.
\bjournal{Preprint arXiv:2005.03557}.
\end{barticle}
\endbibitem

\bibitem{zhang2019convergence}
\begin{binproceedings}[author]
\bauthor{\bsnm{Zhang},~\bfnm{Kaiqing}\binits{K.}},
  \bauthor{\bsnm{Koppel},~\bfnm{Alec}\binits{A.}},
  \bauthor{\bsnm{Zhu},~\bfnm{Hao}\binits{H.}} \AND
  \bauthor{\bsnm{Ba{\c{s}}ar},~\bfnm{Tamer}\binits{T.}}
(\byear{2019}).
\btitle{Convergence and iteration complexity of policy gradient method for
  infinite-horizon reinforcement learning}.
In \bbooktitle{2019 IEEE 58th Conference on Decision and Control (CDC)}
\bpages{7415--7422}.
\bpublisher{IEEE}.
\end{binproceedings}
\endbibitem

\bibitem{zhang2020provably}
\begin{binproceedings}[author]
\bauthor{\bsnm{Zhang},~\bfnm{Shangtong}\binits{S.}},
  \bauthor{\bsnm{Liu},~\bfnm{Bo}\binits{B.}},
  \bauthor{\bsnm{Yao},~\bfnm{Hengshuai}\binits{H.}} \AND
  \bauthor{\bsnm{Whiteson},~\bfnm{Shimon}\binits{S.}}
(\byear{2020}).
\btitle{Provably convergent two-timescale off-policy actor-critic with function
  approximation}.
In \bbooktitle{International Conference on Machine Learning}
\bpages{11204--11213}.
\bpublisher{PMLR}.
\end{binproceedings}
\endbibitem

\end{thebibliography}
