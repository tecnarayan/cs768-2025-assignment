\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, et~al.]{anil2023palm}
Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,
  Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Bengio, Y., L{\'e}onard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2943--2952. PMLR, 2020.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{Frantar2023SparseGPTML}
Frantar, E. and Alistarh, D.
\newblock Sparsegpt: Massive language models can be accurately pruned in
  one-shot.
\newblock \emph{ArXiv}, abs/2301.00774, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:255372747}.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, et~al.]{gao2021framework}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
  Hsu, J., McDonell, K., Muennighoff, N., et~al.
\newblock A framework for few-shot language model evaluation.
\newblock \emph{Version v0. 0.1. Sept}, 2021.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015LearningBW}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Neural Information Processing Systems}, 2015.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:2238772}.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{hassibi1993optimal}
Hassibi, B., Stork, D.~G., and Wolff, G.~J.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE international conference on neural networks}, pp.\
  293--299. IEEE, 1993.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Critch,
  Li, Song, and Steinhardt]{hendrycks2021ethics}
Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D., and
  Steinhardt, J.
\newblock Aligning ai with shared human values.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Basart, Zou,
  Mazeika, Song, and Steinhardt]{hendryckstest2021}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and
  Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021{\natexlab{b}}.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsity}
Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 10882--11005, 2021.

\bibitem[Horn(1990)]{horn1990hadamard}
Horn, R.~A.
\newblock The hadamard product.
\newblock In \emph{Proc. Symp. Appl. Math}, volume~40, pp.\  87--169, 1990.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and
  Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jaiswal et~al.(2023)Jaiswal, Gan, Du, Zhang, Wang, and
  Yang]{jaiswal2023compressing}
Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and Yang, Y.
\newblock Compressing llms: The truth is rarely pure and never simple.
\newblock \emph{arXiv preprint arXiv:2310.01382}, 2023.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{kusupati2020soft}
Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.,
  and Farhadi, A.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5544--5555. PMLR, 2020.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Li et~al.(2023)Li, Niu, Zhang, Liu, Zhu, and Kang]{li2023sparse}
Li, Y., Niu, L., Zhang, X., Liu, K., Zhu, J., and Kang, Z.
\newblock E-sparse: Boosting the large language model inference through
  entropy-based n: M sparsity.
\newblock \emph{arXiv preprint arXiv:2310.15929}, 2023.

\bibitem[Lin et~al.(2020)Lin, Stich, Barba, Dmitriev, and
  Jaggi]{lin2020dynamic}
Lin, T., Stich, S.~U., Barba, L., Dmitriev, D., and Jaggi, M.
\newblock Dynamic model pruning with feedback.
\newblock \emph{arXiv preprint arXiv:2006.07253}, 2020.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, Paul, and
  Bossan]{peft}
Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., and Bossan, B.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh,
  Yu, and Micikevicius]{mishra2021accelerating}
Mishra, A., Latorre, J.~A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G.,
  Yu, C., and Micikevicius, P.
\newblock Accelerating sparse deep neural networks.
\newblock \emph{arXiv preprint arXiv:2104.08378}, 2021.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 2383, 2018.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{2020t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness,
  and Dey]{cerebras2023slimpajama}
Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J.~R., Hestness, J., and Dey,
  N.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of
  RedPajama}.
\newblock
  \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and- \\
  deduplicated-version-of-redpajama}, 2023.

\bibitem[Sun et~al.(2023)Sun, Liu, Bair, and Kolter]{sun2023simple}
Sun, M., Liu, Z., Bair, A., and Kolter, J.~Z.
\newblock A simple and effective pruning approach for large language models.
\newblock \emph{arXiv preprint arXiv:2306.11695}, 2023.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{taori2023alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
  P., and Hashimoto, T.~B.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \emph{Stanford Center for Research on Foundation Models.
  https://crfm. stanford. edu/2023/03/13/alpaca. html}, 3\penalty0
  (6):\penalty0 7, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama1}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and
  Fedus]{wei2022emergent}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,
  D., Bosma, M., Zhou, D., Metzler, D., Chi, E.~H., Hashimoto, T., Vinyals, O.,
  Liang, P., Dean, J., and Fedus, W.
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=yzkSU5zdwD}.
\newblock Survey Certification.

\bibitem[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared}
Xia, M., Gao, T., Zeng, Z., and Chen, D.
\newblock Sheared llama: Accelerating language model pre-training via
  structured pruning.
\newblock \emph{arXiv preprint arXiv:2310.06694}, 2023.

\bibitem[Xu et~al.(2023)Xu, Xie, Gu, Chen, Chang, Zhang, Chen, Zhang, and
  Tian]{xu2023qa}
Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X.,
  and Tian, Q.
\newblock Qa-lora: Quantization-aware low-rank adaptation of large language
  models.
\newblock \emph{arXiv preprint arXiv:2309.14717}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Bai, Lin, Zhao, Hou, and
  Cannistraci]{zhang2023efficient}
Zhang, Y., Bai, H., Lin, H., Zhao, J., Hou, L., and Cannistraci, C.~V.
\newblock An efficient plug-and-play post-training pruning strategy in large
  language models.
\newblock 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhao, Lin, Sun, Yao, Han,
  Tanner, Liu, and Ji]{zhang2023dynamic}
Zhang, Y., Zhao, L., Lin, M., Sun, Y., Yao, Y., Han, X., Tanner, J., Liu, S.,
  and Ji, R.
\newblock Dynamic sparse no training: Training-free fine-tuning for sparse
  llms.
\newblock \emph{arXiv preprint arXiv:2310.08915}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou2021learning}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning n: m fine-grained structured sparse neural networks from
  scratch.
\newblock \emph{arXiv preprint arXiv:2102.04010}, 2021.

\bibitem[Zhou et~al.(2024)Zhou, Wang, Lu, Shi, Luo, Qin, Lu, Jia, Song, Zhan,
  and Li]{zhou2024solving}
Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song,
  L., Zhan, M., and Li, H.
\newblock Solving challenging math word problems using {GPT}-4 code interpreter
  with code-based self-verification.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=c8McWs4Av0}.

\end{thebibliography}
