
@Article{Tight_EG_OGDA,
	author  = {Anonymous authors},
	journal = {Private technical report},
	title   = {Tight Analysis of Extra-gradient and Optimistic Gradient Methods For Nonconvex-Strongly-Concave Minimax Problems},
	year    = {2022},
}


@Article{SGD_Lojasiewicz_ABC,
	author  = {Anonymous authors},
	journal = {Private technical report},
	title   = {Analysis of {SGD} under the Łojasiewicz Inequality},
	year    = {2022},
}


@InProceedings{li2018simple,
	author    = {Li, Zhize and Li, Jian},
	booktitle = {Advances in Neural Information Processing Systems },
	title     = {A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization},
	year      = {2018},
	pages     = {5569--5579},
}

@Article{SpiderBoost,
	author  = {Zhe Wang and Kaiyi Ji and Yi Zhou and Yingbin Liang and Vahid Tarokh},
	journal = {arXiv preprint arXiv:1810.10690},
	title   = {SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms},
	year    = {2018},
}



@Article{Bolte_KL,
	author = {Attouch, Hedy and Bolte, J{\'e}r{\^o}me},
	journal = {Mathematical Programming},
	number = {1},
	pages = {5--16},
	title = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
	volume = {116},
	year = {2009},
}




@Article{Karimi_PL,
	author  = {Hamed Karimi, Julie Nutini, Mark Schmidt},
	journal = {arXiv preprint arXiv:1608.04636v4},
	title   = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition},
	year    = {2016},
}


@Article{ABC_Khaled,
	author  = {Ahmed Khaled, Peter Richtárik},
	journal = {arXiv preprint arXiv:2002.03329v3},
	title   = {Better Theory for {SGD} in the Nonconvex World},
	year    = {2020},
}


@Article{Fontaine_SGD_dynamics,
	author  = {Xavier Fontaine, Valentin De Bortoli, Alain Durmus},
	journal = {arXiv preprint arXiv:2004.04193v2},
	title   = {Convergence rates and approximation results for {SGD} and its continuous-time counterpart},
	year    = {2020},
}




@article{yang-kiyavash-he20,
  title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1153--1165},
  year={2020}
}
@Article{SALSA,
	author  = {Pengchuan Zhang and Hunter Lang and Qiang Liu and Lin Xiao},
	journal = {arXiv preprint arXiv:2002.10597v1},
	title   = {Statistical Adaptive Stochastic Gradient Methods},
	year    = {2020},
}

@InProceedings{SASA,
	author  = {Hunter Lang and Lin Xiao and Pengchuan Zhang},
	booktitle={Advances in Neural Information Processing Systems },
	title   = {Using Statistics to Automate Stochastic Optimization},
	year    = {2019},
}

@InProceedings{Step_decay_LS,
	author    = {Rong Ge and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli},
	booktitle={Advances in Neural Information Processing Systems },
	title     = {The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares},
	year      = {2019},
}


@InProceedings{Step_decay_21,
	author    = {Xiaoyu Wang and Sindri Magnússon and Mikael Johansson},
	booktitle={Advances in Neural Information Processing Systems },
	title     = {On the Convergence of Step Decay Step-Size for Stochastic Optimization},
	year      = {2021},
}

@InProceedings{RL_based_schedule,
	author    = {Zhen Xu and Andrew M. Dai and Jonas Kemp and Luke Metz},
	booktitle={Advances in Neural Information Processing Systems },
	title     = {Learning an Adaptive Learning Rate Schedule
	},
	year      = {2019},
}

@Article{Unified_SGD_Stich,
	author  = {Sebastian U. Stich},
	journal = {arXiv preprint arXiv:1907.04232v2},
	title   = {Unified Optimal Analysis of the (Stochastic) Gradient Method},
	year    = {2019},
}

@InProceedings{SGD_Moulines_Bach,
	author  = {Eric Moulines and Francis Bach},
	booktitle={Advances in Neural Information Processing Systems },
	title   = {Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning},
	year    = {2011},
}

@InProceedings{ExpDecay,
	author  = {Xiaoyu Li and Zhenxun Zhuang and Francesco Orabona},
	booktitle={International Conference on Machine Learning},
	title   = {A second look at exponential and cosine step sizes: Simplicity, adaptivity, and performance},
	year    = {2021},
}


@Article{AdaGDA,
	author  = {Feihu Huang and Heng Huang},
	journal = {arXiv preprint arXiv:2106.16101v4},
	title   = {AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization},
	year    = {2021},
}

@InProceedings{AdaProx,
	title={Adaptive Extra-Gradient Methods for Min-Max Optimization and Games},
	author={Kimon Antonakopoulos and Veronica Belmega and Panayotis Mertikopoulos},
	booktitle={International Conference on Learning Representations},
	year={2021},
}

@InProceedings{DSEG,
	author = {Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J\'{e}r\^{o}me and Mertikopoulos, Panayotis},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling},
	year = {2020}
}

@Article{GANs_training,
	author  = {Lars Mescheder and Andreas Geiger and Sebastian Nowozin},
	journal = {arXiv preprint arXiv:1801.04406v4},
	title   = {Which Training Methods for GANs do actually Converge?},
	year    = {2018},
}

@InProceedings{GANs_Goodfellow,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Generative Adversarial Nets},
	year = {2014}
}

@InProceedings{ADAM,
	author={Diederik P. Kingma and Jimmy Ba},
	title={Adam: A Method for Stochastic Optimization},
	year={2015},
	booktitle={ICLR (Poster)},
}

% Encoding: UTF-8

@inproceedings{mishchenko2020random,
	author = {Mishchenko, Konstantin and Khaled, Ahmed and Richtarik, Peter},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
	pages = {17309--17320},
	publisher = {Curran Associates, Inc.},
	title = {Random Reshuffling: Simple Analysis with Vast Improvements},
	
	volume = {33},
	year = {2020}
}


@Article{SMOMENTUM,
	author  = {Loizou, Nicolas and Richt\'{a}rik, Peter},
	journal = {Computational Optimization and Applications},
	title   = {Momentum and stochastic momentum for stochastic gradient, {N}ewton, proximal point and subspace descent methods},
	year    = {2020},
	pages   = {653--710},
	volume  = {77},
}


@article{PowerSGD,
	title={PowerSGD: Practical low-rank gradient compression for distributed optimization},
	author={Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
	journal={Advances in Neural Information Processing Systems},
	year={2019}
}

@incollection{bottou2012stochastic,
	title={Stochastic gradient descent tricks},
	author={Bottou, L{\'e}on},
	booktitle={Neural networks: Tricks of the trade},
	pages={421--436},
	year={2012},
	publisher={Springer}
}

@inproceedings{bottou2009curiously,
	title={Curiously fast convergence of some stochastic gradient descent algorithms},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of the symposium on learning and data science, Paris},
	volume={8},
	pages={2624--2633},
	year={2009}
}

@article{arjevani2019lower,
	title={Lower bounds for non-convex stochastic optimization},
	author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
	journal={arXiv preprint arXiv:1912.02365},
	year={2019}
}

@article{lojasiewicz1963topological,
	title={A topological property of real analytic subsets},
	author={Lojasiewicz, Stanislaw},
	journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
	volume={117},
	number={87-89},
	pages={2},
	year={1963}
}

@article{polyak1963gradient,
	title={Gradient methods for the minimisation of functionals},
	author={Polyak, Boris T},
	journal={USSR Computational Mathematics and Mathematical Physics},
	volume={3},
	number={4},
	pages={864--878},
	year={1963},
	publisher={Elsevier}
}

@inproceedings{You2020Large,
	title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
	author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
	booktitle={International Conference on Learning Representations},
	year={2020},
}
url={https://openreview.net/forum?id=Syx4wnEtvH}

@article{goyal2017accurate,
	title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
	author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	journal={arXiv preprint arXiv:1706.02677},
	year={2017}
}

@article{gower2020variance,
	title={Variance-reduced methods for machine learning},
	author={Gower, Robert M and Schmidt, Mark and Bach, Francis and Richt{\'a}rik, Peter},
	journal={Proceedings of the IEEE},
	volume={108},
	number={11},
	pages={1968--1983},
	year={2020},
	publisher={IEEE}
}

@inproceedings{gower2019sgd,
	title={{SGD}: {G}eneral analysis and improved rates},
	author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
	booktitle={International Conference on Machine Learning},
	pages={5200--5209},
	year={2019},
	organization={PMLR}
}

@InProceedings{PAGE2021,
	author       = {Zhize Li and Hongyan Bao and Xiangliang Zhang and Peter Richt\'{a}rik},
	booktitle    = {International Conference on Machine Learning },
	title        = {{PAGE}: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
	year         = {2021},
	organization = {PMLR},
	pages        = {6286--6295},
}

@InProceedings{ACH-overparameterized-2018,
	author    = {Sanjeev Arora and Nadav Cohen and Elad Hazan},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning },
	title     = {On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
	year      = {2018},
}

@Article{DMLsurvey,
	author  = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S},
	title   = {A Survey on Distributed Machine Learning},
	journal = {ACM Computing Surveys},
	year    = {2019},
}

@inproceedings{nesterov1,
	title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
	author={Nesterov, Yurii},
	booktitle={Doklady AN USSR},
	volume={269},
	pages={543--547},
	year={1983}
}

@book{shai_book,
	title={Understanding machine learning: from theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge University Press}
}

@Article{NonconvexBook,
	author  = {Jain, Prateek and Kar, Purushottam},
	journal = {Foundations and Trends in Machine Learning},
	title   = {Non-convex Optimization for Machine Learning},
	year    = {2017},
	number  = {3-4},
	pages   = {142--336},
	volume  = {10},
}

@Article{EC-Katyusha,
	author  = {Xun Qian and Peter Richt\'{a}rik and Tong Zhang},
	journal = {arXiv preprint arXiv:2010.00091},
	title   = {Error compensated distributed {SGD} can be accelerated},
	year    = {2020},
}

@InProceedings{Lian2017,
	author    = {Xiangru Lian and Ce Zhang and Huan Zhang and Cho-Jui Hsieh and Wei Zhang and Ji Liu},
	booktitle = {Advances in Neural Information Processing Systems },
	title     = {Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent},
	year      = {2017},
}

@Book{NesterovBook,
	title     = {Introductory lectures on convex optimization: a basic course (Applied Optimization)},
	publisher = {Kluwer Academic Publishers},
	year      = {2004},
	author    = {Yurii Nesterov},
}

@Article{DCGD,
	author  = {Sarit Khirirat and Hamid Reza Feyzmahdavian and Mikael Johansson},
	journal = {arXiv preprint arXiv:1806.06573},
	title   = {Distributed learning with compressed gradients},
	year    = {2018},
}

@Article{DIANA,
	author  = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
	journal = {arXiv preprint arXiv:1901.09269},
	title   = {Distributed Learning with Compressed Gradient Differences},
	year    = {2019},
}

@InProceedings{99percent,
	author    = {Mishchenko, Konstantin and Hanzely, Filip and Richt\'{a}rik, Peter},
	booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
	title     = {99\% of Worker-Master Communication in Distributed Optimization Is Not Needed},
	year      = {2020},
	pages     = {979--988},
	volume    = {124},
}

@Article{DIANA2,
	author  = {Horv\'{a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt\'{a}rik, Peter},
	title   = {Stochastic distributed learning with gradient quantization and variance reduction},
	journal = {arXiv preprint arXiv:1904.05115},
	year    = {2019},
}

@InProceedings{ADIANA,
	author       = {Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richt{\'a}rik, Peter},
	booktitle    = {International Conference on Machine Learning },
	title        = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
	year         = {2020},
	organization = {PMLR},
	pages        = {5895--5904},
}

@Article{CANITA,
	author  = {Li, Zhize and Richt{\'a}rik, Peter},
	journal = {arXiv preprint arXiv:2107.09461},
	title   = {{CANITA}: Faster Rates for Distributed Convex Optimization with Communication Compression},
	year    = {2021},
}


@Article{DFPMCI2019,
	author  = {Chraibi, S\'{e}lim and Khaled, Ahmed and Kovalev, Dmitry and Salim, Adil and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
	title   = {Distributed fixed point methods with compressed iterates},
	journal = {arXiv preprint arXiv:1912.09925},
	year    = {2019},
}

@Article{NL2021,
	author  = {Islamov,  Rustem and Qian,  Xun and Richt\'{a}rik,  Peter},
	journal = {arXiv preprint arXiv:2102.07158},
	title   = {Distributed Second Order Methods with Fast Rates and Compressed Communication},
	year    = {2021},
}

@Article{Nonconvex-sigma_k,
	author  = {Li, Zhize and Richt\'{a}rik, Peter},
	journal = {arXiv preprint arXiv:2006.07013},
	title   = {A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization},
	year    = {2020},
}

@InProceedings{MARINA,
	author       = {Gorbunov, Eduard and Burlachenko, Konstantin and Li, Zhize and Richt{\'a}rik, Peter},
	booktitle    = {International Conference on Machine Learning},
	title        = {{MARINA}: Faster non-convex distributed learning with compression},
	year         = {2021},
	organization = {PMLR},
	pages        = {3788--3798},
}

@TechReport{Dutta-compress-Survey-2020,
	author      = {Hang Xu and Chen-Yu Ho and Ahmed M Abdelmoniem and Aritra Dutta and El Houcine Bergou and Konstantinos Karatsenidis and Marco Canini and Panos Kalnis},
	institution = {KAUST},
	title       = {Compressed communication for distributed deep learning: Survey and quantitative evaluation},
	year        = {2020},
}

@Article{Artemis2020,
	author  = {Constantin Philippenko and Aymeric Dieuleveut},
	journal = {arXiv preprint arXiv:2006.14591},
	title   = {Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees},
	year    = {2020},
}

@Article{sigma_k-convex,
	author  = {Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M. and Richt\'{a}rik, Peter},
	journal = {arXiv preprint arXiv:2006.11573},
	title   = {Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization},
	year    = {2020},
}

@InProceedings{sigma_k,
	author    = {Gorbunov,  Eduard and Hanzely,  Filip and Richt\'{a}rik,  Peter},
	title     = {A Unified Theory of {SGD}: {V}ariance Reduction,  Sampling,  Quantization and Coordinate Descent},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	year      = {2020},
}

@Article{Cnat,
	author  = {Samuel Horv\'{a}th and Chen-Yu Ho and \v{L}udov\'{i}t Horv\'{a}th and Atal Narayan Sahu and Marco Canini and Peter Richt\'{a}rik},
	title   = {Natural compression for distributed deep learning},
	journal = {arXiv preprint arXiv:1905.10988},
	year    = {2019},
}


@InProceedings{Alistarh-EF-NIPS2018,
	author    = {Alistarh, Dan and Hoefler, Torsten and Johansson,  Mikael and Khirirat,  Sarit and Konstantinov,  Nikola and Renggli,  C\'{e}dric},
	title     = {The convergence of sparsified gradient methods},
	booktitle = {Advances in Neural Information Processing Systems },
	year      = {2018},
}

@inproceedings{Seide2014,
	author    = {Seide,  Frank and Fu,  Hao and Droppo,  Jasha and Li,  Gang and Yu,  Dong},
	title     = {1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNN}s},
	booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
	year      = {2014},
}

@article{lei2019stochastic,
	title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
	author={Lei,  Yunwen and Hu,  Ting and Li,  Guiying and Tang,  Ke},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	year={2019},
	publisher={IEEE}
}

%https://arxiv.org/abs/1902.00908 Authors derives an almost sure convergence under Holder smoothness condition and without bounded gradient assumption.

@article{khaled2020better,
	title={Better Theory for {SGD} in the Nonconvex World},
	author={Khaled,  Ahmed and Richt{\'a}rik,  Peter},
	journal={arXiv preprint arXiv:2002.03329},
	year={2020}
}

@article{stich2020biased,
	title={Analysis of {SGD} with Biased Gradient Estimators},
	author={Ajalloeian,  Ahmad and Stich,  Sebastian U},
	journal={arXiv preprint arXiv:2008.00051},
	year={2020}
}

@InProceedings{GDCI,
	author      = {Khaled, Ahmed and Richt\'{a}rik, Peter},
	title       = {Gradient Descent with Compressed Iterates},
	booktitle   = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
	year        = {2019},
}

@InProceedings{FedNL,
  author    = {Safaryan, Mher and Islamov, Rustem and Qian, Xun and Richt\'{a}rik, Peter},
  booktitle = {Internatioanl Conference on Machine Learning},
  title     = {{FedNL}: Making {N}ewton-type methods applicable to federated learning},
  year      = {2022},
}


% Provides with the rate of nonconvex SGD unbiased compressor

@article{beznosikov2020biased,
	title={On Biased Compression for Distributed Learning},
	author={Beznosikov,  Aleksandr and Horv{\'a}th,  Samuel and Richt{\'a}rik,  Peter and Safaryan,  Mher},
	journal={arXiv preprint arXiv:2002.12410},
	year={2020}
}

@Article{UP2021,
	author  = {Mher Safaryan and Egor Shulgin and Peter Richt\'{a}rik},
	journal = {Information and Inference: A Journal of the IMA},
	title   = {Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor},
	year    = {2021},
}

%We use Lemma 2 (from Appendix A) in our work.


@article{Stich_Delayed_2019,
	title={The Error-Feedback Framework: Better Rates for {SGD} with Delayed Gradients and Compressed Communication},
	author={Stich,  Sebastian and Sai Praneeth Karimireddy},
	journal={arXiv preprint arXiv:1909.05350},
	year={2019},
}

@InProceedings{Karimireddy_SignSGD,
	title = 	 {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
	author =       {Karimireddy,  Sai Praneeth and Rebjock,  Quentin and Stich,  Sebastian and Jaggi,  Martin},
	booktitle = 	 {International Conference on Machine Learning },
	year = 	 {2019},
}

@inproceedings{A_better_alternative,
	title={A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning},
	author={Horv\'{a}th,  Samuel and Richt\'{a}rik,  Peter},
	booktitle={International Conference on Learning Representations },
	year={2021},
}

@InProceedings{Stich-EF-NIPS2018,
	author    = {Stich,  Sebastian U.  and Cordonnier,  J.-B.  and Jaggi,  Martin},
	title     = {Sparsified {SGD} with memory},
	booktitle = {Advances in Neural Information Processing Systems },
	year      = {2018},
}


@InProceedings{Lin_EC_SGD,
	author    = {Gorbunov,  Eduard and Kovalev,  Dmitry and Makarenko,  Dmitry and Richt\'{a}rik,  Peter},
	booktitle = {34th Conference on Neural Information Processing Systems },
	title     = {Linearly Converging Error Compensated {SGD}},
	year      = {2020},
}

@inproceedings{Koloskova2019DecentralizedDL,
	title={Decentralized Deep Learning with Arbitrary Communication Compression},
	author={Anastasia Koloskova and Tao Lin and S. Stich and Martin Jaggi},
	booktitle={International Conference on Learning Representations },
	year={2020},
}

@InProceedings{alistarh2017qsgd,
	author    = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	title     = {{QSGD}: {C}ommunication-efficient {SGD} via gradient quantization and encoding},
	booktitle = {Advances in Neural Information Processing Systems },
	year      = {2017},
	pages     = {1709--1720},
}

@InProceedings{Qsparse_local_SGD,
	title={{Q}sparse-local-{SGD}: {D}istributed {SGD} with Quantization,  Sparsification,  and Local Computations},
	author={Debraj Basu and Deepesh Data and Can Karakus and Suhas Diggavi},
	booktitle={Advances in Neural Information Processing Systems },
	year={2019},
}


@InProceedings{CSER,
	title={{CSER:} {C}ommunication-efficient {SGD} with Error Reset},
	author={Cong Xie and Shuai Zheng and Oluwasanmi Koyejo and Indranil Gupta and Mu Li and Haibin Lin},
	booktitle={Advances in Neural Information Processing Systems },
	pages = {12593--12603},
	year={2020},
}

@article{horvath2020better,
	title={A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning},
	author={Horv{\'a}th,  Samuel and Richt{\'a}rik,  Peter},
	journal={arXiv preprint arXiv:2006.11077},
	year={2020}
}


@article{chang2011libsvm,
	title={{LIBSVM}: a library for support vector machines},
	author={Chang, Chih-Chung and Lin, Chih-Jen},
	journal={{ACM} {T}ransactions on {I}ntelligent {S}ystems and {T}echnology (TIST)},
	volume={2},
	number={3},
	pages={1--27},
	year={2011},
	publisher={ACM New York, NY, USA}
}
@article{tran2019hybrid,
	title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
	author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
	journal={arXiv preprint arXiv:1905.05920},
	year={2019}
}

@book{nesterov2018lectures,
	title={Lectures on convex optimization},
	author={Nesterov, Yurii and others},
	volume={137},
	year={2018},
	publisher={Springer}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={770--778},
	year={2016}
}

@techreport{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	jnumber = {Technical Report TR-2009},
	institution = {University of Toronto,  Toronto}
}

@inproceedings{paszke2019pytorch,
	title={Pytorch: An imperative style, high-performance deep learning library},
	author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	booktitle={Advances in Neural Information Processing Systems },
	year={2019}
}


@inproceedings{EF21,
	title={{EF}21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback},
	author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
	booktitle={Advances in Neural Information Processing Systems},
	year={2021}
}

@article{EF21_extension,
	title={Unified analysis of stochastic gradient methods with biased gradient estimators for nonconvex optimization},
	author={Sokolov, Igor and Fatkhullin, Ilyas and Richt{\'a}rik, Peter and  and Gorbunov, Eduard and Li, Zhize},
	month={08.},
	year={2021}
}


@article{Linear_Speedup_PP,
	title={Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning},
	author={Haibo Yang and Minghong Fang and Jia Liu},
	journal={arXiv preprint arXiv:2101.11203v3},
	year={2021}
}

@Article{FL-big,
	author  = {Kairouz, Peter et al},
	title   = {Advances and Open Problems in Federated Learning},
	journal = {arXiv preprint arXiv:1912.04977},
	year    = {2019},
	comment = {and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur\'{e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D’Oliveira, Rafael G.L. and El Rouayheb, Salim and Evans, David and Gardner, Josh and Gasc\'{o}n, Adri\`{a} and Ghazi, Badih and Gibbons, Phillip B. and Garrett, Zachary},
}

@InProceedings{FEDLEARN,
	author    = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Yu, Felix and Richt\'{a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	booktitle = {NIPS Private Multi-Party Machine Learning Workshop},
	title     = {Federated learning: strategies for improving communication efficiency},
	year      = {2016},
	groups    = {richtap:1},
}

@article{Power_of_Choice,
	title={Client Selection in Federated Learning: Convergence Analysis and Power-of-Choice Selection Strategies},
	author={Yae Jee Cho and Jianyu Wang and Gauri Joshi},
	journal={arXiv preprint arXiv:2010.01243v1},
	year={2020}
}

@Book{BeckBook,
	author = {Beck, Amir},
	title = {First-Order Methods in Optimization},
	publisher = {Society for Industrial and Applied Mathematics},
	year = {2017},
}

@article{KL_calculus,
	title={Calculus of the exponent of {K}urdyka-{L}ojasiewicz inequality and its applications to linear convergence of first-order methods},
	author={Guoyin Li and Ting Kei Pong},
	journal={arXiv preprint arXiv:1602.02915v6},
	year={2021}
}

@Article{Schmidt_PL,
	author  = {Hamed Karimi and Julie Nutini and Mark Schmidt},
	journal = {arXiv preprint arXiv:1608.04636v4},
	title   = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the {P}olyak-{\L}ojasiewicz Condition},
	year    = {2020},
}

@Article{Bolte_EB,
	author  = {Jérôme Bolte and Trong Phong Nguyen and Juan Peypouquet and Bruce Suter},
	journal = {arXiv preprint arXiv:1510.08234v3},
	title   = {From error bounds to the complexity of first-order descent methods for convex functions},
	year    = {2016},
}

@Article{SpiderBoost,
	author  = {Zhe Wang and Kaiyi Ji and Yi Zhou and Yingbin Liang and Vahid Tarokh},
	journal = {arXiv preprint arXiv:1810.10690},
	title   = {SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms},
	year    = {2018},
}

@InProceedings{li2018simple,
	author    = {Li, Zhize and Li, Jian},
	booktitle = {Advances in Neural Information Processing Systems },
	title     = {A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization},
	year      = {2018},
	pages     = {5569--5579},
}

@Article{ZeroSARAH,
	author  = {Li, Zhize and Richt{\'a}rik, Peter},
	journal = {arXiv preprint arXiv:2103.01447},
	title   = {{ZeroSARAH}: Efficient Nonconvex Finite-Sum Optimization with Zero Full Gradient Computation},
	year    = {2021},
}

@Article{Heavy-ball,
	author    = {Polyak, Boris T},
	journal   = {Ussr computational mathematics and mathematical physics},
	title     = {Some methods of speeding up the convergence of iteration methods},
	year      = {1964},
	number    = {5},
	pages     = {1--17},
	volume    = {4},
	publisher = {Elsevier},
}



@Article{Arbitrary_Sampling,
	author  = {Zheng Qu and Peter Richt\'{a}rik},
	journal = {arXiv preprint arXiv:1412.8063},
	title   = {Coordinate Descent with Arbitrary Sampling II: Expected Separable Overapproximation},
	year    = {2014},
}

@Article{FedPAGE,
	author  = {Zhao, Haoyu and Li, Zhize and Richt{\'a}rik, Peter},
	journal = {arXiv preprint arXiv:2108.04755},
	title   = {{FedPAGE}: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning},
	year    = {2021},
}

@InProceedings{allen2017katyusha,
	author       = {Allen-Zhu, Zeyuan},
	booktitle    = {Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
	title        = {Katyusha: The first direct acceleration of stochastic gradient methods},
	year         = {2017},
	organization = {ACM},
	pages        = {1200--1205},
}

@Article{lan2015optimal,
	author  = {Lan, Guanghui and Zhou, Yi},
	journal = {arXiv preprint arXiv:1507.02000},
	title   = {An optimal randomized incremental gradient method},
	year    = {2015},
}

@InProceedings{Varag,
	author    = {Lan, Guanghui and Li, Zhize and Zhou, Yi},
	booktitle = {Advances in Neural Information Processing Systems},
	title     = {A Unified Variance-Reduced Accelerated Gradient Method for Convex Optimization},
	year      = {2019},
	pages     = {10462--10472},
}

@Article{li2021anita,
	author  = {Li, Zhize},
	journal = {arXiv preprint arXiv:2103.11333},
	title   = {{ANITA}: An Optimal Loopless Accelerated Variance-Reduced Gradient Method},
	year    = {2021},
}

@InProceedings{D-DIANA,
  author    = {Kovalev, Dmitry and Koloskova, Anastasia and Jaggi, Martin and Richt\'{a}rik, Peter and Stich, Sebastian},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)},
  title     = {A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!},
  year      = {2021},
}

@InProceedings{zhang2021convergence,
  title={On the convergence and sample efficiency of variance-reduced policy gradient method},
  author={Zhang, Junyu and Ni, Chengzhuo and Szepesvari, Csaba and Wang, Mengdi and others},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{KL_PAGER_Fatkhullin,
  title={Sharp Analysis of Stochastic Optimization under Global {K}urdyka-{\L}ojasiewicz Inequality},
  author={Fatkhullin, Ilyas and Etesami, Jalal and He, Niao and Kiyavash, Negar},
  journal={private technical report},
  year={2022}
}


@InProceedings{scaman22a,
  title = 	 {Convergence Rates of Non-Convex Stochastic Gradient Descent Under a Generic Lojasiewicz Condition and Local Smoothness},
  author =       {Scaman, Kevin and Malherbe, Cedric and Santos, Ludovic Dos},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  series = 	 {Proceedings of Machine Learning Research},
}



@Article{Masiha_SCRN_KL,
	author  = {Masiha, Saeed and Salehkaleybar, Saber and He, Niao and Kiyavash, Negar and Thiran,  Patrick},
	journal = {arXiv preprint arXiv:2205.12856v1},
	title   = {Stochastic Second-Order Methods Provably Beat SGD For Gradient-Dominated Functions},
	year    = {2022},
}


@inproceedings{Arjevani_2nd_Order_Stoch,    
    title={Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations},   
    booktitle={Proceedings of Thirty Third Conference on Learning Theory}, 
    author={Arjevani, Yossi and Carmon, Yair and Duchi, John C. and Foster, Dylan J. and Sekhari, Ayush and Sridharan, Karthik}, 
    year={2020}, 
    pages={242–299}
}


@InProceedings{fang19a_escaping_saddles,
  title = 	 {Sharp Analysis for Nonconvex SGD Escaping from Saddle Points},
  author =       {Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1192--1234},
  year = 	 {2019},
  series = 	 {Proceedings of Machine Learning Research},
}



@InProceedings{Leveraging_mei21a,
  title = 	 {Leveraging Non-uniformity in First-order Non-convex Optimization},
  author =       {Mei, Jincheng and Gao, Yue and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7555--7564},
  year = 	 {2021},
}


@Article{Vanilla_PL_Yuan_21,
  title = {A General Sample Complexity Analysis of Vanilla Policy Gradient},
  author = {Yuan, Rui and Gower, Robert M. and Lazaric, Alessandro},
  year = {2022},
  journal = {arXiv preprint arXiv:2107.11433},
}
 


@InProceedings{ROOT_SGD,
  title = 	 {ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm},
  author =       {Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin and Jordan, Michael},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {909--981},
  year = 	 {2022},
}


@article{Adaptivity_SGD_Lei_Jordan_2020,
  title = {On the {{Adaptivity}} of {{Stochastic Gradient-Based Optimization}}},
  author = {Lei, Lihua and Jordan, Michael I.},
  year = {2020},
  journal = {SIAM Journal on Optimization},
  pages = {1473--1500}
}



@article{adam+,
  title = {Adam\$\^+\$: {{A Stochastic Method}} with {{Adaptive Variance Reduction}}},
  author = {Liu, Mingrui and Zhang, Wei and Orabona, Francesco and Yang, Tianbao},
  year = {2020},
  journal = {arXiv preprint arXiv:2011.11985},
}



@article{EF_delay_2021,
  title = {The {{Error-Feedback Framework}}: {{Better Rates}} for {{SGD}} with {{Delayed Gradients}} and {{Compressed Communication}}},
  author = {Stich, Sebastian U. and Karimireddy, Sai Praneeth},
  year = {2021},
  journal = {arXiv preprint arXiv:1909.05350},
}



@article{OnlineOfflineConversions_Levy_2017,
  title = {Online to {{Offline Conversions}}, {{Universality}} and {{Adaptive Minibatch Sizes}}},
  author = {Levy, Kfir Y.},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.10499},
}



@article{AdaGrad_Duchi_2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {JMLR},
}



@article{AMSGrad_Alacaoglu_Nonsmooth_2020,
  title = {Convergence of Adaptive Algorithms for Weakly Convex Constrained Optimization},
  author = {Alacaoglu, Ahmet and Malitsky, Yura and Cevher, Volkan},
  year = {2020},
  journal = {arXiv preprint arXiv:2006.06650},
}


@article{Regularized_GDA_Zeng_2022,
  title = {Regularized {{Gradient Descent Ascent}} for {{Two-Player Zero-Sum Markov Games}}},
  author = {Zeng, Sihan and Doan, Thinh T. and Romberg, Justin},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.13746},
}




@inproceedings{RevisitingStateAugmentation2021a,
  title = {Revisiting {{State Augmentation}} Methods for {{Reinforcement Learning}} with {{Stochastic Delays}}},
  booktitle = {{ACM International Conference} on {Information} \& {Knowledge Management}},
  author = {Nath, Somjit and Baranwal, Mayank and Khadilkar, Harshad},
  year = {2021},
  pages = {1346--1355}
}




@article{DelayedFeedbackEpisodicRL2021,
  title = {Delayed {{Feedback}} in {{Episodic Reinforcement Learning}}},
  author = {Howson, Benjamin and {Pike-Burke}, Ciara and Filippi, Sarah},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.07615},
 
}


@inproceedings{RL_w_Random_delays_Bouteiller_21,
  title = {Reinforcement Learning with Random Delays},
  author = {Bouteiller, Yann and Ramstedt, Simon and Beltrame, Giovanni and Pal, Christopher and Binas, Jonathan},
  year = {2021},
  booktitle={International Conference on Learning Representations },
}





@article{BeyondExactGradientsConvergence2022a,
  title = {Beyond {{Exact Gradients}}: {{Convergence}} of {{Stochastic Soft-Max Policy Gradient Methods}} with {{Entropy Regularization}}},
  author = {Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  year = {2022},
  journal = {arXiv preprint arXiv:2110.10117},
}



@inproceedings{
huang2022lower,
title={Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression},
author={Xinmeng Huang and Yiming Chen and Wotao Yin and Kun Yuan},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

 @article{EF21BW_2021, 
 title={{EF21} with Bells \& Whistles: Practical Algorithmic Extensions of Modern Error Feedback}, 
  journal = {arXiv preprint arXiv:2110.03294},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richtárik, Peter}, 
  year={2021}
  }


 @article{3PC, 
 title={{3PC}: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation}, 
 journal = {arXiv preprint arXiv:2202.00998},
 author={Richtárik, Peter and Sokolov, Igor and Fatkhullin, Ilyas and Gasanov, Elnur and Li, Zhize and Gorbunov, Eduard},
 year={2022} 
 }

@InProceedings{tyurin2022dasha,
  title={{DASHA}: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity},
  author={Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={International Conference on Learning Representations },
  year={2023},
}

 @inproceedings{Sahu_abs_comp_2021,
 	 title={Rethinking gradient sparsification as total error minimization},  booktitle={Advances in Neural Information Processing Systems}, 
 	 author={Sahu, Atal and Dutta, Aritra and M. Abdelmoniem, Ahmed and Banerjee, Trambak and Canini, Marco and Kalnis, Panos}, 
 	 year={2021}, 
 	 pages={8133–8146} 
  }

 @inproceedings{Gupta_DL_limited_presition_2015, title={Deep Learning with Limited Numerical Precision}, 
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish}, year={2015}, 
  month={Feb},
booktitle = {International Conference on Machine Learning},
pages = {1737–1746}
}

 @inproceedings{Sapio_Scaling_Dist_ML_2021, 
 	title={Scaling Distributed Machine Learning with In-Network Aggregation}, 
 	author={Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson, Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy, Arvind and Moshref, Masoud}, 
 	booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation},
 	year = {2021},
 	pages = {785–808}
  }


@inproceedings{DoubleSqueeze,
	title={{D}ouble{S}queeze: {P}arallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression},
	author={Hanlin Tang and Xiangru Lian and Chen Yu and Tong Zhang and Ji Liu},
	booktitle = {International Conference on Machine Learning},
	year={2020},
}

 @article{DASHA_2022, 
 	title={DASHA: Distributed Nonconvex Optimization with Communication Compression, Optimal Oracle Complexity, and No Client Synchronization},  journal={arXiv preprint arXiv:2202.01268},
 author={Tyurin, Alexander and Richtárik, Peter}, year={2022}
 }

@inproceedings{lin2018deep,
	title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
	author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
	booktitle={International Conference on Learning Representations},
	year={2018}
}
 
 @Article{Polyak_Some_methods_1964,
 	author    = {Polyak, Boris T},
 	journal   = {Ussr computational mathematics and mathematical physics},
 	title     = {Some methods of speeding up the convergence of iteration methods},
 	year      = {1964},
 	number    = {5},
 	pages     = {1--17},
 	volume    = {4},
 	publisher = {Elsevier},
 }

@Article{Yang_Unified_momentum_2016,
	author  = {Tianbao Yang and Qihang Lin and Zhe Li},
	journal = {arXiv preprint arXiv:1604.03257},
	title   = {Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
	year    = {2016},
}

 @article{Makarenko_AdaCGD_2022, 
 	title={Adaptive Compression for Communication-Efficient Distributed Training}, 
  journal={arXiv preprint arXiv:2211.00188}, author={Makarenko, Maksim and Gasanov, Elnur and Islamov, Rustem and Sadiev, Abdurakhmon and Richtarik, Peter}, 
  year={2022}, 
  month={Oct} }

 @inproceedings{Ghadikolaei_LENA_2021, 
 title={LENA: Communication-Efficient Distributed Learning with Self-Triggered Gradient Uploads}, 
 booktitle={International Conference on Artificial Intelligence and Statistics}, publisher={PMLR}, author={Ghadikolaei, Hossein Shokri and Stich, Sebastian and Jaggi, Martin}, year={2021}, 
 pages={3943–3951}
}

 @article{Gruntkowska_EF21_P_2022, 
 	title={EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression},  
 	journal={arXiv preprint arXiv:2209.15218},  author={Gruntkowska, Kaja and Tyurin, Alexander and Richtárik, Peter}, 
 	year={2022}, 
 	month={Sep} 
 }

@inproceedings{Zhao_BEER_2022,
	title={{BEER}: Fast {O(1/T)} Rate for Decentralized Nonconvex Optimization with Communication Compression},
	author={Haoyu Zhao and Boyue Li and Zhize Li and Peter Richt{\'a}rik and Yuejie Chi},
	booktitle={Advances in Neural Information Processing Systems},
	year={2022}
}



@inproceedings{Li_COMP_AMS_2022,
	title={On Distributed Adaptive Optimization with Gradient Compression},
	author={Xiaoyun Li and Belhal Karimi and Ping Li},
	booktitle={International Conference on Learning Representations},
	year={2022}
}


@InProceedings{Wang_CD_ADAM-AMSGrad_2022,
	title = 	 { Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization },
	author =       {Wang, Yujia and Lin, Lu and Chen, Jinghui},
	booktitle = 	 { International Conference on Artificial Intelligence and Statistics},
	pages = 	 {6292--6320},
	year = 	 {2022},
	publisher =    {PMLR}
}


@InProceedings{Karimireddy_SCAFFOLD_2020,
	title = 	 {{SCAFFOLD}: Stochastic Controlled Averaging for Federated Learning},
	author =       {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
	booktitle = 	 {International Conference on Machine Learning},
	pages = 	 {5132--5143},
	year = 	 {2020},
	publisher =    {PMLR},
}

@inproceedings{
	Stich_Local_2019,
	title={Local {SGD} Converges Fast and Communicates Little},
	author={Sebastian U. Stich},
	booktitle={International Conference on Learning Representations},
	year={2019}
}

@inproceedings{Chen_LAG_2018,
	author = {Chen, Tianyi and Giannakis, Georgios and Sun, Tao and Yin, Wotao},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning},
	year = {2018}
}

 @article{Dorfman_DoCoFL_2023, 
 	title={$\texttt{DoCoFL}$: Downlink Compression for Cross-Device Federated Learning},  journal={arXiv preprint arXiv:2302.00543}, author={Dorfman, Ron and Vargaftik, Shay and Ben-Itzhak, Yaniv and Levy, Kfir Y.}, year={2023}
 }

 @article{Islamov_Newton_3PC_2022, 
 title={Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation},  
 journal={arXiv preprint arXiv:2206.03588},  author={Islamov, Rustem and Qian, Xun and Hanzely, Slavomír and Safaryan, Mher and Richtárik, Peter}, 
 year={2022}, 
 }


 @article{Yau_DoCoM_SGT_2022, 
 title={{DoCoM-SGT}: Doubly Compressed Momentum-assisted Stochastic Gradient Tracking Algorithm for Communication Efficient Decentralized Learning}, 
  journal={arXiv preprint arXiv:2202.00255}, author={Yau, Chung-Yiu and Wai, Hoi-To}, year={2022}
 }


 @article{Kale_SGD_Role_of_Implicit_Reg_2021, title={SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs},  journal={arXiv preprint arXiv:2107.05074},  author={Kale, Satyen and Sekhari, Ayush and Sridharan, Karthik}, year={2021}
 }

@inproceedings{
	Keskar_OnLargeBatch_DL_2017,
	title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
	author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
	booktitle={International Conference on Learning Representations},
	year={2017}
}

 @inproceedings{Kleinberg_When_SGD_Escape_Local_2018, title={An Alternative View: When Does SGD Escape Local Minima?}, booktitle={International Conference on Machine Learning}, publisher={PMLR}, author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang}, year={2018}, pages={2698–2707} 
  }
 
  @article{Mitra_TD_EF_2023, title={Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning},   journal={arXiv preprint arXiv:2301.00944}, 
  author={Mitra, Aritra and Pappas, George J. and Hassani, Hamed}, year={2023} 
}
 

@InProceedings{Doan_FT_TD_4_MARL_2019,
	title = 	 {Finite-Time Analysis of Distributed {TD}(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning},
	author =       {Doan, Thinh and Maguluri, Siva and Romberg, Justin},
	booktitle = 	 {International Conference on Machine Learning},
	pages = 	 {1626--1635},
	year = 	 {2019},
	publisher =    {PMLR}
}


@InProceedings{Jin_FedRL_Env_Heterogeneity_2022,
	title = 	 { Federated Reinforcement Learning with Environment Heterogeneity },
	author =       {Jin, Hao and Peng, Yang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
	booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
	pages = 	 {18--37},
	year = 	 {2022},
	volume = 	 {151},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {28--30 Mar},
	publisher =    {PMLR}
}

 @inproceedings{Khodadadian_FedRL_Linear_speed_2022, 
 title={Federated Reinforcement Learning: Linear Speedup Under Markovian Sampling},  booktitle={International Conference on Machine Learning}, 
 publisher={PMLR}, 
 author={Khodadadian, Sajad and Sharma, Pranay and Joshi, Gauri and Maguluri, Siva Theja}, year={2022}, month={Jun}, pages={10997–11057}, language={en} }

 @article{Rieke_Dig_Health_FL_2020, 
 title={The future of digital health with federated learning}, volume={3},  number={11}, journal={npj Digital Medicine}, publisher={Nature Publishing Group}, author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletarì, Fausto and Roth, Holger R. and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N. and Landman, Bennett A. and Maier-Hein, Klaus and Ourselin, Sébastien and Sheller, Micah and Summers, Ronald M. and Trask, Andrew and Xu, Daguang and Baust, Maximilian and Cardoso, M. Jorge}, year={2020}, pages={1–7}
 }


 @inproceedings{Cutkosky_MomNSGD_2020, 
 	title={Momentum Improves Normalized {SGD}}, 
 	booktitle={International Conference on Machine Learning}, 
 	publisher={PMLR},
 	author={Cutkosky, Ashok and Mehta, Harsh}, year={2020},  pages={2260–2268}, language={en} 
 }

 @article{Danilova_Nonmonotone_HB_2018, 
 	title={Non-monotone Behavior of the Heavy Ball Method}, 
 	 journal={arXiv preprint arXiv:1811.00658},  author={Danilova, Marina and Kulakova, Anastasiya and Polyak, Boris}, year={2018}
  }

 
 @inproceedings{Ghadimi_Glob_HB_Convex_2014,
 	title={Global convergence of the Heavy-ball method for convex optimization},
 	author={Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
 	booktitle={2015 European control conference (ECC)},
 	pages={310--315},
 	year={2015},
 	organization={IEEE}
 }
 
  @inproceedings{Liu_Improved_analysis_SGDM_2020, 
  	title={An Improved Analysis of Stochastic Gradient Descent with Momentum}, 
  	author={Liu, Yanli and Gao, Yuan and Yin, Wotao}, language={en},
  	booktitle = {Advances in Neural Information Processing Systems },
  	year = {2020}
  }
 
 
  @inproceedings{Yu_LinearSpeedup_Com_Efficient_SGDM_2019,
  	title={On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization}, 
  	booktitle={International Conference on Machine Learning}, publisher={PMLR}, author={Yu, Hao and Jin, Rong and Yang, Sen}, year={2019}, pages={7184–7193}, language={en} }
 

@InProceedings{Sebbouh_AS_Conv_SHB_2021,
	title = 	 {Almost sure convergence rates for Stochastic Gradient Descent and Stochastic Heavy Ball},
	author =       {Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
	booktitle = 	 {Conference on Learning Theory},
	pages = 	 {3935--3971},
	year = 	 {2021},
	publisher =    {PMLR},
}


@misc{
	Huang_Mom_as_VRSGD,
	title={Momentum as Variance-Reduced Stochastic Gradient},
	author={Zih-Syuan Huang and Ching-pei Lee},
	year={2022},
	url={https://openreview.net/forum?id=kiwu8tcVf38}
}


@article{Plattner_SGDM_Thesis_2022,
	title = {On {SGD} with Momentum},
	author = {Plattner, Maximilian},
	year = {2022},
	journal = {Master's Thesis}
}


@InProceedings{Li_On_Last_Iterate_Mom_2022,
	title = 	 {On the Last Iterate Convergence of Momentum Methods},
	author =       {Li, Xiaoyu and Liu, Mingrui and Orabona, Francesco},
	booktitle = 	 {International Conference on Algorithmic Learning Theory},
	pages = 	 {699--717},
	year = 	 {2022},
	publisher =    {PMLR},
	abstract = 	 {SGD with Momentum (SGDM) is a widely used family of algorithms for large scale optimization of machine learning problems. Yet, when optimizing generic convex functions, no advantage is known for any SGDM algorithm over plain SGD. Moreover, even the most recent results require changes to the SGDM algorithms, like averaging of the iterates and a projection onto a bounded domain, which are rarely used in practice. In this paper, we focus on the convergence rate of the last iterate of SGDM. For the first time, we prove that for any constant momentum factor, there exists a Lipschitz and convex function for which the last iterate of SGDM suffers from a suboptimal convergence rate of $\Omega(\frac{\log T}{\sqrt{T}})$ after $T$ iterations. Based on this fact, we study a class of (both adaptive and non-adaptive) Follow-The-Regularized-Leader-based SGDM algorithms with \emph{increasing momentum} and \emph{shrinking updates}. For these algorithms, we show that the last iterate has optimal convergence $O(\frac{1}{\sqrt{T}})$ for unconstrained convex stochastic optimization problems without projections onto bounded domains nor knowledge of $T$. Further, we show a variety of results for FTRL-based SGDM when used with adaptive stepsizes. Empirical results are shown as well.}
}

 @article{Xiao_Conv_SGD_type_2022, 
 	title={A convergence study of SGD-type methods for stochastic optimization},  
 	 journal={arXiv preprint arXiv:2211.06197}, author={Xiao, Tiannan and Yang, Guoguo}, year={2022} }


 @inproceedings{Wang_Provable_Acceleration_HB_2022, 
 	title={Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out},  abstractNote={Heavy Ball (HB) nowadays is one of the most popular momentum methods in non-convex optimization. It has been widely observed that incorporating the Heavy Ball dynamic in gradient-based methods accelerates the training process of modern machine learning models. However, the progress on establishing its theoretical foundation of acceleration is apparently far behind its empirical success. Existing provable acceleration results are of the quadratic or close-to-quadratic functions, as the current techniques of showing HB’s acceleration are limited to the case when the Hessian is fixed. In this work, we develop some new techniques that help show acceleration beyond quadratics, which is achieved by analyzing how the change of the Hessian at two consecutive time points affects the convergence speed. Based on our technical results, a class of Polyak-Lojasiewicz (PL) optimization problems for which provable acceleration can be achieved via HB is identified. Moreover, our analysis demonstrates a benefit of adaptively setting the momentum parameter.}, booktitle={International Conference on Machine Learning}, publisher={PMLR}, author={Wang, Jun-Kun and Lin, Chi-Heng and Wibisono, Andre and Hu, Bin}, year={2022}, pages={22839–22864},  }


 @article{Ochs_IPiasco_2015, 
 	title={iPiasco: Inertial Proximal Algorithm for Strongly Convex Optimization}, 
 	 abstractNote={In this paper, we present a forward–backward splitting algorithm with additional inertial term for solving a strongly convex optimization problem of a certain type. The strongly convex objective function is assumed to be a sum of a non-smooth convex and a smooth convex function. This additional knowledge is used for deriving a worst-case convergence rate for the proposed algorithm. It is proved to be an optimal algorithm with linear rate of convergence. For certain problems this linear rate of convergence is better than the provably optimal worst-case rate of convergence for smooth strongly convex functions. We demonstrate the efﬁciency of the proposed algorithm in numerical experiments and examples from image processing.}, number={2}, journal={Journal of Mathematical Imaging and Vision}, author={Ochs, Peter and Brox, Thomas and Pock, Thomas}, year={2015}, pages={171–181}, language={en} }



@InProceedings{Jelassi_Mom_Improves_Generalization_2022,
	title = 	 {Towards understanding how momentum improves generalization in deep learning},
	author =       {Jelassi, Samy and Li, Yuanzhi},
	booktitle = 	 {International Conference on Machine Learning},
	pages = 	 {9965--10040},
	year = 	 {2022},
	publisher =    {PMLR}
}

 @article{Takezawa_Mom_Tracking_2022, title={Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data}, 
 	 abstractNote={SGD with momentum acceleration is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum acceleration is Distributed SGD (DSGD) with momentum acceleration (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum acceleration that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and decrease when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum acceleration whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the standard deep learning setting, where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coefficient $betain [0, 1)$. Through image classification tasks, we demonstrate that Momentum Tracking is more robust to data heterogeneity than the existing decentralized learning methods with momentum acceleration and can consistently outperform these existing methods when the data distributions are heterogeneous.}, journal={arXiv preprint arXiv:2209.15505}, author={Takezawa, Yuki and Bao, Han and Niwa, Kenta and Sato, Ryoma and Yamada, Makoto}, year={2022},  }


 @article{Gao_Distr_Stoch_Grad_Tracking_2023, 
 	title={Distributed stochastic gradient tracking methods with momentum acceleration for non-convex optimization}, volume={84}, 
 	 abstractNote={We consider a distributed non-convex optimization problem of minimizing the sum of all local cost functions over a network of agents. This problem often appears in large-scale distributed machine learning, known as non-convex empirical risk minimization. In this paper, we propose two accelerated algorithms, named DSGT-HB and DSGT-NAG, which combine the distributed stochastic gradient tracking (DSGT) method with momentum accelerated techniques. Under appropriate assumptions, we prove that both algorithms sublinearly converge to a neighborhood of a first-order stationary point of the distributed non-convex optimization. Moreover, we derive the conditions under which DSGT-HB and DSGT-NAG achieve a network-independent linear speedup. Numerical experiments for a distributed non-convex logistic regression problem on real data sets and a deep neural network on the MNIST database show the superiorities of DSGT-HB and DSGT-NAG compared with DSGT.}, number={2}, journal={Computational Optimization and Applications}, author={Gao, Juan and Liu, Xin-Wei and Dai, Yu-Hong and Huang, Yakui and Gu, Junhua}, year={2023},  pages={531–572} }
  
  
  @InProceedings{Yuan_DecentLaM_2021,
  	author    = {Yuan, Kun and Chen, Yiming and Huang, Xinmeng and Zhang, Yingya and Pan, Pan and Xu, Yinghui and Yin, Wotao},
  	title     = {DecentLaM: Decentralized Momentum SGD for Large-Batch Deep Training},
  	booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  	year      = {2021},
  	pages     = {3029-3039}
  }

 @article{Wang_Quickly_Finding_HB_2021, 
 	title={Quickly Finding a Benign Region via Heavy Ball Momentum in Non-Convex Optimization}, 
 	 abstractNote={The Heavy Ball Method, proposed by Polyak over five decades ago, is a first-order method for optimizing continuous functions. While its stochastic counterpart has proven extremely popular in training deep networks, there are almost no known functions where deterministic Heavy Ball is provably faster than the simple and classical gradient descent algorithm in non-convex optimization. The success of Heavy Ball has thus far eluded theoretical understanding. Our goal is to address this gap, and in the present work we identify two non-convex problems where we provably show that the Heavy Ball momentum helps the iterate to enter a benign region that contains a global optimal point faster. We show that Heavy Ball exhibits simple dynamics that clearly reveal the benefit of using a larger value of momentum parameter for the problems. The first of these optimization problems is the phase retrieval problem, which has useful applications in physical science. The second of these optimization problems is the cubic-regularized minimization, a critical subroutine required by Nesterov-Polyak cubic-regularized method to find second-order stationary points in general smooth non-convex problems.}, journal={arXiv preprint arXiv:2010.01449}, author={Wang, Jun-Kun and Abernethy, Jacob}, year={2021}, }
  
   @article{Karimireddy_Mime_2021, 
   	title={Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning},
   	abstractNote={Federated learning (FL) is a challenging setting for optimization due to the heterogeneity of the data across different clients which gives rise to the client drift phenomenon. In fact, obtaining an algorithm for FL which is uniformly better than simple centralized training has been a major open problem thus far. In this work, we propose a general algorithmic framework, Mime, which i) mitigates client drift and ii) adapts arbitrary centralized optimization algorithms such as momentum and Adam to the cross-device federated learning setting. Mime uses a combination of control-variates and server-level statistics (e.g. momentum) at every client-update step to ensure that each local update mimics that of the centralized method run on iid data. We prove a reduction result showing that Mime can translate the convergence of a generic algorithm in the centralized setting into convergence in the federated setting. Further, we show that when combined with momentum based variance reduction, Mime is provably faster than any centralized method--the first such result. We also perform a thorough experimental exploration of Mime’s performance on real world datasets.}, journal={arXiv preprint arXiv:2008.03606}, 
   	 author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J. and Stich, Sebastian U. and Suresh, Ananda Theertha}, year={2021}, 
   }
  
  
  
  @InProceedings{Mishchenko_Proxskip_2022,
  	title = 	 {{P}rox{S}kip: Yes! {L}ocal Gradient Steps Provably Lead to Communication Acceleration! {F}inally!},
  	author =       {Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richtarik, Peter},
  	booktitle = 	 {International Conference on Machine Learning},
  	pages = 	 {15750--15769},
  	year = 	 {2022},
  	publisher =    {PMLR},
  	abstract = 	 {We introduce ProxSkip—a surprisingly simple and provably efficient method for minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable ($\psi$) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of $f$ and the prox operator of $\psi$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is $\mathcal{O}(\kappa \log \nicefrac{1}{\varepsilon})$, where $\kappa$ is the condition number of $f$, the number of prox evaluations is $\mathcal{O}(\sqrt{\kappa} \log \nicefrac{1}{\varepsilon})$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corresponds to taking a local GD step independently on all devices, and evaluation of prox corresponds to (expensive) communication in the form of gradient averaging. In this context, ProxSkip offers an effective <em>acceleration</em> of communication complexity. Unlike other local gradient-type methods, such as FedAvg, SCAFFOLD, S-Local-GD and FedLin, whose theoretical communication complexity is worse than, or at best matching, that of vanilla GD in the heterogeneous data regime, we obtain a provable and large improvement without any heterogeneity-bounding assumptions.}
  }
  

 @article{Defazio_Mom_PrimalAvg_2021, 
 	title={Momentum via Primal Averaging: Theoretical Insights and Learning Rate Schedules for Non-Convex Optimization},  abstractNote={Momentum methods are now used pervasively within the machine learning community for training non-convex models such as deep neural networks. Empirically, they out perform traditional stochastic gradient descent (SGD) approaches. In this work we develop a Lyapunov analysis of SGD with momentum (SGD+M), by utilizing a equivalent rewriting of the method known as the stochastic primal averaging (SPA) form. This analysis is much tighter than previous theory in the non-convex case, and due to this we are able to give precise insights into when SGD+M may out-perform SGD, and what hyper-parameter schedules will work and why.}, 
 	journal={arXiv preprint arXiv:2010.00406},  author={Defazio, Aaron}, year={2021},  }


@InProceedings{Cutkosky_STORM_2019,
	title={Momentum-based variance reduction in non-convex {SGD}},
	author={Cutkosky, Ashok and Orabona, Francesco},
	booktitle={Advances in Neural Information Processing Systems},
	year={2019}
}

@article{Singh_SquarmSGD_2021, 
	title={Squarm-sgd: Communication-efficient momentum sgd for decentralized optimization},
	author={Singh, Navjot and Data, Deepesh and George, Jemin and Diggavi, Suhas},
	journal={IEEE Journal on Selected Areas in Information Theory},
	volume={2},
	number={3},
	pages={954--969},
	year={2021},
	publisher={IEEE}
}

@InProceedings{Zheng_EF_SGD_Nesterov_2019,
	title={Communication-efficient distributed blockwise momentum {SGD} with error-feedback},
	author={Zheng, Shuai and Huang, Ziyue and Kwok, James},
	booktitle={Advances in Neural Information Processing Systems},
	year={2019}
}


@InProceedings{Yang_Two_Sides_2023,
	title={Two Sides of One Coin: the Limits of Untuned {SGD}
	and the Power of Adaptive Methods},
	author={Yang, Junchi and Li, Xiang and Fatkhullin, Ilyas and He, Niao},
	booktitle={Advances in Neural Information Processing Systems},
	year={2023}
}

@article{Chen_Network_Revenue_2022,
	title={Efficient Algorithms for Minimizing Compositions of Convex Functions and Random Functions and Its Applications in Network Revenue Management},
	author={Chen, Xin and He, Niao and Hu, Yifan and Ye, Zikun},
	journal={arXiv preprint arXiv:2205.01774},
	year={2022}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

@article{Zavriev_heavy_ball_1993,
	title={Heavy-ball method in nonconvex optimization problems},
	author={Zavriev, SK and Kostyuk, FV},
	journal={Computational Mathematics and Modeling},
	volume={4},
	number={4},
	pages={336--341},
	year={1993},
	publisher={Springer}
}

@article{Li_Orabona_High_Prob_2020,
	title={A high probability analysis of adaptive {SGD} with momentum},
	author={Li, Xiaoyu and Orabona, Francesco},
	journal={arXiv preprint arXiv:2007.14294},
	year={2020}
}

@InProceedings{Cutkosky_High_Prob_Tails_2021,
	title={High-probability bounds for non-convex stochastic optimization with heavy tails},
	author={Cutkosky, Ashok and Mehta, Harsh},
	journal={Advances in Neural Information Processing Systems},
	pages={4883--4895},
	year={2021}
}


@article{sebbouh2019nesterov,
	title={{Nesterov}'s acceleration and {Polyak}'s heavy ball method in continuous time: convergence rate analysis under geometric conditions and perturbations},
	author={Sebbouh, Othmane and Dossal, Charles and Rondepierre, Aude},
	journal={arXiv preprint arXiv:1907.02710},
	year={2019}
}

@inproceedings{xu2022detached,
	title={Detached Error Feedback for Distributed SGD with Random Sparsification},
	author={Xu, An and Huang, Heng},
	booktitle={International Conference on Machine Learning},
	pages={24550--24575},
	year={2022},
	organization={PMLR}
}

@article{szlendak2021permutation,
	title={Permutation compressors for provably faster distributed nonconvex optimization},
	author={Szlendak, Rafa{\l} and Tyurin, Alexander and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2110.03300},
	year={2021}
}

@article{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Toronto, ON, Canada}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

