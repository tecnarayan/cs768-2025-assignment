\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Abeille et~al.(2021)Abeille, Faury, and
  Calauz{\`e}nes]{abeille2021instance}
Marc Abeille, Louis Faury, and Cl{\'e}ment Calauz{\`e}nes.
\newblock Instance-wise minimax-optimal algorithms for logistic bandits.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3691--3699. PMLR, 2021.

\bibitem[Afsar et~al.(2022)Afsar, Crump, and Far]{afsar2022reinforcement}
M~Mehdi Afsar, Trafford Crump, and Behrouz Far.
\newblock Reinforcement learning based recommender systems: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (7):\penalty0 1--38, 2022.

\bibitem[Amani and Thrampoulidis(2021)]{amani2021ucb}
Sanae Amani and Christos Thrampoulidis.
\newblock Ucb-based algorithms for multinomial logistic regression bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 2913--2924, 2021.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{auer2008near}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Bacchus et~al.(1996)Bacchus, Boutilier, and Grove]{BBG:NMRDPs}
Fahiem Bacchus, Craig Boutilier, and Adam Grove.
\newblock Rewarding behaviors.
\newblock In \emph{Proceedings of the Thirteenth National Conference on
  Artificial Intelligence (AAAI-96)}, pages 1160--1167, Portland, OR, 1996.

\bibitem[Bhagwat and Subramanian(1978)]{bhagwat1978inequalities}
KV~Bhagwat and R~Subramanian.
\newblock Inequalities between means of positive operators.
\newblock In \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, volume~83, pages 393--401. Cambridge University Press, 1978.

\bibitem[Cen et~al.(2020)Cen, Zhang, Zou, Zhou, Yang, and Tang]{cen:kdd20}
Yukuo Cen, Jianwei Zhang, Xu~Zou, Chang Zhou, Hongxia Yang, and Jie Tang.
\newblock Controllable multi-interest framework for recommendation.
\newblock In \emph{Proceedings of the 26th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery \& Data Mining (KDD-20)}, pages 2942--2951,
  2020.

\bibitem[Chen and Pu(2012)]{chen_critiquing_survey:umuai2012}
Li~Chen and Pearl Pu.
\newblock Critiquing-based recommenders: Survey and emerging trends.
\newblock \emph{User Modeling and User-Adapted Interaction}, 22\penalty0
  (1):\penalty0 125--150, 2012.

\bibitem[Chen et~al.(2022)Chen, Zhu, Zheng, Zhang, Zhao, Cheng, CHENG, Xiong,
  Qin, Chen, et~al.]{chenadaptive}
Xiaoyu Chen, Xiangming Zhu, Yufeng Zheng, Pushi Zhang, Li~Zhao, Wenxue Cheng,
  Peng CHENG, Yongqiang Xiong, Tao Qin, Jianyu Chen, et~al.
\newblock An adaptive deep rl method for non-stationary environments with
  piecewise stable context.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Coulom(2007)]{coulom2007efficient}
R{\'e}mi Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In \emph{International conference on computers and games}, pages
  72--83. Springer, 2007.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem[Efroni et~al.(2020)Efroni, Merlis, and
  Mannor]{efroni2020reinforcement}
Yonathan Efroni, Nadav Merlis, and Shie Mannor.
\newblock Reinforcement learning with trajectory feedback.
\newblock \emph{arXiv preprint arXiv:2008.06036}, 2020.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, Saha, and
  Mannor]{efroni2021confidence}
Yonathan Efroni, Nadav Merlis, Aadirupa Saha, and Shie Mannor.
\newblock Confidence-budget matching for sequential budgeted learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2937--2947. PMLR, 2021.

\bibitem[Fayjie et~al.(2018)Fayjie, Hossain, Oualid, and
  Lee]{fayjie2018driverless}
Abdur~R Fayjie, Sabir Hossain, Doukhi Oualid, and Deok-Jin Lee.
\newblock Driverless car: Autonomous driving using deep reinforcement learning
  in urban environment.
\newblock In \emph{2018 15th international conference on ubiquitous robots
  (ur)}, pages 896--901. IEEE, 2018.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Hallak et~al.(2015)Hallak, Di~Castro, and
  Mannor]{hallak2015contextual}
Assaf Hallak, Dotan Di~Castro, and Shie Mannor.
\newblock Contextual markov decision processes.
\newblock \emph{arXiv preprint arXiv:1502.02259}, 2015.

\bibitem[Harper and Konstan(2015)]{harper2015movielens}
F~Maxwell Harper and Joseph~A Konstan.
\newblock The movielens datasets: History and context.
\newblock \emph{Acm transactions on interactive intelligent systems (tiis)},
  5\penalty0 (4):\penalty0 1--19, 2015.

\bibitem[Hohnhold et~al.(2015)Hohnhold, O'Brien, and Tang]{hohnhold:kdd15}
Henning Hohnhold, Deirdre O'Brien, and Diane Tang.
\newblock Focusing on the long-term: It's good for users and business.
\newblock In \emph{Proceedings of the Twenty-first {ACM} International
  Conference on Knowledge Discovery and Data Mining (KDD-15)}, pages
  1849--1858, Sydney, 2015.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and Mannor]{kwon2021rl}
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor.
\newblock Rl for latent mdps: Regret guarantees and a lower bound.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 24523--24534, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Chung, Szepesv{\'a}ri, and
  Jin]{liu2022partially}
Qinghua Liu, Alan Chung, Csaba Szepesv{\'a}ri, and Chi Jin.
\newblock When is partially observable reinforcement learning not scary?
\newblock \emph{arXiv preprint arXiv:2204.08967}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Netrapalli, Szepesvari, and
  Jin]{liu2022optimistic}
Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin.
\newblock Optimistic mle--a generic model-based algorithm for partially
  observable sequential decision making.
\newblock \emph{arXiv preprint arXiv:2209.14997}, 2022{\natexlab{b}}.

\bibitem[Mao et~al.(2018)Mao, Venkatakrishnan, Schwarzkopf, and
  Alizadeh]{maovariance}
Hongzi Mao, Shaileshh~Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad
  Alizadeh.
\newblock Variance reduction for reinforcement learning in input-driven
  environments.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mladenov et~al.(2019)Mladenov, Meshi, Ooi, Schuurmans, and
  Boutilier]{advamp:ijcai19}
Martin Mladenov, Ofer Meshi, Jayden Ooi, Dale Schuurmans, and Craig Boutilier.
\newblock Advantage amplification in slowly evolving latent-state environments.
\newblock In \emph{Proceedings of the Twenty-eighth International Joint
  Conference on Artificial Intelligence (IJCAI-19)}, pages 3165--3172, Macau,
  2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Papadimitriou and Tsitsiklis(1987)]{papadimitriou1987complexity}
Christos~H Papadimitriou and John~N Tsitsiklis.
\newblock The complexity of markov decision processes.
\newblock \emph{Mathematics of operations research}, 12\penalty0 (3):\penalty0
  441--450, 1987.

\bibitem[Ren et~al.(2022)Ren, Sootla, Jafferjee, Shen, Wang, and
  Ammar]{renreinforcement}
Hang Ren, Aivar Sootla, Taher Jafferjee, Junxiao Shen, Jun Wang, and
  Haitham~Bou Ammar.
\newblock Reinforcement learning in presence of discrete markovian context
  evolution.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Rescorla(1972)]{rescorla1972theory}
Robert~A Rescorla.
\newblock A theory of pavlovian conditioning: Variations in the effectiveness
  of reinforcement and nonreinforcement.
\newblock \emph{Current research and theory}, pages 64--99, 1972.

\bibitem[Ronca and Giacomo(2021)]{ronca:ijcai21}
Alessandro Ronca and Giuseppe~De Giacomo.
\newblock Efficient pac reinforcement learning in regular decision processes.
\newblock In \emph{Proceedings of the Thirtieth International Joint Conference
  on Artificial Intelligence (IJCAI-21)}, pages 2026--2032, Montreal, 2021.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Sun and Tran-Dinh(2019)]{sun2019generalized}
Tianxiao Sun and Quoc Tran-Dinh.
\newblock Generalized self-concordant functions: a recipe for newton-type
  methods.
\newblock \emph{Mathematical Programming}, 178\penalty0 (1):\penalty0 145--213,
  2019.

\bibitem[Tennenholtz et~al.(2020)Tennenholtz, Shalit, and
  Mannor]{tennenholtz2020off}
Guy Tennenholtz, Uri Shalit, and Shie Mannor.
\newblock Off-policy evaluation in partially observable environments.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 10276--10283, 2020.

\bibitem[Tennenholtz et~al.(2022)Tennenholtz, Merlis, Shani, Mannor, Shalit,
  Chechik, Hallak, and Dalal]{tennenholtz2022reinforcement}
Guy Tennenholtz, Nadav Merlis, Lior Shani, Shie Mannor, Uri Shalit, Gal
  Chechik, Assaf Hallak, and Gal Dalal.
\newblock Reinforcement learning with a terminator.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Tessler et~al.(2019)Tessler, Tennenholtz, and
  Mannor]{tessler2019distributional}
Chen Tessler, Guy Tennenholtz, and Shie Mannor.
\newblock Distributional policy optimization: An alternative approach for
  continuous control.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Vlassis et~al.(2012)Vlassis, Littman, and
  Barber]{vlassis2012computational}
Nikos Vlassis, Michael~L Littman, and David Barber.
\newblock On the computational complexity of stochastic controller optimization
  in pomdps.
\newblock \emph{ACM Transactions on Computation Theory (TOCT)}, 4\penalty0
  (4):\penalty0 1--8, 2012.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo~J
  Weinberger.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Wilhelm et~al.(2018)Wilhelm, Ramanathan, Bonomo, Jain, Chi, and
  Gillenwater]{wilhelm:cikm18}
Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed~H. Chi, and
  Jennifer Gillenwater.
\newblock Practical diversified recommendations on {YouTube} with determinantal
  point processes.
\newblock In \emph{Proceedings of the 27th {ACM} International Conference on
  Information and Knowledge Management (CIKM18)}, pages 2165--2173, Torino,
  Italy, 2018.

\bibitem[Xiong et~al.(2022)Xiong, Chen, Gao, and Zhou]{xiong2022sublinear}
Yi~Xiong, Ningyuan Chen, Xuefeng Gao, and Xiang Zhou.
\newblock Sublinear regret for learning pomdps.
\newblock \emph{Production and Operations Management}, 31\penalty0
  (9):\penalty0 3491--3504, 2022.

\bibitem[Zhao et~al.(2013)Zhao, Zhang, and Wang]{zhao13interactive}
Xiaoxue Zhao, Weinan Zhang, and Jun Wang.
\newblock Interactive collaborative filtering.
\newblock In \emph{Proceedings of the 22nd {ACM} International Conference on
  Information and Knowledge Management}, pages 1411--1420, 2013.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin
  Gal, Katja Hofmann, and Shimon Whiteson.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1910.08348}, 2019.

\end{thebibliography}
