\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{dpsgd_2016}
Martin Abadi, Andy Chu, Ian Goodfellow, H.~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer and
  Communications Security}, Oct 2016.
\newblock \doi{10.1145/2976749.2978318}.
\newblock URL \url{http://dx.doi.org/10.1145/2976749.2978318}.

\bibitem[Agarwal and Duchi(2011)]{Agarwal11:delayedSGD}
Alekh Agarwal and John~C Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~24. Curran Associates, Inc., 2011.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2011/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf}.

\bibitem[Authors(2019)]{stackoverflow}
The TensorFlow~Federated Authors.
\newblock Tensor{F}low {F}ederated {Stack Overflow} dataset, 2019.
\newblock URL
  \url{https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data}.

\bibitem[Bassily et~al.(2014)Bassily, Smith, and Thakurta]{bassily2014private}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In \emph{2014 IEEE 55th annual symposium on foundations of computer
  science}, pages 464--473. IEEE, 2014.

\bibitem[Bassily et~al.(2019)Bassily, Feldman, Talwar, and
  Guha~Thakurta]{bassily2019private}
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha~Thakurta.
\newblock Private stochastic convex optimization with optimal rates.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Bubeck(2015)]{Bubeck15:optimization}
S\'{e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Found. Trends Mach. Learn.}, 8\penalty0 (3–4):\penalty0
  231–357, nov 2015.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000050}.
\newblock URL \url{https://doi.org/10.1561/2200000050}.

\bibitem[Choquette-Choo et~al.(2022)Choquette-Choo, McMahan, Rush, and
  Thakurta]{choquette22:multi-epochs}
Christopher~A. Choquette-Choo, H.~Brendan McMahan, Keith Rush, and Abhradeep
  Thakurta.
\newblock Multi-epoch matrix factorization mechanisms for private machine
  learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.06530}.

\bibitem[Das et~al.(2022)Das, Kale, Xu, Zhang, and Sanghavi]{das2022beyond}
Rudrajit Das, Satyen Kale, Zheng Xu, Tong Zhang, and Sujay Sanghavi.
\newblock Beyond uniform {L}ipschitz condition in differentially private
  optimization.
\newblock \emph{arXiv preprint arXiv:2206.10713}, 2022.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{Dekel12:sgd_convex_proof}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{J. Mach. Learn. Res.}, 13\penalty0 (null):\penalty0 165–202,
  jan 2012.
\newblock ISSN 1532-4435.

\bibitem[Denisov et~al.(2022)Denisov, McMahan, Rush, Smith, and
  Thakurta]{denisov2022:matrix-fact}
Sergey Denisov, Brendan McMahan, Keith Rush, Adam Smith, and Abhradeep~Guha
  Thakurta.
\newblock Improved differential privacy for {SGD} via optimal private linear
  operators on adaptive streams.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi2012randomized}
John~C Duchi, Peter~L Bartlett, and Martin~J Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[Dutta et~al.(2018)Dutta, Joshi, Ghosh, Dube, and
  Nagpurkar]{dutta2018slow}
Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube, and Priya
  Nagpurkar.
\newblock Slow and stale gradients can win the race: Error-runtime trade-offs
  in distributed sgd.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pages 803--812. PMLR, 2018.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and Smith]{dp_def}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In Shai Halevi and Tal Rabin, editors, \emph{Theory of Cryptography},
  pages 265--284, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-32732-5.

\bibitem[Edmonds et~al.(2020)Edmonds, Nikolov, and Ullman]{edmonds2020power}
Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman.
\newblock The power of factorization mechanisms in local and central
  differential privacy.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 425--438, 2020.

\bibitem[Erlingsson et~al.(2019)Erlingsson, Feldman, Mironov, Raghunathan,
  Talwar, and Thakurta]{erlingsson2019amplification}
{\'U}lfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal
  Talwar, and Abhradeep Thakurta.
\newblock Amplification by shuffling: From local to central differential
  privacy via anonymity.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2468--2479. SIAM, 2019.

\bibitem[Feldman et~al.(2022)Feldman, McMillan, and Talwar]{feldman2022hiding}
Vitaly Feldman, Audra McMillan, and Kunal Talwar.
\newblock Hiding among the clones: A simple and nearly optimal analysis of
  privacy amplification by shuffling.
\newblock In \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 954--964. IEEE, 2022.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt{\'a}rik]{gorbunov2020linearly}
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt{\'a}rik.
\newblock Linearly converging error compensated sgd.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20889--20900, 2020.

\bibitem[Henzinger and Upadhyay(2022)]{constant_matters}
Monika Henzinger and Jalaj Upadhyay.
\newblock Constant matters: Fine-grained complexity of differentially private
  continual observation using completely bounded norms.
\newblock Cryptology ePrint Archive, Paper 2022/225, 2022.
\newblock URL \url{https://eprint.iacr.org/2022/225}.
\newblock \url{https://eprint.iacr.org/2022/225}.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{jin2021nonconvex}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29,
  2021.

\bibitem[Kairouz et~al.(2021{\natexlab{a}})Kairouz, Mcmahan, Song, Thakkar,
  Thakurta, and Xu]{kairouz21-dp-ftrl}
Peter Kairouz, Brendan Mcmahan, Shuang Song, Om~Thakkar, Abhradeep Thakurta,
  and Zheng Xu.
\newblock Practical and private (deep) learning without sampling or shuffling.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 5213--5225. PMLR,
  18--24 Jul 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v139/kairouz21b.html}.

\bibitem[Kairouz et~al.(2021{\natexlab{b}})Kairouz, McMahan, Avent, Bellet,
  Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings,
  et~al.]{kairouz2021advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021{\natexlab{b}}.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova20:unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U. Stich.
\newblock A unified theory of decentralized {SGD} with changing topology and
  local updates.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Koskela et~al.(2021)Koskela, J{\"a}lk{\"o}, Prediger, and
  Honkela]{koskela2021tight}
Antti Koskela, Joonas J{\"a}lk{\"o}, Lukas Prediger, and Antti Honkela.
\newblock Tight differential privacy for discrete-valued mechanisms and for the
  subsampled gaussian mechanism using {FFT}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3358--3366. PMLR, 2021.

\bibitem[Li et~al.(2010)Li, Hay, Rastogi, Miklau, and
  McGregor]{li2010optimizing}
Chao Li, Michael Hay, Vibhor Rastogi, Gerome Miklau, and Andrew McGregor.
\newblock Optimizing linear counting queries under differential privacy.
\newblock In \emph{Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART
  symposium on Principles of database systems}, pages 123--134, 2010.

\bibitem[Li et~al.(2015)Li, Miklau, Hay, Mcgregor, and Rastogi]{Li2015TheMM}
Chao Li, Gerome Miklau, Michael Hay, Andrew Mcgregor, and Vibhor Rastogi.
\newblock The matrix mechanism: optimizing linear counting queries under
  differential privacy.
\newblock \emph{The VLDB Journal}, 24:\penalty0 757--781, 2015.

\bibitem[Lucchi et~al.(2022)Lucchi, Proske, Orvieto, Bach, and
  Kersting]{lucchi22:noise_correlation}
Aurelien Lucchi, Frank Proske, Antonio Orvieto, Francis Bach, and Hans
  Kersting.
\newblock On the theoretical properties of noise correlation in stochastic
  optimization.
\newblock \emph{Neural Information Processing Systems}, 2022.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{mania17:perturbed_analysis}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I. Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.
\newblock \doi{10.1137/16M1057000}.
\newblock URL \url{https://doi.org/10.1137/16M1057000}.

\bibitem[McKenna et~al.(2018)McKenna, Miklau, Hay, and
  Machanavajjhala]{mckenna2018optimizing}
Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala.
\newblock Optimizing error of high-dimensional statistical queries under
  differential privacy.
\newblock \emph{arXiv preprint arXiv:1808.03537}, 2018.

\bibitem[McKenna et~al.(2021)McKenna, Miklau, Hay, and
  Machanavajjhala]{mckenna2021hdmm}
Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala.
\newblock Hdmm: Optimizing error of high-dimensional statistical queries under
  differential privacy.
\newblock \emph{arXiv preprint arXiv:2106.12118}, 2021.

\bibitem[McMahan and Thakurta(2022)]{mcmahan2022federated}
Brendan McMahan and Abhradeep Thakurta.
\newblock Federated learning with formal differential privacy guarantees.
\newblock \emph{Google AI Blog}, 2022.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and
  Richt{\'a}rik]{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17309--17320, 2020.

\bibitem[Mitra et~al.(2021)Mitra, Jaafar, Pappas, and Hassani]{mitra2021linear}
Aritra Mitra, Rayana Jaafar, George~J Pappas, and Hamed Hassani.
\newblock Linear convergence in federated learning: Tackling client
  heterogeneity and sparse gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14606--14619, 2021.

\bibitem[Nesterov(1983)]{nesterov1983:momentum}
Yurii Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock \emph{Proceedings of the USSR Academy of Sciences}, 269:\penalty0
  543--547, 1983.

\bibitem[Nguyen et~al.(2022)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and
  Huba]{nguyen2022federated}
John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani
  Malek, and Dzmitry Huba.
\newblock Federated learning with buffered asynchronous aggregation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3581--3607. PMLR, 2022.

\bibitem[Orvieto et~al.(2022{\natexlab{a}})Orvieto, Kersting, Proske, Bach, and
  Lucchi]{orvieto2022anticorrelated}
Antonio Orvieto, Hans Kersting, Frank Proske, Francis Bach, and Aurelien
  Lucchi.
\newblock Anticorrelated noise injection for improved generalization.
\newblock \emph{arXiv preprint arXiv:2202.02831}, 2022{\natexlab{a}}.

\bibitem[Orvieto et~al.(2022{\natexlab{b}})Orvieto, Raj, Kersting, and
  Bach]{orvieto2022explicit}
Antonio Orvieto, Anant Raj, Hans Kersting, and Francis Bach.
\newblock Explicit regularization in overparametrized models via noise
  injection.
\newblock \emph{arXiv preprint arXiv:2206.04613}, 2022{\natexlab{b}}.

\bibitem[Polyak(1964)]{polyak1964:momentum}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.
\newblock ISSN 0041-5553.
\newblock \doi{https://doi.org/10.1016/0041-5553(64)90137-5}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0041555364901375}.

\bibitem[Robbins and Monro(1951)]{Robbins51:sgd_original}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400 -- 407, 1951.
\newblock \doi{10.1214/aoms/1177729586}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177729586}.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{ShalevShwartz2009:StochasticCO}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{Annual Conference Computational Learning Theory}, 2009.

\bibitem[Stich(2019)]{stich2018local}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1g2JnRcFX}.

\bibitem[Stich and Karimireddy(2022)]{stich21:error-feedback}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed updates.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (1), jun 2022.
\newblock ISSN 1532-4435.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Thakkar et~al.(2021)Thakkar, Andrew, and
  McMahan]{Thakkar19:adaptive_clipping}
Om~Thakkar, Galen Andrew, and H.~B. McMahan.
\newblock Differentially private learning with adaptive clipping.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Tran and Cutkosky(2022)]{tran2022momentumFTRL}
Hoang Tran and Ashok Cutkosky.
\newblock Momentum aggregation for private non-convex erm, 2022.

\bibitem[Vardhan and Stich(2022)]{vardhan22:smoothing}
Harsh Vardhan and Sebastian~U. Stich.
\newblock Tackling benign nonconvexity with smoothing and stochastic gradients,
  2022.
\newblock URL \url{https://arxiv.org/abs/2202.09052}.

\bibitem[Wang et~al.(2017)Wang, Ye, and Xu]{wang2017differentially}
Di~Wang, Minwei Ye, and Jinhui Xu.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wang et~al.(2019)Wang, Balle, and Kasiviswanathan]{wang2019subsampled}
Yu-Xiang Wang, Borja Balle, and Shiva~Prasad Kasiviswanathan.
\newblock Subsampled {R\'e}nyi differential privacy and analytical moments
  accountant.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1226--1235. PMLR, 2019.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7184--7193. PMLR, 2019.

\bibitem[Yuan et~al.(2016)Yuan, Yang, Zhang, and Hao]{newton_step_mm}
Ganzhao Yuan, Yin Yang, Zhenjie Zhang, and Zhifeng Hao.
\newblock Convex optimization for linear query processing under approximate
  differential privacy, 2016.
\newblock URL \url{https://arxiv.org/abs/1602.04302}.

\bibitem[Yuan and Ma(2020)]{yuan2020federated}
Honglin Yuan and Tengyu Ma.
\newblock Federated accelerated stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5332--5344, 2020.

\bibitem[Yun et~al.(2022)Yun, Rajput, and Sra]{yun2022minibatch}
Chulhee Yun, Shashank Rajput, and Suvrit Sra.
\newblock Minibatch vs local {SGD} with shuffling: Tight convergence bounds and
  beyond.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=LdlwbBP2mlq}.

\bibitem[Zhou et~al.(2019)Zhou, Liu, Li, Lin, Zhou, and Zhao]{zhou2019toward}
Mo~Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao.
\newblock Toward understanding the importance of noise in training neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  7594--7602. PMLR, 2019.

\bibitem[Zhu and Wang(2019)]{zhu2019poission}
Yuqing Zhu and Yu-Xiang Wang.
\newblock Poission subsampled {R\'e}nyi differential privacy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7634--7642. PMLR, 2019.

\end{thebibliography}
