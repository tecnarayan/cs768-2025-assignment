\begin{thebibliography}{10}

\bibitem{abraham2011optimization}
Mark~J Abraham and Jill~E Gready.
\newblock Optimization of parameters for molecular dynamics simulation using
  smooth particle-mesh ewald in gromacs 4.5.
\newblock {\em Journal of computational chemistry}, 32(9):2031--2040, 2011.

\bibitem{GEANT4}
S.~Agostinelli et~al.
\newblock {GEANT4: A Simulation toolkit}.
\newblock {\em Nucl. Instrum. Meth.}, A506:250--303, 2003.

\bibitem{FairRoot}
M~Al-Turany, D~Bertini, R~Karabowicz, D~Kresan, P~Malzacher, T~Stockmanns, and
  F~Uhlig.
\newblock The fairroot framework.
\newblock {\em Journal of Physics: Conference Series}, 396(2):022001, 2012.

\bibitem{Amadio_2017}
G.~Amadio, J.~Apostolakis, M.~Bandieramonte, S.P. Behera, R.~Brun, P.~Canal,
  F.~Carminati, G.~Cosmo, L.~Duhem, D.~Elvira, G.~Folger, A.~Gheata, M.~Gheata,
  I.~Goulas, F.~Hariri, S.Y. Jun, D.~Konstantinov, H.~Kumawat, V.~Ivantchenko,
  G.~Lima, T.~Nikitina, M.~Novak, W.~Pokorski, A.~Ribon, R.~Seghal, O.~Shadura,
  S.~Vallecorsa, and S.~Wenzel.
\newblock Stochastic optimization of {GeantV} code by use of genetic
  algorithms.
\newblock {\em Journal of Physics: Conference Series}, 898:042026, oct 2017.

\bibitem{genetic_book}
W.~Banzhaf, P.~Nordin, R.E. Keller, and F.D. Francone.
\newblock {\em Genetic Programming: An Introduction}.
\newblock Morgan Kaufmann Series in Arti. Elsevier Science, 1998.

\bibitem{shield_opt_ship}
A~Baranov, E~Burnaev, D~Derkach, A~Filatov, N~Klyuchnikov, O~Lantwin,
  F~Ratnikov, A~Ustyuzhanin, and A~Zaitsev.
\newblock Optimising the active muon shield for the {SHiP} experiment at
  {CERN}.
\newblock {\em Journal of Physics: Conference Series}, 934:012050, dec 2017.

\bibitem{cramer_gan}
Marc~G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji
  Lakshminarayanan, Stephan Hoyer, and R{\'e}mi Munos.
\newblock The cramer distance as a solution to biased wasserstein gradients.
\newblock {\em arXiv preprint arXiv:1705.10743}, 2017.

\bibitem{opt_design}
Atul Bhaskar.
\newblock Principles of optimal design: Modeling and computation p. y.
  papalambros and d. j. wilde second edition. cambridge university press, the
  edinburgh building, cambridge cb2 2ru, uk. 2000. 390pp. illustrated. £27.95.
  isbn 0-521-62727-3.
\newblock {\em The Aeronautical Journal (1968)}, 105(1050):458–459, 2001.

\bibitem{Pyro}
Eli Bingham, Jonathan~P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj
  Pradhan, Theofanis Karaletsos, Rohit Singh, Paul~A. Szerlip, Paul Horsfall,
  and Noah~D. Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock {\em CoRR}, abs/1810.09538, 2018.

\bibitem{Brookes2019ConditioningBA}
David~H. Brookes, Hahnbeom Park, and Jennifer Listgarten.
\newblock Conditioning by adaptive sampling for robust design.
\newblock In {\em ICML}, 2019.

\bibitem{chang2016compositional}
Michael~B Chang, Tomer Ullman, Antonio Torralba, and Joshua~B Tenenbaum.
\newblock A compositional object-based approach to learning physical dynamics.
\newblock {\em arXiv preprint arXiv:1612.00341}, 2016.

\bibitem{Chen2016LearningTL}
Yutian Chen, Matthew~W. Hoffman, Sergio~Gomez Colmenarejo, Misha Denil,
  Timothy~P. Lillicrap, and Nando de~Freitas.
\newblock Learning to learn for global optimization of black box functions.
\newblock {\em ArXiv}, abs/1611.03824, 2016.

\bibitem{cranmer2019frontier}
Kyle Cranmer, Johann Brehmer, and Gilles Louppe.
\newblock The frontier of simulation-based inference.
\newblock {\em arXiv preprint arXiv:1911.01429}, 2019.

\bibitem{BelbutePeres2018EndtoEndDP}
Filipe de~Avila Belbute-Peres, Kevin~A. Smith, Kelsey~R. Allen, Joshua~B.
  Tenenbaum, and J.~Zico Kolter.
\newblock End-to-end differentiable physics for learning and control.
\newblock In {\em NeurIPS}, 2018.

\bibitem{Degrave2017ADP}
Jonas Degrave, Michiel Hermans, Joni Dambre, and Francis Wyffels.
\newblock A differentiable physics engine for deep learning in robotics.
\newblock In {\em Front. Neurorobot.}, 2017.

\bibitem{djolonga2013high}
Josip Djolonga, Andreas Krause, and Volkan Cevher.
\newblock High-dimensional gaussian process bandits.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1025--1033, 2013.

\bibitem{NIPS2019_8788}
David Eriksson, Michael Pearce, Jacob Gardner, Ryan~D Turner, and Matthias
  Poloczek.
\newblock Scalable global optimization via local bayesian optimization.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  5496--5507. Curran Associates, Inc., 2019.

\bibitem{10.5555/3020751.3020776}
Roman Garnett, Michael~A. Osborne, and Philipp Hennig.
\newblock Active learning of linear embeddings for gaussian processes.
\newblock In {\em Proceedings of the Thirtieth Conference on Uncertainty in
  Artificial Intelligence}, UAI’14, page 230–239, Arlington, Virginia, USA,
  2014. AUAI Press.

\bibitem{GmezBombarelli2018AutomaticCD}
Rafael G{\'o}mez-Bombarelli, David Duvenaud, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, Jorge Aguilera-Iparraguirre, Timothy~D. Hirzel, Ryan~P.
  Adams, and Al{\'a}n Aspuru-Guzik.
\newblock Automatic chemical design using a data-driven continuous
  representation of molecules.
\newblock In {\em ACS central science}, 2018.

\bibitem{gan_paper}
Ian~J. {Goodfellow}, Jean {Pouget-Abadie}, Mehdi {Mirza}, Bing {Xu}, David
  {Warde-Farley}, Sherjil {Ozair}, Aaron {Courville}, and Yoshua {Bengio}.
\newblock {Generative Adversarial Networks}.
\newblock {\em arXiv e-prints}, page arXiv:1406.2661, Jun 2014.

\bibitem{gp_kernel_selection}
Nico~S. Gorbach, Andrew~An Bian, Benjamin Fischer, Stefan Bauer, and Joachim~M.
  Buhmann.
\newblock Model selection for gaussian process regression.
\newblock In Volker Roth and Thomas Vetter, editors, {\em Pattern Recognition},
  pages 306--318, Cham, 2017. Springer International Publishing.

\bibitem{ffjord_paper}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David
  Duvenaud.
\newblock {FFJORD:} free-form continuous dynamics for scalable reversible
  generative models.
\newblock {\em CoRR}, abs/1810.01367, 2018.

\bibitem{grathwohl2018backpropagation}
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Greenberg2019AutomaticPT}
David~S. Greenberg, Marcel Nonnenmacher, and Jakob~H. Macke.
\newblock Automatic posterior transformation for likelihood-free inference.
\newblock In {\em ICML}, 2019.

\bibitem{Gupta2019FeedbackGF}
Anvita Gupta and James Zou.
\newblock Feedback gan for dna optimizes protein functions.
\newblock {\em Nature Machine Intelligence}, 1:105--111, 2019.

\bibitem{NIPS2019_9350}
Huong Ha, Santu Rana, Sunil Gupta, Thanh Nguyen, Hung Tran-The, and Svetha
  Venkatesh.
\newblock Bayesian optimization with unknown search space.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~dAlch\'{e} Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 11795--11804. Curran Associates, Inc., 2019.

\bibitem{bostonD}
David Harrison and Daniel Rubinfeld.
\newblock Hedonic housing prices and the demand for clean air.
\newblock {\em Journal of Environmental Economics and Management}, 5:81--102,
  03 1978.

\bibitem{hoos2014efficient}
Holger Hoos and Kevin Leyton-Brown.
\newblock An efficient approach for assessing hyperparameter importance.
\newblock In {\em International conference on machine learning}, pages
  754--762, 2014.

\bibitem{ChainQueen}
Y.~{Hu}, J.~{Liu}, A.~{Spielberg}, J.~B. {Tenenbaum}, W.~T. {Freeman}, J.~{Wu},
  D.~{Rus}, and W.~{Matusik}.
\newblock Chainqueen: A real-time differentiable physical simulator for soft
  robotics.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 6265--6271, May 2019.

\bibitem{lhc_sampling}
Ronald Iman, James Davenport, and D.~Zeigler.
\newblock {\em Latin hypercube sampling (program user's guide). [LHC, in
  FORTRAN]}.
\newblock 01 1980.

\bibitem{Jamil2013ALS}
Momin Jamil and Xin-She Yang.
\newblock A literature survey of benchmark functions for global optimisation
  problems.
\newblock {\em IJMNO}, 4:150--194, 2013.

\bibitem{Keskar2017ImprovingGP}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock {\em ArXiv}, abs/1712.07628, 2017.

\bibitem{adam_opt}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem{vae_well}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2013.

\bibitem{design_opt_overview}
Gang Lei, Jianguo Zhu, Youguang Guo, Chengcheng Liu, and Bo~Ma.
\newblock A review of design optimization methods for electrical machines.
\newblock {\em Energies}, 10:1962, 11 2017.

\bibitem{Li2018MeasuringTI}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock {\em ArXiv}, abs/1804.08838, 2018.

\bibitem{Louppe2017AdversarialVO}
Gilles Louppe, Joeri Hermans, and Kyle Cranmer.
\newblock Adversarial variational optimization of non-differentiable
  simulators.
\newblock {\em ArXiv}, abs/1707.07113, 2017.

\bibitem{Lueckmann2018LikelihoodfreeIW}
Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob~H.
  Macke.
\newblock Likelihood-free inference with emulator networks.
\newblock In {\em AABI}, 2018.

\bibitem{Maheswaranathan2018GuidedES}
Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha
  Sohl-Dickstein.
\newblock Guided evolutionary strategies: augmenting random search with
  surrogate gradients.
\newblock In {\em ICML}, 2018.

\bibitem{mohamed2019monte}
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
\newblock Monte {Carlo} gradient estimation in machine learning.
\newblock {\em arXiv preprint arXiv:1906.10652}, 2019.

\bibitem{Nakada2019AdaptiveAA}
Ryumei Nakada and Masaaki Imaizumi.
\newblock Adaptive approximation and estimation of deep neural network to
  intrinsic dimensionality.
\newblock {\em ArXiv}, abs/1907.02177, 2019.

\bibitem{Oh2018BOCKB}
ChangYong Oh, Efstratios Gavves, and Max Welling.
\newblock Bock : Bayesian optimization with cylindrical kernels.
\newblock In {\em ICML}, 2018.

\bibitem{papalambros_wilde_2000}
Panos~Y. Papalambros and Douglass~J. Wilde.
\newblock {\em Principles of Optimal Design: Modeling and Computation}.
\newblock Cambridge University Press, 2000.

\bibitem{Papamakarios2018SequentialNL}
George Papamakarios, David~C. Sterratt, and Iain Murray.
\newblock Sequential neural likelihood: Fast likelihood-free inference with
  autoregressive flows.
\newblock {\em ArXiv}, abs/1805.07226, 2018.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~dAlch\'{e} Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 8024--8035. Curran Associates, Inc., 2019.

\bibitem{rezende_nf}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock {\em arXiv preprint arXiv:1505.05770}, 2015.

\bibitem{vae_rezende}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning - Volume 32}, ICML’14, page
  II–1278–II–1286. JMLR.org, 2014.

\bibitem{richter2016playing}
Stephan~R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
\newblock Playing for data: Ground truth from computer games.
\newblock In {\em European Conference on Computer Vision}, pages 102--118.
  Springer, 2016.

\bibitem{ros2016synthia}
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio~M
  Lopez.
\newblock The synthia dataset: A large collection of synthetic images for
  semantic segmentation of urban scenes.
\newblock In {\em Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, pages 3234--3243, 2016.

\bibitem{Rosenbrock}
H.~H. Rosenbrock.
\newblock {An Automatic Method for Finding the Greatest or Least Value of a
  Function}.
\newblock {\em The Computer Journal}, 3(3):175--184, 01 1960.

\bibitem{ruiz2018learning}
Nataniel Ruiz, Samuel Schulter, and Manmohan Chandraker.
\newblock Learning to simulate.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{SchmidtHieber2019DeepRN}
Johannes Schmidt-Hieber.
\newblock Deep relu network approximation of functions on a manifold.
\newblock {\em ArXiv}, abs/1908.00695, 2019.

\bibitem{pmlr-v51-shahriari16}
Bobak Shahriari, Alexandre Bouchard-Cote, and Nando Freitas.
\newblock Unbounded bayesian optimization via regularization.
\newblock In Arthur Gretton and Christian~C. Robert, editors, {\em Proceedings
  of the 19th International Conference on Artificial Intelligence and
  Statistics}, volume~51 of {\em Proceedings of Machine Learning Research},
  pages 1168--1176, Cadiz, Spain, 09--11 May 2016. PMLR.

\bibitem{shahriari2015taking}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P Adams, and Nando De~Freitas.
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock {\em Proceedings of the IEEE}, 104(1):148--175, 2015.

\bibitem{sjostrand2008brief}
Torbj{\"o}rn Sj{\"o}strand, Stephen Mrenna, and Peter Skands.
\newblock A brief introduction to pythia 8.1.
\newblock {\em Computer Physics Communications}, 178(11):852--867, 2008.

\bibitem{gan_batch_size}
Samuel~L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V. Le.
\newblock Don't decay the learning rate, increase the batch size, 2017.

\bibitem{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  2951--2959, 2012.

\bibitem{trustRegion}
D.~C. Sorensen.
\newblock Newton’s method with a model trust region modification.
\newblock {\em SIAM J. Numer. Anal.}, 19(2):409--426, 1982.

\bibitem{Stulp2013PolicyI}
Freek Stulp and Olivier Sigaud.
\newblock Policy improvement : Between black-box optimization and episodic
  reinforcement learning.
\newblock 2013.

\bibitem{Suzuki2019DeepLI}
Taiji Suzuki and Atsushi Nitanda.
\newblock Deep learning is adaptive to intrinsic dimensionality of model
  smoothness in anisotropic besov space.
\newblock {\em ArXiv}, abs/1910.12799, 2019.

\bibitem{moving_ass}
Krister Svanberg.
\newblock The method of moving asymptotes—a new method for structural
  optimization.
\newblock 1987.

\bibitem{Wang2016ParallelBG}
Jialei Wang, Scott~C. Clark, Eric Liu, and Peter~I. Frazier.
\newblock Parallel bayesian global optimization of expensive functions.
\newblock 2016.

\bibitem{wang2013bayesian}
Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando De~Freitas.
\newblock Bayesian optimization in high dimensions via random embeddings.
\newblock In {\em Twenty-Third International Joint Conference on Artificial
  Intelligence}, 2013.

\bibitem{Williams_reinforce}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Mach. Learn.}, 8(3-4):229--256, May 1992.

\bibitem{Zhang2019HighDB}
Miao Zhang, Huiqi Li, and Steven~W. Su.
\newblock High dimensional bayesian optimization via supervised dimension
  reduction.
\newblock In {\em IJCAI}, 2019.

\end{thebibliography}
