\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari et~al.(2000)Amari, Park, and Fukumizu]{amari2000adaptive}
S.-I. Amari, H.~Park, and K.~Fukumizu.
\newblock Adaptive method of realizing natural gradient learning for multilayer
  perceptrons.
\newblock \emph{Neural computation}, 12\penalty0 (6):\penalty0 1399--1409,
  2000.

\bibitem[Ba et~al.(2017)Ba, Grosse, and Martens]{Ba2017DistributedSO}
J.~Ba, R.~B. Grosse, and J.~Martens.
\newblock Distributed second-order optimization using kronecker-factored
  approximations.
\newblock In \emph{ICLR}, 2017.

\bibitem[Badreddine et~al.(2014)Badreddine, Vandewalle, and
  Meyers]{badreddine2014sequential}
H.~Badreddine, S.~Vandewalle, and J.~Meyers.
\newblock Sequential quadratic programming (sqp) for optimal control in direct
  numerical simulation of turbulent flow.
\newblock \emph{Journal of Computational Physics}, 256:\penalty0 1--16, 2014.

\bibitem[Bordes et~al.(2009)Bordes, Bottou, and Gallinari]{bordes2009sgd}
A.~Bordes, L.~Bottou, and P.~Gallinari.
\newblock Sgd-qn: Careful quasi-newton stochastic gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Jul):\penalty0 1737--1754, 2009.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
A.~Botev, H.~Ritter, and D.~Barber.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 557--565. JMLR. org, 2017.

\bibitem[Broyden(1970)]{broyden1970convergence}
C.~G. Broyden.
\newblock The convergence of a class of double-rank minimization algorithms 1.
  general considerations.
\newblock \emph{IMA Journal of Applied Mathematics}, 6\penalty0 (1):\penalty0
  76--90, 1970.

\bibitem[Byrd et~al.(1994)Byrd, Nocedal, and Schnabel]{byrd1994representations}
R.~H. Byrd, J.~Nocedal, and R.~B. Schnabel.
\newblock Representations of quasi-newton matrices and their use in limited
  memory methods.
\newblock \emph{Mathematical Programming}, 63\penalty0 (1-3):\penalty0
  129--156, 1994.

\bibitem[Byrd et~al.(2011)Byrd, Chin, Neveitt, and Nocedal]{byrd2011use}
R.~H. Byrd, G.~M. Chin, W.~Neveitt, and J.~Nocedal.
\newblock On the use of stochastic hessian information in optimization methods
  for machine learning.
\newblock \emph{SIAM Journal on Optimization}, 21\penalty0 (3):\penalty0
  977--995, 2011.

\bibitem[Byrd et~al.(2016)Byrd, Hansen, Nocedal, and
  Singer]{byrd2016stochastic}
R.~H. Byrd, S.~L. Hansen, J.~Nocedal, and Y.~Singer.
\newblock A stochastic quasi-newton method for large-scale optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1008--1031, 2016.

\bibitem[Dangel et~al.(2019)Dangel, Hennig, and Harmeling]{dangel2019modular}
F.~Dangel, P.~Hennig, and S.~Harmeling.
\newblock Modular block-diagonal curvature approximations for feedforward
  architectures.
\newblock \emph{arXiv preprint arXiv:1902.01813}, 2019.

\bibitem[Desjardins et~al.(2015)Desjardins, Simonyan, Pascanu,
  et~al.]{desjardins2015natural}
G.~Desjardins, K.~Simonyan, R.~Pascanu, et~al.
\newblock Natural neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2071--2079, 2015.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Fletcher(1970)]{fletcher1970new}
R.~Fletcher.
\newblock A new approach to variable metric algorithms.
\newblock \emph{The computer journal}, 13\penalty0 (3):\penalty0 317--322,
  1970.

\bibitem[Fujimoto and Ohira(2018)]{fujimoto2018neural}
Y.~Fujimoto and T.~Ohira.
\newblock A neural network model with bidirectional whitening.
\newblock In \emph{International Conference on Artificial Intelligence and Soft
  Computing}, pages 47--57. Springer, 2018.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and
  Vincent]{george2018fast}
T.~George, C.~Laurent, X.~Bouthillier, N.~Ballas, and P.~Vincent.
\newblock Fast approximate natural gradient descent in a kronecker factored
  eigenbasis.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9550--9560, 2018.

\bibitem[Goldfarb(1970)]{goldfarb1970family}
D.~Goldfarb.
\newblock A family of variable-metric methods derived by variational means.
\newblock \emph{Mathematics of computation}, 24\penalty0 (109):\penalty0
  23--26, 1970.

\bibitem[Gower et~al.(2016)Gower, Goldfarb, and
  Richt{\'a}rik]{gower2016stochastic}
R.~Gower, D.~Goldfarb, and P.~Richt{\'a}rik.
\newblock Stochastic block bfgs: Squeezing more curvature out of data.
\newblock In \emph{International Conference on Machine Learning}, pages
  1869--1878, 2016.

\bibitem[Gower and Richt{\'a}rik(2017)]{gower2017randomized}
R.~M. Gower and P.~Richt{\'a}rik.
\newblock Randomized quasi-newton updates are linearly convergent matrix
  inversion algorithms.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 38\penalty0
  (4):\penalty0 1380--1409, 2017.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{gupta2018shampoo}
V.~Gupta, T.~Koren, and Y.~Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 1842--1850. PMLR, 2018.

\bibitem[Heskes(2000)]{heskes2000}
T.~Heskes.
\newblock On "natural" learning and pruning in multilayered perceptrons.
\newblock \emph{Neural Computation}, 12, 01 2000.
\newblock \doi{10.1162/089976600300015637}.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
G.~Hinton, N.~Srivastava, and K.~Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8), 2012.

\bibitem[Hinton and Salakhutdinov(2006)]{hinton2006reducing}
G.~E. Hinton and R.~R. Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li and Mont{\'u}far(2018)]{li2018natural}
W.~Li and G.~Mont{\'u}far.
\newblock Natural gradient via optimal transport.
\newblock \emph{Information Geometry}, 1\penalty0 (2):\penalty0 181--214, 2018.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1-3):\penalty0
  503--528, 1989.

\bibitem[Lucchi et~al.(2015)Lucchi, McWilliams, and
  Hofmann]{lucchi2015variance}
A.~Lucchi, B.~McWilliams, and T.~Hofmann.
\newblock A variance reduced stochastic newton method.
\newblock \emph{arXiv preprint arXiv:1503.08316}, 2015.

\bibitem[Martens(2010)]{martens2010deep}
J.~Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{ICML}, volume~27, pages 735--742, 2010.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417, 2015.

\bibitem[Mokhtari and Ribeiro(2014)]{mokhtari2014res}
A.~Mokhtari and A.~Ribeiro.
\newblock Res: Regularized stochastic bfgs algorithm.
\newblock \emph{IEEE Transactions on Signal Processing}, 62\penalty0
  (23):\penalty0 6089--6104, 2014.

\bibitem[Mokhtari and Ribeiro(2015)]{mokhtari2015global}
A.~Mokhtari and A.~Ribeiro.
\newblock Global convergence of online limited memory bfgs.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3151--3181, 2015.

\bibitem[Moritz et~al.(2016)Moritz, Nishihara, and Jordan]{moritz2016linearly}
P.~Moritz, R.~Nishihara, and M.~Jordan.
\newblock A linearly-convergent stochastic l-bfgs algorithm.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 249--258,
  2016.

\bibitem[Nocedal and Wright(2006)]{nocedal2006numerical}
J.~Nocedal and S.~Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Powell(1978)]{powell1978algorithms}
M.~J. Powell.
\newblock Algorithms for nonlinear constraints that use lagrangian functions.
\newblock \emph{Mathematical programming}, 14\penalty0 (1):\penalty0 224--248,
  1978.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Roux and Fitzgibbon(2010)]{le2010fast}
N.~L. Roux and A.~W. Fitzgibbon.
\newblock A fast natural newton method.
\newblock In \emph{ICML}, 2010.

\bibitem[Schraudolph et~al.(2007)Schraudolph, Yu, and
  G{\"u}nter]{schraudolph2007stochastic}
N.~N. Schraudolph, J.~Yu, and S.~G{\"u}nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In \emph{Artificial intelligence and statistics}, pages 436--443,
  2007.

\bibitem[Shanno(1970)]{shanno1970conditioning}
D.~F. Shanno.
\newblock Conditioning of quasi-newton methods for function minimization.
\newblock \emph{Mathematics of computation}, 24\penalty0 (111):\penalty0
  647--656, 1970.

\bibitem[Vinyals and Povey(2012)]{vinyals2012krylov}
O.~Vinyals and D.~Povey.
\newblock Krylov subspace descent for deep learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1261--1268,
  2012.

\bibitem[Wang et~al.(2017)Wang, Ma, Goldfarb, and Liu]{wang2017stochastic}
X.~Wang, S.~Ma, D.~Goldfarb, and W.~Liu.
\newblock Stochastic quasi-newton methods for nonconvex stochastic
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (2):\penalty0
  927--956, 2017.

\bibitem[Wang and Li(2020)]{wang2020information}
Y.~Wang and W.~Li.
\newblock Information newton's flow: second-order optimization method in
  probability space.
\newblock \emph{arXiv preprint arXiv:2001.04341}, 2020.

\bibitem[Xu et~al.(2019)Xu, Roosta, and Mahoney]{xu2019newton}
P.~Xu, F.~Roosta, and M.~W. Mahoney.
\newblock Newton-type methods for non-convex optimization under inexact hessian
  information.
\newblock \emph{Mathematical Programming}, pages 1--36, 2019.

\end{thebibliography}
