@inproceedings{iyengar2010feasible,
  title={Feasible and accurate algorithms for covering semidefinite programs},
  author={Iyengar, Garud and Phillips, David J and Stein, Cliff},
  booktitle={Scandinavian Workshop on Algorithm Theory},
  pages={150--162},
  year={2010},
  organization={Springer}
}

@article{weinberger2009distance,
  title={Distance metric learning for large margin nearest neighbor classification.},
  author={Weinberger, Kilian Q and Saul, Lawrence K},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={2},
  year={2009}
}


@book{bauschke2011convex,
  title={Convex analysis and monotone operator theory in Hilbert spaces},
  author={Bauschke, Heinz H and Combettes, Patrick L and others},
  volume={408},
  publisher={Springer}
}


@inproceedings{dasgupta2016cost,
  title={A cost function for similarity-based hierarchical clustering},
  author={Dasgupta, Sanjoy},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={118--127},
  year={2016}
}


@article{chatziafratis2018hierarchical,
  title={Hierarchical clustering with structural constraints},
  author={Chatziafratis, Vaggos and Niazadeh, Rad and Charikar, Moses},
  journal={arXiv preprint arXiv:1805.09476},
  year={2018}
}


@article{bhatt1984framework,
  title={A framework for solving VLSI graph layout problems},
  author={Bhatt, Sandeep N and Leighton, Frank Thomson},
  journal={Journal of Computer and System Sciences},
  volume={28},
  number={2},
  pages={300--343},
  year={1984},
  publisher={Elsevier}
}



%%%%%%%%%%%%%%% INTRO%%%%%%%%%%%%%%%%%%
@article{peng2007approximating,
  title={Approximating k-means-type clustering via semidefinite programming},
  author={Peng, Jiming and Wei, Yu},
  journal={SIAM journal on optimization},
  volume={18},
  number={1},
  pages={186--205},
  year={2007},
  publisher={SIAM}
}

@article{yang2015sdpnal,
  title={SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints},
  author={Yang, Liuqin and Sun, Defeng and Toh, Kim-Chuan},
  journal={Mathematical Programming Computation},
  volume={7},
  number={3},
  pages={331--366},
  year={2015},
  publisher={Springer}
}



@book{ledoux2001concentration,
  title={The concentration of measure phenomenon},
  author={Ledoux, Michel},
  number={89},
  year={2001},
  publisher={American Mathematical Soc.}
}

@inproceedings{nr,
      title = {The Network Data Repository with Interactive Graph Analytics and Visualization},
      author={Ryan A. Rossi and Nesreen K. Ahmed},
      booktitle = {AAAI},
      url={http://networkrepository.com},
      year={2015}
  }

@article{shi2000normalized,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={Ieee}
}



@inproceedings{gerla1974cut,
  title={A cut saturation algorithm for topological design of packet switched communication networks},
  author={Gerla, Mario and Frank, H and Chou, Wushow and Eckl, John},
  booktitle={Proceedings of the National Telecommunication Conference},
  pages={1074--1085},
  year={1974}
}




@article{garber2016sublinear,
  title={Sublinear time algorithms for approximate semidefinite programming},
  author={Garber, Dan and Hazan, Elad},
  journal={Mathematical Programming},
  volume={158},
  number={1-2},
  pages={329--361},
  year={2016},
  publisher={Springer}
}



@article{lecun-mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{mixon2016clustering,
  title={Clustering subgaussian mixtures by semidefinite programming},
  author={Mixon, Dustin G and Villar, Soledad and Ward, Rachel},
  journal={arXiv preprint arXiv:1602.06612},
  year={2016}
}


@inproceedings{arora2005fast,
  title={Fast algorithms for approximate semidefinite programming using the multiplicative weights update method},
  author={Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
  booktitle={46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)},
  pages={339--348},
  year={2005},
  organization={IEEE}
}



@article{martinet1970breve,
  title={Br{\`e}ve communication. R{\'e}gularisation d'in{\'e}quations variationnelles par approximations successives},
  author={Martinet, Bernard},
  journal={Revue fran{\c{c}}aise d'informatique et de recherche op{\'e}rationnelle. S{\'e}rie rouge},
  volume={4},
  number={R3},
  pages={154--158},
  year={1970},
  publisher={EDP Sciences}
}

@book{bertsekas2014constrained,
  title={Constrained optimization and Lagrange multiplier methods},
  author={Bertsekas, Dimitri P},
  year={2014},
  publisher={Academic press}
}

@article{slavakis2014modeling,
  title={Modeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge},
  author={Slavakis, Konstantinos and Giannakis, Georgios B and Mateos, Gonzalo},
  journal={IEEE Signal Processing Magazine},
  volume={31},
  number={5},
  pages={18--31},
  year={2014},
  publisher={IEEE}
}

\cite{@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}}

@inproceedings{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={451--459},
  year={2011}
}

@article{cevher2014convex,
  title={Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics},
  author={Cevher, Volkan and Becker, Stephen and Schmidt, Mark},
  journal={IEEE Signal Processing Magazine},
  volume={31},
  number={5},
  pages={32--43},
  year={2014},
  publisher={IEEE}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@book{spall2005introduction,
  title={Introduction to stochastic search and optimization: estimation, simulation, and control},
  author={Spall, James C},
  volume={65},
  year={2005},
  publisher={John Wiley \& Sons}
}

@article{friedman2008sparse,
  title={Sparse inverse covariance estimation with the graphical lasso},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={Biostatistics},
  volume={9},
  number={3},
  pages={432--441},
  year={2008},
  publisher={Oxford University Press}
}

@article{chambolle2004algorithm,
  title={An algorithm for total variation minimization and applications},
  author={Chambolle, Antonin},
  journal={Journal of Mathematical imaging and vision},
  volume={20},
  number={1-2},
  pages={89--97},
  year={2004},
  publisher={Springer}
}

@book{campisi2016blind,
  title={Blind image deconvolution: theory and applications},
  author={Campisi, Patrizio and Egiazarian, Karen},
  year={2016},
  publisher={CRC press}
}

@article{bertsekas1997nonlinear,
  title={Nonlinear programming},
  author={Bertsekas, Dimitri P},
  journal={Journal of the Operational Research Society},
  volume={48},
  number={3},
  pages={334--334},
  year={1997},
  publisher={Taylor \& Francis}
}
%%%%%%%%%%%%%%% LAGR %%%%%%%%%%%%%%%%%%5

@article{hestenes1969multiplier,
  title={Multiplier and gradient methods},
  author={Hestenes, Magnus R},
  journal={Journal of optimization theory and applications},
  volume={4},
  number={5},
  pages={303--320},
  year={1969},
  publisher={Springer}
}

@article{powell1969method,
  title={A method for nonlinear constraints in minimization problems},
  author={Powell, Michael JD},
  journal={Optimization},
  pages={283--298},
  year={1969},
  publisher={Academic Press}
}

@incollection{SABACH2019,
title = "Lagrangian methods for composite optimization",
series = "Handbook of Numerical Analysis",
publisher = "Elsevier",
year = "2019",
issn = "1570-8659",
doi = "https://doi.org/10.1016/bs.hna.2019.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S1570865919300031",
author = "Shoham Sabach and Marc Teboulle",
keywords = "Lagrangian multiplier methods, Proximal multiplier algorithms, Convex and nonconvex composite minimization, Alternating minimization, Decomposition schemes, Global rate of convergence analysis, KurdykaâÅosiajewicz property, Semialgebraic optimization, Global pointwise convergence, 90C25, 65K05, 90C06, 90C26",
abstract = "Lagrangian-based methods have been on the market for over 50 years. These methods are robust and often can handle optimization problems with complex geometries through efficient computational steps. The last decade of research have generated a large volume of literature on various practical and theoretical aspects of many Lagrangian-based algorithms. This chapter reviews the basic elements of Lagrangian-based methods for composite minimization in the convex and nonconvex setting. In the convex case, the focus is on global rate of convergence results, which are derived here through a novel approach and very simple proof technique. In the much harder nonconvex case, we survey a very recent methodology which allows to establish global pointwise convergence results for a broad class of genuine nonlinear composite semialgebraic problems."
}


@article{rockafellar1976augmented,
  title={Augmented Lagrangians and applications of the proximal point algorithm in convex programming},
  author={Rockafellar, R Tyrrell},
  journal={Mathematics of operations research},
  volume={1},
  number={2},
  pages={97--116},
  year={1976},
  publisher={INFORMS}
}


@article{bolte2018nonconvex,
  title={Nonconvex Lagrangian-based optimization: monitoring schemes and global convergence},
  author={Bolte, J{\'e}r{\^o}me and Sabach, Shoham and Teboulle, Marc},
  journal={Mathematics of Operations Research},
  volume={43},
  number={4},
  pages={1210--1232},
  year={2018},
  publisher={INFORMS}
}


@article{chen1994proximal,
  title={A proximal-based decomposition method for convex minimization problems},
  author={Chen, Gong and Teboulle, Marc},
  journal={Mathematical Programming},
  volume={64},
  number={1-3},
  pages={81--101},
  year={1994},
  publisher={Springer}
}

@article{eckstein1994some,
  title={Some saddle-function splitting methods for convex programming},
  author={Eckstein, Jonathan},
  journal={Optimization Methods and Software},
  volume={4},
  number={1},
  pages={75--83},
  year={1994},
  publisher={Taylor \& Francis}
}

@article{tibshirani2005sparsity,
  title={Sparsity and smoothness via the fused lasso},
  author={Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={67},
  number={1},
  pages={91--108},
  year={2005},
  publisher={Wiley Online Library}
}

%%%%%%%%%%%%%%% ALMOST SURE %%%%%%%%%%%%%%%%%%5

@article{fercoq2019almost,
  title={Almost surely constrained convex optimization},
  author={Fercoq, Olivier and Alacaoglu, Ahmet and Necoara, Ion and Cevher, Volkan},
  journal={arXiv preprint arXiv:1902.00126},
  year={2019}
}



@article{nesterov2005smooth,
  title={Smooth minimization of non-smooth functions},
  author={Nesterov, Yu},
  journal={Mathematical programming},
  volume={103},
  number={1},
  pages={127--152},
  year={2005},
  publisher={Springer}
}

@article{tran2018smooth,
  title={A smooth primal-dual optimization framework for nonsmooth composite convex minimization},
  author={Tran-Dinh, Quoc and Fercoq, Olivier and Cevher, Volkan},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={1},
  pages={96--134},
  year={2018},
  publisher={SIAM}
}


@inproceedings{yu2017online,
  title={Online convex optimization with stochastic constraints},
  author={Yu, Hao and Neely, Michael and Wei, Xiaohan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1428--1438},
  year={2017}
}

@article{rosasco2014convergence,
  title={Convergence of stochastic proximal gradient algorithm},
  author={Rosasco, Lorenzo and Villa, Silvia and V{\~u}, Bang C{\^o}ng},
  journal={arXiv preprint arXiv:1403.5074},
  year={2014}
}
%%%%%%%%%%%%%%% ZEROTH ORDER %%%%%%%%%%%%%%%%%%5

@phdthesis{garber2016projection,
  title={Projection-free Algorithms for Convex Optimization and Online Learning},
  author={Garber, Dan},
  year={2016},
  school={Technion-Israel Institute of Technology, Faculty of Industrial and~…}
}




@inproceedings{balasubramanian2018zeroth,
  title={Zeroth-order (non)-convex stochastic optimization via conditional gradient and gradient updates},
  author={Balasubramanian, Krishnakumar and Ghadimi, Saeed},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3455--3464},
  year={2018}
}
@article{nesterov2006cubic,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}

@book{rubinstein2016simulation,
  title={Simulation and the Monte Carlo method},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  volume={10},
  year={2016},
  publisher={John Wiley \& Sons}
}

@inproceedings{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@InProceedings{revisitingfw,
  title = 	 {Revisiting {Frank-Wolfe}: Projection-Free Sparse Convex Optimization},
  author = 	 {Martin Jaggi},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {427--435},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/jaggi13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/jaggi13.html},
  abstract = 	 {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions.    On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices.    We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.}
}


@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017},
  publisher={Springer}
}

%%%%%%%%%%%%%%%PROPOSAL %%%%%%%%%%%%%%%%%%5
@InProceedings{spiderfw,
  title = 	 {Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator},
  author = 	 {Yurtsever, Alp and Sra, Suvrit and Cevher, Volkan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7282--7291},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/yurtsever19b/yurtsever19b.pdf},
  url = 	 {http://proceedings.mlr.press/v97/yurtsever19b.html},
  abstract = 	 {We propose a class of variance-reduced stochastic conditional gradient methods. By adopting the recent stochastic path-integrated differential estimator technique (SPIDER) of Fang et. al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for finite-sum minimization as well as the more general expectation minimization problems. SPIDER-FW enjoys superior complexity guarantees in the non-convex setting, while matching the best known FW variants in the convex case. We also extend our framework a la conditional gradient sliding (CGS) of Lan & Zhou. (2016), and propose SPIDER-CGS.}
}

@article{lan2016conditional,
  title={Conditional gradient sliding for convex optimization},
  author={Lan, Guanghui and Zhou, Yi},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={2},
  pages={1379--1409},
  year={2016},
  publisher={SIAM}
}

@article{liu2019nonergodic,
  title={On the nonergodic convergence rate of an inexact augmented lagrangian framework for composite convex programming},
  author={Liu, Ya-Feng and Liu, Xin and Ma, Shiqian},
  journal={Mathematics of Operations Research},
  volume={44},
  number={2},
  pages={632--650},
  year={2019},
  publisher={INFORMS}
}




@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}

@article{arv,
  title={Expander flows, geometric embeddings and graph partitioning},
  author={Arora, Sanjeev and Rao, Satish and Vazirani, Umesh},
  journal={Journal of the ACM (JACM)},
  volume={56},
  number={2},
  pages={5},
  year={2009},
  publisher={ACM}
}




@article{locatello2019stochastic,
  title={Stochastic Conditional Gradient Method for Composite Convex Minimization},
  author={Locatello, Francesco and Yurtsever, Alp and Fercoq, Olivier and Cevher, Volkan},
  journal={arXiv preprint arXiv:1901.10348},
  year={2019}
}
@article{cgs,
  title={Conditional gradient sliding for convex optimization},
  author={Lan, Guanghui and Zhou, Yi},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={2},
  pages={1379--1409},
  year={2016},
  publisher={SIAM}
}
@book{williamson2011design,
  title={The design of approximation algorithms},
  author={Williamson, David P and Shmoys, David B},
  year={2011},
  publisher={Cambridge university press}
}

@article{mokhtari,
  title={Stochastic conditional gradient methods: From convex minimization to submodular maximization},
  author={Mokhtari, Aryan and Hassani, Hamed and Karbasi, Amin},
  journal={arXiv preprint arXiv:1804.09554},
  year={2018}
}



@inproceedings{svrf,
  title={Variance-reduced and projection-free stochastic optimization},
  author={Hazan, Elad and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  pages={1263--1271},
  year={2016}
}

@article{originalfw,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An algorithm for quadratic programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number ={},
pages = {95-110},
doi = {10.1002/nav.3800030109},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800030109},
year = {1956}
}

@article{xiao2014proximal,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}


@inproceedings{allen2016improved,
  title={Improved SVRG for non-strongly-convex or sum-of-non-convex objectives},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  booktitle={International conference on machine learning},
  pages={1080--1089},
  year={2016}
}



@article{zhang2019one,
  title={One Sample Stochastic Frank-Wolfe},
  author={Zhang, Mingrui and Shen, Zebang and Mokhtari, Aryan and Hassani, Hamed and Karbasi, Amin},
  journal={arXiv preprint arXiv:1910.04322},
  year={2019}
}




@inproceedings{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Advances in neural information processing systems},
  pages={2663--2671},
  year={2012}
}





@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2613--2621},
  year={2017},
  organization={JMLR. org}
}


@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii},
  volume={137},
  publisher={Springer}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, YE},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={269},
  pages={543--547},
  year={1983}
}


@article{lee2017first,
  title={First-order methods almost always avoid saddle points},
  author={Lee, Jason D and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={arXiv preprint arXiv:1710.07406},
  year={2017}
}

@article{nemirovsky1983problem,
  title={Problem complexity and method efficiency in optimization.},
  author={Nemirovsky, Arkadii Semenovich and Yudin, David Borisovich},
  year={1983}
}

@article{agarwal2014lower,
  title={A lower bound for the optimization of finite sums},
  author={Agarwal, Alekh and Bottou, Leon},
  journal={arXiv preprint arXiv:1410.0723},
  year={2014}
}

@inproceedings{huang2014scalable,
  title={Scalable semidefinite relaxation for maximum a posterior estimation},
  author={Huang, Qixing and Chen, Yuxin and Guibas, Leonidas},
  booktitle={International Conference on Machine Learning},
  pages={64--72},
  year={2014}
}


@article{burer2005local,
  title={Local minima and convergence in low-rank semidefinite programming},
  author={Burer, Samuel and Monteiro, Renato DC},
  journal={Mathematical Programming},
  volume={103},
  number={3},
  pages={427--444},
  year={2005},
  publisher={Springer}
}

@article{patrascu2017nonasymptotic,
  title={Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization.},
  author={Patrascu, Andrei and Necoara, Ion},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={198--1},
  year={2017}
}

@article{xu2018primal,
  title={Primal-dual stochastic gradient method for convex programs with many functional constraints},
  author={Xu, Yangyang},
  journal={arXiv preprint arXiv:1802.02724},
  year={2018}
}


@inproceedings{yurtsever2018conditional,
  title={A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming},
  author={Yurtsever, A and Fercoq, O and Locatello, F and Cevher, V},
  booktitle={35th International Conference on Machine Learning (ICML)},
  pages={5727--5736},
  year={2018},
  organization={PMLR}
}

@article{lan2013complexity,
  title={The complexity of large-scale convex programming under a linear optimization oracle},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:1309.5550},
  year={2013}
}

@inproceedings{gidel2018frank,
  title={Frank-Wolfe Splitting via Augmented Lagrangian Method},
  author={Gidel, Gauthier and Pedregosa, Fabian and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1456--1465},
  year={2018}
}

@article{silveti2019generalized,
  title={Generalized Conditional Gradient with Augmented Lagrangian for Composite Minimization},
  author={Silveti-Falls, Antonio and Molinari, Cesare and Fadili, Jalal},
  journal={arXiv preprint arXiv:1901.01287},
  year={2019}
}


@inproceedings{yurtsever2015universal,
  title={A universal primal-dual convex optimization framework},
  author={Yurtsever, Alp and Dinh, Quoc Tran and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3150--3158},
  year={2015}
}



@inproceedings{yurtsever2019conditional,
  title={A Conditional-Gradient-Based Augmented Lagrangian Framework},
  author={Yurtsever, Alp and Fercoq, Olivier and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={7272--7281},
  year={2019}
}

@article{xu2017accelerated,
  title={Accelerated first-order primal-dual proximal methods for linearly constrained composite convex programming},
  author={Xu, Yangyang},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={3},
  pages={1459--1484},
  year={2017},
  publisher={SIAM}
}

@article{clarkson2010coresets,
  title={Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm},
  author={Clarkson, Kenneth L},
  journal={ACM Transactions on Algorithms (TALG)},
  volume={6},
  number={4},
  pages={63},
  year={2010},
  publisher={ACM}
}

@inproceedings{hazan2012projection,
  title={Projection-free online learning},
  author={Hazan, Elad E and Kale, Satyen},
  booktitle={29th International Conference on Machine Learning, ICML 2012},
  pages={521--528},
  year={2012}
}

@inproceedings{mahdavi2013mixed,
  title={Mixed optimization for smooth functions},
  author={Mahdavi, Mehrdad and Zhang, Lijun and Jin, Rong},
  booktitle={Advances in neural information processing systems},
  pages={674--682},
  year={2013}
}



@inproceedings{hazan2008sparse,
  title={Sparse approximate solutions to semidefinite programs},
  author={Hazan, Elad},
  booktitle={Latin American symposium on theoretical informatics},
  pages={306--316},
  year={2008},
  organization={Springer}
}














