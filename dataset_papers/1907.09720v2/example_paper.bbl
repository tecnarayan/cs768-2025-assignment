\begin{thebibliography}{10}

\bibitem{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3981--3989, 2016.

\bibitem{ba2016using}
Jimmy Ba, Geoffrey~E Hinton, Volodymyr Mnih, Joel~Z Leibo, and Catalin Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4331--4339, 2016.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{duan2016rl}
Yan Duan, John Schulman, Xi~Chen, Peter~L Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Rl$^2$: Fast reinforcement learning via slow reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.02779}, 2016.

\bibitem{dumoulin2016learned}
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
\newblock A learned representation for artistic style.
\newblock {\em CoRR, abs/1610.07629}, 2(4):5, 2016.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 1126--1135, International
  Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem{finn2017one}
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine.
\newblock One-shot visual imitation learning via meta-learning.
\newblock {\em arXiv preprint arXiv:1709.04905}, 2017.

\bibitem{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock {\em arXiv preprint arXiv:1410.5401}, 2014.

\bibitem{graves2016hybrid}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
  Grabska-Barwi{\'n}ska, Sergio~G{\'o}mez Colmenarejo, Edward Grefenstette,
  Tiago Ramalho, John Agapiou, et~al.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock {\em Nature}, 538(7626):471, 2016.

\bibitem{ha2017hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock In {\em ICLR 2017}, 2017.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In {\em Advances in neural information processing systems}, pages
  164--171, 1993.

\bibitem{hebb2005organization}
Donald~Olding Hebb.
\newblock {\em The organization of behavior: A neuropsychological theory}.
\newblock Psychology Press, 1949.

\bibitem{hinton1987using}
Geoffrey~E Hinton and David~C Plaut.
\newblock Using fast weights to deblur old memories.
\newblock In {\em Proceedings of the ninth annual conference of the Cognitive
  Science Society}, pages 177--186, 1987.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{hochreiter2001learning}
Sepp Hochreiter, A~Steven Younger, and Peter~R Conwell.
\newblock Learning to learn using gradient descent.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 87--94. Springer, 2001.

\bibitem{jaderberg2017decoupled}
Max Jaderberg, Wojciech~Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
  Graves, David Silver, and Koray Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1627--1635. JMLR. org, 2017.

\bibitem{kanerva1988sparse}
Pentti Kanerva.
\newblock {\em Sparse distributed memory}.
\newblock MIT press, 1988.

\bibitem{khan2017memory}
Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar,
  and Daniel~D Lee.
\newblock Memory augmented control networks.
\newblock {\em arXiv preprint arXiv:1709.05706}, 2017.

\bibitem{kumar2016ask}
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan
  Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher.
\newblock Ask me anything: Dynamic memory networks for natural language
  processing.
\newblock In {\em International Conference on Machine Learning}, pages
  1378--1387, 2016.

\bibitem{lei2015predicting}
Jimmy Lei~Ba, Kevin Swersky, Sanja Fidler, et~al.
\newblock Predicting deep zero-shot convolutional neural networks using textual
  descriptions.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 4247--4255, 2015.

\bibitem{lillicrap2016random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7, 2016.

\bibitem{miconi2018differentiable}
Thomas Miconi, Jeff Clune, and Kenneth~O Stanley.
\newblock Differentiable plasticity: training plastic neural networks with
  backpropagation.
\newblock {\em arXiv preprint arXiv:1804.02464}, 2018.

\bibitem{mishra2018simple}
Nikhil Mishra, Mostafa Rohaninejad, Xi~Chen, and Pieter Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock 2018.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem{munkhdalai2018metalearning}
Tsendsuren Munkhdalai and Adam Trischler.
\newblock Metalearning with hebbian fast weights.
\newblock {\em arXiv preprint arXiv:1807.05076}, 2018.

\bibitem{pmlr-v70-munkhdalai17a}
Tsendsuren Munkhdalai and Hong Yu.
\newblock Meta networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 2554--2563, International
  Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem{E17-1038}
Tsendsuren Munkhdalai and Hong Yu.
\newblock Neural semantic encoders.
\newblock In {\em Proceedings of the 15th Conference of the European Chapter of
  the Association for Computational Linguistics: Volume 1, Long Papers}, pages
  397--407. Association for Computational Linguistics, 2017.

\bibitem{munkhdalai2018rapid}
Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler.
\newblock Rapid adaptation with conditionally shifted neurons.
\newblock In {\em International Conference on Machine Learning}, pages
  3661--3670, 2018.

\bibitem{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1037--1045, 2016.

\bibitem{perez2017film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock {\em arXiv preprint arXiv:1709.07871}, 2017.

\bibitem{rae2016scaling}
Jack Rae, Jonathan~J Hunt, Ivo Danihelka, Timothy Harley, Andrew~W Senior,
  Gregory Wayne, Alex Graves, and Timothy Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3621--3629, 2016.

\bibitem{Sachin2017}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In {\em ICLR 2017}, 2017.

\bibitem{rosenblatt1958perceptron}
Frank Rosenblatt.
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological review}, 65(6):386, 1958.

\bibitem{santoro2016meta}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy
  Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In {\em International conference on machine learning}, pages
  1842--1850, 2016.

\bibitem{schlag2018learning}
Imanol Schlag and J{\"u}rgen Schmidhuber.
\newblock Learning to reason with third order tensor products.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10003--10014, 2018.

\bibitem{schmidhuber1993reducing}
J~Schmidhuber.
\newblock Reducing the ratio between learning complexity and number of time
  varying variables in fully recurrent nets.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 460--463. Springer, 1993.

\bibitem{schmidhuber1992learning}
J{\"u}rgen Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock {\em Neural Computation}, 4(1):131--139, 1992.

\bibitem{smolensky1990tensor}
Paul Smolensky.
\newblock Tensor product variable binding and the representation of symbolic
  structures in connectionist systems.
\newblock {\em Artificial intelligence}, 46(1-2):159--216, 1990.

\bibitem{sukhbaatar2015end}
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et~al.
\newblock End-to-end memory networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2440--2448, 2015.

\bibitem{sun2018contextual}
Wen Sun, Alina Beygelzimer, Hal Daum{\'e}~III, John Langford, and Paul Mineiro.
\newblock Contextual memory trees.
\newblock {\em arXiv preprint arXiv:1807.06473}, 2018.

\bibitem{trischler2016computational}
Adam Trischler.
\newblock {\em A Computational Model for Episodic Memory Inspired by the
  Brain}.
\newblock PhD thesis, 2016.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3630--3638, 2016.

\bibitem{wang2016learning}
Jane~X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel~Z Leibo,
  Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
\newblock Learning to reinforcement learn.
\newblock {\em arXiv preprint arXiv:1611.05763}, 2016.

\bibitem{weston2015towards}
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart van
  Merri{\"e}nboer, Armand Joulin, and Tomas Mikolov.
\newblock Towards ai-complete question answering: A set of prerequisite toy
  tasks.
\newblock {\em arXiv preprint arXiv:1502.05698}, 2015.

\bibitem{Weston2014MemoryN}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock {\em CoRR}, abs/1410.3916, 2014.

\bibitem{wu2018kanerva}
Yan Wu, Greg Wayne, Alex Graves, and Timothy Lillicrap.
\newblock The kanerva machine: A generative distributed memory.
\newblock {\em ICLR 2018}, 2018.

\bibitem{wu2018learning}
Yan Wu, Gregory Wayne, Karol Gregor, and Timothy Lillicrap.
\newblock Learning attractor dynamics for generative memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9401--9410, 2018.

\bibitem{zemel2000generative}
Richard~S Zemel and Michael~C Mozer.
\newblock A generative model for attractor dynamics.
\newblock In {\em Advances in neural information processing systems}, pages
  80--88, 2000.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
