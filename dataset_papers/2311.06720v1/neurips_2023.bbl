\begin{thebibliography}{10}

\bibitem{Bach2022PromptSourceAI}
S.~H. Bach, V.~Sanh, Z.~X. Yong, A.~Webson, C.~Raffel, N.~V. Nayak, A.~Sharma, T.~Kim, M.~S. Bari, T.~F{\'e}vry, Z.~Alyafeai, M.~Dey, A.~Santilli, Z.~Sun, S.~Ben-David, C.~Xu, G.~Chhablani, H.~Wang, J.~A. Fries, M.~S. Al-shaibani, S.~Sharma, U.~Thakker, K.~Almubarak, X.~Tang, M.~T.-J. Jiang, and A.~M. Rush.
\newblock Promptsource: An integrated development environment and repository for natural language prompts.
\newblock {\em ArXiv}, abs/2202.01279, 2022.

\bibitem{bakker2022fine}
M.~Bakker, M.~Chadwick, H.~Sheahan, M.~Tessler, L.~Campbell-Gillingham, J.~Balaguer, N.~McAleese, A.~Glaese, J.~Aslanides, M.~Botvinick, et~al.
\newblock Fine-tuning language models to find agreement among humans with diverse preferences.
\newblock {\em Advances in Neural Information Processing Systems}, 35:38176--38189, 2022.

\bibitem{vicuna2023}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang, Y.~Zhuang, J.~E. Gonzalez, I.~Stoica, and E.~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{Chung2022ScalingIL}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang, M.~Dehghani, S.~Brahma, A.~Webson, S.~S. Gu, Z.~Dai, M.~Suzgun, X.~Chen, A.~Chowdhery, D.~Valter, S.~Narang, G.~Mishra, A.~W. Yu, V.~Zhao, Y.~Huang, A.~M. Dai, H.~Yu, S.~Petrov, E.~H. hsin Chi, J.~Dean, J.~Devlin, A.~Roberts, D.~Zhou, Q.~V. Le, and J.~Wei.
\newblock Scaling instruction-finetuned language models.
\newblock {\em ArXiv}, abs/2210.11416, 2022.

\bibitem{Dong2022ASF}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui.
\newblock A survey for in-context learning.
\newblock {\em ArXiv}, abs/2301.00234, 2022.

\bibitem{glaese2022improving}
A.~Glaese, N.~McAleese, M.~Tr{\k{e}}bacz, J.~Aslanides, V.~Firoiu, T.~Ewalds, M.~Rauh, L.~Weidinger, M.~Chadwick, P.~Thacker, et~al.
\newblock Improving alignment of dialogue agents via targeted human judgements.
\newblock {\em arXiv preprint arXiv:2209.14375}, 2022.

\bibitem{Hu2021LoRALA}
J.~E. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, and W.~Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em ArXiv}, abs/2106.09685, 2021.

\bibitem{Hu2023LLMAdaptersAA}
Z.~Hu, Y.~Lan, L.~Wang, W.~Xu, E.-P. Lim, R.~K.-W. Lee, L.~Bing, and S.~Poria.
\newblock Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.
\newblock {\em ArXiv}, abs/2304.01933, 2023.

\bibitem{Huang2022TowardsRI}
J.~Huang and K.~C.-C. Chang.
\newblock Towards reasoning in large language models: A survey.
\newblock {\em ArXiv}, abs/2212.10403, 2022.

\bibitem{Iyer2022OPTIMLSL}
S.~Iyer, X.~Lin, R.~Pasunuru, T.~Mihaylov, D.~Simig, P.~Yu, K.~Shuster, T.~Wang, Q.~Liu, P.~S. Koura, X.~Li, B.~O'Horo, G.~Pereyra, J.~Wang, C.~Dewan, A.~Celikyilmaz, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Opt-iml: Scaling language model instruction meta learning through the lens of generalization.
\newblock {\em ArXiv}, abs/2212.12017, 2022.

\bibitem{jouppi2023tpu}
N.~Jouppi, G.~Kurian, S.~Li, P.~Ma, R.~Nagarajan, L.~Nai, N.~Patil, S.~Subramanian, A.~Swing, B.~Towles, et~al.
\newblock Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings.
\newblock In {\em Proceedings of the 50th Annual International Symposium on Computer Architecture}, pages 1--14, 2023.

\bibitem{Kratzwald2019RankQANQ}
B.~Kratzwald, A.~Eigenmann, and S.~Feuerriegel.
\newblock Rankqa: Neural question answering with answer re-ranking.
\newblock {\em ArXiv}, abs/1906.03008, 2019.

\bibitem{lambert2022illustrating}
N.~Lambert, L.~Castricato, L.~von Werra, and A.~Havrilla.
\newblock Illustrating reinforcement learning from human feedback (rlhf).
\newblock {\em Hugging Face Blog}, 2022.
\newblock https://huggingface.co/blog/rlhf.

\bibitem{Lee2018RankingPF}
J.~Lee, S.~Yun, H.~Kim, M.~Ko, and J.~Kang.
\newblock Ranking paragraphs for improving answer recall in open-domain question answering.
\newblock {\em ArXiv}, abs/1810.00494, 2018.

\bibitem{Li2021PrefixTuningOC}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, abs/2101.00190, 2021.

\bibitem{lin2022unsupervised}
B.~Y. Lin, K.~Tan, C.~Miller, B.~Tian, and X.~Ren.
\newblock Unsupervised cross-task generalization via retrieval augmentation.
\newblock {\em NeurIPS}, 2022.

\bibitem{lin-2004-rouge}
C.-Y. Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In {\em Text Summarization Branches Out}, pages 74--81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

\bibitem{Liu2021PretrainPA}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55:1 -- 35, 2021.

\bibitem{liu2009learning}
T.-Y. Liu et~al.
\newblock Learning to rank for information retrieval.
\newblock {\em Foundations and Trends{\textregistered} in Information Retrieval}, 3(3):225--331, 2009.

\bibitem{Liu2019RoBERTaAR}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em ArXiv}, abs/1907.11692, 2019.

\bibitem{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{Ravaut2022SummaRerankerAM}
M.~Ravaut, S.~R. Joty, and N.~F. Chen.
\newblock Summareranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization.
\newblock In {\em Annual Meeting of the Association for Computational Linguistics}, 2022.

\bibitem{Sanh2022MultitaskPT}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai, A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja, M.~Dey, M.~S. Bari, C.~Xu, U.~Thakker, S.~Sharma, E.~Szczechla, T.~Kim, G.~Chhablani, N.~V. Nayak, D.~Datta, J.~Chang, M.~T.-J. Jiang, H.~Wang, M.~Manica, S.~Shen, Z.~X. Yong, H.~Pandey, R.~Bawden, T.~Wang, T.~Neeraj, J.~Rozen, A.~Sharma, A.~Santilli, T.~F{\'e}vry, J.~A. Fries, R.~Teehan, S.~R. Biderman, L.~Gao, T.~Bers, T.~Wolf, and A.~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock {\em ICLR}, 2022.

\bibitem{sileo2023tasksource}
D.~Sileo.
\newblock tasksource: Structured dataset preprocessing annotations for frictionless extreme multi-task learning and evaluation.
\newblock {\em arXiv preprint arXiv:2301.05948}, 2023.

\bibitem{Srivastava2022BeyondTI}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R. Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso, A.~Kluska, A.~Lewkowycz, A.~Agarwal, A.~Power, A.~Ray, A.~Warstadt, A.~W. Kocurek, A.~Safaya, A.~Tazarv, A.~Xiang, A.~Parrish, A.~Nie, A.~Hussain, A.~Askell, A.~Dsouza, A.~A. Rahane, A.~S. Iyer, A.~Andreassen, A.~Santilli, A.~Stuhlmuller, A.~M. Dai, A.~D. La, A.~K. Lampinen, A.~Zou, A.~Jiang, A.~Chen, A.~Vuong, A.~Gupta, A.~Gottardi, A.~Norelli, A.~Venkatesh, A.~Gholamidavoodi, A.~Tabassum, A.~Menezes, A.~Kirubarajan, A.~Mullokandov, A.~Sabharwal, A.~Herrick, A.~Efrat, A.~Erdem, A.~Karakacs, B.~R. Roberts, B.~S. Loe, B.~Zoph, B.~Bojanowski, B.~Ozyurt, B.~Hedayatnia, B.~Neyshabur, B.~Inden, B.~Stein, B.~Ekmekci, B.~Y. Lin, B.~S. Howald, C.~Diao, C.~Dour, C.~Stinson, C.~Argueta, C.~F. Ram'irez, C.~Singh, C.~Rathkopf, C.~Meng, C.~Baral, C.~Wu, C.~Callison-Burch, C.~Waites, C.~Voigt, C.~D. Manning, C.~Potts, C.~T. Ramirez, C.~Rivera, C.~Siro, C.~Raffel, C.~Ashcraft, C.~Garbacea, D.~Sileo,
  D.~H. Garrette, D.~Hendrycks, D.~Kilman, D.~Roth, D.~Freeman, D.~Khashabi, D.~Levy, D.~Gonz'alez, D.~Hernandez, D.~Chen, D.~Ippolito, D.~Gilboa, D.~Dohan, D.~Drakard, D.~Jurgens, D.~Datta, D.~Ganguli, D.~Emelin, D.~Kleyko, D.~Yuret, D.~Chen, D.~Tam, D.~Hupkes, D.~Misra, D.~Buzan, D.~C. Mollo, D.~Yang, D.-H. Lee, E.~Shutova, E.~D. Cubuk, E.~Segal, E.~Hagerman, E.~Barnes, E.~P. Donoway, E.~Pavlick, E.~Rodol{\`a}, E.~F. Lam, E.~Chu, E.~Tang, E.~Erdem, E.~Chang, E.~A. Chi, E.~Dyer, E.~J. Jerzak, E.~Kim, E.~E. Manyasi, E.~Zheltonozhskii, F.~Xia, F.~Siar, F.~Mart'inez-Plumed, F.~Happ'e, F.~Chollet, F.~Rong, G.~Mishra, G.~I. Winata, G.~de~Melo, G.~Kruszewski, G.~Parascandolo, G.~Mariani, G.~Wang, G.~Jaimovitch-L'opez, G.~Betz, G.~Gur-Ari, H.~Galijasevic, H.~S. Kim, H.~Rashkin, H.~Hajishirzi, H.~Mehta, H.~Bogar, H.~Shevlin, H.~Sch{\"u}tze, H.~Yakura, H.~Zhang, H.~Wong, I.~A.-S. Ng, I.~Noble, J.~Jumelet, J.~Geissinger, J.~Kernion, J.~Hilton, J.~Lee, J.~F. Fisac, J.~B. Simon, J.~Koppel, J.~Zheng, J.~Zou, J.~Koco'n,
  J.~Thompson, J.~Kaplan, J.~Radom, J.~N. Sohl-Dickstein, J.~Phang, J.~Wei, J.~Yosinski, J.~Novikova, J.~Bosscher, J.~Marsh, J.~Kim, J.~Taal, J.~Engel, J.~O. Alabi, J.~Xu, J.~Song, J.~Tang, J.~W. Waweru, J.~Burden, J.~Miller, J.~U. Balis, J.~Berant, J.~Frohberg, J.~Rozen, J.~Hern{\'a}ndez-Orallo, J.~Boudeman, J.~Jones, J.~B. Tenenbaum, J.~S. Rule, J.~Chua, K.~Kanclerz, K.~Livescu, K.~Krauth, K.~Gopalakrishnan, K.~Ignatyeva, K.~Markert, K.~D. Dhole, K.~Gimpel, K.~O. Omondi, K.~W. Mathewson, K.~Chiafullo, K.~Shkaruta, K.~Shridhar, K.~McDonell, K.~Richardson, L.~Reynolds, L.~Gao, L.~Zhang, L.~Dugan, L.~Qin, L.~Contreras-Ochando, L.-P. Morency, L.~Moschella, L.~Lam, L.~Noble, L.~Schmidt, L.~He, L.~O. Col'on, L.~Metz, L.~K. cSenel, M.~Bosma, M.~Sap, M.~ter Hoeve, M.~Andrea, M.~S. Farooqi, M.~Faruqui, M.~Mazeika, M.~Baturan, M.~Marelli, M.~Maru, M.~Quintana, M.~Tolkiehn, M.~Giulianelli, M.~Lewis, M.~Potthast, M.~Leavitt, M.~Hagen, M.~Schubert, M.~Baitemirova, M.~Arnaud, M.~A. McElrath, M.~A. Yee, M.~Cohen, M.~Gu,
  M.~I. Ivanitskiy, M.~Starritt, M.~Strube, M.~Swkedrowski, M.~Bevilacqua, M.~Yasunaga, M.~Kale, M.~Cain, M.~Xu, M.~Suzgun, M.~Tiwari, M.~Bansal, M.~Aminnaseri, M.~Geva, M.~Gheini, T.~MukundVarma, N.~Peng, N.~Chi, N.~Lee, N.~G.-A. Krakover, N.~Cameron, N.~S. Roberts, N.~Doiron, N.~Nangia, N.~Deckers, N.~Muennighoff, N.~S. Keskar, N.~Iyer, N.~Constant, N.~Fiedel, N.~Wen, O.~Zhang, O.~Agha, O.~Elbaghdadi, O.~Levy, O.~Evans, P.~A.~M. Casares, P.~Doshi, P.~Fung, P.~P. Liang, P.~Vicol, P.~Alipoormolabashi, P.~Liao, P.~Liang, P.~W. Chang, P.~Eckersley, P.~M. Htut, P.-B. Hwang, P.~Milkowski, P.~S. Patil, P.~Pezeshkpour, P.~Oli, Q.~Mei, Q.~LYU, Q.~Chen, R.~Banjade, R.~E. Rudolph, R.~Gabriel, R.~Habacker, R.~R. Delgado, R.~Milli{\`e}re, R.~Garg, R.~Barnes, R.~A. Saurous, R.~Arakawa, R.~Raymaekers, R.~Frank, R.~Sikand, R.~Novak, R.~Sitelew, R.~L. Bras, R.~Liu, R.~Jacobs, R.~Zhang, R.~Salakhutdinov, R.~Chi, R.~Lee, R.~Stovall, R.~Teehan, R.~Yang, S.~J. Singh, S.~M. Mohammad, S.~Anand, S.~Dillavou, S.~Shleifer,
  S.~Wiseman, S.~Gruetter, S.~Bowman, S.~S. Schoenholz, S.~Han, S.~Kwatra, S.~A. Rous, S.~Ghazarian, S.~Ghosh, S.~Casey, S.~Bischoff, S.~Gehrmann, S.~Schuster, S.~Sadeghi, S.~S. Hamdan, S.~Zhou, S.~Srivastava, S.~Shi, S.~Singh, S.~Asaadi, S.~S. Gu, S.~Pachchigar, S.~Toshniwal, S.~Upadhyay, S.~Debnath, S.~Shakeri, S.~Thormeyer, S.~Melzi, S.~Reddy, S.~P. Makini, S.~hwan Lee, S.~B. Torene, S.~Hatwar, S.~Dehaene, S.~Divic, S.~Ermon, S.~R. Biderman, S.~C. Lin, S.~Prasad, S.~T. Piantadosi, S.~M. Shieber, S.~Misherghi, S.~Kiritchenko, S.~Mishra, T.~Linzen, T.~Schuster, T.~Li, T.~Yu, T.~A. Ali, T.~Hashimoto, T.-L. Wu, T.~Desbordes, T.~Rothschild, T.~Phan, T.~Wang, T.~Nkinyili, T.~Schick, T.~N. Kornev, T.~Telleen-Lawton, T.~Tunduny, T.~Gerstenberg, T.~Chang, T.~Neeraj, T.~Khot, T.~O. Shultz, U.~Shaham, V.~Misra, V.~Demberg, V.~Nyamai, V.~Raunak, V.~V. Ramasesh, V.~U. Prabhu, V.~Padmakumar, V.~Srikumar, W.~Fedus, W.~Saunders, W.~Zhang, W.~Vossen, X.~Ren, X.~Tong, X.~Wu, X.~Shen, Y.~Yaghoobzadeh, Y.~Lakretz, Y.~Song,
  Y.~Bahri, Y.~J. Choi, Y.~Yang, Y.~Hao, Y.~Chen, Y.~Belinkov, Y.~Hou, Y.~Hou, Y.~Bai, Z.~Seid, Z.~Xinran, Z.~Zhao, Z.~F. Wang, Z.~J. Wang, Z.~Wang, Z.~Wu, S.~Singh, and U.~Shaham.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em ArXiv}, abs/2206.04615, 2022.

\bibitem{stiennon2020learning}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~Ziegler, R.~Lowe, C.~Voss, A.~Radford, D.~Amodei, and P.~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 33:3008--3021, 2020.

\bibitem{tan2023redco}
B.~Tan, Y.~Zhu, L.~Liu, H.~Wang, Y.~Zhuang, J.~Chen, E.~Xing, and Z.~Hu.
\newblock Redco: A lightweight tool to automate distributed training of llms on any gpu/tpus.
\newblock {\em arXiv preprint arXiv:2310.16355}, 2023.

\bibitem{wang2022self}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~Le, E.~Chi, S.~Narang, A.~Chowdhery, and D.~Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock {\em arXiv preprint arXiv:2203.11171}, 2022.

\bibitem{wang2022benchmarking}
Y.~Wang, S.~Mishra, P.~Alipoormolabashi, Y.~Kordi, A.~Mirzaei, A.~Arunkumar, A.~Ashok, A.~S. Dhanasekaran, A.~Naik, D.~Stap, et~al.
\newblock Benchmarking generalization via in-context instructions on 1,600+ language tasks.
\newblock {\em EMNLP}, 2022.

\bibitem{Wei2021FinetunedLM}
J.~Wei, M.~Bosma, V.~Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai, and Q.~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em ICLR}, 2022.

\bibitem{Yuan2023HowWD}
Z.~Yuan, H.~Yuan, C.~Tan, W.~Wang, and S.~Huang.
\newblock How well do large language models perform in arithmetic tasks?
\newblock {\em ArXiv}, abs/2304.02015, 2023.

\bibitem{zha2023text}
Y.~Zha, Y.~Yang, R.~Li, and Z.~Hu.
\newblock Text alignment is an efficient unified model for massive nlp tasks.
\newblock {\em arXiv preprint arXiv:2307.02729}, 2023.

\bibitem{Zheng2023Chatbot}
L.~Zheng, Y.~Sheng, W.-L. Chiang, H.~Zhang, J.~E. Gonzalez, and I.~Stoica.
\newblock Chatbot arena: Benchmarking llms in the wild with elo ratings, 2023.

\end{thebibliography}
