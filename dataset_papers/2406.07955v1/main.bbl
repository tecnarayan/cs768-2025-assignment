\begin{thebibliography}{127}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alvarez{-}Melis \& Jaakkola(2018)Alvarez{-}Melis and
  Jaakkola]{robust_xnn}
Alvarez{-}Melis, D. and Jaakkola, T.~S.
\newblock Towards robust interpretability with self-explaining neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7786--7795, 2018.

\bibitem[Amara et~al.(2022)Amara, Ying, Zhang, Han, Shan, Brandes, Schemm, and
  Zhang]{GraphFramEx}
Amara, K., Ying, R., Zhang, Z., Han, Z., Shan, Y., Brandes, U., Schemm, S., and
  Zhang, C.
\newblock Graphframex: Towards systematic evaluation of explainability methods
  for graph neural networks.
\newblock In \emph{Learning on Graphs Conference}, 2022.

\bibitem[Bajaj et~al.(2021)Bajaj, Chu, Xue, Pei, Wang, Lam, and
  Zhang]{RCExplainer}
Bajaj, M., Chu, L., Xue, Z.~Y., Pei, J., Wang, L., Lam, P. C.-H., and Zhang, Y.
\newblock Robust counterfactual explanations on graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Bapst et~al.(2020)Bapst, Keck, Grabska-Barwinska, Donner, Cubuk,
  Schoenholz, Obika, Nelson, Back, Hassabis, and Kohli]{ai4sci_phy}
Bapst, V., Keck, T., Grabska-Barwinska, A., Donner, C., Cubuk, E.~D.,
  Schoenholz, S.~S., Obika, A., Nelson, A. W.~R., Back, T., Hassabis, D., and
  Kohli, P.
\newblock Unveiling the predictive power of static structure in glassy systems.
\newblock \emph{Nature Physics}, 16:\penalty0 448--454, 2020.

\bibitem[Berman et~al.(2000)Berman, Westbrook, Feng, Gilliland, Bhat, Weissig,
  Shindyalov, and Bourne]{PDB}
Berman, H.~M., Westbrook, J.~D., Feng, Z., Gilliland, G.~L., Bhat, T.~N.,
  Weissig, H., Shindyalov, I.~N., and Bourne, P.~E.
\newblock The protein data bank.
\newblock \emph{Nucleic acids research}, 28 1:\penalty0 235--42, 2000.

\bibitem[Bevilacqua et~al.(2021)Bevilacqua, Zhou, and Ribeiro]{size_gen2}
Bevilacqua, B., Zhou, Y., and Ribeiro, B.
\newblock Size-invariant graph representations for graph classification
  extrapolations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  837--851, 2021.

\bibitem[Bian et~al.(2019)Bian, Buhmann, and Krause]{optimal_drsub}
Bian, Y., Buhmann, J., and Krause, A.
\newblock Optimal continuous {DR}-submodular maximization and applications to
  provable mean field inference.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  644--653, 2019.

\bibitem[Bian et~al.(2020)Bian, Buhmann, and Krause]{2020csfm}
Bian, Y., Buhmann, J.~M., and Krause, A.
\newblock Continuous submodular function maximization.
\newblock \emph{arXiv preprint arXiv:2006.13474}, 2020.

\bibitem[Bian et~al.(2022)Bian, Rong, Xu, Wu, Krause, and
  Huang]{bian2022energybased}
Bian, Y., Rong, Y., Xu, T., Wu, J., Krause, A., and Huang, J.
\newblock Energy-based learning for cooperative games, with applications to
  valuation problems in machine learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bierlich et~al.(2022)Bierlich, Chakraborty, Desai, Gellersen,
  Helenius, Ilten, Lonnblad, Mrenna, Prestel, Preuss, Sjostrand, Skands,
  Utheim, and Verheyen]{Bierlich2022ACG}
Bierlich, C., Chakraborty, S., Desai, N., Gellersen, L., Helenius, I.~J.,
  Ilten, P., Lonnblad, L., Mrenna, S., Prestel, S., Preuss, C.~T., Sjostrand,
  T., Skands, P., Utheim, M., and Verheyen, R.
\newblock A comprehensive guide to the physics and usage of pythia 8.3.
\newblock \emph{SciPost Physics Codebases}, 2022.

\bibitem[Calinescu et~al.(2011)Calinescu, Chekuri, P\'{a}l, and
  Vondr\'{a}k]{Calinescu11}
Calinescu, G., Chekuri, C., P\'{a}l, M., and Vondr\'{a}k, J.
\newblock Maximizing a monotone submodular function subject to a matroid
  constraint.
\newblock \emph{SIAM Journal on Computing}, 40\penalty0 (6):\penalty0
  1740--1766, 2011.

\bibitem[Chang et~al.(2020)Chang, Zhang, Yu, and Jaakkola]{inv_rat}
Chang, S., Zhang, Y., Yu, M., and Jaakkola, T.~S.
\newblock Invariant rationalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1448--1458, 2020.

\bibitem[Chekuri et~al.(2014)Chekuri, Vondr\'{a}k, and Zenklusen]{Chekuri14}
Chekuri, C., Vondr\'{a}k, J., and Zenklusen, R.
\newblock Submodular function maximization via the multilinear relaxation and
  contention resolution schemes.
\newblock \emph{SIAM Journal on Computing}, 43\penalty0 (6):\penalty0
  1831--1879, 2014.

\bibitem[Chekuri et~al.(2015)Chekuri, Jayram, and Vondrak]{Chekuri15}
Chekuri, C., Jayram, T., and Vondrak, J.
\newblock On multiplicative weight updates for concave and submodular function
  maximization.
\newblock In \emph{Conference on Innovations in Theoretical Computer Science},
  pp.\  201–210, 2015.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Yang, Zhang, Ma, Liu, Han, and
  Cheng]{hao}
Chen, Y., Yang, H., Zhang, Y., Ma, K., Liu, T., Han, B., and Cheng, J.
\newblock Understanding and improving graph injection attack by promoting
  unnoticeability.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Zhang, Bian, Yang, Ma, Xie, Liu,
  Han, and Cheng]{ciga}
Chen, Y., Zhang, Y., Bian, Y., Yang, H., Ma, K., Xie, B., Liu, T., Han, B., and
  Cheng, J.
\newblock Learning causally invariant representations for out-of-distribution
  generalization on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Bian, Zhou, Xie, Han, and
  Cheng]{gala}
Chen, Y., Bian, Y., Zhou, K., Xie, B., Han, B., and Cheng, J.
\newblock Does invariant graph learning via environment augmentation learn
  invariance?
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Huang, Zhou, Bian, Han, and
  Cheng]{feat}
Chen, Y., Huang, W., Zhou, K., Bian, Y., Han, B., and Cheng, J.
\newblock Understanding and improving feature learning for out-of-distribution
  generalization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Zhou, Bian, Xie, Wu, Zhang,
  KAILI, Yang, Zhao, Han, and Cheng]{pair}
Chen, Y., Zhou, K., Bian, Y., Xie, B., Wu, B., Zhang, Y., KAILI, M., Yang, H.,
  Zhao, P., Han, B., and Cheng, J.
\newblock Pareto invariant risk minimization: Towards mitigating the
  optimization dilemma in out-of-distribution generalization.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{c}}.

\bibitem[Collaboration(2020)]{tau3mu_discovery}
Collaboration, A.
\newblock Search for charged-lepton-flavour violation in z-boson decays with
  the atlas detector.
\newblock \emph{Nature Physics}, 17:\penalty0 819 -- 825, 2020.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`{o}}, and
  Velickovic]{pna}
Corso, G., Cavalleri, L., Beaini, D., Li{\`{o}}, P., and Velickovic, P.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Crabb{\'{e}} \& van~der Schaar(2023)Crabb{\'{e}} and van~der
  Schaar]{xgnn_equi}
Crabb{\'{e}}, J. and van~der Schaar, M.
\newblock Evaluating the robustness of interpretability methods through
  explanation invariance and equivariance.
\newblock \emph{arXiv preprint}, arXiv:2304.06715, 2023.

\bibitem[Cranmer et~al.(2020)Cranmer, Sanchez-Gonzalez, Battaglia, Xu, Cranmer,
  Spergel, and Ho]{ai4sci_xgnn1}
Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P.~W., Xu, R., Cranmer, K.,
  Spergel, D.~N., and Ho, S.
\newblock Discovering symbolic models from deep learning with inductive biases.
\newblock \emph{arXiv preprint}, arXiv:2006.11287, 2020.

\bibitem[Călinescu et~al.(2007)Călinescu, Chekuri, P{\'a}l, and
  Vondr{\'a}k]{mt}
Călinescu, G., Chekuri, C., P{\'a}l, M., and Vondr{\'a}k, J.
\newblock Maximizing a submodular set function subject to a matroid constraint
  (extended abstract).
\newblock In \emph{Conference on Integer Programming and Combinatorial
  Optimization}, 2007.

\bibitem[Dai et~al.(2021)Dai, Demirel, Liang, and Hu]{ai4sci_xgnn2}
Dai, M., Demirel, M.~F., Liang, Y., and Hu, J.-M.
\newblock Graph neural networks for an accurate and interpretable prediction of
  the properties of polycrystalline materials.
\newblock \emph{npj Computational Materials}, 7\penalty0 (1):\penalty0 103,
  2021.

\bibitem[Debnath et~al.(1991)Debnath, Compadre, Debnath, Shusterman, and
  Hansch]{mutag}
Debnath, A.~K., Compadre, R.~L., Debnath, G., Shusterman, A.~J., and Hansch, C.
\newblock Structure-activity relationship of mutagenic aromatic and
  heteroaromatic nitro compounds. correlation with molecular orbital energies
  and hydrophobicity.
\newblock \emph{Journal of medicinal chemistry}, 34 2:\penalty0 786--97, 1991.

\bibitem[Deng \& Shen(2024)Deng and Shen]{sunny_gnn}
Deng, J. and Shen, Y.
\newblock Self-interpretable graph learning with sufficient and necessary
  explanations.
\newblock In \emph{{AAAI} Conference on Artificial Intelligence}, pp.\
  11749--11756, 2024.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pp.\  4171--4186, 2019.

\bibitem[Du et~al.(2016)Du, Li, Xia, Ai, Liang, Sang, lai Ji, and
  Liu]{Du2016InsightsIP}
Du, X., Li, Y., Xia, Y.-L., Ai, S.-M., Liang, J., Sang, P., lai Ji, X., and
  Liu, S.-Q.
\newblock Insights into protein–ligand interactions: Mechanisms, models, and
  methods.
\newblock \emph{International Journal of Molecular Sciences}, 17, 2016.

\bibitem[Erdos \& R{\'e}nyi(1984)Erdos and R{\'e}nyi]{er_graph}
Erdos, P.~L. and R{\'e}nyi, A.
\newblock On the evolution of random graphs.
\newblock \emph{Transactions of the American Mathematical Society},
  286:\penalty0 257--257, 1984.

\bibitem[Fan et~al.(2022)Fan, Wang, Mo, Shi, and Tang]{disc}
Fan, S., Wang, X., Mo, Y., Shi, C., and Tang, J.
\newblock Debiasing graph neural networks via learning disentangled causal
  substructure.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{pytorch_geometric}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Fountoulakis et~al.(2023)Fountoulakis, Levi, Yang, Baranwal, and
  Jagannath]{gatv2}
Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A.
\newblock Graph attention retrospective.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (246):\penalty0 1--52, 2023.

\bibitem[Funke et~al.(2023)Funke, Khosla, Rathee, and Anand]{Zorro}
Funke, T., Khosla, M., Rathee, M., and Anand, A.
\newblock Zorro: Valid, sparse, and stable explanations in graph neural
  networks.
\newblock \emph{{IEEE} Transactions on Knowledge and Data Engineering},
  35\penalty0 (8):\penalty0 8687--8698, 2023.

\bibitem[Gardner et~al.(2018)Gardner, Grus, Neumann, Tafjord, Dasigi, Liu,
  Peters, Schmitz, and Zettlemoyer]{biaffine}
Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N.~F.,
  Peters, M.~E., Schmitz, M., and Zettlemoyer, L.
\newblock Allennlp: {A} deep semantic natural language processing platform.
\newblock \emph{arXiv preprint}, arXiv:1803.07640, 2018.

\bibitem[Giles et~al.(1998)Giles, Bollacker, and Lawrence]{citeseer}
Giles, C.~L., Bollacker, K.~D., and Lawrence, S.
\newblock Citeseer: An automatic citation indexing system.
\newblock In \emph{Proceedings of the 3rd {ACM} International Conference on
  Digital Libraries}, pp.\  89--98, 1998.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{ai4sci_qchem}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1272, 2017.

\bibitem[Gui et~al.(2022)Gui, Li, Wang, and Ji]{good_bench}
Gui, S., Li, X., Wang, L., and Ji, S.
\newblock {GOOD}: A graph out-of-distribution benchmark.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Gui et~al.(2023)Gui, Liu, Li, Luo, and Ji]{joint_causal_indep}
Gui, S., Liu, M., Li, X., Luo, Y., and Ji, S.
\newblock Joint learning of label and environment causal independence for graph
  out-of-distribution generalization.
\newblock \emph{arXiv preprint}, arXiv:2306.01103, 2023.

\bibitem[Guo et~al.(2023)Guo, Xiao, Aggarwal, Liu, and Wang]{counterfactual_gl}
Guo, Z., Xiao, T., Aggarwal, C., Liu, H., and Wang, S.
\newblock Counterfactual learning on graphs: {A} survey.
\newblock \emph{arXiv preprint}, arXiv:2304.01391, 2023.

\bibitem[Holstein(2006)]{tau3mu}
Holstein, B.
\newblock { The Theory of Almost Everything: The Standard Model, the Unsung
  Triumph of Modern Physics }.
\newblock \emph{Physics Today}, 59\penalty0 (7):\penalty0 49--50, 07 2006.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{ogb}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Huang et~al.(2023)Huang, Yamada, Tian, Singh, and Chang]{graphlime}
Huang, Q., Yamada, M., Tian, Y., Singh, D., and Chang, Y.
\newblock Graphlime: Local interpretable model explanations for graph neural
  networks.
\newblock \emph{{IEEE} Transactions on Knowledge and Data Engineering},
  35\penalty0 (7):\penalty0 6968--6972, 2023.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batch_norm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Jain \& Wallace(2019)Jain and Wallace]{att_not_exp}
Jain, S. and Wallace, B.~C.
\newblock Attention is not explanation.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  3543--3556, 2019.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{gumbel}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[{Ji} et~al.(2022){Ji}, {Zhang}, {Wu}, {Wu}, {Huang}, {Xu}, {Rong},
  {Li}, {Ren}, {Xue}, {Lai}, {Xu}, {Feng}, {Liu}, {Luo}, {Zhou}, {Huang},
  {Zhao}, and {Bian}]{drugood}
{Ji}, Y., {Zhang}, L., {Wu}, J., {Wu}, B., {Huang}, L.-K., {Xu}, T., {Rong},
  Y., {Li}, L., {Ren}, J., {Xue}, D., {Lai}, H., {Xu}, S., {Feng}, J., {Liu},
  W., {Luo}, P., {Zhou}, S., {Huang}, J., {Zhao}, P., and {Bian}, Y.
\newblock {DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for
  AI-aided Drug Discovery -- A Focus on Affinity Prediction Problems with Noise
  Annotations}.
\newblock \emph{arXiv preprint}, arXiv:2201.09637, 2022.

\bibitem[Jin et~al.(2022)Jin, Zhao, Ding, Liu, Tang, and Shah]{graph_ttt}
Jin, W., Zhao, T., Ding, J., Liu, Y., Tang, J., and Shah, N.
\newblock Empowering graph representation learning with test-time graph
  transformation.
\newblock \emph{arXiv preprint}, arXiv:2210.03561, 2022.

\bibitem[Johndrow et~al.(2020)Johndrow, Pillai, and Smith]{no_free_lunch_MCMC}
Johndrow, J.~E., Pillai, N.~S., and Smith, A.
\newblock No free lunch for approximate mcmc.
\newblock \emph{arXiv preprint}, arXiv:2010.12514, 2020.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, Z{\'i}dek, Potapenko, Bridgland, Meyer,
  Kohl, Ballard, Cowie, Romera-Paredes, Nikolov, Jain, Adler, Back, Petersen,
  Reiman, Clancy, Zielinski, Steinegger, Pacholska, Berghammer, Bodenstein,
  Silver, Vinyals, Senior, Kavukcuoglu, Kohli, and Hassabis]{ai4sci_bchem}
Jumper, J.~M., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger,
  O., Tunyasuvunakool, K., Bates, R., Z{\'i}dek, A., Potapenko, A., Bridgland,
  A., Meyer, C., Kohl, S. A.~A., Ballard, A., Cowie, A., Romera-Paredes, B.,
  Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D.~A.,
  Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T.,
  Bodenstein, S., Silver, D., Vinyals, O., Senior, A.~W., Kavukcuoglu, K.,
  Kohli, P., and Hassabis, D.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596:\penalty0 583 -- 589, 2021.

\bibitem[Kamhoua et~al.(2022)Kamhoua, Zhang, Chen, Yang, KAILI, Han, Li, and
  Cheng]{shape_matching}
Kamhoua, B.~F., Zhang, L., Chen, Y., Yang, H., KAILI, M., Han, B., Li, B., and
  Cheng, J.
\newblock Exact shape correspondence via 2d graph convolution.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Karalias et~al.(2022)Karalias, Robinson, Loukas, and
  Jegelka]{neural_set}
Karalias, N., Robinson, J., Loukas, A., and Jegelka, S.
\newblock Neural set function extensions: Learning with discrete functions in
  high dimensions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Karimi et~al.(2023)Karimi, Muandet, Kornblith, Sch{\"{o}}lkopf, and
  Kim]{relation_exp_pred}
Karimi, A., Muandet, K., Kornblith, S., Sch{\"{o}}lkopf, B., and Kim, B.
\newblock On the relationship between explanation and prediction: {A} causal
  view.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  15861--15883, 2023.

\bibitem[Karimi et~al.(2019)Karimi, Wu, Wang, and
  Shen]{Karimi2018DeepAffinityID}
Karimi, M., Wu, D., Wang, Z., and Shen, Y.
\newblock Deepaffinity: Interpretable deep learning of compound-protein
  affinity through unified recurrent and convolutional neural networks.
\newblock \emph{Bioinformatics}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Knyazev et~al.(2019)Knyazev, Taylor, and Amer]{understand_att}
Knyazev, B., Taylor, G.~W., and Amer, M.~R.
\newblock Understanding attention and generalization in graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4204--4214, 2019.

\bibitem[Kochkov et~al.(2021)Kochkov, Pfaff, Sanchez-Gonzalez, Battaglia, and
  Clark]{ai4sci_qua}
Kochkov, D., Pfaff, T., Sanchez-Gonzalez, A., Battaglia, P.~W., and Clark,
  B.~K.
\newblock Learning ground states of quantum hamiltonians with graph networks.
\newblock \emph{arXiv preprint}, arXiv:2110.06390, 2021.

\bibitem[Laskowski(2001)]{Laskowski2001PDBsumSS}
Laskowski, R.~A.
\newblock {PDBsum: summaries and analyses of PDB structures}.
\newblock \emph{Nucleic Acids Research}, 29\penalty0 (1):\penalty0 221--222, 01
  2001.

\bibitem[Lee et~al.(2023)Lee, Bu, Yoo, and Shin]{aero_gnn}
Lee, S.~Y., Bu, F., Yoo, J., and Shin, K.
\newblock Towards deep attention in graph neural networks: Problems and
  remedies.
\newblock In \emph{International Conference on Machine Learning}, volume 202,
  pp.\  18774--18795, 2023.

\bibitem[Li et~al.(2022)Li, Zhang, Wang, and Zhu]{gil}
Li, H., Zhang, Z., Wang, X., and Zhu, W.
\newblock Learning invariant graph representations for out-of-distribution
  generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Li et~al.(2023)Li, Gui, Luo, and Ji]{graph_joint_extra}
Li, X., Gui, S., Luo, Y., and Ji, S.
\newblock Graph structure and feature extrapolation for out-of-distribution
  generalization.
\newblock \emph{arXiv preprint}, arXiv:2306.08076, 2023.

\bibitem[Lin et~al.(2021)Lin, Lan, and Li]{gen_xgnn}
Lin, W., Lan, H., and Li, B.
\newblock Generative causal explanations for graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, volume 139,
  pp.\  6666--6679, 2021.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Lan, Wang, and Li]{orphicx}
Lin, W., Lan, H., Wang, H., and Li, B.
\newblock Orphicx: {A} causality-inspired latent variable model for
  interpreting graph neural networks.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pp.\  13719--13728, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Zhu, Tan, and Cui]{zin}
Lin, Y., Zhu, S., Tan, L., and Cui, P.
\newblock {ZIN}: When and how to learn invariance without environment
  partition?
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Lipton(2018)]{mythos_inter}
Lipton, Z.~C.
\newblock The mythos of model interpretability.
\newblock \emph{Commun. {ACM}}, 61\penalty0 (10):\penalty0 36--43, 2018.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Zhao, Xu, Luo, and Jiang]{grea}
Liu, G., Zhao, T., Xu, J., Luo, T., and Jiang, M.
\newblock Graph rationalization with environment-based augmentations.
\newblock \emph{arXiv preprint arXiv:2206.02886}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Luo, Uchino, Maruhashi, and
  Ji]{Liu2022Generating3M}
Liu, M., Luo, Y., Uchino, K., Maruhashi, K., and Ji, S.
\newblock Generating 3d molecules for target protein binding.
\newblock \emph{arXiv preprint}, arXiv:2204.09410, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2017)Liu, Su, Han, Liu, Yang, Li, and Wang]{PDBind}
Liu, Z., Su, M., Han, L., Liu, J., Yang, Q., Li, Y., and Wang, R.
\newblock Forging the basis for developing protein-ligand interaction scoring
  functions.
\newblock \emph{Accounts of chemical research}, 50 2:\penalty0 302--309, 2017.

\bibitem[Lov{\'{a}}sz \& Szegedy(2006)Lov{\'{a}}sz and Szegedy]{graphon}
Lov{\'{a}}sz, L. and Szegedy, B.
\newblock Limits of dense graph sequences.
\newblock \emph{Journal of Combinatorial Theory, Series B}, 96\penalty0
  (6):\penalty0 933--957, 2006.

\bibitem[Luo et~al.(2020)Luo, Cheng, Xu, Yu, Zong, Chen, and Zhang]{pge}
Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., and Zhang, X.
\newblock Parameterized explainer for graph neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  19620--19631, 2020.

\bibitem[Ma et~al.(2022)Ma, Guo, Mishra, Zhang, and Li]{clear}
Ma, J., Guo, R., Mishra, S., Zhang, A., and Li, J.
\newblock Clear: Generative counterfactual explanations on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  25895--25907, 2022.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{gumbel2}
Maddison, C.~J., Mnih, A., and Teh, Y.~W.
\newblock The concrete distribution: {A} continuous relaxation of discrete
  random variables.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mahdavi et~al.(2022)Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis,
  and Liao]{OOD_CLRS}
Mahdavi, S., Swersky, K., Kipf, T., Hashemi, M., Thrampoulidis, C., and Liao,
  R.
\newblock Towards better out-of-distribution generalization of neural
  algorithmic reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.00692}, 2022.

\bibitem[McCloskey et~al.(2018)McCloskey, Taly, Monti, Brenner, and
  Colwell]{McCloskey2018UsingAT}
McCloskey, K., Taly, A., Monti, F., Brenner, M.~P., and Colwell, L.~J.
\newblock Using attribution to decode binding mechanism in neural network
  models for chemistry.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116:\penalty0
  11624 -- 11629, 2018.

\bibitem[Miao et~al.(2022)Miao, Liu, and Li]{gsat}
Miao, S., Liu, M., and Li, P.
\newblock Interpretable and generalizable graph learning via stochastic
  attention mechanism.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Miao et~al.(2023)Miao, Luo, Liu, and Li]{lri}
Miao, S., Luo, Y., Liu, M., and Li, P.
\newblock Interpretable geometric deep learning via learnable randomness
  injection.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Murray \& Rees(2009)Murray and Rees]{xgnn_ai4sci0}
Murray, C.~W. and Rees, D.~C.
\newblock The rise of fragment-based drug discovery.
\newblock \emph{Nature chemistry}, 1 3:\penalty0 187--92, 2009.

\bibitem[Owen(1972)]{mt_game}
Owen, G.
\newblock Multilinear extensions of games.
\newblock \emph{Management Science}, 18:\penalty0 64--79, 1972.

\bibitem[Papamarkou et~al.(2022)Papamarkou, Hinkle, Young, and
  Womble]{papamarkou2022a}
Papamarkou, T., Hinkle, J., Young, M.~T., and Womble, D.
\newblock Challenges in {M}arkov chain {M}onte {C}arlo for {B}ayesian
  neuralnetworks.
\newblock \emph{Statistical Science}, 37\penalty0 (3):\penalty0 425--442, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Prado-Romero et~al.(2022)Prado-Romero, Prenkaj, Stilo, and
  Giannotti]{counterfactual_xgnn_sur}
Prado-Romero, M.~A., Prenkaj, B., Stilo, G., and Giannotti, F.
\newblock A survey on graph counterfactual explanations: Definitions, methods,
  evaluation, and research challenges.
\newblock \emph{ACM Computing Surveys}, 2022.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{fidelity}
Ribeiro, M.~T., Singh, S., and Guestrin, C.
\newblock "why should i trust you?": Explaining the predictions of any
  classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  1135–1144, 2016.

\bibitem[Rudin(2018)]{Rudin2018StopEB}
Rudin, C.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock \emph{Nature Machine Intelligence}, 1:\penalty0 206--215, 2018.

\bibitem[Sahin et~al.(2020)Sahin, Bian, Buhmann, and Krause]{sets2multisets}
Sahin, A., Bian, Y., Buhmann, J.~M., and Krause, A.
\newblock From sets to multisets: Provable variational inference for
  probabilistic integer submodular models.
\newblock In \emph{International Conference on Machine Learning}, volume 119,
  pp.\  8388--8397, 2020.

\bibitem[Satorras et~al.(2021)Satorras, Hoogeboom, and Welling]{egnn}
Satorras, V.~G., Hoogeboom, E., and Welling, M.
\newblock E(n) equivariant graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9323--9332, 2021.

\bibitem[Schlichtkrull et~al.(2021)Schlichtkrull, Cao, and Titov]{graphmask}
Schlichtkrull, M.~S., Cao, N.~D., and Titov, I.
\newblock Interpreting graph neural networks for {NLP} with differentiable edge
  masking.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Schulte et~al.(2004)Schulte, Bashkirov, Li, Liang, Mueller, Heimann,
  Johnson, Keeney, Sadrozinski, Seiden, Williams, Zhang, Li, Peggs, Satogata,
  and Woody]{act}
Schulte, R., Bashkirov, V., Li, T., Liang, Z., Mueller, K., Heimann, J.,
  Johnson, L., Keeney, B., Sadrozinski, H.-W., Seiden, A., Williams, D., Zhang,
  L., Li, Z., Peggs, S., Satogata, T., and Woody, C.
\newblock Conceptual design of a proton computed tomography system for
  applications in proton radiation therapy.
\newblock \emph{IEEE Transactions on Nuclear Science}, 51\penalty0
  (3):\penalty0 866--872, 2004.

\bibitem[Sch{\"u}tt et~al.(2017)Sch{\"u}tt, Sauceda, Kindermans, Tkatchenko,
  and M{\"u}ller]{ai4sci_mat}
Sch{\"u}tt, K.~T., Sauceda, H.~E., Kindermans, P.~J., Tkatchenko, A., and
  M{\"u}ller, K.-R.
\newblock Schnet - a deep learning architecture for molecules and materials.
\newblock \emph{The Journal of chemical physics}, 148 24:\penalty0 241722,
  2017.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{gradcam}
Selvaraju, R.~R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra,
  D.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{{IEEE} International Conference on Computer Vision}, pp.\
  618--626. {IEEE} Computer Society, 2017.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Gallagher, and
  Eliassi-Rad]{Sen2008CollectiveCI}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B., and Eliassi-Rad, T.
\newblock Collective classification in network data.
\newblock In \emph{The AI Magazine}, 2008.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"u}nnemann]{Shchur2018PitfallsOG}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock \emph{arXiv preprint}, arXiv:1811.05868, 2018.

\bibitem[Shrikumar et~al.(2017)Shrikumar, Greenside, and Kundaje]{gradgeo}
Shrikumar, A., Greenside, P., and Kundaje, A.
\newblock Learning important features through propagating activation
  differences.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3145--3153, 2017.

\bibitem[Snijders \& Nowicki(1997)Snijders and Nowicki]{sbm}
Snijders, T.~A. and Nowicki, K.
\newblock Estimation and prediction for stochastic blockmodels for graphs with
  latent block structure.
\newblock In \emph{Journal of Classification}, volume~14, pp.\  75--100, 1997.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst25}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, pp.\  1631--1642, 2013.

\bibitem[Spinelli et~al.(2024)Spinelli, Scardapane, and Uncini]{meta_ex_gnn}
Spinelli, I., Scardapane, S., and Uncini, A.
\newblock A meta-learning approach for training explainable graph neural
  networks.
\newblock \emph{{IEEE} Transactions on Neural Networks and Learning Systems},
  35\penalty0 (4):\penalty0 4647--4655, 2024.

\bibitem[St{\'a}rk et~al.(2022)St{\'a}rk, Ganea, Pattanaik, Barzilay, and
  Jaakkola]{Strk2022EquiBindGD}
St{\'a}rk, H., Ganea, O.-E., Pattanaik, L., Barzilay, R., and Jaakkola, T.
\newblock Equibind: Geometric deep learning for drug binding structure
  prediction.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Sui et~al.(2022)Sui, Wang, Wu, Lin, He, and Chua]{cal}
Sui, Y., Wang, X., Wu, J., Lin, M., He, X., and Chua, T.
\newblock Causal attention for interpretable and generalizable graph
  classification.
\newblock In \emph{The 28th {ACM} {SIGKDD} Conference on Knowledge Discovery
  and Data Mining}, pp.\  1696--1705, 2022.

\bibitem[Taghanaki et~al.(2020)Taghanaki, Hassani, Jayaraman, Ahmadi, and
  Custis]{pointmask}
Taghanaki, S.~A., Hassani, K., Jayaraman, P.~K., Ahmadi, A. H.~K., and Custis,
  T.
\newblock Pointmask: Towards interpretable and bias-resilient point cloud
  processing.
\newblock \emph{arXiv preprint}, arXiv:2007.04525, 2020.

\bibitem[Tishby et~al.(1999)Tishby, Pereira, and Bialek]{ib}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock In \emph{Annual Allerton Conference on Communication, Control and
  Computing}, pp.\  368--377, 1999.

\bibitem[Villanueva-Domingo et~al.(2021)Villanueva-Domingo,
  Villaescusa-Navarro, Angl'es-Alc'azar, Genel, Marinacci, Spergel, Hernquist,
  Vogelsberger, Dav{\'e}, and Narayanan]{ai4sci_cos}
Villanueva-Domingo, P., Villaescusa-Navarro, F., Angl'es-Alc'azar, D., Genel,
  S., Marinacci, F., Spergel, D.~N., Hernquist, L.~E., Vogelsberger, M.,
  Dav{\'e}, R., and Narayanan, D.
\newblock Inferring halo masses with graph neural networks.
\newblock \emph{The Astrophysical Journal}, 935, 2021.

\bibitem[Vondrak(2008)]{Vondrak08}
Vondrak, J.
\newblock Optimal approximation for the submodular welfare problem in the value
  oracle model.
\newblock In \emph{Annual ACM Symposium on Theory of Computing}, pp.\  67–74,
  2008.

\bibitem[Vu \& Thai(2020)Vu and Thai]{pgm_explainer}
Vu, M.~N. and Thai, M.~T.
\newblock Pgm-explainer: Probabilistic graphical model explanations for graph
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Wang \& Zhang(2017)Wang and Zhang]{Wang2017ImprovingSP}
Wang, C. and Zhang, Y.
\newblock Improving scoring‐docking‐screening powers of protein–ligand
  scoring functions using random forest.
\newblock \emph{Journal of Computational Chemistry}, 38:\penalty0 169 -- 177,
  2017.

\bibitem[Wang et~al.(2023)Wang, Fu, Du, Gao, Huang, Liu, Chandak, Liu, Katwyk,
  Deac, Anandkumar, Bergen, Gomes, Ho, Kohli, Lasenby, Leskovec, Liu, Manrai,
  Marks, Ramsundar, Song, Sun, Tang, Velickovic, Welling, Zhang, Coley, Bengio,
  and Zitnik]{ai4sci0}
Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S.,
  Katwyk, P.~V., Deac, A., Anandkumar, A., Bergen, K.~J., Gomes, C.~P., Ho, S.,
  Kohli, P., Lasenby, J., Leskovec, J., Liu, T.-Y., Manrai, A.~K., Marks, D.,
  Ramsundar, B., Song, L., Sun, J., Tang, J., Velickovic, P., Welling, M.,
  Zhang, L., Coley, C.~W., Bengio, Y., and Zitnik, M.
\newblock Scientific discovery in the age of artificial intelligence.
\newblock \emph{Nature}, 620:\penalty0 47 -- 60, 2023.

\bibitem[Wencel-Delord \& Glorius(2013)Wencel-Delord and Glorius]{xgnn_ai4sci1}
Wencel-Delord, J. and Glorius, F.
\newblock C-h bond activation enables the rapid construction and late-stage
  diversification of functional molecules.
\newblock \emph{Nature chemistry}, 5 5:\penalty0 369--75, 2013.

\bibitem[Wu et~al.(2019)Wu, Souza, Zhang, Fifty, Yu, and Weinberger]{sgnn}
Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  6861--6871, 2019.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Zhang, Yan, and Wipf]{eerm}
Wu, Q., Zhang, H., Yan, J., and Wipf, D.
\newblock Handling distribution shifts on graphs: An invariance perspective.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Wu et~al.(2020)Wu, Ren, Li, and Leskovec]{gib_node}
Wu, T., Ren, H., Li, P., and Leskovec, J.
\newblock Graph information bottleneck.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  20437--20448, 2020.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Wang, Zhang, He, and Chua]{dir}
Wu, Y., Wang, X., Zhang, A., He, X., and Chua, T.-S.
\newblock Discovering invariant rationales for graph neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Xie \& Grossman(2017)Xie and Grossman]{ai4sci_xgnn0}
Xie, T. and Grossman, J.~C.
\newblock Crystal graph convolutional neural networks for an accurate and
  interpretable prediction of material properties.
\newblock \emph{Physical review letters}, 120 14:\penalty0 145301, 2017.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{gin}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{nn_extrapo}
Xu, K., Zhang, M., Li, J., Du, S.~S., Kawarabayashi, K., and Jegelka, S.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yang et~al.(2023)Yang, Zheng, Wang, Liu, Huang, Hong, Zhang, and
  Cui]{gib_hiera}
Yang, L., Zheng, J., Wang, H., Liu, Z., Huang, Z., Hong, S., Zhang, W., and
  Cui, B.
\newblock Individual and structural graph information bottlenecks for
  out-of-distribution generalization.
\newblock \emph{arXiv preprint}, arXiv:2306.15902, 2023.

\bibitem[Yang et~al.(2022)Yang, Zeng, Wu, Jia, and Yan]{moleood}
Yang, N., Zeng, K., Wu, Q., Jia, X., and Yan, J.
\newblock Learning substructure invariance for out-of-distribution molecular
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yang et~al.(2016)Yang, Cohen, and Salakhutdinov]{cora}
Yang, Z., Cohen, W.~W., and Salakhutdinov, R.
\newblock Revisiting semi-supervised learning with graph embeddings.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning}, pp.\  40--48, 2016.

\bibitem[Yehudai et~al.(2021)Yehudai, Fetaya, Meirom, Chechik, and
  Maron]{size_gen1}
Yehudai, G., Fetaya, E., Meirom, E., Chechik, G., and Maron, H.
\newblock From local structures to size generalization in graph neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11975--11986, 2021.

\bibitem[Ying et~al.(2019)Ying, Bourgeois, You, Zitnik, and
  Leskovec]{gnn_explainer}
Ying, Z., Bourgeois, D., You, J., Zitnik, M., and Leskovec, J.
\newblock Gnnexplainer: Generating explanations for graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9240--9251, 2019.

\bibitem[Yu et~al.(2021)Yu, Xu, Rong, Bian, Huang, and He]{gib}
Yu, J., Xu, T., Rong, Y., Bian, Y., Huang, J., and He, R.
\newblock Graph information bottleneck for subgraph recognition.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yu et~al.(2022)Yu, Cao, and He]{vgib}
Yu, J., Cao, J., and He, R.
\newblock Improving subgraph recognition with variational graph information
  bottleneck.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pp.\  19374--19383. {IEEE}, 2022.

\bibitem[Yu et~al.(2023)Yu, Liang, and He]{dps}
Yu, J., Liang, J., and He, R.
\newblock Mind the label shift of augmentation-based graph {OOD}
  generalization.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pp.\  11620--11630. {IEEE}, 2023.

\bibitem[Yuan et~al.(2020{\natexlab{a}})Yuan, Tang, Hu, and Ji]{xgnn}
Yuan, H., Tang, J., Hu, X., and Ji, S.
\newblock {XGNN:} towards model-level explanations of graph neural networks.
\newblock In \emph{The 26th {ACM} {SIGKDD} Conference on Knowledge Discovery
  and Data Mining}, pp.\  430--438, 2020{\natexlab{a}}.

\bibitem[Yuan et~al.(2020{\natexlab{b}})Yuan, Yu, Gui, and Ji]{xgnn_tax}
Yuan, H., Yu, H., Gui, S., and Ji, S.
\newblock Explainability in graph neural networks: {A} taxonomic survey.
\newblock \emph{arXiv preprint}, arXiv:2012.15445, 2020{\natexlab{b}}.

\bibitem[Yuan et~al.(2021)Yuan, Yu, Wang, Li, and Ji]{subgraphxgn}
Yuan, H., Yu, H., Wang, J., Li, K., and Ji, S.
\newblock On explainability of graph neural networks via subgraph explorations.
\newblock In \emph{International Conference on Machine Learning}, volume 139,
  pp.\  12241--12252, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Helwig, Luo, Fu, Xie, Liu, Lin, Xu,
  Yan, Adams, Weiler, Li, Fu, Wang, Yu, Xie, Fu, Strasser, Xu, Liu, Du, Saxton,
  Ling, Lawrence, St{\"{a}}rk, Gui, Edwards, Gao, Ladera, Wu, Hofgard, Tehrani,
  Wang, Daigavane, Bohde, Kurtin, Huang, Phung, Xu, Joshi, Mathis,
  Azizzadenesheli, Fang, Aspuru{-}Guzik, Bekkers, Bronstein, Zitnik,
  Anandkumar, Ermon, Li{\`{o}}, Yu, G{\"{u}}nnemann, Leskovec, Ji, Sun,
  Barzilay, Jaakkola, Coley, Qian, Qian, Smidt, and Ji]{ai4sci}
Zhang, X., Wang, L., Helwig, J., Luo, Y., Fu, C., Xie, Y., Liu, M., Lin, Y.,
  Xu, Z., Yan, K., Adams, K., Weiler, M., Li, X., Fu, T., Wang, Y., Yu, H.,
  Xie, Y., Fu, X., Strasser, A., Xu, S., Liu, Y., Du, Y., Saxton, A., Ling, H.,
  Lawrence, H., St{\"{a}}rk, H., Gui, S., Edwards, C., Gao, N., Ladera, A., Wu,
  T., Hofgard, E.~F., Tehrani, A.~M., Wang, R., Daigavane, A., Bohde, M.,
  Kurtin, J., Huang, Q., Phung, T., Xu, M., Joshi, C.~K., Mathis, S.~V.,
  Azizzadenesheli, K., Fang, A., Aspuru{-}Guzik, A., Bekkers, E., Bronstein,
  M.~M., Zitnik, M., Anandkumar, A., Ermon, S., Li{\`{o}}, P., Yu, R.,
  G{\"{u}}nnemann, S., Leskovec, J., Ji, H., Sun, J., Barzilay, R., Jaakkola,
  T.~S., Coley, C.~W., Qian, X., Qian, X., Smidt, T.~E., and Ji, S.
\newblock Artificial intelligence for science in quantum, atomistic, and
  continuum systems.
\newblock \emph{arXiv preprint}, arXiv:2307.08423, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Liu, Wang, Lu, and Lee]{protGNN}
Zhang, Z., Liu, Q., Wang, H., Lu, C., and Lee, C.
\newblock Protgnn: Towards self-explaining graph neural networks.
\newblock In \emph{Thirty-Sixth {AAAI} Conference on Artificial Intelligence},
  pp.\  9127--9135, 2022.

\bibitem[Zheng et~al.(2024)Zheng, Shirani, Wang, Cheng, Chen, Chen, Wei, and
  Luo]{zheng2024robust_fid}
Zheng, X., Shirani, F., Wang, T., Cheng, W., Chen, Z., Chen, H., Wei, H., and
  Luo, D.
\newblock Towards robust fidelity for evaluating explainability of graph neural
  networks.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Zhou et~al.(2022)Zhou, Kutyniok, and Ribeiro]{size_gen3}
Zhou, Y., Kutyniok, G., and Ribeiro, B.
\newblock {OOD} link prediction generalization capabilities of message-passing
  {GNN}s in larger test graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}
