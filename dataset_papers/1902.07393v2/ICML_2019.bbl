\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel et~al.(2007)Abbeel, Coates, Quigley, and Ng]{Abbeel2007_NIPS}
Abbeel, P., Coates, A., Quigley, M., and Ng, A.
\newblock An application of reinforcement learning to aerobatic helicopter
  flight.
\newblock In \emph{Advances in Neural Information Processing Systems 19}, pp.\
  1--8. 2007.

\bibitem[Bennis et~al.(2013)Bennis, Perlaza, Blasco, Han, and
  Poor]{Bennis2013_CellNetworks}
Bennis, M., Perlaza, S.~M., Blasco, P., Han, Z., and Poor, H.~V.
\newblock Self-organization in small cell networks: A reinforcement learning
  approach.
\newblock \emph{IEEE Transactions on Wireless Communications}, 12\penalty0
  (7):\penalty0 3202--3212, 2013.

\bibitem[Bertsekas \& Tsitsiklis(1999)Bertsekas and
  Tsitsiklis]{Bertsekas1999_book}
Bertsekas, D. and Tsitsiklis, J.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, Belmont, MA, 2nd edition, 1999.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and
  Singal]{Bhandari2018_FiniteTD0}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{COLT}, 2018.

\bibitem[Borkar(2008)]{borkar2008}
Borkar, V.
\newblock \emph{Stochastic Approximation: A Dynamical Systems Viewpoint}.
\newblock Cambridge University Press, 2008.

\bibitem[Borkar \& Meyn(2000)Borkar and Meyn]{Borkar2000_ODE}
Borkar, V. and Meyn, S.
\newblock The o.d.e. method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \emph{SIAM Journal on Control and Optimization}, 38\penalty0
  (2):\penalty0 447--469, 2000.

\bibitem[Bradtke \& Barto(1996)Bradtke and Barto]{Bradtke1996_LSTD}
Bradtke, S. and Barto, A.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine Learning}, 22\penalty0 (1):\penalty0 33--57, Mar 1996.

\bibitem[Chen et~al.(2015)Chen, Seff, Kornhauser, and Xiao]{Chen2015_DeepDrive}
Chen, C., Seff, A., Kornhauser, A., and Xiao, J.
\newblock Deepdriving: Learning affordance for direct perception in autonomous
  driving.
\newblock In \emph{Proceedings of the 2015 IEEE International Conference on
  Computer Vision (ICCV)}, ICCV '15, pp.\  2722--2730, Washington, DC, USA,
  2015.

\bibitem[Cortes et~al.(2004)Cortes, Martinez, Karatas, and
  Bullo]{CortesMKB2004}
Cortes, J., Martinez, S., Karatas, T., and Bullo, F.
\newblock Coverage control for mobile sensing networks.
\newblock \emph{IEEE Transactions on Robotics and Automation}, 20\penalty0
  (2):\penalty0 243--255, 2004.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{Dalal2018_FiniteTD0}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analyses for td(0) with function approximation.
\newblock In \emph{AAAI}, 2018.

\bibitem[Dayan(1992)]{Dayan1992_TD}
Dayan, P.
\newblock The convergence of {TD}($\lambda$) for general $\lambda$, 1992.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{Gu2017_DeepRL}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock \emph{2017 IEEE International Conference on Robotics and Automation
  (ICRA)}, pp.\  3389--3396, 2017.

\bibitem[Gurvits et~al.(1994)Gurvits, Lin, and Hanson]{Gurvits1994_TD}
Gurvits, L., Lin, L.~J., and Hanson, S.~J.
\newblock Incremental learning of evaluation functions for absorbing {M}arkov
  chains: New methods and theorems, 1994.

\bibitem[Kar et~al.(2013)Kar, Moura, and Poor]{Kar2013_QDLearning}
Kar, S., Moura, J. M.~F., and Poor, H.~V.
\newblock Qd-learning: A collaborative distributed strategy for multi-agent
  reinforcement learning through consensus + innovations.
\newblock \emph{IEEE Trans. Signal Processing}, 61:\penalty0 1848--1862, 2013.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{Liu2015_FiniteGTD}
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{Proceedings of the Thirty-First Conference on Uncertainty in
  Artificial Intelligence}, pp.\  504--513, 2015.

\bibitem[Macua et~al.(2015)Macua, Chen, Zazo, and Sayed]{Macua_2015_TAC}
Macua, S.~V., Chen, J., Zazo, S., and Sayed, A.~H.
\newblock Distributed policy evaluation under multiple behavior strategies.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (5):\penalty0 1260--1274, 2015.

\bibitem[Mathkar \& Borkar(2017)Mathkar and Borkar]{Mathkar2017_GossipRL}
Mathkar, A. and Borkar, V.~S.
\newblock Distributed reinforcement learning via gossip.
\newblock \emph{IEEE Transactions on Automatic Control}, 62\penalty0
  (3):\penalty0 1465--1470, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,
  G.~Bellemare, Graves, Riedmiller, K.~Fidjeland, Ostrovski, Petersen, Beattie,
  Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and
  Hassabis]{Mnih2015_Nature}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A., Veness, J., G.~Bellemare, M.,
  Graves, A., Riedmiller, M., K.~Fidjeland, A., Ostrovski, G., Petersen, S.,
  Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
  Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518:\penalty0 529--33, 02 2015.

\bibitem[Nedi{\'{c}} \& Bertsekas(2003)Nedi{\'{c}} and
  Bertsekas]{Nedic2003_LSPE}
Nedi{\'{c}}, A. and Bertsekas, D.~P.
\newblock Least squares policy evaluation algorithms with linear function
  approximation.
\newblock \emph{Discrete Event Dynamic Systems}, 13\penalty0 (1):\penalty0
  79--110, Jan 2003.

\bibitem[Nedi\'{c} et~al.(2010)Nedi\'{c}, Ozdaglar, and Parrilo]{NedicOP2010}
Nedi\'{c}, A., Ozdaglar, A., and Parrilo, P.~A.
\newblock Constrained consensus and optimization in multi-agent networks.
\newblock \emph{IEEE Transactions on Automatic Control}, 55\penalty0
  (4):\penalty0 922--938, 2010.

\bibitem[Nedi\'{c} et~al.(2018)Nedi\'{c}, Olshevsky, and Rabbat]{NedicOR2018}
Nedi\'{c}, A., Olshevsky, A., and Rabbat, M.~G.
\newblock Network topology and communication-computation tradeoffs in
  decentralized optimization.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (5):\penalty0 953--976,
  2018.

\bibitem[Ogren et~al.(2004)Ogren, Fiorelli, and Leonard]{Ogren2004_TAC}
Ogren, P., Fiorelli, E., and Leonard, N.~E.
\newblock Cooperative control of mobile sensor networks:adaptive gradient
  climbing in a distributed environment.
\newblock \emph{IEEE Transactions on Automatic Control}, 49\penalty0
  (8):\penalty0 1292--1302, 2004.

\bibitem[Pineda(1997)]{Pineda1997_TD}
Pineda, F.~J.
\newblock Mean-field theory for batched {TD}($\lambda$).
\newblock \emph{Neural Computation}, 9:\penalty0 1403--1419, 1997.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{Silver2016_MasteringTG}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T.~P., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529:\penalty0 484--489, 2016.

\bibitem[Stankovi\'{c} \& Stankovi\'{c}(2016)Stankovi\'{c} and
  Stankovi\'{c}]{Stankovic2016_ACC}
Stankovi\'{c}, M.~S. and Stankovi\'{c}, S.~S.
\newblock Multi-agent temporal-difference learning with linear function
  approximation: Weak convergence under time-varying network topologies.
\newblock In \emph{2016 American Control Conference (ACC)}, pp.\  167--172,
  2016.

\bibitem[Sutton(1988)]{Sutton1988_TD}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9--44, Aug 1988.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{Sutton1998_book}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 1st edition, 1998.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv\'{a}ri, and Wiewiora]{Sutton2009_FGTD}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv\'{a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, ICML '09, pp.\  993--1000, 2009{\natexlab{a}}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, and
  Szepesv\'{a}ri]{Sutton2008_GTD}
Sutton, R.~S., Maei, H.~R., and Szepesv\'{a}ri, C.
\newblock A convergent o(n) temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 21}, pp.\
  1609--1616. 2009{\natexlab{b}}.

\bibitem[Szepesvari(2010)]{Szepesvari2010_book}
Szepesvari, C.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan and Claypool Publishers, 2010.

\bibitem[Tesauro(1995)]{Tesauro1995_TD0}
Tesauro, G.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Commun. ACM}, 38\penalty0 (3):\penalty0 58--68, 1995.

\bibitem[Thoppe \& Borkar()Thoppe and Borkar]{Thoppe2018_ConcentrationBound}
Thoppe, G. and Borkar, V.~S.
\newblock {A Concentration Bound for Stochastic Approximation via Alekseev's
  Formula}.
\newblock Available at: \url{https://arxiv.org/abs/1506.08657}.

\bibitem[Tsitsiklis \& Roy(1997)Tsitsiklis and Roy]{Tsitsiklis1997_TD}
Tsitsiklis, J.~N. and Roy, B.~V.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[Tsitsiklis \& Roy(1999)Tsitsiklis and Roy]{Tsitsiklis1999_TDAverage}
Tsitsiklis, J.~N. and Roy, B.~V.
\newblock Average cost temporal-difference learning.
\newblock \emph{Automatica}, 35:\penalty0 1799--1808, 1999.

\bibitem[Tu \& Recht(2018)Tu and Recht]{Tu2018_ICML}
Tu, S. and Recht, B.
\newblock Least-squares temporal difference learning for the linear quadratic
  regulator.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pp.\  5012--5021, 2018.

\bibitem[Wai et~al.(2018)Wai, Yang, Wang, and Hong]{Wai2018_NIPS}
Wai, H.-T., Yang, Z., Wang, Z., and Hong, M.
\newblock Multi-agent reinforcement learning via double averaging primal-dual
  optimization.
\newblock In \emph{Annual Conference on Neural Information Processing Systems},
  pp.\  9672--9683, 2018.

\bibitem[Yu \& Bertsekas(2009)Yu and Bertsekas]{Yu2009_TAC}
Yu, H. and Bertsekas, D.~P.
\newblock Convergence results for some temporal difference methods based on
  least squares.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (7):\penalty0 1515--1531, 2009.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Liu, Zhang, and Basar]{zhang2018_ICML}
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
   5872--5881, 2018.

\end{thebibliography}
