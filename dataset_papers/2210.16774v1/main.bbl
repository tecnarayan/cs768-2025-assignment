\begin{thebibliography}{10}

\bibitem{bachem2017practical}
Olivier Bachem, Mario Lucic, and Andreas Krause.
\newblock Practical coreset constructions for machine learning.
\newblock {\em arXiv preprint arXiv:1703.06476}, 2017.

\bibitem{belouadah2020scail}
Eden Belouadah and Adrian Popescu.
\newblock Scail: Classifier weights scaling for class incremental learning.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision}, pages 1266--1275, 2020.

\bibitem{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1):151--175, 2010.

\bibitem{bohdal2020flexible}
Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock {\em arXiv preprint arXiv:2006.08572}, 2020.

\bibitem{castro2018end}
Francisco~M Castro, Manuel~J Mar{\'\i}n-Jim{\'e}nez, Nicol{\'a}s Guil, Cordelia
  Schmid, and Karteek Alahari.
\newblock End-to-end incremental learning.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 233--248, 2018.

\bibitem{cazenavette2022dataset}
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei~A Efros, and
  Jun-Yan Zhu.
\newblock Dataset distillation by matching training trajectories.
\newblock {\em arXiv preprint arXiv:2203.11932}, 2022.

\bibitem{chen2022bidirectional}
Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates.
\newblock Bidirectional learning for offline infinite-width model-based
  optimization.
\newblock In {\em Thirty-Sixth Conference on Neural Information Processing
  Systems}, 2022.

\bibitem{chen2012super}
Yutian Chen, Max Welling, and Alex Smola.
\newblock Super-samples from kernel herding.
\newblock {\em arXiv preprint arXiv:1203.3472}, 2012.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dong2022privacy}
Tian Dong, Bo~Zhao, and Lingjuan Lyu.
\newblock Privacy for free: How does dataset condensation help privacy?
\newblock {\em arXiv preprint arXiv:2206.00240}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{farahani2009facility}
Reza~Zanjirani Farahani and Masoud Hekmatfar.
\newblock {\em Facility location: concepts, models, algorithms and case
  studies}.
\newblock Springer Science \& Business Media, 2009.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{gou2021knowledge}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{har2007smaller}
Sariel Har-Peled and Akash Kushal.
\newblock Smaller coresets for k-median and k-means clustering.
\newblock {\em Discrete \& Computational Geometry}, 37(1):3--19, 2007.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2019robustness}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{huang2017arbitrary}
Xun Huang and Serge Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance
  normalization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1501--1510, 2017.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{jing2020dynamic}
Yongcheng Jing, Xiao Liu, Yukang Ding, Xinchao Wang, Errui Ding, Mingli Song,
  and Shilei Wen.
\newblock Dynamic instance normalization for arbitrary style transfer.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4369--4376, 2020.

\bibitem{Jing_2018_ECCV}
Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng, Yizhou Yu, Dacheng Tao, and
  Mingli Song.
\newblock Stroke controllable fast style transfer with adaptive receptive
  fields.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18661--18673, 2020.

\bibitem{kim2022dataset}
Jang-Hyun Kim, Jinuk Kim, Seong~Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun
  Jeong, Jung-Woo Ha, and Hyun~Oh Song.
\newblock Dataset condensation via efficient synthetic-data parameterization.
\newblock {\em arXiv preprint arXiv:2205.14959}, 2022.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2022dataset}
Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon.
\newblock Dataset condensation with contrastive signals.
\newblock {\em arXiv preprint arXiv:2202.02916}, 2022.

\bibitem{HuihuiAAAI21}
Huihui Liu, Yiding Yang, and Xinchao Wang.
\newblock Overcoming catastrophic forgetting in graph neural networks.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2021.

\bibitem{SonghuaECCV22}
Songhua Liu, Jingwen Ye, Sucheng Ren, and Xinchao Wang.
\newblock Dynast: Dynamic sparse transformer for exemplar-guided image
  generation.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  2022.

\bibitem{masarczyk2020reducing}
Wojciech Masarczyk and Ivona Tautkute.
\newblock Reducing catastrophic forgetting with learning on synthetic data.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), Workshop}, 2020.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{nguyen2020dataset}
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock {\em arXiv preprint arXiv:2011.00050}, 2020.

\bibitem{nguyen2021dataset}
Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{redmon2018yolov3}
Joseph Redmon and Ali Farhadi.
\newblock Yolov3: An incremental improvement.
\newblock {\em arXiv preprint arXiv:1804.02767}, 2018.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{SuchengCVPR22}
Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang.
\newblock Shunted self-attention via multi-scale token aggregation.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem{riba2020kornia}
Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski.
\newblock Kornia: an open source differentiable computer vision library for
  pytorch.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision}, pages 3674--3683, 2020.

\bibitem{rosasco2021distilled}
Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo Lomonaco, and Davide
  Bacciu.
\newblock Distilled replay: Overcoming forgetting through synthetic samples.
\newblock {\em arXiv preprint arXiv:2103.15851}, 2021.

\bibitem{sangermano2022sample}
Mattia Sangermano, Antonio Carta, Andrea Cossu, and Davide Bacciu.
\newblock Sample condensation in online continual learning.
\newblock In {\em Proceedings of the International Joint Conference on Neural
  Networks (IJCNN)}, pages 1--8, 2022.

\bibitem{sener2017active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock {\em arXiv preprint arXiv:1708.00489}, 2017.

\bibitem{shelhamer2017fully}
Evan Shelhamer, Jonathan Long, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  39(4):640--651, 2017.

\bibitem{shokri2015privacy}
Reza Shokri and Vitaly Shmatikov.
\newblock Privacy-preserving deep learning.
\newblock In {\em Proceedings of the 22nd ACM SIGSAC conference on computer and
  communications security}, pages 1310--1321, 2015.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{sucholutsky2021soft}
Ilia Sucholutsky and Matthias Schonlau.
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock In {\em 2021 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi Tachet~des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock {\em arXiv preprint arXiv:1812.05159}, 2018.

\bibitem{tsang2005core}
Ivor~W Tsang, James~T Kwok, Pak-Ming Cheung, and Nello Cristianini.
\newblock Core vector machines: Fast svm training on very large data sets.
\newblock {\em Journal of Machine Learning Research}, 6(4), 2005.

\bibitem{van2018representation}
Aaron Van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv e-prints}, pages arXiv--1807, 2018.

\bibitem{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of machine learning research}, 9(11), 2008.

\bibitem{wang2022cafe}
Kai Wang, Bo~Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang,
  Hakan Bilen, Xinchao Wang, and Yang You.
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock {\em arXiv preprint arXiv:2203.01531}, 2022.

\bibitem{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv:1804.03209}, 2018.

\bibitem{wiewel2021soft}
Felix Wiewel and Bin Yang.
\newblock Condensed composite memory continual learning.
\newblock In {\em Proceedings of the International Joint Conference on Neural
  Networks (IJCNN)}, pages 1--8, 2021.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{xie2021segformer}
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose~M Alvarez, and Ping
  Luo.
\newblock Segformer: Simple and efficient design for semantic segmentation with
  transformers.
\newblock {\em arXiv preprint arXiv:2105.15203}, 2021.

\bibitem{XingyiECCV22}
Xingyi Yang, Jingwen Ye, and Xinchao Wang.
\newblock Factorizing knowledge in neural networks.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  2022.

\bibitem{YidingNIPS20}
Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang.
\newblock Factorizable graph convolutional networks.
\newblock In {\em Conference on Neural Information Processing Systems}, 2020.

\bibitem{YidingCVPR20}
Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang.
\newblock Distilling knowledge from graph convolutional networks.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2020.

\bibitem{zhao2021dataset}
Bo~Zhao and Hakan Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  12674--12685. PMLR, 2021.

\bibitem{zhao2021distribution}
Bo~Zhao and Hakan Bilen.
\newblock Dataset condensation with distribution matching.
\newblock {\em arXiv preprint arXiv:2110.04181}, 2021.

\bibitem{zhao2020dataset}
Bo~Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock {\em arXiv preprint arXiv:2006.05929}, 2020.

\bibitem{zhou2022dataset}
Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.
\newblock Dataset distillation using neural feature regression.
\newblock {\em arXiv preprint arXiv:2206.00719}, 2022.

\end{thebibliography}
