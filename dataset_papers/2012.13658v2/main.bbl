\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{NIPS2017_7090}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  5048--5058. Curran Associates,
  Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf}.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  9--16, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  263--272. JMLR. org, 2017.

\bibitem[Bartlett \& Tewari(2007)Bartlett and Tewari]{bartlett2007sample}
Bartlett, P.~L. and Tewari, A.
\newblock Sample complexity of policy search with known dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  97--104, 2007.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Ciosek et~al.(2019)Ciosek, Vuong, Loftin, and
  Hofmann]{ciosek2019better}
Ciosek, K., Vuong, Q., Loftin, R., and Hofmann, K.
\newblock Better exploration with optimistic actor-critic.
\newblock \emph{arXiv preprint arXiv:1910.12807}, 2019.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{colas2018gep}
Colas, C., Sigaud, O., and Oudeyer, P.-Y.
\newblock {GEP}-{PG}: Decoupling exploration and exploitation in deep
  reinforcement learning algorithms.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
   1039--1048, 2018.

\bibitem[Dabney et~al.(2020)Dabney, Ostrovski, and
  Barreto]{dabney2020temporally}
Dabney, W., Ostrovski, G., and Barreto, A.
\newblock Temporally-extended, epsilon-greedy exploration.
\newblock \emph{arXiv preprint arXiv:2006.01782}, 2020.

\bibitem[de~Gennes(1979)]{de1979scaling}
de~Gennes, P.-G.
\newblock \emph{Scaling concepts in polymer physics}.
\newblock Cornel University Press, 1979.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, Peters,
  et~al.]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  2\penalty0 (1--2):\penalty0 1--142, 2013.

\bibitem[Doi \& Edwards(1988)Doi and Edwards]{doi1988theory}
Doi, M. and Edwards, S.~F.
\newblock \emph{The theory of polymer dynamics}, volume~73.
\newblock oxford university press, 1988.

\bibitem[Fortunato et~al.(2017)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, et~al.]{fortunato2017noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., et~al.
\newblock Noisy networks for exploration.
\newblock \emph{arXiv preprint arXiv:1706.10295}, 2017.

\bibitem[Fu et~al.(2017)Fu, Co-Reyes, and Levine]{fu2017ex2}
Fu, J., Co-Reyes, J., and Levine, S.
\newblock Ex2: Exploration with exemplar models for deep reinforcement
  learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2577--2587, 2017.

\bibitem[Gupta et~al.(2018)Gupta, Mendonca, Liu, Abbeel, and
  Levine]{gupta2018meta}
Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5302--5311, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870, 2018.

\bibitem[Hare(2019)]{hare2019dealing}
Hare, J.
\newblock Dealing with sparse rewards in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.09281}, 2019.

\bibitem[Houthooft et~al.(2016{\natexlab{a}})Houthooft, Chen, Chen, Duan,
  Schulman, Turck, and Abbeel]{VIME}
Houthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., Turck, F.~D., and
  Abbeel, P.
\newblock {VIME:} variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pp.\  1109--1117, 2016{\natexlab{a}}.

\bibitem[Houthooft et~al.(2016{\natexlab{b}})Houthooft, Chen, Duan, Schulman,
  De~Turck, and Abbeel]{houthooft2016vime}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel, P.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1109--1117, 2016{\natexlab{b}}.

\bibitem[Kang et~al.(2018)Kang, Jie, and Feng]{kang2018policy}
Kang, B., Jie, Z., and Feng, J.
\newblock Policy optimization with demonstrations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2469--2478, 2018.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Kober, J., Bagnell, J.~A., and Peters, J.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Sharma, and
  Ravindran]{lakshminarayanan2017dynamic}
Lakshminarayanan, A.~S., Sharma, S., and Ravindran, B.
\newblock Dynamic action repetition for deep reinforcement learning.
\newblock In \emph{Proceedings of the Thirty-First AAAI Conference on
  Artificial Intelligence}, pp.\  2133--2139, 2017.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lopes et~al.(2012)Lopes, Lang, Toussaint, and
  Oudeyer]{lopes2012exploration}
Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y.
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  206--214, 2012.

\bibitem[Morimoto \& Doya(2001)Morimoto and Doya]{morimoto2001acquisition}
Morimoto, J. and Doya, K.
\newblock Acquisition of stand-up behavior by a real robot using hierarchical
  reinforcement learning.
\newblock \emph{Robotics and Autonomous Systems}, 36\penalty0 (1):\penalty0
  37--51, 2001.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[Ng \& Jordan(2000)Ng and Jordan]{ng2000pegasus}
Ng, A.~Y. and Jordan, M.
\newblock Pegasus: a policy search method for large mdps and pomdps.
\newblock In \emph{Proceedings of the Sixteenth conference on Uncertainty in
  artificial intelligence}, pp.\  406--415, 2000.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, van~den Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2721--2730. JMLR. org, 2017.

\bibitem[Perez \& Silander(2017)Perez and Silander]{perez2017non}
Perez, J. and Silander, T.
\newblock Non-markovian control with gated end-to-end memory policy networks.
\newblock \emph{arXiv preprint arXiv:1705.10993}, 2017.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  and othere]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  and othere.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Plappert et~al.(2018)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2018parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Rubenstein \& Colby(2003)Rubenstein and Colby]{rubenstein2003polymer}
Rubenstein, M. and Colby, R.
\newblock Polymer physics: Oxford university press, 2003.

\bibitem[Sehnke et~al.(2010)Sehnke, Osendorfer, R{\"u}ckstie{\ss}, Graves,
  Peters, and Schmidhuber]{sehnke2010parameter}
Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J., and
  Schmidhuber, J.
\newblock Parameter-exploring policy gradients.
\newblock \emph{Neural Networks}, 23\penalty0 (4):\penalty0 551--559, 2010.

\bibitem[Sharma et~al.(2017)Sharma, Lakshminarayanan, and
  Ravindran]{sharma2017learning}
Sharma, S., Lakshminarayanan, A.~S., and Ravindran, B.
\newblock Learning to repeat: Fine grained action repetition for deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1702.06054}, 2017.

\bibitem[Strehl \& Littman(2004)Strehl and Littman]{strehl2004exploration}
Strehl, A. and Littman, M.
\newblock Exploration via model based interval estimation.
\newblock In \emph{International Conference on Machine Learning}. Citeseer,
  2004.

\bibitem[Ta{\"\i}ga et~al.(2019)Ta{\"\i}ga, Fedus, Machado, Courville, and
  Bellemare]{taiga2019benchmarking}
Ta{\"\i}ga, A.~A., Fedus, W., Machado, M.~C., Courville, A., and Bellemare,
  M.~G.
\newblock Benchmarking bonus-based exploration methods on the arcade learning
  environment.
\newblock \emph{arXiv preprint arXiv:1908.02388}, 2019.

\bibitem[Theodorou et~al.(2010)Theodorou, Buchli, and
  Schaal]{theodorou2010generalized}
Theodorou, E., Buchli, J., and Schaal, S.
\newblock A generalized path integral control approach to reinforcement
  learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Nov):\penalty0 3137--3181, 2010.

\bibitem[Uhlenbeck \& Ornstein(1930)Uhlenbeck and
  Ornstein]{uhlenbeck1930theory}
Uhlenbeck, G.~E. and Ornstein, L.~S.
\newblock On the theory of the brownian motion.
\newblock \emph{Physical review}, 36\penalty0 (5):\penalty0 823, 1930.

\bibitem[Wang et~al.(2020)Wang, Dong, Chen, and Wang]{Wang2020Q-learning}
Wang, Y., Dong, K., Chen, X., and Wang, L.
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Wawrzy{\'n}ski(2009)]{wawrzynski2009real}
Wawrzy{\'n}ski, P.
\newblock Real-time reinforcement learning by sequential actor--critics and
  experience replay.
\newblock \emph{Neural Networks}, 22\penalty0 (10):\penalty0 1484--1497, 2009.

\bibitem[White \& White(2010)White and White]{white2010interval}
White, M. and White, A.
\newblock Interval estimation for reinforcement-learning algorithms in
  continuous-state domains.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2433--2441, 2010.

\bibitem[Xu et~al.(2018)Xu, Liu, Zhao, and Peng]{xu2018learning}
Xu, T., Liu, Q., Zhao, L., and Peng, J.
\newblock Learning to explore with meta-policy gradient.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~80, pp.\  5459--5468, 2018.

\end{thebibliography}
