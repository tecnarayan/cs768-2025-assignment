\begin{thebibliography}{10}

\bibitem{angluin1988learning}
Dana Angluin and Philip Laird.
\newblock Learning from noisy examples.
\newblock {\em Machine Learning}, 2(4):343--370, 1988.

\bibitem{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em NeurIPS}, pages 6240--6249, 2017.

\bibitem{bartlett2006convexity}
Peter~L Bartlett, Michael~I Jordan, and Jon~D McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{blanchard2010semi}
Gilles Blanchard, Gyemin Lee, and Clayton Scott.
\newblock Semi-supervised novelty detection.
\newblock {\em Journal of Machine Learning Research}, 11(Nov):2973--3009, 2010.

\bibitem{boucheron2005theory}
St{\'e}phane Boucheron, Olivier Bousquet, and G{\'a}bor Lugosi.
\newblock Theory of classification: A survey of some recent advances.
\newblock {\em ESAIM: probability and statistics}, 9:323--375, 2005.

\bibitem{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem{cheng2017learning}
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao.
\newblock Learning with bounded instance-and label-dependent label noise.
\newblock {\em arXiv preprint arXiv:1709.03768}, 2017.

\bibitem{goldberger2016training}
Jacob Goldberger and Ehud Ben-Reuven.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In {\em ICLR}, 2017.

\bibitem{golowich2018size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In {\em COLT}, pages 297--299, 2018.

\bibitem{gretton2009covariate}
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten
  Borgwardt, and Bernhard Sch{\"o}lkopf.
\newblock Covariate shift by kernel mean matching.
\newblock {\em Dataset shift in machine learning}, pages 131--160, 2009.

\bibitem{guo2018curriculumnet}
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew~R
  Scott, and Dinglong Huang.
\newblock Curriculumnet: Weakly supervised learning from large-scale web
  images.
\newblock In {\em ECCV}, pages 135--150, 2018.

\bibitem{han2018masking}
Bo~Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya~Zhang, and
  Masashi Sugiyama.
\newblock Masking: A new perspective of noisy supervision.
\newblock In {\em NeurIPS}, pages 5836--5846, 2018.

\bibitem{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In {\em NeurIPS}, pages 8527--8537, 2018.

\bibitem{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock {MentorNet}: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In {\em ICML}, pages 2309--2318, 2018.

\bibitem{kawaguchi2017generalization}
Kenji Kawaguchi, Leslie~Pack Kaelbling, and Yoshua Bengio.
\newblock Generalization in deep learning.
\newblock {\em arXiv preprint arXiv:1710.05468}, 2017.

\bibitem{kremer2018robust}
Jan Kremer, Fei Sha, and Christian Igel.
\newblock Robust active label correction.
\newblock In {\em AISTATS}, pages 308--316, 2018.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{LeCunmnist}
Yann LeCun, Corinna Cortes, and Christopher~J.C. Burges.
\newblock The {MNIST} database of handwritten digits.
\newblock {\em http://yann.lecun.com/exdb/mnist/}.

\bibitem{ledoux2013probability}
Michel Ledoux and Michel Talagrand.
\newblock {\em Probability in Banach Spaces: isoperimetry and processes}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{li2017learning}
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
  Li.
\newblock Learning from noisy labels with distillation.
\newblock In {\em ICCV}, pages 1910--1918, 2017.

\bibitem{liu2016classification}
Tongliang Liu and Dacheng Tao.
\newblock Classification with noisy labels by importance reweighting.
\newblock {\em IEEE Transactions on pattern analysis and machine intelligence},
  38(3):447--461, 2016.

\bibitem{ma2018dimensionality}
Xingjun Ma, Yisen Wang, Michael~E Houle, Shuo Zhou, Sarah~M Erfani, Shu-Tao
  Xia, Sudanthi Wijewickrema, and James Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In {\em ICML}, pages 3361--3370, 2018.

\bibitem{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock In {\em NeurIPS}, pages 960--970, 2017.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of Machine Learning}.
\newblock MIT Press, 2018.

\bibitem{natarajan2013learning}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock In {\em NeurIPS}, pages 1196--1204, 2013.

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em NeurIPS}, pages 5947--5956, 2017.

\bibitem{neyshabur2018pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-{B}ayesian approach to spectrally-normalized margin bounds
  for neural networks.
\newblock In {\em ICLR}, 2018.

\bibitem{northcuttlearning}
Curtis~G Northcutt, Tailin Wu, and Isaac~L Chuang.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock In {\em UAI}, 2017.

\bibitem{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In {\em CVPR}, pages 1944--1952, 2017.

\bibitem{ramaswamy2016mixture}
Harish Ramaswamy, Clayton Scott, and Ambuj Tewari.
\newblock Mixture proportion estimation via kernel embeddings of distributions.
\newblock In {\em ICML}, pages 2052--2060, 2016.

\bibitem{reed2014training}
Scott~E Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock In {\em ICLR}, 2015.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em ICML}, pages 4331--4340, 2018.

\bibitem{scott2012calibrated}
Clayton Scott.
\newblock Calibrated asymmetric surrogate losses.
\newblock {\em Electronic Journal of Statistics}, 6:958--992, 2012.

\bibitem{scott2015rate}
Clayton Scott.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In {\em AISTATS}, pages 838--846, 2015.

\bibitem{scott2013classification}
Clayton Scott, Gilles Blanchard, and Gregory Handy.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In {\em COLT}, pages 489--511, 2013.

\bibitem{tanaka2018joint}
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In {\em CVPR}, pages 5552--5560, 2018.

\bibitem{thekumparampil2018robustness}
Kiran~K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh.
\newblock Robustness of conditional gans to noisy labels.
\newblock In {\em NeurIPS}, pages 10271--10282, 2018.

\bibitem{vahdat2017toward}
Arash Vahdat.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In {\em NeurIPS}, pages 5596--5605, 2017.

\bibitem{vandermeulen2016operator}
Robert~A Vandermeulen and Clayton~D Scott.
\newblock An operator theoretic approach to nonparametric mixture models.
\newblock {\em arXiv preprint arXiv:1607.00071}, 2016.

\bibitem{vandermeulen2019operator}
Robert~A Vandermeulen and Clayton~D Scott.
\newblock An operator theoretic approach to nonparametric mixture models.
\newblock {\em accepted to The Annals of Statistics}, 2019.

\bibitem{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem{veit2017learning}
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge
  Belongie.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In {\em CVPR}, pages 839--847, 2017.

\bibitem{xiao2015learning}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In {\em CVPR}, pages 2691--2699, 2015.

\bibitem{yu2019does}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor~W Tsang, and Masashi
  Sugiyama.
\newblock How does disagreement benefit co-teaching?
\newblock In {\em ICML}, 2019.

\bibitem{yu2018efficient}
Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao.
\newblock An efficient and provable approach for mixture proportion estimation
  using linear independence assumption.
\newblock In {\em CVPR}, pages 4480--4489, 2018.

\bibitem{yu2018learning}
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao.
\newblock Learning with biased complementary labels.
\newblock In {\em ECCV}, pages 68--83, 2018.

\bibitem{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em ICLR}, 2017.

\bibitem{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In {\em NeurIPS}, pages 8778--8788, 2018.

\end{thebibliography}
