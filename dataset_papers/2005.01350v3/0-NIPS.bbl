\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2016)Bahdanau, Brakel, Xu, Goyal, Lowe, Pineau,
  Courville, and Bengio]{bahdanau2016actor}
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle
  Pineau, Aaron Courville, and Yoshua Bengio.
\newblock An actor-critic algorithm for sequence prediction.
\newblock \emph{arXiv preprint arXiv:1607.07086}, 2016.

\bibitem[{Barto} et~al.(1983){Barto}, {Sutton}, and
  {Anderson}]{barto1983neuronlike}
A.~G. {Barto}, R.~S. {Sutton}, and C.~W. {Anderson}.
\newblock Neuronlike adaptive elements that can solve difficult learning
  control problems.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics},
  SMC-13\penalty0 (5):\penalty0 834--846, 1983.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:1806.02450}, 2018.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and
  Lee]{bhatnagar2009natural}
Shalabh Bhatnagar, Richard~S Sutton, Mohammad Ghavamzadeh, and Mark Lee.
\newblock Natural actor--critic algorithms.
\newblock \emph{Automatica}, 45\penalty0 (11):\penalty0 2471--2482, 2009.

\bibitem[Borkar(1997)]{borkar1997stochastic}
Vivek~S Borkar.
\newblock Stochastic approximation with two time scales.
\newblock \emph{Systems \& Control Letters}, 29\penalty0 (5):\penalty0
  291--294, 1997.

\bibitem[Borkar and Konda(1997)]{borkar1997actor}
Vivek~S Borkar and Vijaymohan~R Konda.
\newblock The actor-critic algorithm as multi-time-scale stochastic
  approximation.
\newblock \emph{Sadhana}, 22\penalty0 (4):\penalty0 525--543, 1997.

\bibitem[Cai et~al.(2019)Cai, Yang, Jin, and Wang]{cai2019provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv preprint arXiv:1912.05830}, 2019.

\bibitem[Castro and Meir(2010)]{castro2010convergent}
Dotan~Di Castro and Ron Meir.
\newblock A convergent online single time scale actor critic algorithm.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Jan):\penalty0 367--410, 2010.

\bibitem[Cen et~al.(2020)Cen, Cheng, Chen, Wei, and Chi]{cen2020fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{arXiv preprint arXiv:2007.06558}, 2020.

\bibitem[Chen et~al.(2019)Chen, Zhang, Doan, Maguluri, and
  Clarke]{chen2019performance}
Zaiwei Chen, Sheng Zhang, Thinh~T Doan, Siva~Theja Maguluri, and John-Paul
  Clarke.
\newblock Performance of q-learning with linear function approximation:
  Stability and finite-time analysis.
\newblock \emph{arXiv preprint arXiv: 1905.11425}, 2019.

\bibitem[Dalal et~al.(2017)Dalal, Szorenyi, Thoppe, and
  Mannor]{dalal2017finite}
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.05376}, 2017.

\bibitem[Gupta et~al.(2019)Gupta, Srikant, and Ying]{gupta2019finite}
Harsh Gupta, R~Srikant, and Lei Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4706--4715, 2019.

\bibitem[Hu and Syed(2019)]{hu2019characterizing}
Bin Hu and Usman Syed.
\newblock Characterizing the exact behaviors of temporal difference learning
  algorithms using markov jump linear system theory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8477--8488, 2019.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Kaledin et~al.(2020)Kaledin, Moulines, Naumov, Tadic, and
  Wai]{kaledin2020finite}
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai.
\newblock Finite time analysis of linear two-timescale stochastic approximation
  with markovian noise.
\newblock \emph{arXiv preprint arXiv:2002.01268}, 2020.

\bibitem[Konda and Tsitsiklis(2000)]{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1008--1014, 2000.

\bibitem[Konda et~al.(2004)Konda, Tsitsiklis, et~al.]{konda2004convergence}
Vijay~R Konda, John~N Tsitsiklis, et~al.
\newblock Convergence rate of linear two-time-scale stochastic approximation.
\newblock \emph{The Annals of Applied Probability}, 14\penalty0 (2):\penalty0
  796--819, 2004.

\bibitem[Kumar et~al.(2019)Kumar, Koppel, and Ribeiro]{kumar2019sample}
Harshat Kumar, Alec Koppel, and Alejandro Ribeiro.
\newblock On the sample complexity of actor-critic method for reinforcement
  learning with function approximation.
\newblock \emph{arXiv preprint arXiv:1910.08412}, 2019.

\bibitem[Mitrophanov(2005)]{mitrophanov2005sensitivity}
A~Yu Mitrophanov.
\newblock Sensitivity and convergence of uniformly ergodic markov chains.
\newblock \emph{Journal of Applied Probability}, 42\penalty0 (4):\penalty0
  1003--1014, 2005.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Yurii Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and
  Restelli]{papini2018stochastic}
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello
  Restelli.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pages
  4023--4032, 2018.

\bibitem[Qiu et~al.(2019)Qiu, Yang, Ye, and Wang]{qiu2019finite}
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang.
\newblock On the finite-time convergence of actor-critic algorithm.
\newblock \emph{NeurIPS 2019 Optimization Foundations of Reinforcement Learning
  Workshop}, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, volume~37,
  pages 1889--1897, 2015.

\bibitem[Srikant and Ying(2019)]{srikant2019finite}
R~Srikant and Lei Ying.
\newblock Finite-time error bounds for linear stochastic approximation andtd
  learning.
\newblock In \emph{Conference on Learning Theory}, pages 2803--2830, 2019.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1057--1063, 2000.

\bibitem[Tadic and Meyn(2003)]{tadic2003asymptotic}
Vladislav~B Tadic and Sean~P Meyn.
\newblock Asymptotic properties of two time-scale stochastic approximation
  algorithms with constant step sizes.
\newblock In \emph{Proceedings of the 2003 American Control Conference, 2003.},
  volume~5, pages 4426--4431. IEEE, 2003.

\bibitem[Wang et~al.(2020)Wang, Cai, Yang, and Wang]{wang2020neural}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang et~al.(2016)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{wang2016sample}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2016.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Wiering(2004)]{wiering2004convergence}
Marco~A Wiering.
\newblock Convergence and divergence in standard and averaging reinforcement
  learning.
\newblock In \emph{European Conference on Machine Learning}, pages 477--488.
  Springer, 2004.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Xu and Gu(2019)]{xu2019finite}
Pan Xu and Quanquan Gu.
\newblock A finite-time analysis of q-learning with neural network function
  approximation.
\newblock \emph{arXiv preprint arXiv:1912.04511}, 2019.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Gao, and Gu]{xu2019improved}
Pan Xu, Felicia Gao, and Quanquan Gu.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In \emph{International Conference on Uncertainty in Artificial
  Intelligence}, 2019{\natexlab{a}}.

\bibitem[Xu et~al.(2020{\natexlab{a}})Xu, Gao, and Gu]{xu2020sample}
Pan Xu, Felicia Gao, and Quanquan Gu.
\newblock Sample efficient policy gradient methods with recursive variance
  reduction.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Zou, and Liang]{xu2019two}
Tengyu Xu, Shaofeng Zou, and Yingbin Liang.
\newblock Two time-scale off-policy td learning: Non-asymptotic analysis over
  markovian samples.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10633--10643, 2019{\natexlab{b}}.

\bibitem[Xu et~al.(2020{\natexlab{b}})Xu, Wang, and Liang]{xu2020non}
Tengyu Xu, Zhe Wang, and Yingbin Liang.
\newblock Non-asymptotic convergence analysis of two time-scale (natural)
  actor-critic algorithms.
\newblock \emph{arXiv preprint arXiv:2005.03557}, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2018)Yang, Zhang, Hong, and Ba{\c{s}}ar]{yang2018finite}
Zhuoran Yang, Kaiqing Zhang, Mingyi Hong, and Tamer Ba{\c{s}}ar.
\newblock A finite sample analysis of the actor-critic algorithm.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)}, pages
  2759--2764. IEEE, 2018.

\bibitem[Yang et~al.(2019)Yang, Chen, Hong, and Wang]{yang2019global}
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang.
\newblock On the global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Koppel, Zhu, and
  Ba{\c{s}}ar]{zhang2019global}
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba{\c{s}}ar.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock \emph{arXiv preprint arXiv:1906.08383}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Liu, Yao, and
  Whiteson]{zhang2019provably}
Shangtong Zhang, Bo~Liu, Hengshuai Yao, and Shimon Whiteson.
\newblock Provably convergent two-timescale off-policy actor-critic with
  function approximation.
\newblock \emph{arXiv}, pages arXiv--1911, 2019{\natexlab{b}}.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8665--8675, 2019.

\end{thebibliography}
