@article{vargaftik2021drive,
  title={DRIVE: One-bit Distributed Mean Estimation},
  author={Vargaftik, Shay and Basat, Ran Ben and Portnoy, Amit and Mendelson, Gal and Ben-Itzhak, Yaniv and Mitzenmacher, Michael},
  journal={arXiv preprint arXiv:2105.08339},
  year={2021}
}

@article{fino1976unified,
  title={Unified matrix treatment of the fast Walsh-Hadamard transform},
  author={Fino, Bernard J. and Algazi, V. Ralph},
  journal={IEEE Transactions on Computers},
  volume={25},
  number={11},
  pages={1142--1146},
  year={1976},
  publisher={IEEE Computer Society Washington, DC, USA}
}
@inproceedings{NEURIPS2019_093f65e0,
 author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, zhifeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}},
 url = {https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{vargaftik2021communication,
  title={Communication-Efficient Federated Learning via Robust Distributed Mean Estimation},
  author={Vargaftik, Shay and Basat, Ran Ben and Portnoy, Amit and Mendelson, Gal and Ben-Itzhak, Yaniv and Mitzenmacher, Michael},
  journal={arXiv preprint arXiv:2108.08842},
  year={2021}
}


@inproceedings{NEURIPS2018_b440509a,
 author = {Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Sparsified SGD with Memory}},
 url = {https://proceedings.neurips.cc/paper/2018/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf},
 volume = {31},
 year = {2018}
}



@inproceedings{NIPS2012_6aca9700,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Large Scale Distributed Deep Networks}},
 url = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{shoeybi2019megatron,
  title={{Megatron-lm: Training multi-billion parameter language models using model parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}



@INPROCEEDINGS{icde2021,
   author={Alfassi, Yuval and Gabel, Moshe and Yehuda, Gal and Keren, Daniel},
   booktitle={2021 IEEE 37th International Conference on Data Engineering ({ICDE})},
   title = {A {D}istance-{B}ased {S}cheme for {R}educing {B}andwidth in {D}istributed {G}eometric {M}onitoring},
   year={2021},
}

@article{beznosikov2020biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={arXiv preprint arXiv:2002.12410},
  year={2020}
}

@article{iaab006,
  title={{Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor}},
  author={Safaryan, Mher and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.08958},
  year={2020}
}

@article{szablowski1998uniform,
  title={{Uniform Distributions on Spheres in Finite Dimensional $L_\alpha$ and Their Generalizations}},
  author={Szab{\l}owski, Pawe{\l} J},
  journal={Journal of multivariate analysis},
  volume={64},
  number={2},
  pages={103--117},
  year={1998},
  publisher={Elsevier}
}

@article{muller1959note,
  title={{A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres}},
  author={Muller, Mervin E},
  journal={Communications of the ACM},
  volume={2},
  number={4},
  pages={19--20},
  year={1959},
  publisher={ACM New York, NY, USA}
}

@article{safaryan2020uncertainty,
  title={{Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor}},
  author={Safaryan, Mher and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.08958},
  year={2020}
}

@article{lyubarskii2010uncertainty,
  title={{Uncertainty Principles and Vector Quantization}},
  author={Lyubarskii, Yurii and Vershynin, Roman},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={7},
  pages={3491--3501},
  year={2010},
  publisher={IEEE}
}

@misc{reddi2020adaptive,
      title={Adaptive {F}ederated {O}ptimization}, 
      author={Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Konečný and Sanjiv Kumar and H. Brendan McMahan},
      year={2020},
      eprint={2003.00295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{horadam2012hadamard,
  title={{Hadamard Matrices and Their Applications}},
  author={Horadam, Kathy J},
  year={2012},
  publisher={Princeton university press}
}

@article{ailon2009fast,
  title={{The Fast Johnson--Lindenstrauss Transform and Approximate Nearest Neighbors}},
  author={Ailon, Nir and Chazelle, Bernard},
  journal={SIAM Journal on computing},
  volume={39},
  number={1},
  pages={302--322},
  year={2009},
  publisher={SIAM}
}

@article{gronlund2017fast,
  title={{Fast Exact K-Means, K-Medians and Bregman Divergence Clustering in 1D}},
  author={Gr{\o}nlund, Allan and Larsen, Kasper Green and Mathiasen, Alexander and Nielsen, Jesper Sindahl and Schneider, Stefan and Song, Mingzhou},
  journal={arXiv preprint arXiv:1701.07204},
  year={2017}
}
@article{latala1994best,
  title={{On the Best Constant In the Khinchin-Kahane Inequality}},
  author={Lata{\l}a, Rafa{\l} and Oleszkiewicz, Krzysztof},
  journal={Studia Mathematica},
  volume={109},
  number={1},
  pages={101--104},
  year={1994}
}
@article{filmus2012khintchine,
  title={{Khintchine-Kahane using Fourier Analysis}},
  author={Filmus, Yuval},
  journal={Posted at \url{http://www.cs.toronto.edu/yuvalf/KK.pdf}},
  year={2012},
  publisher={Citeseer}
}

@article{szarek1976best,
  title={{On the Best Constants in the Khinchin Inequality}},
  author={Szarek, S},
  journal={Studia Mathematica},
  volume={2},
  number={58},
  pages={197--208},
  year={1976}
}

@article{khintchine1923dyadische,
  title={{\"U}ber {D}yadische {B}r{\"u}che},
  author={Khintchine, Aleksandr},
  journal={Mathematische Zeitschrift},
  volume={18},
  number={1},
  pages={109--116},
  year={1923},
  publisher={Springer}
}

@inproceedings{charikar2002finding,
  title={Finding frequent items in data streams},
  author={Charikar, Moses and Chen, Kevin and Farach-Colton, Martin},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={693--703},
  year={2002},
  organization={Springer}
}


@incollection{NIPS2019_9015,
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8026--8037},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{stackoverflowdb,
    title = {{Stack Overflow Data}},
    howpublished = {\url{https://www.kaggle.com/stackoverflow/stackoverflow}},
    note = "accessed 01-Mar-21"
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}


@misc{CCASA3,
    title = {{Creative Commons Attribution-Share Alike 3.0 license.}},
    howpublished = {\url{https://creativecommons.org/licenses/by-sa/3.0/}},
    note = "accessed 01-Mar-21"
}


@misc{tensorflowfed,
    author = {Google},
    title = {{TensorFlow Federated: Machine Learning on Decentralized Data}},
    year = {2020},
    howpublished = {\url{https://www.tensorflow.org/federated}},
    note = "accessed 25-Mar-20"
}

@misc{tensorflowmo,
    author = {Google},
    title = {{TensorFlow Model Optimization Toolkit}},
    year = {2021},
    howpublished = {\url{https://www.tensorflow.org/model_optimization}},
    note = "accessed 19-Mar-21"
}

@misc{apache2,
 title = {Apache License, Version 2.0},
 howpublished = {\url{https://www.apache.org/licenses/LICENSE-2.0}},
 note = "accessed 19-Mar-21"
}

@misc{pytorchlicense,
title = {Pytorch license},
howpublished = {\url{https://github.com/pytorch/pytorch/blob/aaccdc39965ade4b61b7852329739e777e244c25/LICENSE}},
note = "accessed 19-Mar-21"
}

@inproceedings{bai2021gradient,
  title={Gradient Compression Supercharged High-Performance Data Parallel DNN Training},
  author={Bai, Youhui and Li, Cheng and Zhou, Quan and Yi, Jun and Gong, Ping and Yan, Feng and Chen, Ruichuan and Xu, Yinlong},
  booktitle={The 28th ACM Symposium on Operating Systems Principles (SOSP 2021)},
  year={2021}
}

@misc{2BSD,
 title = {The 2-Clause BSD License},
 howpublished = {\url{https://opensource.org/licenses/BSD-2-Clause}},
 note = "accessed 19-Mar-21"
}

@misc{openSource,
    author = {Vargaftik, Shay and Basat, Ran Ben and Portnoy, Amit and Mendelson, Gal and Ben-Itzhak, Yaniv and Mitzenmacher, Michael},
    title = {{DRIVE open source code}},
    year = {2021},
howpublished = {\url{https://github.com/amitport/DRIVE-One-bit-Distributed-Mean-Estimation}}
}

@misc{shakespeare,
author = {William Shakespeare},
title = {{The Complete Works of William Shakespeare}},
howpublished = {\url{https://www.gutenberg.org/ebooks/100}}
}

@misc{tensorflowfedkashincode,
    author = {The TensorFlow Authors},
    title = {{TensorFlow Federated: Compression via Kashin's representation from Hadamard transform}},
    howpublished = {\url{https://github.com/tensorflow/model-optimization/blob/9193d70f6e7c9f78f7c63336bd68620c4bc6c2ca/tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research/kashin.py#L92}},
    note = "accessed 16-May-21"
}

@inproceedings{uberHadamard,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  note = {Code available at: \url{https://github.com/uber-research/intrinsic-dimension}},
  year={2018}
}



@misc{tensorflowfedencsum,
    author = {The TensorFlow Authors},
    title = {{TensorFlow Federated: EncodedSumFactory class}},
    howpublished = {\url{https://github.com/tensorflow/federated/blob/v0.19.0/tensorflow_federated/python/aggregators/encoded.py#L40-L142}},
    note = "accessed 19-May-21"
}


@misc{pytorchqrfact,
    author = {Mario Lezcano Casado},
    title = {{GeoTorch’s Documentation. A Library for Constrained Optimization and Manifold Optimization for Deep Learning in Pytorch.}},
    year = {2021},
    howpublished = {\url{https://geotorch.readthedocs.io/en/latest/_modules/geotorch/so.html}},
    note = "accessed 17-May-21"
}




@article{konevcny2015federated,
  title={{Federated Optimization: Distributed Optimization Beyond the Datacenter}},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, Brendan and Ramage, Daniel},
  journal={arXiv preprint arXiv:1511.03575},
  year={2015}
}

@inproceedings{mcmahan2017communication,
  title={{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
  author={H. Brendan McMahan and
              Eider Moore and
              Daniel Ramage and
              Seth Hampson and
              Blaise Ag{\"{u}}era y Arcas},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@misc{caldas2019leaf,
      title={{LEAF: A Benchmark for Federated Settings}}, 
      author={Sebastian Caldas and Sai Meher Karthik Duddu and Peter Wu and Tian Li and Jakub Konečný and H. Brendan McMahan and Virginia Smith and Ameet Talwalkar},
      year={2019},
      eprint={1812.01097},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kairouz2019advances,
    title={{Advances and Open Problems in Federated Learning}},
    author={Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Keith Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
    year={2019},
    eprint={1912.04977},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{bonawitz2019towards,
  title={{Towards Federated Learning at Scale: System Design}},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Konečný, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
  journal={arXiv preprint arXiv:1902.01046},
  year={2019}
}

@inproceedings{bernstein2018signsgd,
  title={{signSGD: Compressed Optimisation for Non-Convex Problems}},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018}
}

@inproceedings{
bernstein2019signsgd,
title={{sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant}},
author={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJxhijAcY7},
}

@inproceedings{li2014scaling,
  title={{Scaling Distributed Machine Learning With the Parameter Server}},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}


@inproceedings{seide20141,
  title={{1-Bit Stochastic Gradient Descent and Its Application to Data-Parallel Distributed Training of Speech DNNs}},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@article{dean2012large,
  title={{Large Scale Distributed Deep Networks}},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1223--1231},
  year={2012}
}

@inproceedings{he2016deep,
  title={{Deep Residual Learning for Image Recognition}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{he2016identity,
  title={{Identity Mappings in Deep Residual Networks}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{Hochreiter1997LongSM,
  title={{Long Short-Term Memory}},
  author={S. Hochreiter and J. Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@article{krizhevsky2009learning,
  title={{Learning Multiple Layers of Features From Tiny Images}},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{krizhevsky2017imagenet,
  title={{Imagenet Classification With Deep Convolutional Neural Networks}},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{russakovsky2015imagenet,
  title={{Imagenet Large Scale Visual Recognition Challenge}},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{wen2017terngrad,
  title={{TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning}},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={1509--1519},
  year={2017}
}

@inproceedings{cohen2017emnist,
  title={{EMNIST: Extending MNIST to Handwritten Letters}},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}


@inproceedings{
lin2018deep,
title={{Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}},
author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SkhQHMW0W},
}

@inproceedings{aji2017sparse,
  title={{Sparse Communication for Distributed Gradient Descent}},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={440--445},
  year={2017}
}

@inproceedings{strom2015scalable,
  title={{Scalable Distributed Dnn Training Using Commodity Gpu Cloud Computing}},
  author={Str{\"o}m, Nikko},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}

@INPROCEEDINGS{659000,

  author={Str{\"o}m, Nikko},

  booktitle={1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings}, 

  title={{A Tonotopic Artificial Neural Network Architecture for Phoneme Probability Estimation}}, 

  year={1997},

  volume={},

  number={},

  pages={156-163},

  doi={10.1109/ASRU.1997.659000}}


@inproceedings{strom1997sparse,
  title={Sparse connection and pruning in large dynamic artificial neural networks},
  author={Str{\"o}m, Nikko},
  booktitle={Fifth European Conference on Speech Communication and Technology},
  year={1997}
}

@INPROCEEDINGS{7835789,

  author={N. {Dryden} and T. {Moon} and S. A. {Jacobs} and B. {Van Essen}},

  booktitle={2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC)}, 

  title={Communication Quantization for Data-Parallel Training of Deep Neural Networks}, 

  year={2016},

  volume={},

  number={},

  pages={1-8},

  doi={10.1109/MLHPC.2016.004}}

@article{NIPS2017_6c340f25,
  title={{QSGD: Communication-Efficient Sgd via Gradient Quantization and Encoding}},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={1709--1720},
  year={2017}
}

@inproceedings{NEURIPS2018_3328bdf9,
 author = {Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {1299--1309},
 publisher = {Curran Associates, Inc.},
 title = {{Gradient Sparsification for Communication-Efficient Distributed Optimization}},
 url = {https://proceedings.neurips.cc/paper/2018/file/3328bdf9a4b9504b9398284244fe97c2-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{karimireddy2019error,
  title={{Error Feedback Fixes SignSGD and other Gradient Compression Schemes}},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019}
}

@inproceedings{WangSLCPW18,
  author={Hongyi Wang and Scott Sievert and Shengchao Liu and Zachary B. Charles and Dimitris S. Papailiopoulos and Stephen Wright},
  title={{ATOMO: Communication-efficient Learning via Atomic Sparsification}},
  year={2018},
  cdate={1514764800000},
  pages={9872-9883},
  url={http://papers.nips.cc/paper/8191-atomo-communication-efficient-learning-via-atomic-sparsification},
  booktitle={NeurIPS},
}

@misc{konecy2017federated,
      title={{Federated Learning: Strategies for Improving Communication Efficiency}}, 
      author={Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtárik and Ananda Theertha Suresh and Dave Bacon},
      year={2017},
      eprint={1610.05492},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
davies2021new,
title={{New Bounds For Distributed Mean Estimation and Variance Reduction}},
author={Peter Davies and Vijaykrishna Gurunanthan and Niusha Moshrefi and Saleh Ashkboos and Dan Alistarh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=t86MwoUCCNe}
}

@inproceedings{NEURIPS2019_d9fbed9d,
 author = {Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 title = {{PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization}},
 volume = {32},
 year = {2019}
}




@article{konevcny2018randomized,
  title={{Randomized Distributed Mean Estimation: Accuracy vs. Communication}},
  author={Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter},
  journal={Frontiers in Applied Mathematics and Statistics},
  volume={4},
  pages={62},
  year={2018},
  publisher={Frontiers}
}

@article{mishchenko2019distributed,
  title={{Distributed Learning With Compressed Gradient Differences}},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}


@inproceedings{pmlr-v70-suresh17a,
  title={{Distributed Mean Estimation With Limited Communication}},
  author={Suresh, Ananda Theertha and Felix, X Yu and Kumar, Sanjiv and McMahan, H Brendan},
  booktitle={International Conference on Machine Learning},
  pages={3329--3337},
  year={2017},
  organization={PMLR}
}

 
@inproceedings{NEURIPS2018_33b3214d,
 author = {Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {9850--9861},
 publisher = {Curran Associates, Inc.},
 title = {{ATOMO: Communication-efficient Learning via Atomic Sparsification}},
 url = {https://proceedings.neurips.cc/paper/2018/file/33b3214d792caf311e1f00fd22b392c5-Paper.pdf},
 volume = {31},
 year = {2018}
}



@INPROCEEDINGS{8852172,

  author={F. {Sattler} and S. {Wiedemann} and K. {Müller} and W. {Samek}},

  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)}, 

  title={{Sparse Binary Compression: Towards Distributed Deep Learning With Minimal Communication}}, 

  year={2019},

  volume={},

  number={},

  pages={1-8},

  doi={10.1109/IJCNN.2019.8852172}}

@ARTICLE{8939562,

  author={S. R. {Dubey} and S. {Chakraborty} and S. K. {Roy} and S. {Mukherjee} and S. K. {Singh} and B. B. {Chaudhuri}},

  journal={IEEE Transactions on Neural Networks and Learning Systems}, 

  title={diffGrad: An Optimization Method for Convolutional Neural Networks}, 

  year={2020},

  volume={31},

  number={11},

  pages={4500-4511},

  doi={10.1109/TNNLS.2019.2955777}}


 @InProceedings{pmlr-v119-fu20c, title = {Don’t Waste Your Bits! {S}queeze Activations and Gradients for Deep Neural Networks via {T}iny{S}cript}, author = {Fu, Fangcheng and Hu, Yuzheng and He, Yihan and Jiang, Jiawei and Shao, Yingxia and Zhang, Ce and Cui, Bin}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {3304--3314}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/fu20c/fu20c.pdf}, url = {http://proceedings.mlr.press/v119/fu20c.html}, abstract = {Recent years have witnessed intensive research interests on training deep neural networks (DNNs) more efficiently by quantization-based compression methods, which facilitate DNNs training in two ways: (1) activations are quantized to shrink the memory consumption, and (2) gradients are quantized to decrease the communication cost. However, existing methods mostly use a uniform mechanism that quantizes the values evenly. Such a scheme may cause a large quantization variance and slow down the convergence in practice. In this work, we introduce TinyScript, which applies a non-uniform quantization algorithm to both activations and gradients. TinyScript models the original values by a family of Weibull distributions and searches for ”quantization knobs” that minimize quantization variance. We also discuss the convergence of the non-uniform quantization algorithm on DNNs with varying depths, shedding light on the number of bits required for convergence. Experiments show that TinyScript always obtains lower quantization variance, and achieves comparable model qualities against full precision training using 1-2 bits less than the uniform-based counterpart.} }
 
 @article{ramezani2019nuqsgd,
  title={{NUQSGD: Improved Communication Efficiency for Data-Parallel SGD via Nonuniform Quantization}},
  author={Ramezani-Kebrya, Ali and Faghri, Fartash and Roy, Daniel M},
  journal={arXiv preprint arXiv:1908.06077},
  year={2019}
}

@article{horvath2019natural,
  title={{Natural Compression for Distributed Deep Learning}},
  author={Horvath, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richtarik, Peter},
  journal={arXiv preprint arXiv:1905.10988},
  year={2019}
}

 @InProceedings{pmlr-v108-mayekar20a, title = {{RATQ: A Universal Fixed-Length Quantizer for Stochastic Optimization}}, author = {Mayekar, Prathamesh and Tyagi, Himanshu}, booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, pages = {1399--1409}, year = {2020}, editor = {Silvia Chiappa and Roberto Calandra}, volume = {108}, series = {Proceedings of Machine Learning Research}, address = {Online}, month = {26--28 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v108/mayekar20a/mayekar20a.pdf}, url = {http://proceedings.mlr.press/v108/mayekar20a.html}, abstract = {We present Rotated Adaptive Tetra-iterated Quantizer (RATQ), afixed-length quantizer for gradients in first order stochasticoptimization. RATQ is easy to implement and involves only a Hadamard transform computation and adaptive uniform quantization with appropriately chosen dynamic ranges. For noisy gradients with almost surely bounded Euclidean norms, we establish an informationtheoretic lower bound for optimization accuracy using finite precisiongradients and show that RATQ almost attains this lower bound. For mean square bounded noisy gradients, we use a gain-shape quantizer which separately quantizes the Euclidean norm and uses RATQ to quantize the normalized unit norm vector. We establish lower bounds for performance of any optimization procedure and shape quantizer, when used with a uniform gain quantizer. Finally, we propose an adaptive quantizer for gain which when used with RATQ for shape quantizer outperforms uniform gain quantization and is, in fact, close to optimal.  } }
 
 @misc{jin2020stochasticsign,
      title={{Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees}}, 
      author={Richeng Jin and Yufan Huang and Xiaofan He and Tianfu Wu and Huaiyu Dai},
      year={2020},
      eprint={2002.10940},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2020distributed,
  title={{Distributed Training With Heterogeneous Data: Bridging Median-and Mean-Based Algorithms}},
  author={Chen, Xiangyi and Chen, Tiancong and Sun, Haoran and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@misc{shlezinger2020uveqfed,
      title={{UVeQFed: Universal Vector Quantization for Federated Learning}}, 
      author={Nir Shlezinger and Mingzhe Chen and Yonina C. Eldar and H. Vincent Poor and Shuguang Cui},
      year={2020},
      eprint={2006.03262},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2020breaking,
  title={{Breaking the Communication-Privacy-Accuracy Trilemma}},
  author={Chen, Wei-Ning and Kairouz, Peter and Ozgur, Ayfer},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{caldas2018expanding,
  title={{Expanding the Reach of Federated Learning by Reducing Client Resource Requirements}},
  author={Caldas, Sebastian and Konečný, Jakub and McMahan, H Brendan and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.07210},
  year={2018}
}

@article{newman1991private,
  title={{Private vs. Common Random Bits in Communication Complexity}},
  author={Newman, Ilan},
  journal={Information processing letters},
  volume={39},
  number={2},
  pages={67--71},
  year={1991}
}

@article{mezzadri2006generate,
  title={{How to Generate Random Matrices From the Classical Compact Groups}},
  author={Mezzadri, Francesco},
  journal={arXiv preprint math-ph/0609050},
  year={2006}
}

@article{wedderburn1975generating,
  title={Generating {R}andom {R}otations},
  author={Wedderburn, RWM},
  journal={Research Report},
  year={1975},
  publisher={Rothamsted Experimental Station}
}

@article{heiberger1978generation,
  title={{Generation of Random Orthogonal Matrices}},
  author={Heiberger, Richard M},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={27},
  number={2},
  pages={199--206},
  year={1978},
  publisher={Wiley Online Library}
}

@article{tanner1982remark,
  title={{Remark as r42: A Remark on as 127. Generation of Random Orthogonal Matrices}},
  author={Tanner, Martin A and Thisted, Ronald A},
  journal={Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume={31},
  number={2},
  pages={190--192},
  year={1982},
  publisher={JSTOR}
}

@article{stewart1980efficient,
  title={{The Efficient Generation of Random Orthogonal Matrices With an Application to Condition Estimators}},
  author={Stewart, Gilbert W},
  journal={SIAM Journal on Numerical Analysis},
  volume={17},
  number={3},
  pages={403--409},
  year={1980},
  publisher={SIAM}
}

@article{ben2020send,
  title={{How to Send a Real Number Using a Single Bit (and Some Shared Randomness)}},
  author={Ben-Basat, Ran and Mitzenmacher, Michael and Vargaftik, Shay},
  journal={arXiv preprint arXiv:2010.02331},
  year={2020}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{herendi1997fast,
  title={{Fast Gaussian Random Number Generation Using Linear Transformations}},
  author={Herendi, Tam{\'a}s and Siegl, Thomas and Tichy, Robert F},
  journal={Computing},
  volume={59},
  number={2},
  pages={163--181},
  year={1997},
  publisher={Springer}
}

@inproceedings{thomas2013parallel,
  title={{Parallel Generation of Gaussian Random Numbers Using the Table-Hadamard Transform}},
  author={Thomas, David B},
  booktitle={2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines},
  pages={161--168},
  year={2013},
  organization={IEEE}
}

@techreport{rader1969new,
  title={{A New Method of Generating Gaussian Random Variables by Computer}},
  author={Rader, Charles M},
  year={1969},
  institution={Massachusetts Institute of Technology, Lincoln Lab}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{bailey1992distributional,
  title={{Distributional Identities of Beta and Chi-Squared Variates: A Geometrical Interpretation}},
  author={Bailey, Ralph W},
  journal={The American Statistician},
  volume={46},
  number={2},
  pages={117--120},
  year={1992},
  publisher={Taylor \& Francis}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{chmiel2020neural,
  title={{Neural Gradients Are Near-Lognormal: Improved Quantized and Sparse Training}},
  author={Chmiel, Brian and Ben-Uri, Liad and Shkolnik, Moran and Hoffer, Elad and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2006.08173},
  year={2020}
}

@inproceedings{ye2020accelerating,
  title={{Accelerating CNN Training by Pruning Activation Gradients}},
  author={Ye, Xucheng and Dai, Pengcheng and Luo, Junyu and Guo, Xin and Qi, Yingjie and Yang, Jianlei and Chen, Yiran},
  booktitle={European Conference on Computer Vision},
  pages={322--338},
  year={2020},
  organization={Springer}
}

@article{banner2018post,
  title={{Post-Training 4-Bit Quantization of Convolution Networks for Rapid-Deployment}},
  author={Banner, Ron and Nahshan, Yury and Hoffer, Elad and Soudry, Daniel},
  journal={arXiv preprint arXiv:1810.05723},
  year={2018}
}

@article{esseen1956moment,
  title={{A Moment Inequality With an Application to the Central Limit Theorem}},
  author={Esseen, Carl-Gustav},
  journal={Scandinavian Actuarial Journal},
  volume={1956},
  number={2},
  pages={160--170},
  year={1956},
  publisher={Taylor \& Francis}
}

@article{bennett1962probability,
  title={{Probability Inequalities for the Sum of Independent Random Variables}},
  author={Bennett, George},
  journal={Journal of the American Statistical Association},
  volume={57},
  number={297},
  pages={33--45},
  year={1962},
  publisher={Taylor \& Francis Group}
}

@article{maurer2003bound,
  title={{A Bound on the Deviation Probability for Sums of Non-Negative Random Variables}},
  author={Maurer, Andreas and others},
  journal={J. Inequalities in Pure and Applied Mathematics},
  volume={4},
  number={1},
  pages={15},
  year={2003}
}

@article{lecun1998gradient,
  title={{Gradient-Based Learning Applied to Document Recognition}},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{ivkin2019communication,
  title={{Communication-Efficient Distributed Sgd With Sketching}},
  author={Ivkin, Nikita and Rothchild, Daniel and Ullah, Enayat and Braverman, Vladimir and Stoica, Ion and Arora, Raman},
  journal={arXiv preprint arXiv:1903.04488},
  year={2019}
}

@inproceedings{rothchild2020fetchsgd,
  title={{FetchSGD: Communication-Efficient Federated Learning With Sketching}},
  author={Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={8253--8265},
  year={2020},
  organization={PMLR}
}

@article{spruill2007asymptotic,
  title={{Asymptotic Distribution of Coordinates on High Dimensional Spheres}},
  author={Spruill, Marcus and others},
  journal={Electronic communications in probability},
  volume={12},
  pages={234--247},
  year={2007},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{stam1982limit,
  title={{Limit Theorems for Uniform Distributions on Spheres in High-Dimensional Euclidean Spaces}},
  author={Stam, Adriaan J},
  journal={Journal of Applied probability},
  volume={19},
  number={1},
  pages={221--228},
  year={1982},
  publisher={Cambridge University Press}
}

@article{rachev1991approximate,
  title={{Approximate Independence of Distributions on Spheres and Their Stability Properties}},
  author={Rachev, Svetlozar T and Ruschendorf, L and others},
  journal={The Annals of Probability},
  volume={19},
  number={3},
  pages={1311--1337},
  year={1991},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{diaconis1987dozen,
  title={{A Dozen de Finetti-Style Results in Search of a Theory}},
  author={Diaconis, Persi and Freedman, David},
  booktitle={Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume={23},
  number={S2},
  pages={397--423},
  year={1987}
}