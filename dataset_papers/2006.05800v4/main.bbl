\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{BVDBS{\etalchar{+}}15}

\bibitem[ABG{\etalchar{+}}20]{amari2020does}
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji
  Suzuki, Denny Wu, and Ji~Xu, \emph{When does preconditioning help or hurt
  generalization?}, arXiv preprint arXiv:2006.10732 (2020).

\bibitem[AKT19]{ali2019continuous}
Alnur Ali, J~Zico Kolter, and Ryan~J Tibshirani, \emph{A continuous-time view
  of early stopping for least squares}, International Conference on Artificial
  Intelligence and Statistics, vol.~22, 2019.

\bibitem[AS17]{advani2017high}
Madhu~S Advani and Andrew~M Saxe, \emph{High-dimensional dynamics of
  generalization error in neural networks}, arXiv preprint arXiv:1710.03667
  (2017).

\bibitem[BES{\etalchar{+}}20]{ba2020generalization}
Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang,
  \emph{Generalization of two-layer neural networks: An asymptotic viewpoint},
  International Conference on Learning Representations, 2020.

\bibitem[BHMM18]{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal, \emph{Reconciling
  modern machine learning and the bias-variance trade-off}, arXiv preprint
  arXiv:1812.11118 (2018).

\bibitem[BHX19]{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu, \emph{Two models of double descent for
  weak features}, arXiv preprint arXiv:1903.07571 (2019).

\bibitem[BLLT19]{bartlett2019benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler,
  \emph{Benign overfitting in linear regression}, arXiv preprint
  arXiv:1906.11300 (2019).

\bibitem[BS99]{bjorkstrom1999generalized}
Anders Bj{\"o}rkstr{\"o}m and Rolf Sundberg, \emph{A generalized view on
  continuum regression}, Scandinavian Journal of Statistics \textbf{26} (1999),
  no.~1, 17--30.

\bibitem[BVDBS{\etalchar{+}}15]{bogdan2015slope}
Ma{\l}gorzata Bogdan, Ewout Van Den~Berg, Chiara Sabatti, Weijie Su, and
  Emmanuel~J Cand{\`e}s, \emph{Slopeâ€”adaptive variable selection via convex
  optimization}, The annals of applied statistics \textbf{9} (2015), no.~3,
  1103.

\bibitem[Cas80]{casella1980minimax}
George Casella, \emph{Minimax ridge regression estimation}, The Annals of
  Statistics (1980), 1036--1056.

\bibitem[CWB08]{candes2008enhancing}
Emmanuel~J Candes, Michael~B Wakin, and Stephen~P Boyd, \emph{Enhancing
  sparsity by reweighted $\ell_1$ minimization}, Journal of Fourier analysis
  and applications \textbf{14} (2008), no.~5-6, 877--905.

\bibitem[Dic16]{dicker2016ridge}
Lee~H Dicker, \emph{Ridge regression and asymptotic minimax estimation over
  spheres of growing dimension}, Bernoulli \textbf{22} (2016), no.~1, 1--37.

\bibitem[DKT19]{deng2019model}
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis, \emph{A model of double
  descent for high-dimensional binary linear classification}, arXiv preprint
  arXiv:1911.05822 (2019).

\bibitem[DLM19]{derezinski2019exact}
Micha{\l} Derezi{\'n}ski, Feynman Liang, and Michael~W Mahoney, \emph{Exact
  expressions for double descent and implicit regularization via surrogate
  random design}, arXiv preprint arXiv:1912.04533 (2019).

\bibitem[DM16]{donoho2016high}
David Donoho and Andrea Montanari, \emph{High dimensional robust m-estimation:
  Asymptotic variance via approximate message passing}, Probability Theory and
  Related Fields \textbf{166} (2016), no.~3-4, 935--969.

\bibitem[dRBK20]{d2020double}
St{\'e}phane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala,
  \emph{Double trouble in double descent: Bias and variance (s) in the lazy
  regime}, arXiv preprint arXiv:2003.01054 (2020).

\bibitem[DS20]{dobriban2020wonder}
Edgar Dobriban and Yue Sheng, \emph{Wonder: Weighted one-shot distributed ridge
  regression in high dimensions.}, Journal of Machine Learning Research
  \textbf{21} (2020), no.~66, 1--52.

\bibitem[DW18]{dobriban2018high}
Edgar Dobriban and Stefan Wager, \emph{High-dimensional asymptotics of
  prediction: Ridge regression and classification}, The Annals of Statistics
  \textbf{46} (2018), no.~1, 247--279.

\bibitem[HG83]{hua1983generalized}
Tsushung~A Hua and Richard~F Gunst, \emph{Generalized ridge regression: a note
  on negative ridge parameters}, Communications in Statistics-Theory and
  Methods \textbf{12} (1983), no.~1, 37--45.

\bibitem[HK70]{hoerl1970ridge}
Arthur~E Hoerl and Robert~W Kennard, \emph{Ridge regression: Biased estimation
  for nonorthogonal problems}, Technometrics \textbf{12} (1970), no.~1, 55--67.

\bibitem[HL20]{hu2020universality}
Hong Hu and Yue~M Lu, \emph{Universality laws for high-dimensional learning
  with random features}, arXiv preprint arXiv:2009.07669 (2020).

\bibitem[HMRT19]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani,
  \emph{Surprises in high-dimensional ridgeless least squares interpolation},
  arXiv preprint arXiv:1903.08560 (2019).

\bibitem[Kar13]{karoui2013asymptotic}
Noureddine~El Karoui, \emph{Asymptotic behavior of unregularized and
  ridge-regularized high-dimensional robust regression estimators: rigorous
  results}, arXiv preprint arXiv:1311.2445 (2013).

\bibitem[KH92]{krogh1992simple}
Anders Krogh and John~A Hertz, \emph{A simple weight decay can improve
  generalization}, Advances in neural information processing systems, 1992,
  pp.~950--957.

\bibitem[KLS20]{kobak2020optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez, \emph{The optimal ridge
  penalty for real-world high-dimensional data can be zero or negative due to
  the implicit ridge regularization}, Journal of Machine Learning Research
  \textbf{21} (2020), no.~169, 1--16.

\bibitem[KPR{\etalchar{+}}17]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al., \emph{Overcoming catastrophic forgetting in neural
  networks}, Proceedings of the national academy of sciences \textbf{114}
  (2017), no.~13, 3521--3526.

\bibitem[LD19]{liu2019ridge}
Sifan Liu and Edgar Dobriban, \emph{Ridge regression: Structure,
  cross-validation, and sketching}, arXiv preprint arXiv:1910.02373 (2019).

\bibitem[LH17]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter, \emph{Decoupled weight decay regularization},
  arXiv preprint arXiv:1711.05101 (2017).

\bibitem[Lol20]{lolas2020regularization}
Panagiotis Lolas, \emph{Regularization in high-dimensional regression and
  classification via random matrix theory}, arXiv preprint arXiv:2003.13723
  (2020).

\bibitem[LP11]{ledoit2011eigenvectors}
Olivier Ledoit and Sandrine P{\'e}ch{\'e}, \emph{Eigenvectors of some large
  sample covariance matrix ensembles}, Probability Theory and Related Fields
  \textbf{151} (2011), no.~1-2, 233--264.

\bibitem[LPRS17]{liang2017fisher}
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes,
  \emph{Fisher-rao metric, geometry, and complexity of neural networks}, arXiv
  preprint arXiv:1711.01530 (2017).

\bibitem[LW17]{louizos2017multiplicative}
Christos Louizos and Max Welling, \emph{Multiplicative normalizing flows for
  variational bayesian neural networks}, Proceedings of the 34th International
  Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp.~2218--2227.

\bibitem[MM19]{mei2019generalization}
Song Mei and Andrea Montanari, \emph{The generalization error of random
  features regression: Precise asymptotics and double descent curve}, arXiv
  preprint arXiv:1908.05355 (2019).

\bibitem[MRSY19]{montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan, \emph{The
  generalization error of max-margin linear classifiers: High-dimensional
  asymptotics in the overparametrized regime}, arXiv preprint arXiv:1911.01544
  (2019).

\bibitem[MS05]{maruyama2005new}
Yuzo Maruyama and William~E Strawderman, \emph{A new class of generalized bayes
  minimax ridge regression estimators}, The Annals of Statistics \textbf{33}
  (2005), no.~4, 1753--1770.

\bibitem[MS18]{mori2018generalized}
Yuichi Mori and Taiji Suzuki, \emph{Generalized ridge estimator and model
  selection criteria in multivariate linear regression}, Journal of
  Multivariate Analysis \textbf{165} (2018), 243--261.

\bibitem[RC15]{ryan2015semi}
Kenneth~Joseph Ryan and Mark~Vere Culp, \emph{On semi-supervised linear
  regression in covariate shift problems}, The Journal of Machine Learning
  Research \textbf{16} (2015), no.~1, 3183--3217.

\bibitem[RM11]{rubio2011spectral}
Francisco Rubio and Xavier Mestre, \emph{Spectral convergence for a general
  class of random matrices}, Statistics \& probability letters \textbf{81}
  (2011), no.~5, 592--602.

\bibitem[RMR20]{richards2020asymptotics}
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco, \emph{Asymptotics of
  ridge (less) regression under general source condition}, arXiv preprint
  arXiv:2006.06386 (2020).

\bibitem[SC95]{silverstein1995analysis}
Jack~W Silverstein and Sang-Il Choi, \emph{Analysis of the limiting spectral
  distribution of large dimensional random matrices}, Journal of Multivariate
  Analysis \textbf{54} (1995), no.~2, 295--309.

\bibitem[Str78]{strawderman1978minimax}
William~E Strawderman, \emph{Minimax adaptive generalized ridge regression
  estimators}, Journal of the American Statistical Association \textbf{73}
  (1978), no.~363, 623--627.

\bibitem[TAH18]{thrampoulidis2018precise}
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi, \emph{Precise error
  analysis of regularized $ m $-estimators in high dimensions}, IEEE
  Transactions on Information Theory \textbf{64} (2018), no.~8, 5592--5628.

\bibitem[TB20]{tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett, \emph{Benign overfitting in ridge
  regression}, arXiv preprint arXiv:2009.14286 (2020).

\bibitem[TCG20]{tony2020semisupervised}
T~Tony~Cai and Zijian Guo, \emph{Semisupervised inference for explained
  variance in high dimensional linear regression and its applications}, Journal
  of the Royal Statistical Society: Series B (Statistical Methodology) (2020).

\bibitem[XH19]{xu2019number}
Ji~Xu and Daniel~J Hsu, \emph{On the number of variables to use in principal
  component regression}, Advances in Neural Information Processing Systems,
  2019, pp.~5095--5104.

\bibitem[Zou06]{zou2006adaptive}
Hui Zou, \emph{The adaptive lasso and its oracle properties}, Journal of the
  American statistical association \textbf{101} (2006), no.~476, 1418--1429.

\bibitem[ZTSG19]{zhao2019learning}
Han Zhao, Yao-Hung~Hubert Tsai, Russ~R Salakhutdinov, and Geoffrey~J Gordon,
  \emph{Learning neural networks with adaptive regularization}, Advances in
  Neural Information Processing Systems, 2019, pp.~11389--11400.

\bibitem[ZWXG18]{zhang2018three}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse, \emph{Three mechanisms
  of weight decay regularization}, arXiv preprint arXiv:1810.12281 (2018).

\end{thebibliography}
