\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Baird(1995)]{Baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Proceedings of the 12th International Conference on Machine
  Learning (ICML)}, 1995.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesvari, and
  Schuurmans]{dai2020coindice}
Dai, B., Nachum, O., Chow, Y., Li, L., Szepesvari, C., and Schuurmans, D.
\newblock Coin{DICE}: Off-policy confidence interval estimation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Image{N}et: {A} large-scale hierarchical image database.
\newblock In \emph{Proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 2019.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2005.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2016taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Proceedings of the 32nd Conference on Uncertainty in
  Artificial Intelligence (UAI)}, 2016.

\bibitem[Fu et~al.(2021)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock {D4RL}: Datasets for deep data-driven reinforcement learning, 2021.
\newblock URL \url{https://openreview.net/forum?id=px0-N3_KjA}.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
  Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2672--2680, 2014.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Iyengar(2005)]{Iyengar2005robust}
Iyengar, G.~N.
\newblock Robust dynamic programming.
\newblock \emph{Mathematics of Operations Research}, 2005.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog, 2019.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock {MOReL} : Model-based offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Kostrikov et~al.(2019{\natexlab{a}})Kostrikov, Agrawal, Dwibedi,
  Levine, and Tompson]{kostrikov2018discriminator}
Kostrikov, I., Agrawal, K.~K., Dwibedi, D., Levine, S., and Tompson, J.
\newblock Discriminator-{A}ctor-{C}ritic: Addressing sample inefficiency and
  reward bias in adversarial imitation learning.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, 2019{\natexlab{a}}.

\bibitem[Kostrikov et~al.(2019{\natexlab{b}})Kostrikov, Nachum, and
  Tompson]{kostrikov2019imitation}
Kostrikov, I., Nachum, O., and Tompson, J.
\newblock Imitation learning via off-policy distribution matching.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{alex2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Image{N}et classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2012.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock \emph{Reinforcement learning: State-of-the-art}.
\newblock Springer Berlin Heidelberg, 2012.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Laroche, R., Trichelair, P., and Des~Combes, R.~T.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem[Lee et~al.(2020)Lee, Lee, Vrancx, Kim, and Kim]{lee2020batch}
Lee, B.-J., Lee, J., Vrancx, P., Kim, D., and Kim, K.-E.
\newblock Batch reinforcement learning with hyperparameter gradients.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{Proceedings of the 4th International Conference on Learning
  Representations (ICLR)}, 2016.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock Dual{DICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.
\newblock Algae{DICE}: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Nilim \& El~Ghaoui(2005)Nilim and El~Ghaoui]{Nilim2005robust}
Nilim, A. and El~Ghaoui, L.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock \emph{Operations Research}, 2005.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning, 2019.

\bibitem[Petrik et~al.(2016)Petrik, Ghavamzadeh, and Chow]{petrik2016safe}
Petrik, M., Ghavamzadeh, M., and Chow, Y.
\newblock Safe policy improvement by minimizing robust baseline regret.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: Discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, Inc., 1st edition, 1994.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft {Q}-learning, 2017.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998rlbook}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: {A}n introduction}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 1999.

\bibitem[Szita \& L\"{o}rincz(2008)Szita and L\"{o}rincz]{szita2008the}
Szita, I. and L\"{o}rincz, A.
\newblock The many faces of optimism: A unifying approach.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning (ICML)}, 2008.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning, 2019.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and
  Schuurmans]{yang2020offpolicy}
Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Liu, and Nemati]{yu2020reinforcement}
Yu, C., Liu, J., and Nemati, S.
\newblock Reinforcement learning in healthcare: A survey, 2020{\natexlab{a}}.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Chen, Wang, Xian, Chen, Liu,
  Madhavan, and Darrell]{yu2020bdd100k}
Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., and
  Darrell, T.
\newblock {BDD100K}: A diverse driving dataset for heterogeneous multitask
  learning.
\newblock \emph{Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2020{\natexlab{b}}.

\bibitem[Yu et~al.(2020{\natexlab{c}})Yu, Thomas, Yu, Ermon, Zou, Levine, Finn,
  and Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock {MOPO}: Model-based offline policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{c}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2019gendice}
Zhang, R., Dai, B., Li, L., and Schuurmans, D.
\newblock Gen{DICE}: Generalized offline estimation of stationary values.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Zhang, S., Liu, B., and Whiteson, S.
\newblock Gradient{DICE}: Rethinking generalized offline estimation of
  stationary values.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2020{\natexlab{b}}.

\end{thebibliography}
