\begin{thebibliography}{10}

\bibitem{achiam2017cpo}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In {\em International conference on machine learning}, pages 22--31. PMLR, 2017.

\bibitem{agarwal2019reinforcementth}
Alekh Agarwal, Nan Jiang, Sham~M Kakade, and Wen Sun.
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 32:96, 2019.

\bibitem{agarwal2022reincarnating}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and Marc Bellemare.
\newblock Reincarnating reinforcement learning: Reusing prior computation to accelerate progress.
\newblock {\em Advances in Neural Information Processing Systems}, 35:28955--28971, 2022.

\bibitem{ball2023efficient}
Philip~J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine.
\newblock Efficient online reinforcement learning with offline data.
\newblock {\em arXiv preprint arXiv:2302.02948}, 2023.

\bibitem{beeson2022td3refine}
Alex Beeson and Giovanni Montana.
\newblock Improving td3-bc: Relaxed policy constraint for offline learning and stable online fine-tuning.
\newblock {\em arXiv preprint arXiv:2211.11802}, 2022.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems}, 34:15084--15097, 2021.

\bibitem{dalal2018safe}
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa.
\newblock Safe exploration in continuous action spaces.
\newblock {\em arXiv preprint arXiv:1801.08757}, 2018.

\bibitem{diehl2023driving}
Christopher Diehl, Timo~Sebastian Sievernich, Martin Kr{\"u}ger, Frank Hoffmann, and Torsten Bertram.
\newblock Uncertainty-aware model-based offline reinforcement learning for automated driving.
\newblock {\em IEEE Robotics and Automation Letters}, 8(2):1167--1174, 2023.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{fujimoto2021td3bc}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145, 2021.

\bibitem{fujimoto2018td3}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages 1587--1596. PMLR, 2018.

\bibitem{fujimoto2019bcq}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem{garg2023extreme}
Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon.
\newblock Extreme q-learning: Maxent rl without entropy.
\newblock {\em arXiv preprint arXiv:2301.02328}, 2023.

\bibitem{fcs_rl}
Cong GUAN, Ke~XUE, Chunpeng FAN, Feng CHEN, Lichao ZHANG, Lei YUAN, Chao QIAN, and Yang YU.
\newblock Open and real-world human-ai coordination by heterogeneous training with communication.
\newblock {\em Frontiers of Computer Science}, 19(4):194314, 2025.

\bibitem{guo2023sung}
Siyuan Guo, Yanchao Sun, Jifeng Hu, Sili Huang, Hechang Chen, Haiyin Piao, Lichao Sun, and Yi~Chang.
\newblock A simple unified uncertainty-guided framework for offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2306.07541}, 2023.

\bibitem{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International conference on machine learning}, pages 1352--1361. PMLR, 2017.

\bibitem{haarnoja2018sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages 1861--1870. PMLR, 2018.

\bibitem{hansen2023idql}
Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub~Grudzien Kuba, and Sergey Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion policies.
\newblock {\em arXiv preprint arXiv:2304.10573}, 2023.

\bibitem{jang2022UQ}
Ingook Jang and Seonghyun Kim.
\newblock Uncertainty-driven pessimistic q-ensemble for offline-to-online reinforcement learning.
\newblock In {\em 3rd Offline RL Workshop: Offline RL as a''Launchpad''}, 2022.

\bibitem{kostrikov2021iql}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{kumar2020cql}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191, 2020.

\bibitem{le2019fqe}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In {\em International Conference on Machine Learning}, pages 3703--3712. PMLR, 2019.

\bibitem{lee2022br}
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin.
\newblock Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.
\newblock In {\em Conference on Robot Learning}, pages 1702--1712. PMLR, 2022.

\bibitem{lei2023uni}
Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, and Huazhe Xu.
\newblock Uni-o4: Unifying online and offline deep reinforcement learning with multi-step on-policy optimization.
\newblock {\em arXiv preprint arXiv:2311.03351}, 2023.

\bibitem{li2023proto}
Jianxiong Li, Xiao Hu, Haoran Xu, Jingjing Liu, Xianyuan Zhan, and Ya-Qin Zhang.
\newblock Proto: Iterative policy regularized offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.15669}, 2023.

\bibitem{luo2023td3lmbda}
Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc~Peter Deisenroth.
\newblock Finetuning from offline reinforcement learning: Challenges, trade-offs and practical solutions.
\newblock {\em arXiv preprint arXiv:2303.17396}, 2023.

\bibitem{lyu2022mildlyql}
Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu.
\newblock Mildly conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:1711--1724, 2022.

\bibitem{mao2022moore}
Yihuan Mao, Chao Wang, Bin Wang, and Chongjie Zhang.
\newblock Moore: Model-based offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2201.10070}, 2022.

\bibitem{mao2023stro}
Yixiu Mao, Hongchang Zhang, Chen Chen, Yi~Xu, and Xiangyang Ji.
\newblock Supported trust region optimization for offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 23829--23851. PMLR, 2023.

\bibitem{mark2022o3f}
Max~Sobol Mark, Ali Ghadirzadeh, Xi~Chen, and Chelsea Finn.
\newblock Fine-tuning offline policies with optimistic action selection.
\newblock In {\em Deep Reinforcement Learning Workshop NeurIPS 2022}, 2022.

\bibitem{mcinroe2023planning}
Trevor McInroe, Stefano~V Albrecht, and Amos Storkey.
\newblock Planning to go out-of-distribution in offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2310.05723}, 2023.

\bibitem{nair2020awac}
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.
\newblock Awac: Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{nakamoto2023cal}
Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max~Sobol Mark, Yi~Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine.
\newblock Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.
\newblock {\em arXiv preprint arXiv:2303.05479}, 2023.

\bibitem{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems}, 34:11702--11716, 2021.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages 1889--1897. PMLR, 2015.

\bibitem{schulman2015gae}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{schulman2017ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{Sutton_Barto_2005}
RichardS. Sutton and AndrewG. Barto.
\newblock Reinforcement learning: An introduction.
\newblock {\em IEEE Transactions on Neural Networks}, page 285â€“286, Jan 2005.

\bibitem{tang2022health}
Shengpu Tang, Maggie Makar, Michael Sjoding, Finale Doshi-Velez, and Jenna Wiens.
\newblock Leveraging factored action spaces for efficient offline reinforcement learning in healthcare.
\newblock {\em Advances in Neural Information Processing Systems}, 35:34272--34286, 2022.

\bibitem{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.
\newblock {CORL}: Research-oriented deep offline reinforcement learning library.
\newblock In {\em 3rd Offline RL Workshop: Offline RL as a ''Launchpad''}, 2022.

\bibitem{tessler2018rcpo}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock {\em arXiv preprint arXiv:1805.11074}, 2018.

\bibitem{uchendu2023jump}
Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos{\'e}phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et~al.
\newblock Jump-start reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 34556--34583. PMLR, 2023.

\bibitem{wang2023miql}
Shenzhi Wang, Qisen Yang, Jiawei Gao, Matthieu~Gaetan Lin, HAO CHEN, Liwei Wu, Ning Jia, Shiji Song, and Gao Huang.
\newblock Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{wu2022supported}
Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long.
\newblock Supported policy optimization for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:31278--31291, 2022.

\bibitem{xie2021policy}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:27395--27407, 2021.

\bibitem{xu2022guide}
Haoran Xu, Li~Jiang, Li~Jianxiong, and Xianyuan Zhan.
\newblock A policy-guided imitation approach for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:4085--4098, 2022.

\bibitem{xu2023ivr}
Haoran Xu, Li~Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai~Kin Chan, and Xianyuan Zhan.
\newblock Offline rl with no ood actions: In-sample learning via implicit value regularization.
\newblock {\em arXiv preprint arXiv:2303.15810}, 2023.

\bibitem{yu2023aca}
Zishun Yu and Xinhua Zhang.
\newblock Actor-critic alignment for offline-to-online reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 40452--40474. PMLR, 2023.

\bibitem{yue2023understanding}
Yang Yue, Rui Lu, Bingyi Kang, Shiji Song, and Gao Huang.
\newblock Understanding, predicting and better resolving q-value divergence in offline-rl.
\newblock {\em arXiv preprint arXiv:2310.04411}, 2023.

\bibitem{zhang2023pex}
Haichao Zhang, We~Xu, and Haonan Yu.
\newblock Policy expansion for bridging offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2302.00935}, 2023.

\bibitem{zhao2023ensembleq}
Kai Zhao, Yi~Ma, Jinyi Liu, Yan Zheng, and Zhaopeng Meng.
\newblock Ensemble-based offline-to-online reinforcement learning: From pessimistic learning to optimistic exploration.
\newblock {\em arXiv preprint arXiv:2306.06871}, 2023.

\bibitem{zhao2022td3ada}
Yi~Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, and Joni Pajarinen.
\newblock Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2210.13846}, 2022.

\bibitem{zheng2022odt}
Qinqing Zheng, Amy Zhang, and Aditya Grover.
\newblock Online decision transformer.
\newblock In {\em international conference on machine learning}, pages 27042--27059. PMLR, 2022.

\end{thebibliography}
