\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Liang]{allen2018learning}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{arXiv preprint arXiv:1811.04918}, 2018.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  244--253. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:1902.09229}, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{baldi1989neural}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeshwar, Ozair, Bengio,
  Courville, and Hjelm]{belghazi2018mutual}
Belghazi, M.~I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Courville,
  A., and Hjelm, D.
\newblock Mutual information neural estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  531--540. PMLR, 2018.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and
  Douze]{Caron2018DeepCF}
Caron, M., Bojanowski, P., Joulin, A., and Douze, M.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In \emph{ECCV}, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020swav}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simclr}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Chen \& He(2020)Chen and He]{chen2020simsiam}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, 2011.

\bibitem[Coria et~al.(2020)Coria, Bredin, Ghannay, and
  Rosset]{coria2020comparison}
Coria, J.~M., Bredin, H., Ghannay, S., and Rosset, S.
\newblock A comparison of metric learning loss functions for end-to-end speaker
  verification.
\newblock In \emph{International Conference on Statistical Language and Speech
  Processing}, pp.\  137--148. Springer, 2020.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Tian, Singh, and
  Poczos]{du2018gradient}
Du, S., Lee, J., Tian, Y., Singh, A., and Poczos, B.
\newblock Gradient descent learns one-hidden-layer cnn: Donâ€™t be afraid of
  spurious local minima.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1339--1348. PMLR, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{arXiv preprint arXiv:1806.00900}, 2018{\natexlab{b}}.

\bibitem[Fisher(1936)]{fisher1936use}
Fisher, R.~A.
\newblock The use of multiple measurements in taxonomic problems.
\newblock \emph{Annals of eugenics}, 7\penalty0 (2):\penalty0 179--188, 1936.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{byol}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~A., Guo, Z.~D., Azar, M.~G., et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Hadsell et~al.(2006)Hadsell, Chopra, and
  LeCun]{hadsell2006dimensionality}
Hadsell, R., Chopra, S., and LeCun, Y.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock In \emph{2006 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition (CVPR'06)}, volume~2, pp.\  1735--1742. IEEE, 2006.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Hardt \& Ma(2017)Hardt and Ma]{hardt2016identity}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock \emph{ICLR}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{He2020MomentumCF}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.~B.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  9726--9735, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4700--4708, 2017.

\bibitem[Ji et~al.(2021)Ji, Deng, Nakada, Zou, and Zhang]{ji2021power}
Ji, W., Deng, Z., Nakada, R., Zou, J., and Zhang, L.
\newblock The power of contrast for feature learning: A theoretical analysis.
\newblock \emph{arXiv preprint arXiv:2110.02473}, 2021.

\bibitem[Jing et~al.(2022)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Jing, L., Vincent, P., LeCun, Y., and Tian, Y.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock \emph{ICLR}, 2022.

\bibitem[Kalantidis et~al.(2020)Kalantidis, Sariyildiz, Pion, Weinzaepfel, and
  Larlus]{kalantidis2020hard}
Kalantidis, Y., Sariyildiz, M.~B., Pion, N., Weinzaepfel, P., and Larlus, D.
\newblock Hard negative mixing for contrastive learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
  A., Liu, C., and Krishnan, D.
\newblock Supervised contrastive learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kokiopoulou et~al.(2011)Kokiopoulou, Chen, and
  Saad]{kokiopoulou2011trace}
Kokiopoulou, E., Chen, J., and Saad, Y.
\newblock Trace optimization and eigenproblems in dimension reduction methods.
\newblock \emph{Numerical Linear Algebra with Applications}, 18\penalty0
  (3):\penalty0 565--602, 2011.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Laurent \& Brecht(2018)Laurent and Brecht]{laurent2018deep}
Laurent, T. and Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International conference on machine learning}, pp.\
  2902--2907. PMLR, 2018.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Misra \& Maaten(2020)Misra and Maaten]{misra2020pirl}
Misra, I. and Maaten, L. v.~d.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{CVPR}, 2020.

\bibitem[Oh~Song et~al.(2016)Oh~Song, Xiang, Jegelka, and Savarese]{oh2016deep}
Oh~Song, H., Xiang, Y., Jegelka, S., and Savarese, S.
\newblock Deep metric learning via lifted structured feature embedding.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4004--4012, 2016.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Robinson et~al.(2021)Robinson, Chuang, Sra, and
  Jegelka]{robinson2020contrastive}
Robinson, J., Chuang, C.-Y., Sra, S., and Jegelka, S.
\newblock Contrastive learning with hard negative samples.
\newblock \emph{ICLR}, 2021.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{safran2018spurious}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4433--4441. PMLR, 2018.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{ICLR}, 2014.

\bibitem[Schroff et~al.(2015)Schroff, Kalenichenko, and
  Philbin]{schroff2015facenet}
Schroff, F., Kalenichenko, D., and Philbin, J.
\newblock Facenet: A unified embedding for face recognition and clustering.
\newblock In \emph{CVPR}, 2015.

\bibitem[Sohn(2016)]{sohn2016improved}
Sohn, K.
\newblock Improved deep metric learning with multi-class n-pair loss objective.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1857--1865, 2016.

\bibitem[Tian(2018)]{tian2018theoretical}
Tian, Y.
\newblock A theoretical framework for deep locally connected relu network.
\newblock \emph{arXiv preprint arXiv:1809.10829}, 2018.

\bibitem[Tian(2020)]{tian2019student}
Tian, Y.
\newblock Student specialization in deep relu networks with finite width and
  input dimension.
\newblock \emph{ICML}, 2020.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Krishnan, and
  Isola]{tian2020contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16}, pp.\  776--794.
  Springer, 2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning?
\newblock \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Tian et~al.(2020{\natexlab{c}})Tian, Yu, Chen, and
  Ganguli]{tian2020understanding}
Tian, Y., Yu, L., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning with dual deep networks.
\newblock \emph{arXiv preprint arXiv:2010.00578}, 2020{\natexlab{c}}.

\bibitem[Wen \& Li(2021)Wen and Li]{wen2021toward}
Wen, Z. and Li, Y.
\newblock Toward understanding the feature learning process of self-supervised
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2105.15134}, 2021.

\bibitem[Wold et~al.(1987)Wold, Esbensen, and Geladi]{wold1987principal}
Wold, S., Esbensen, K., and Geladi, P.
\newblock Principal component analysis.
\newblock \emph{Chemometrics and intelligent laboratory systems}, 2\penalty0
  (1-3):\penalty0 37--52, 1987.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3733--3742, 2018.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,
  Y., Wang, L., and Liu, T.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10524--10533. PMLR, 2020.

\bibitem[Yeh et~al.(2021)Yeh, Hong, Hsu, Liu, Chen, and
  LeCun]{yeh2021decoupled}
Yeh, C.-H., Hong, C.-Y., Hsu, Y.-C., Liu, T.-L., Chen, Y., and LeCun, Y.
\newblock Decoupled contrastive learning.
\newblock \emph{arXiv preprint arXiv:2110.06848}, 2021.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock \emph{arXiv preprint arxiv:2103.03230}, 2021.

\bibitem[Zhou \& Liang(2018)Zhou and Liang]{zhou2018critical}
Zhou, Y. and Liang, Y.
\newblock Critical points of linear neural networks: Analytical forms and
  landscape properties.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
