\begin{thebibliography}{10}

\bibitem{prudencio2023survey}
Rafael~Figueiredo Prudencio, Marcos~ROA Maximo, and Esther~Luna Colombini.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and open problems.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 2023.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{stachowicz2024racer}
Kyle Stachowicz and Sergey Levine.
\newblock Racer: Epistemic risk-sensitive rl enables fast driving with fewer crashes.
\newblock {\em arXiv preprint arXiv:2405.04714}, 2024.

\bibitem{lin2024safety}
Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, and Ding Zhao.
\newblock Safety-aware causal representation for trustworthy offline reinforcement learning in autonomous driving.
\newblock {\em IEEE Robotics and Automation Letters}, 2024.

\bibitem{zhang2023spatial}
Zhili Zhang, Songyang Han, Jiangwei Wang, and Fei Miao.
\newblock Spatial-temporal-aware safe multi-agent reinforcement learning of connected autonomous vehicles in challenging scenarios.
\newblock In {\em 2023 IEEE International Conference on Robotics and Automation (ICRA)}, pages 5574--5580. IEEE, 2023.

\bibitem{fang2022offline}
Xing Fang, Qichao Zhang, Yinfeng Gao, and Dongbin Zhao.
\newblock Offline reinforcement learning for autonomous driving with real world driving data.
\newblock In {\em 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)}, pages 3417--3422. IEEE, 2022.

\bibitem{chi2023diffusion}
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock {\em arXiv preprint arXiv:2303.04137}, 2023.

\bibitem{ding2024seeing}
Wenhao Ding, Laixi Shi, Yuejie Chi, and Ding Zhao.
\newblock Seeing is not believing: Robust reinforcement learning against spurious correlation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{zhao2023guard}
Weiye Zhao, Rui Chen, Yifan Sun, Ruixuan Liu, Tianhao Wei, and Changliu Liu.
\newblock Guard: A safe reinforcement learning benchmark.
\newblock {\em arXiv preprint arXiv:2305.13681}, 2023.

\bibitem{li2023guided}
Jinning Li, Xinyi Liu, Banghua Zhu, Jiantao Jiao, Masayoshi Tomizuka, Chen Tang, and Wei Zhan.
\newblock Guided online distillation: Promoting safe reinforcement learning by offline demonstration.
\newblock {\em arXiv preprint arXiv:2309.09408}, 2023.

\bibitem{zhan2022deepthermal}
Xianyuan Zhan, Haoran Xu, Yue Zhang, Xiangyu Zhu, Honglei Yin, and Yu~Zheng.
\newblock Deepthermal: Combustion optimization for thermal power generating units using offline reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 4680--4688, 2022.

\bibitem{kim2022safety}
Dohyeong Kim, Yunho Kim, Kyungjae Lee, and Songhwai Oh.
\newblock Safety guided policy optimization.
\newblock In {\em 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, pages 2462--2467. IEEE, 2022.

\bibitem{he2024agile}
Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, and Guanya Shi.
\newblock Agile but safe: Learning collision-free high-speed legged locomotion.
\newblock {\em arXiv preprint arXiv:2401.17583}, 2024.

\bibitem{xiao2023safe}
Wenli Xiao, Tairan He, John Dolan, and Guanya Shi.
\newblock Safe deep policy adaptation.
\newblock {\em arXiv preprint arXiv:2310.08602}, 2023.

\bibitem{huang2023safedreamer}
Weidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, and Yaodong Yang.
\newblock Safedreamer: Safe reinforcement learning with world models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{chen2023progressive}
Zhaorun Chen, Binhao Chen, Tairan He, Liang Gong, and Chengliang Liu.
\newblock Progressive adaptive chance-constrained safeguards for reinforcement learning.
\newblock {\em arXiv preprint arXiv:2310.03379}, 2023.

\bibitem{zou2024policy}
Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo~Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, and Keqiang Li.
\newblock Policy bifurcation in safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2403.12847}, 2024.

\bibitem{yang2023model}
Yujie Yang, Yuxuan Jiang, Yichen Liu, Jianyu Chen, and Shengbo~Eben Li.
\newblock Model-free safe reinforcement learning through neural barrier certificate.
\newblock {\em IEEE Robotics and Automation Letters}, 8(3):1295--1302, 2023.

\bibitem{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 16(1):1437--1480, 2015.

\bibitem{brunke2021safe}
Lukas Brunke, Melissa Greeff, Adam~W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela~P Schoellig.
\newblock Safe learning in robotics: From learning-based control to safe reinforcement learning.
\newblock {\em Annual Review of Control, Robotics, and Autonomous Systems}, 5, 2021.

\bibitem{wachi2024survey}
Akifumi Wachi, Xun Shen, and Yanan Sui.
\newblock A survey of constraint formulations in safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2402.02025}, 2024.

\bibitem{lee2022coptidice}
Jongmin Lee, Cosmin Paduraru, Daniel~J Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung Kim, and Arthur Guez.
\newblock Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation.
\newblock {\em arXiv preprint arXiv:2204.08957}, 2022.

\bibitem{lee2021optidice}
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim.
\newblock Optidice: Offline policy optimization via stationary distribution correction estimation.
\newblock In {\em International Conference on Machine Learning}, pages 6120--6130. PMLR, 2021.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum.
\newblock Offline reinforcement learning with fisher divergence critic regularization.
\newblock In {\em International Conference on Machine Learning}, pages 5774--5783. PMLR, 2021.

\bibitem{xu2022constraints}
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu.
\newblock Constraints penalized q-learning for safe offline reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 8753--8760, 2022.

\bibitem{le2019batch}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In {\em International Conference on Machine Learning}, pages 3703--3712. PMLR, 2019.

\bibitem{liu2023constrained}
Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao.
\newblock Constrained decision transformer for offline safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2302.07351}, 2023.

\bibitem{lin2023safe}
Qian Lin, Bo~Tang, Zifan Wu, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, and Dong Wang.
\newblock Safe offline reinforcement learning with real-time budget constraints.
\newblock {\em arXiv preprint arXiv:2306.00603}, 2023.

\bibitem{zheng2024safe}
Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo~Eben Li, Xianyuan Zhan, and Jingjing Liu.
\newblock Safe offline reinforcement learning with feasibility-guided diffusion model.
\newblock {\em arXiv preprint arXiv:2401.10700}, 2024.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem{shi2022pessimistic}
Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi.
\newblock Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity.
\newblock In {\em International conference on machine learning}, pages 19967--20025. PMLR, 2022.

\bibitem{yao2023gradient}
Yihang Yao, Zuxin Liu, Zhepeng Cen, Peide Huang, Tingnan Zhang, Wenhao Yu, and Ding Zhao.
\newblock Gradient shaping for multi-constraint safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2312.15127}, 2023.

\bibitem{hong2023beyond}
Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, and Pulkit Agrawal.
\newblock Beyond uniform sampling: Offline reinforcement learning with imbalanced datasets.
\newblock {\em Advances in Neural Information Processing Systems}, 36:4985--5009, 2023.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191, 2020.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{sinha2022s4rl}
Samarth Sinha, Ajay Mandlekar, and Animesh Garg.
\newblock S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics.
\newblock In {\em Conference on Robot Learning}, pages 907--917. PMLR, 2022.

\bibitem{yu2022leverage}
Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine.
\newblock How to leverage unlabeled data in offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 25611--25635. PMLR, 2022.

\bibitem{li2024survival}
Anqi Li, Dipendra Misra, Andrey Kolobov, and Ching-An Cheng.
\newblock Survival instinct in offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{pitis2020counterfactual}
Silviu Pitis, Elliot Creager, and Animesh Garg.
\newblock Counterfactual data augmentation using locally factored dynamics.
\newblock {\em Advances in Neural Information Processing Systems}, 33:3976--3990, 2020.

\bibitem{aloui2023counterfactual}
Ahmed Aloui, Juncheng Dong, Cat~P Le, and Vahid Tarokh.
\newblock Counterfactual data augmentation with contrastive learning.
\newblock {\em arXiv preprint arXiv:2311.03630}, 2023.

\bibitem{lu2020sample}
Chaochao Lu, Biwei Huang, Ke~Wang, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Kun Zhang, and Bernhard Sch{\"o}lkopf.
\newblock Sample-efficient reinforcement learning via counterfactual-based data augmentation.
\newblock {\em arXiv preprint arXiv:2012.09092}, 2020.

\bibitem{pitis2022mocoda}
Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg.
\newblock Mocoda: Model-based counterfactual data augmentation.
\newblock {\em Advances in Neural Information Processing Systems}, 35:18143--18156, 2022.

\bibitem{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 22--31. PMLR, 2017.

\bibitem{gu2022review}
Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll.
\newblock A review of safe reinforcement learning: Methods, theory and applications.
\newblock {\em arXiv preprint arXiv:2205.10330}, 2022.

\bibitem{liu2022robustness}
Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo~Li, and Ding Zhao.
\newblock On the robustness of safe reinforcement learning under observational perturbations.
\newblock {\em arXiv preprint arXiv:2205.14691}, 2022.

\bibitem{kim2024trust}
Dohyeong Kim, Kyungjae Lee, and Songhwai Oh.
\newblock Trust region-based safe distributional reinforcement learning for multiple constraints.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{xu2023uncertainty}
Sheng Xu and Guiliang Liu.
\newblock Uncertainty-aware constraint inference in inverse constrained reinforcement learning.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{kim2024scale}
Dohyeong Kim, Mineui Hong, Jeongho Park, and Songhwai Oh.
\newblock Scale-invariant gradient aggregation for constrained multi-objective reinforcement learning.
\newblock {\em arXiv preprint arXiv:2403.00282}, 2024.

\bibitem{chow2018risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk criteria.
\newblock {\em Journal of Machine Learning Research}, 18(167):1--51, 2018.

\bibitem{tessler2018reward}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock {\em arXiv preprint arXiv:1805.11074}, 2018.

\bibitem{ray2019benchmarking}
Alex Ray, Joshua Achiam, and Dario Amodei.
\newblock Benchmarking safe exploration in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.01708}, 7, 2019.

\bibitem{ding2020natural}
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic.
\newblock Natural policy gradient primal-dual method for constrained markov decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 33:8378--8390, 2020.

\bibitem{zhang2020first}
Yiming Zhang, Quan Vuong, and Keith Ross.
\newblock First order constrained optimization in policy space.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{cen2024feasibility}
Zhepeng Cen, Yihang Yao, Zuxin Liu, and Ding Zhao.
\newblock Feasibility consistent representation learning for safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2405.11718}, 2024.

\bibitem{wu2024off}
Zifan Wu, Bo~Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, and Dong Wang.
\newblock Off-policy primal-dual safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2401.14758}, 2024.

\bibitem{ding2024resilient}
Dongsheng Ding, Zhengyan Huan, and Alejandro Ribeiro.
\newblock Resilient constrained reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 3412--3420. PMLR, 2024.

\bibitem{stooke2020responsive}
Adam Stooke, Joshua Achiam, and Pieter Abbeel.
\newblock Responsive safety in reinforcement learning by pid lagrangian methods.
\newblock In {\em International Conference on Machine Learning}, pages 9133--9143. PMLR, 2020.

\bibitem{liu2022constrained}
Zuxin Liu, Zhepeng Cen, Vladislav Isenbaev, Wei Liu, Steven Wu, Bo~Li, and Ding Zhao.
\newblock Constrained variational policy optimization for safe reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 13644--13668. PMLR, 2022.

\bibitem{huang2022constrained}
Sandy Huang, Abbas Abdolmaleki, Giulia Vezzani, Philemon Brakel, Daniel~J Mankowitz, Michael Neunert, Steven Bohez, Yuval Tassa, Nicolas Heess, Martin Riedmiller, et~al.
\newblock A constrained multi-objective reinforcement learning framework.
\newblock In {\em Conference on Robot Learning}, pages 883--893. PMLR, 2022.

\bibitem{guan2024poce}
Jiayi Guan, Li~Shen, Ao~Zhou, Lusong Li, Han Hu, Xiaodong He, Guang Chen, and Changjun Jiang.
\newblock Poce: Primal policy optimization with conservative estimation for multi-constraint offline reinforcement learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26243--26253, 2024.

\bibitem{zhang2023saformer}
Qin Zhang, Linrui Zhang, Haoran Xu, Li~Shen, Bowen Wang, Yongzhe Chang, Xueqian Wang, Bo~Yuan, and Dacheng Tao.
\newblock Saformer: A conditional sequence modeling approach to offline safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2301.12203}, 2023.

\bibitem{polosky2022constrained}
Nicholas Polosky, Bruno~C Da~Silva, Madalina Fiterau, and Jithin Jagannath.
\newblock Constrained offline policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 17801--17810. PMLR, 2022.

\bibitem{hong2024primal}
Kihyuk Hong, Yuhang Li, and Ambuj Tewari.
\newblock A primal-dual-critic algorithm for offline constrained reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 280--288. PMLR, 2024.

\bibitem{guan2024voce}
Jiayi Guan, Guang Chen, Jiaming Ji, Long Yang, Zhijun Li, et~al.
\newblock Voce: Variational optimization with conservative estimation for offline safe reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{guo2024temporal}
Zijian Guo, Weichao Zhou, and Wenchao Li.
\newblock Temporal logic specification-conditioned decision transformer for offline safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2402.17217}, 2024.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em arXiv preprint arXiv:2106.01345}, 2021.

\bibitem{zhu2023diffusion}
Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang.
\newblock Diffusion models for reinforcement learning: A survey.
\newblock {\em arXiv preprint arXiv:2311.01223}, 2023.

\bibitem{lee2024gta}
Jaewoo Lee, Sujin Yun, Taeyoung Yun, and Jinkyoo Park.
\newblock Gta: Generative trajectory augmentation with guidance for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2405.16907}, 2024.

\bibitem{yuan2024reward}
Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Minshuo Chen, and Mengdi Wang.
\newblock Reward-directed conditional diffusion: Provable distribution estimation and reward improvement.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{janner2022planning}
Michael Janner, Yilun Du, Joshua~B Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock {\em arXiv preprint arXiv:2205.09991}, 2022.

\bibitem{ajay2022conditional}
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock {\em arXiv preprint arXiv:2211.15657}, 2022.

\bibitem{xiao2023safediffuser}
Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, and Daniela Rus.
\newblock Safediffuser: Safe planning with diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2306.00148}, 2023.

\bibitem{liang2023adaptdiffuser}
Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo.
\newblock Adaptdiffuser: Diffusion models as adaptive self-evolving planners.
\newblock {\em arXiv preprint arXiv:2302.01877}, 2023.

\bibitem{lu2024synthetic}
Cong Lu, Philip Ball, Yee~Whye Teh, and Jack Parker-Holder.
\newblock Synthetic experience replay.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{he2024diffusion}
Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li.
\newblock Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{altman1998constrained}
Eitan Altman.
\newblock Constrained markov decision processes with total cost criteria: Lagrangian approach and dual linear program.
\newblock {\em Mathematical methods of operations research}, 48(3):387--417, 1998.

\bibitem{lin2024policy}
Qian Lin, Chao Yu, Zongkai Liu, and Zifan Wu.
\newblock Policy-regularized offline multi-objective reinforcement learning.
\newblock {\em arXiv preprint arXiv:2401.02244}, 2024.

\bibitem{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145, 2021.

\bibitem{hong2023harnessing}
Zhang-Wei Hong, Pulkit Agrawal, R{\'e}mi Tachet~des Combes, and Romain Laroche.
\newblock Harnessing mixed offline reinforcement learning datasets via trajectory weighting.
\newblock {\em arXiv preprint arXiv:2306.13085}, 2023.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{xu2020error}
Tian Xu, Ziniu Li, and Yang Yu.
\newblock Error bounds of imitating policies and environments.
\newblock {\em Advances in Neural Information Processing Systems}, 33:15737--15749, 2020.

\bibitem{cen2024learning}
Zhepeng Cen, Zuxin Liu, Zitong Wang, Yihang Yao, Henry Lam, and Ding Zhao.
\newblock Learning from sparse offline datasets via conservative density estimation.
\newblock {\em arXiv preprint arXiv:2401.08819}, 2024.

\bibitem{lee2022convergence}
Holden Lee, Jianfeng Lu, and Yixin Tan.
\newblock Convergence for score-based generative modeling with polynomial complexity.
\newblock {\em Advances in Neural Information Processing Systems}, 35:22870--22882, 2022.

\bibitem{chen2022sampling}
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru~R Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions.
\newblock {\em arXiv preprint arXiv:2209.11215}, 2022.

\bibitem{gronauer2022bullet}
Sven Gronauer.
\newblock Bullet-safety-gym: Aframework for constrained reinforcement learning.
\newblock 2022.

\bibitem{liu2023datasets}
Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, et~al.
\newblock Datasets and benchmarks for offline safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2306.09303}, 2023.

\bibitem{kingma2014semi}
Durk~P Kingma, Shakir Mohamed, Danilo Jimenez~Rezende, and Max Welling.
\newblock Semi-supervised learning with deep generative models.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems}, 34:8780--8794, 2021.

\bibitem{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International conference on machine learning}, pages 8162--8171. PMLR, 2021.

\end{thebibliography}
