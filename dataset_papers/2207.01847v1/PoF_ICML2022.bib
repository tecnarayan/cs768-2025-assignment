@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@string{ICLR = "ICLR"}
@string{ICIP = "ICIP"}
@string{NIPS = "NeurIPS"}
@string{CVPR = "CVPR"}
@string{ICML = "ICML"}
@string{NIPS-DC = "NeurIPS Disentanglement Challenge"}
@string{AAAI = "AAAI"}
@string{ICCV = "ICCV"}
@string{ECCV = "ECCV"}
@string{BMVC = "BMVC"}
@string{SIGIR = "SIGIR"}
@string{WWW = "WWW"}

@Article{paper:fmnist,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  journal={arXiv1708.07747},
  year      = {2017},
}

% landscape visualization
@inproceedings{NEURIPS2018_a41b3bb3,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = NIPS,
 title = {Visualizing the Loss Landscape of Neural Nets},
 year = {2018}
}
% editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
% pages = {},
% publisher = {Curran Associates, Inc.},
%url = {https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
% volume = {31},
 

@inproceedings{Goodfellow2015,
 author={Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.}, 
 title={Qualitatively characterizing neural network
optimization problems},
booktitle={ICLR}, 
year={2015}
}

@article{DBLP:journals/corr/ImTB16,
  author    = {Daniel Jiwoong Im and
               Michael Tao and
               Kristin Branson},
  title     = {An Empirical Analysis of Deep Network Loss Surfaces},
  journal   = {arXiv:1612.04010},
  year      = {2016},
  eprinttype = {arXiv},
  eprint    = {1612.04010},
  timestamp = {Mon, 24 Jan 2022 08:04:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ImTB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%  url       = {http://arxiv.org/abs/1612.04010},

% convergence in distribution
@inproceedings{
chaudhari2018stochastic,
title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
author={Pratik Chaudhari and Stefano Soatto},
booktitle= ICLR,
year={2018},
}
%url={https://openreview.net/forum?id=HyWrIgW0W},

% batch size & flatness
@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv:1706.02677},
  year={2017}
}

% optimizer & generalization
@inproceedings{10.5555/3491440.3491892,
author = {Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
title = {Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks},
booktitle = IJCAI,
year = {2020}
}
%articleno = {452},
%numpages = {9},
%location = {Yokohama, Yokohama, Japan},
%series = {IJCAI'20}
%{Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence}
%isbn = {9780999241165},


@article{Keskar2017ImprovingGP,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Nitish Shirish Keskar and Richard Socher},
  journal={arXiv:1712.07628},
  year={2017}
}

@inproceedings{AshiaWilson,
author={Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nati Srebro and Benjamin Recht}, title={The marginal value of adaptive gradient methods in machine learning},
booktitle= NIPS, 
year={2017}
}

@inproceedings{NEURIPS2020_08fb104b,
 author = {Zhou, Yingxue and Karimi, Belhal and Yu, Jinxing and Xu, Zhiqiang and Li, Ping},
 booktitle = NIPS,
 title = {Towards Better Generalization of Adaptive Gradient Methods},
 year = {2020}
}
% editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
% pages = {810--821},
% publisher = {Curran Associates, Inc.},
%url = {https://proceedings.neurips.cc/paper/2020/file/08fb104b0f2f838f3ce2d2b3741a12c2-Paper.pdf},
% volume = {33},
 

% information bottleneck
@inproceedings{Saxe2018OnTI,
  title={On the Information Bottleneck Theory of Deep Learning},
  author={Andrew M. Saxe and Yamini Bansal and Joel Dapello and Madhu S. Advani and Artemy Kolchinsky and Brendan D. Tracey and David D. Cox},
  booktitle=ICLR,
  year={2018}
}
%{Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data}

% PAC & flat
@inproceedings{PAC_flat,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Gintare Karolina Dziugaite and Daniel M Roy},
  booktitle={UAI},
  year={2017}
}

%flat-minimaと汎化向上の関係性に関する研究（SAMで引用されていたもの）
@Inproceedings{paper:keskar17,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      booktitle=ICLR,
}
@Inproceedings{paper:dziugaite17,
      title={Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy},
      year={2017},
      eprint={1703.11008},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@Inproceedings{paper:Jiang20,
    title={Fantastic Generalization Measures and Where to Find Them},
    author={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
    year={2020},
    booktitle=ICLR,
}

%flat-minimaに対する反論
@Inproceedings{paper:dinh17,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle=ICML,
  year={2017},
}
%  pages={1019--1028},
%  organization={PMLR}

%flatnessの定義方法
@InProceedings{paper:tsuzuku20,
  title = 	 {Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks Using {PAC}-{B}ayesian Analysis},
  author =       {Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning (PMLR)},
  pages = 	 {9636--9647},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tsuzuku20a/tsuzuku20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tsuzuku20a.html},
}

@Inproceedings{paper:petzka21,
    title={Relative Flatness and Generalization},
    author={Henning Petzka and Michael Kamp and Linara Adilova and Cristian Sminchisescu and Mario Boley},
    booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
    year={2021},
    url={https://openreview.net/forum?id=sygvo7ctb_}
}

@Inproceedings{
    anonymous2022why,
    title={{WHY} {FLATNESS} {DOES} {AND} {DOES} {NOT} {CORRELATE} {WITH} {GENERALIZATION} {FOR} {DEEP} {NEURAL} {NETWORKS} },
    author={Anonymous},
    booktitle={Submitted to The Tenth International Conference on Learning Representations },
    year={2022},
    url={https://openreview.net/forum?id=L1L2G43k14n},
    note={under review}
}


@Inproceedings{paper:sam,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle=ICLR,
year={2021},
}


@InProceedings{paper:asam,
  title = 	 {ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks},
  author =       {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5905--5914},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kwon21b/kwon21b.pdf},
}

@Inproceedings{paper:Donxian,
title={Adversarial weight perturbation helps robust generalization},
author={Dongxian Wu and Shu-Tao Xia and Yisen Wang},
booktitle=NIPS,
year={2020},
}

@Inproceedings{paper:AMP,
title={Regularizing Neural Networks via Adversarial Model Perturbation},
author={Yaowei Zheng and Richong Zhang and Yongyi Mao},
booktitle=CVPR,
year={2021}
}


@inproceedings{paper:Izmailov,
  title={Averaging weights leads to wider optima and better generalization},
  author={Pavel Izmailov and Dmitrii Podoprikhin ad Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
  booktitle={UAI},
  year={2018}
}


@Inproceedings{paper:Hochreiter97,
    author = {Sepp Hochreiter and J\"{u}rgen Schmidhuber},
    title = {Flat minima},
    booktitle = {Neural Computation},
    volume = {9(1)},
    pages = {1–42},
    year = 1997,
}

@Inproceedings{paper:Rissanen78,
    author = {Jorma Rissanen},
    title = {Modeling by shortest data description},
    booktitle = {Automatica, 14(5):465–471},
    year = 1978,
}

@Inproceedings{paper:Hinton93,
    author = {Hinton, G. E. and Camp, D. van},
    title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
    booktitle = {Annual Conference on Computational Learning Theory (COLT)},
    pages = {5–13},
    year = 1993,
}

@Inproceedings{paper:Honkela04,
    author = {Honkela, A. and Valpola, H. Variational},
    title = {Variational Learning and Bits-back Coding: An Information-theoretic View to Bayesian Learning},
    booktitle = {IEEE Trans. on Neural Networks, 15(4): 800–810},
    year = 2004,
}

@Inproceedings{paper:Sagun16,
    author = {Levent Sagun and Leon Bottou and Yann LeCun},
    title = {Eigenvalues of the hessian in deep learning: Singularity and beyond},
    booktitle = {arXiv:1611.07476},
    year = 2016,
}

@Inproceedings{paper:Wu17,
    author = {Lei Wu and Zhanxing Zhu and Weinan E},
    title = {Towards understanding generalization of deep learning: Perspective of loss landscapes},
    booktitle = {ICML PADL Workshop},
    year = 2017,
}

@Article{paper:Maddox20,
    author = {Wesley J. Maddox and Gregory Benton and Andrew Gordon Wilson},
    title = {Rethinking parameter counting in deep models: Effective dimensionality revisited},
    journal = {arXiv:2003.02139},
    year = 2020,
}

@Inproceedings{paper:Martens15,
      author={J. Martens and R. Grosse},
      title={Optimizing neural networks with Kronecker-factored approximate curvature},
      booktitle = ICML,
      pages={2408–2417},
      year={2015},
}

@Inproceedings{paper:Grosse16,
      title={A Kronecker-factored approximate Fisher matrix for convolution layers}, 
      author={Roger Grosse and James Martens},
      booktitle = ICML,
      year={2016},
}

@Inproceedings{paper:Osawa19,
      title={Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks}, 
      author={Kazuki Osawa and Yohei Tsuji and Yuichiro Ueno and Akira Naruse and Rio Yokota and Satoshi Matsuoka},
      year={2019},
      booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)}
}

@Inproceedings{paper:Botev17,
     author={Aleksandar Botev and Hippolyt Ritter and David Barber},
      title={Practical Gauss-Newton optimisation for deep learning},
    booktitle = ICML,
    year=2017
}

@Inproceedings{paper:Pauloski21,
   title={KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks},
   booktitle={International Conference for High Performance Computing, Networking, Storage, and Analysis (SC)},
   author={Pauloski, J. Gregory and Huang, Qi and Huang, Lei and Venkataraman, Shivaram and Chard, Kyle and Foster, Ian and Zhang, Zhao},
   year={2021}
}

@Inproceedings{paper:Roux,
    title={Topmoumoute online natural gradient algorithm},
    author={N. Le Roux and P. A. Manzagol and Y. Bengio},
    booktitle = NIPS,
    year={2008}
}

@Inproceedings{paper:Zhang18,
    author = {Yao Zhang and Andrew M Saxe and Madhu S Advani and Alpha A Lee},
    title = {Energy–entropy competition and the
effectiveness of stochastic gradient descent in machine learning},
    booktitle = {Molecular Physics},
    volume = {116(21-22)},
    pages = {3214–3223},
    year = 2018,
}

@Inproceedings{paper:Yao18,
    author = {Yao, Z. and Gholami, A. and Lei, Q. and Keutzer, K. and Mahoney, M. W.},
    title = {Hessian-based Analysis of Large Batch Training and Robustness to Adversaries},
    booktitle = NIPS,
    year = 2018,
}

@Inproceedings{paper:Chen20,
    author={Chen, Zhihong and Xiao, Rong and Li, Chenliang and Ye, Gangfeng and Sun, Haochuan and Deng, Hongbo},
    title={ESAM: Discriminative Domain Adaptation with Non-Displayed Items to Improve Long-Tail Performance},
    booktitle = SIGIR,
    year={2020},
}
%{International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},

@Inproceedings{paper:Jungmin21,
      title={ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks}, 
      author={Jungmin Kwon and Jeongseop Kim and Hyunseo Park and In Kwon Choi},
    booktitle = ICML,
      year={2021},
}

@INPROCEEDINGS{paper:Hojjat20,
    author={Moayed, Hojjat and Mansoori, Eghbal G.},
    title={Regularization of Neural Network Using DropCoadapt}, 
    booktitle={International Conference on Computer and Knowledge Engineering (ICCKE)}, 
    year={2020},
}

@INPROCEEDINGS{paper:Wei20,
title = {Decouple co-adaptation: Classifier randomization for person re-identification},
 booktitle={Elsevier Neurocomputing},
 volume = {383},
 pages = {1-9},
 year = {2020},
 author = {Long Wei and Zhenyong Wei and Zhongming Jin and Qianxiao Wei and Jianqiang Huang and Xian-Sheng Hua and Deng Cai and Xiaofei He},
}

@inproceedings{paper:Jastrzebski18,
author = {Stanislaw Jastrzebski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos J Storkey},
title = {Finding flatter minima with sgd},
  booktitle={ICLR Workshop},
  year=2018,
 }

@inproceedings{paper:Becker03,
    author = {Langford, John and Shawe-Taylor, John},
    booktitle = NIPS,
    title = {PAC-Bayes \& Margins},
    year = 2003,
}

@INPROCEEDINGS{paper:corruptions,
    author = {Dan Hendrycks and Thomas Dietterich},
    title = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
    booktitle=ICLR,
    year = 2019,
}

@INPROCEEDINGS{paper:wideresnet,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {British Machine Vision Virtual Conference (BMVC)},
    year = 2016,
}

@INPROCEEDINGS{paper:foca,
    author = {Ikuro Sato and Kohta Ishikawa and Guoqing Liu and Masayuki Tanaka},
    title = {Breaking Inter-Layer Co-Adaptation by Classifier Anonymization},
    booktitle = ICML,
    year = 2019,
}

@INPROCEEDINGS{paper:foca2,
    author = {Ikuro Sato and Kohta Ishikawa and Guoqing Liu and Masayuki Tanaka},
    title = {Does End-to-End Trained Deep Model Always Perform Better than Non-End-to-End Counterpart?},
    booktitle = {Electronic Imaging},
    year = 2021,
}

@INPROCEEDINGS{paper:vae,
    author = {Diederik P Kingma and Max Welling},
    title = {Auto-Encoding Variational Bayes},
    booktitle=ICLR,
    year = 2014,
}

@INPROCEEDINGS{paper:icip1,
    author = {Zongyao Li and Ren Togo and Takahiro Ogawa and Miki Haseyama},
    title = {Variational Autoencoder Based Unsupervised Domain Adaptation For Semantic Segmentation},
    booktitle = ICIP,
    year = 2020
}

@INPROCEEDINGS{paper:icip3,
    author = {Jiaxin Zhou and Takashi Komuro},
    title = {Recognizing Fall Actions from Videos Using Reconstruction Error of Variational Autoencoder},
    booktitle= ICIP,
    year = 2019
}

@INPROCEEDINGS{paper:icip2,
    author = {Nicolas Vercheval and Hendrik De Bie and Aleksandra Pi{\v z}urica},
    title = {Variational Auto-Encoders Without Graph Coarsening For Fine Mesh Learning},
    booktitle = ICIP,
    year = 2020
}

@INPROCEEDINGS{paper:dada,
    author = {Xingchao Peng and Zijun Huang and Ximeng Sun and Kate Saenko},
    title = {Domain Agnostic Learning with Disentangled Representations},
    booktitle = ICML,
    year = {2019},
}

@INPROCEEDINGS{paper:life-long,
    author={Alessandro Achille and Tom Eccles and Loic Matthey and Christopher P. Burgess and Nick Watters and Alexander Lerchner and Irina Higgins},
    title={Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies},
    booktitle = NIPS,
    year = {2018},
}

@INPROCEEDINGS{paper:beta,
    author = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
    title = {Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
    booktitle = ICLR, 
    year = {2017},
}

@INPROCEEDINGS{paper:infovae,
    author = {Shengjia Zhao and Jiaming Song and Stefano Ermon},
    title = {InfoVAE: Balancing Learning and Inference in Variational Autoencoders},
    booktitle = AAAI, 
    year = {2019},
}

@INPROCEEDINGS{paper:factor,
    author = {Hyunjik Kim and Andriy Mnih},
    title = {Disentangling by Factorising},
    booktitle = ICML,
    year = {2018},
}

@INPROCEEDINGS{paper:tcvae,
    author = {Ricky T. Q. Chen and Xuechen Li and Roger Grosse and David Duvenaud},
    title = {Isolating Sources of Disentanglement in Variational Autoencoders},
    booktitle = NIPS,
    year = {2018},
}

@INPROCEEDINGS{paper:dipvae,
    author = {Abhishek Kumar and Prasanna Sattigeri and Avinash BalakrishnanRicky},
    title = {Variational Inference of Disentangled Latent Concepts from Unlabeled Observations},
    booktitle = ICLR,
    year = {2018},
}

@INPROCEEDINGS{wang2018cosface,
    author = {Hao Wang and Yitong Wang and Zheng Zhou and Xing Ji and Dihong Gong and Jingchao Zhou and Zhifeng Li and Wei Liu},
    title = {Cosface: Large Margin Cosine Loss for Deep Face Recognition},
    booktitle = CVPR,
    year = {2018},
}

@INPROCEEDINGS{deng2019arcface,
    author = {Jiankang Deng and Jia Guo and Niannan Xue and Stefanos Zafeiriou},
    title = {Arcface: Additive Angular Margin Loss for Deep Face Recognition},
    booktitle = CVPR,
    year = {2019},
}

@INPROCEEDINGS{paper:sphere,
    author = {Weiyang Liu and Yandong Wen and Zhiding Yu and Ming Li and Bhiksha Raj and Le Song},
    title = {SphereFace: Deep Hypersphere Embedding for Face Recognition},
    booktitle = CVPR,
    year = {2017},
}

@INPROCEEDINGS{paper:reality,
    author = {Kevin Musgrave and Serge Belongie and Ser-Nam Lim},
    title = {A Metric Learning Reality Check},
    booktitle = ECCV,
    year = 2020,
}


@INPROCEEDINGS{hadsell2006dimensionality,
    author = {Raia Hadsell and Sumit Chopra and Yann LeCun},
    title = {Dimensionality Reduction by Learning an Invariant Mapping},
    booktitle = CVPR,  
    year = 2006,
}

@INPROCEEDINGS{paper:softtriplet,
    author = {Qi Qian and Lei Shang and Baigui Sun and Juhua Hu and Hao Li and Rong Jin},
    title = {Softtriple Loss: Deep Metric Learning without Triplet Sampling},
    booktitle = ICCV,
    year = {2019},
}

@INPROCEEDINGS{paper:jigsaw,
    author = {Mehdi Noroozi and Paolo Favaro},
    title = {Unsupervised Learning of Visual Representions by solving Jigsaw Puzzles},
    booktitle = ECCV,  
    year = 2016,
}

@INPROCEEDINGS{paper:simclr,
    author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
    title = {A Simple Framework for Contrastive Learning of Visual Representations},
    booktitle = ICML, 
    year=2020,
}

@INPROCEEDINGS{paper:moco,
    author = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
    title = {Momentum Contrast for Unsupervised Visual Representation Learning},
    booktitle = CVPR,  
    year = 2020,
}

@INPROCEEDINGS{paper:kmeans,
    author = {D.~Sculley},
    title = {Web-Scale k-Means Clustering},
    booktitle = WWW,  
    year = 2010,
}


@INPROCEEDINGS{paper:mpi3d,
    author = {Muhammad Waleed Gondal and Manuel W{\"u}thrich and Dorđe Miladinovi{\'c} and Francesco Locatello and Martin Breidt and Valentin Volchkov and Joel Akpo and Olivier Bachem and Bernhard Schölkopf and Stefan Bauer},
    title = {On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset},
    booktitle = NIPS,
    year = {2019},
}  
 
@INPROCEEDINGS{paper:mig,
    author = {Ricky T. Q. Chen and Xuechen Li and Roger Grosse and David Duvenaud},
    title = {Isolating Sources of Disentanglement in Variational Autoencoders},
    booktitle = NIPS,
    year = {2018},
}

@INPROCEEDINGS{paper:disentanglementpytorch,
    author = {Amir H. Abdi and Purang Abolmaesumi and Sidney Fels},
    title = {Variational Learning with Disentanglement-PyTorch},
    booktitle = NIPS-DC, 
    year = 2019,
}

@INPROCEEDINGS{paper:challenge,
    author = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar R{\"a}tsch and Sylvain Gelly and Bernhard Sch{\"o}lkopf and Olivier Bachem},
    title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
    booktitle = ICML, 
    year = 2019,
}

@Article{LeCun1989,
author = "Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel",
title = "Backpropagation Applied to Handwritten Zip Code Recognition",
journal = "Neural Computation",
volume = "1",
number = "4",
pages = "541--551",
year = "1989"
}

@INPROCEEDINGS{He2016,
author={K. He and X. Zhang and S. Ren and J. Sun}, 
booktitle={Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Residual Learning for Image Recognition}, 
year={2016}, 
volume={}, 
number={}, 
keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
}
%booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
%month={June},}
%pages={770-778}, 

@inproceedings{Li2018,
author = "Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein",
title = "Visualizing the Loss Landscape of Neural Nets",
booktitle = "Neural Information Processing Systems (NeurIPS)",
year = "2018"
}
%booktitle = "Thirty-second Conference on Neural Information Processing Systems (NeurIPS)",

@article{Lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal=ICLR,
  year={2014}
}

@INPROCEEDINGS{Szegedy2015,
author={C. Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich}, 
booktitle={Computer Vision and Pattern Recognition (CVPR)}, 
title={Going deeper with convolutions}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-9}, 
keywords={convolution;decision making;feature extraction;Hebbian learning;image classification;neural net architecture;resource allocation;convolutional neural network architecture;resource utilization;architectural decision;Hebbian principle;object classification;object detection;Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision}
}
%booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
%doi={10.1109/CVPR.2015.7298594}, 
%ISSN={1063-6919}, 
%month={June},}

@inproceedings{Hoffer2018,
  author    = {Elad Hoffer and
               Itay Hubara and
               Daniel Soudry},
  title     = {Fix your classifier: the marginal value of training the last weight layer},
  booktitle = ICLR,
  year      = {2018}
}
%  url       = {http://arxiv.org/abs/1801.04540},
%  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-04540},
%  archivePrefix = {arXiv},
%  eprint    = {1801.04540},
%  timestamp = {Mon, 13 Aug 2018 16:47:19 +0200},

@article{Yosinski2014,
  author    = {Jason Yosinski and
               Jeff Clune and
               Yoshua Bengio and
               Hod Lipson},
  title     = {How transferable are features in deep neural networks?},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2014}
}
%  journal = {Advances in Neural Information Processing Systems 27 (NIPS)},

@article{Breiman1996,
author = {L. Breiman},
title = {Bagging Predictors},
journal = {Machine Learning}, 
volume = {24}, 
pages = {123--140},
year = {1996}
}

@inproceedings{Ioffe2015,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Sergey Ioffe and Christian Szegedy},
  booktitle=ICML,
  year={2015}
}

@article{Hinton2012,
  author    = {Geoffrey E. Hinton and
               Nitish Srivastava and
               Alex Krizhevsky and
               Ilya Sutskever and
               Ruslan Salakhutdinov},
  title     = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal   = {arXiv:1207.0580},
  year      = {2012}
}
%  url       = {http://arxiv.org/abs/1207.0580},
%  archivePrefix = {arXiv},
%  eprint    = {1207.0580},
%  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
%  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-0580},
%  bibsource = {dblp computer science bibliography, https://dblp.org}

@book{Watanabe2009,
author = {Sumio Watanabe}, 
title = {Algebraic geometry and statistical learning theory},
publisher = {Cambridge University Press}, 
year = {2009}
}

@inproceedings{Kobayashi2017,
author = {T. Kobayashi},
title = {Sharing ConvNet Across Heterogeneous Tasks},
booktitle = {International Conference on Neural Information Processing (ICONIP)},
year = {2017}
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevskyf and Geoffrey Hinton},
    title = {Learning multiple layers of features from tiny images},
    volume = {1},
    number = {4},
    year = {2009}
}

@article{alexnet2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
doi = {10.1145/3065386}
}


















@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@inproceedings{alexnet2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS’12}
}
  

@InProceedings{sato19a,
  title = 	 {Breaking Inter-Layer Co-Adaptation by Classifier Anonymization},
  author = 	 {Sato, Ikuro and Ishikawa, Kohta and Liu, Guoqing and Tanaka, Masayuki},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5619--5627},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/sato19a/sato19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/sato19a.html},
  abstract = 	 {This study addresses an issue of co-adaptation between a feature extractor and a classifier in a neural network. A naive joint optimization of a feature extractor and a classifier often brings situations in which an excessively complex feature distribution adapted to a very specific classifier degrades the test performance. We introduce a method called Feature-extractor Optimization through Classifier Anonymization (FOCA), which is designed to avoid an explicit co-adaptation between a feature extractor and a particular classifier by using many randomly-generated, weak classifiers during optimization. We put forth a mathematical proposition that states the FOCA features form a point-like distribution within the same class in a class-separable fashion under special conditions. Real-data experiments under more general conditions provide supportive evidences.}
}

@inproceedings{BMVC2016_87,
	title={Wide Residual Networks},
	author={Sergey Zagoruyko and Nikos Komodakis},
	year={2016},
	month={September},
	pages={87.1-87.12},
	articleno={87},
	numpages={12},
	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
	publisher={BMVA Press},
	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
	doi={10.5244/C.30.87},
	isbn={1-901725-59-6},
	url={https://dx.doi.org/10.5244/C.30.87}
}

@inproceedings{dalal2005histograms,
  title={Histograms of oriented gradients for human detection},
  author={Dalal, Navneet and Triggs, Bill},
  booktitle={2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)},
  volume={1},
  pages={886--893},
  year={2005},
  organization={IEEE}
}

@inproceedings{lowe1999object,
  title={Object recognition from local scale-invariant features},
  author={Lowe, David G},
  booktitle={Proceedings of the seventh IEEE international conference on computer vision},
  volume={2},
  pages={1150--1157},
  year={1999},
  organization={Ieee}
}

@inproceedings{bay2006surf,
  title={Surf: Speeded up robust features},
  author={Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  booktitle={European conference on computer vision},
  pages={404--417},
  year={2006},
  organization={Springer}
}

@article{belongie2002shape,
  title={Shape matching and object recognition using shape contexts},
  author={Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={24},
  number={4},
  pages={509--522},
  year={2002},
  publisher={IEEE}
}

@inproceedings{ojala1994performance,
  title={Performance evaluation of texture measures with classification based on Kullback discrimination of distributions},
  author={Ojala, Timo and Pietikainen, Matti and Harwood, David},
  booktitle={Proceedings of 12th International Conference on Pattern Recognition},
  volume={1},
  pages={582--585},
  year={1994},
  organization={IEEE}
}

@article{ojala1996comparative,
  title={A comparative study of texture measures with classification based on featured distributions},
  author={Ojala, Timo and Pietik{\"a}inen, Matti and Harwood, David},
  journal={Pattern recognition},
  volume={29},
  number={1},
  pages={51--59},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{wang2009hog,
  title={An HOG-LBP human detector with partial occlusion handling},
  author={Wang, Xiaoyu and Han, Tony X and Yan, Shuicheng},
  booktitle={2009 IEEE 12th international conference on computer vision},
  pages={32--39},
  year={2009},
  organization={IEEE}
}

@inproceedings{calonder2010brief,
  title={Brief: Binary robust independent elementary features},
  author={Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
  booktitle={European conference on computer vision},
  pages={778--792},
  year={2010},
  organization={Springer}
}

@inproceedings{ambai2011card,
  title={CARD: Compact and real-time descriptors},
  author={Ambai, Mitsuru and Yoshida, Yuichi},
  booktitle={2011 International Conference on Computer Vision},
  pages={97--104},
  year={2011},
  organization={IEEE}
}

@inproceedings{sivic2003video,
  title={Video Google: A text retrieval approach to object matching in videos},
  author={Sivic, Josef and Zisserman, Andrew},
  booktitle={null},
  pages={1470},
  year={2003},
  organization={IEEE}
}

@inproceedings{csurka2004visual,
  title={Visual categorization with bags of keypoints},
  author={Csurka, Gabriella and Dance, Christopher and Fan, Lixin and Willamowski, Jutta and Bray, C{\'e}dric},
  booktitle={Workshop on statistical learning in computer vision, ECCV},
  volume={1},
  number={1-22},
  pages={1--2},
  year={2004},
  organization={Prague}
}

@inproceedings{perronnin2007fisher,
  title={Fisher kernels on visual vocabularies for image categorization},
  author={Perronnin, Florent and Dance, Christopher},
  booktitle={2007 IEEE conference on computer vision and pattern recognition},
  pages={1--8},
  year={2007},
  organization={IEEE}
}

@inproceedings{perronnin2010improving,
  title={Improving the fisher kernel for large-scale image classification},
  author={Perronnin, Florent and S{\'a}nchez, Jorge and Mensink, Thomas},
  booktitle={European conference on computer vision},
  pages={143--156},
  year={2010},
  organization={Springer}
}

@inproceedings{jegou2010aggregating,
  title={Aggregating local descriptors into a compact image representation},
  author={J{\'e}gou, Herv{\'e} and Douze, Matthijs and Schmid, Cordelia and P{\'e}rez, Patrick},
  booktitle={2010 IEEE computer society conference on computer vision and pattern recognition},
  pages={3304--3311},
  year={2010},
  organization={IEEE}
}

@article{vapnik1963pattern,
  title={Pattern recognition using generalized portrait method},
  author={Vapnik, Vladimir},
  journal={Automation and remote control},
  volume={24},
  pages={774--780},
  year={1963}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@inproceedings{freund1996experiments,
  title={Experiments with a new boosting algorithm},
  author={Freund, Yoav and Schapire, Robert E and others},
  booktitle={icml},
  volume={96},
  pages={148--156},
  year={1996},
  organization={Citeseer}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{ho1998random,
  title={The random subspace method for constructing decision forests},
  author={Ho, Tin Kam},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={20},
  number={8},
  pages={832--844},
  year={1998},
  publisher={IEEE}
}

@article{schapire1998boosting,
  title={Boosting the margin: A new explanation for the effectiveness of voting methods},
  author={Schapire, Robert E and Freund, Yoav and Bartlett, Peter and Lee, Wee Sun and others},
  journal={The annals of statistics},
  volume={26},
  number={5},
  pages={1651--1686},
  year={1998},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{jaderberg2015spatial,
  title={Spatial transformer networks},
  author={Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
  booktitle=NIPS,
  pages={2017--2025},
  year={2015}
}

@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle=NIPS,
  pages={91--99},
  year={2015}
}

@article{uijlings2013selective,
  title={Selective search for object recognition},
  author={Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
  journal={International journal of computer vision},
  volume={104},
  number={2},
  pages={154--171},
  year={2013},
  publisher={Springer}
}

@inproceedings{godard2017unsupervised,
  title={Unsupervised monocular depth estimation with left-right consistency},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={270--279},
  year={2017}
}

@inproceedings{farneback2003two,
  title={Two-frame motion estimation based on polynomial expansion},
  author={Farneb{\"a}ck, Gunnar},
  booktitle={Scandinavian conference on Image analysis},
  pages={363--370},
  year={2003},
  organization={Springer}
}

@inproceedings{brox2004high,
  title={High accuracy optical flow estimation based on a theory for warping},
  author={Brox, Thomas and Bruhn, Andr{\'e}s and Papenberg, Nils and Weickert, Joachim},
  booktitle={European conference on computer vision},
  pages={25--36},
  year={2004},
  organization={Springer}
}

@article{brox2010large,
  title={Large displacement optical flow: descriptor matching in variational motion estimation},
  author={Brox, Thomas and Malik, Jitendra},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={3},
  pages={500--513},
  year={2010},
  publisher={IEEE}
}

@inproceedings{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={652--660},
  year={2017}
}

@inproceedings{kendall2015posenet,
  title={Posenet: A convolutional network for real-time 6-dof camera relocalization},
  author={Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2938--2946},
  year={2015}
}

@inproceedings{cao2017realtime,
  title={Realtime multi-person 2d pose estimation using part affinity fields},
  author={Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7291--7299},
  year={2017}
}

@article{cao2018openpose,
  title={OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields},
  author={Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  journal={arXiv preprint arXiv:1812.08008},
  year={2018}
}

@inproceedings{pishchulin2012articulated,
  title={Articulated people detection and pose estimation: Reshaping the future},
  author={Pishchulin, Leonid and Jain, Arjun and Andriluka, Mykhaylo and Thorm{\"a}hlen, Thorsten and Schiele, Bernt},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3178--3185},
  year={2012},
  organization={IEEE}
}

@inproceedings{gkioxari2014using,
  title={Using k-poselets for detecting people and localizing their keypoints},
  author={Gkioxari, Georgia and Hariharan, Bharath and Girshick, Ross and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3582--3589},
  year={2014}
}

@inproceedings{sun2011articulated,
  title={Articulated part-based model for joint object detection and pose estimation},
  author={Sun, Min and Savarese, Silvio},
  booktitle={2011 International Conference on Computer Vision},
  pages={723--730},
  year={2011},
  organization={IEEE}
}

@inproceedings{iqbal2016multi,
  title={Multi-person pose estimation with local joint-to-person associations},
  author={Iqbal, Umar and Gall, Juergen},
  booktitle={European Conference on Computer Vision},
  pages={627--642},
  year={2016},
  organization={Springer}
}

@inproceedings{papandreou2017towards,
  title={Towards accurate multi-person pose estimation in the wild},
  author={Papandreou, George and Zhu, Tyler and Kanazawa, Nori and Toshev, Alexander and Tompson, Jonathan and Bregler, Chris and Murphy, Kevin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4903--4911},
  year={2017}
}

@inproceedings{maturana2015voxnet,
  title={Voxnet: A 3d convolutional neural network for real-time object recognition},
  author={Maturana, Daniel and Scherer, Sebastian},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={922--928},
  year={2015},
  organization={IEEE}
}

@inproceedings{wu20153d,
  title={3d shapenets: A deep representation for volumetric shapes},
  author={Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1912--1920},
  year={2015}
}

@inproceedings{qi2016volumetric,
  title={Volumetric and multi-view cnns for object classification on 3d data},
  author={Qi, Charles R and Su, Hao and Nie{\ss}ner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5648--5656},
  year={2016}
}

@inproceedings{hinton2011transforming,
  title={Transforming auto-encoders},
  author={Hinton, Geoffrey E and Krizhevsky, Alex and Wang, Sida D},
  booktitle={International conference on artificial neural networks},
  pages={44--51},
  year={2011},
  organization={Springer}
}

@article{Kingma2014,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

%%%%%% representation learning
% SimCLR
@inproceedings{Chen2020ASF,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

% fixmatch
@article{sohn2020fixmatch,
  title={FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
  author={Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:2001.07685},
  year={2020}
}

% moco
@article{he2019momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  journal={arXiv preprint arXiv:1911.05722},
  year={2019}
}

% greedy infomax
@inproceedings{lowe2019putting,
  title={Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
  author={L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan},
  booktitle=NIPS,
  pages={3033--3045},
  year={2019}
}

% AM Deep Infomax
@inproceedings{bachman2019learning,
  title={Learning representations by maximizing mutual information across views},
  author={Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  booktitle=NIPS,
  pages={15509--15519},
  year={2019}
}

% Deep Infomax
@article{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

% CPC
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

% MINE
@article{belghazi2018mine,
  title={Mine: mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1801.04062},
  year={2018}
}

% transfer learning
@inproceedings{joulin2016learning,
  title={Learning visual features from large weakly supervised data},
  author={Joulin, Armand and van der Maaten, Laurens and Jabri, Allan and Vasilache, Nicolas},
  booktitle={European Conference on Computer Vision},
  pages={67--84},
  year={2016},
  organization={Springer}
}

@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{Kolesnikov2019LargeSL,
  title={Large Scale Learning of General Visual Representations for Transfer},
  author={Alexander I Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.11370}
}

% domain adaptation
@InProceedings{zhao19a,
  title = 	 {On Learning Invariant Representations for Domain Adaptation},
  author = 	 {Zhao, Han and Combes, Remi Tachet Des and Zhang, Kun and Gordon, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7523--7532},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/zhao19a.html},
  abstract = 	 {Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits \emph{conditional shift}: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of \emph{any} domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.}
}

@InProceedings{you19a,
  title = 	 {Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation},
  author = 	 {You, Kaichao and Wang, Ximei and Long, Mingsheng and Jordan, Michael},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7124--7133},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/you19a/you19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/you19a.html},
  abstract = 	 {Deep unsupervised domain adaptation (Deep UDA) methods successfully leverage rich labeled data in a source domain to boost the performance on related but unlabeled data in a target domain. However, algorithm comparison is cumbersome in Deep UDA due to the absence of accurate and standardized model selection method, posing an obstacle to further advances in the field. Existing model selection methods for Deep UDA are either highly biased, restricted, unstable, or even controversial (requiring labeled target data). To this end, we propose Deep Embedded Validation (DEV), which embeds adapted feature representation into the validation procedure to obtain unbiased estimation of the target risk with bounded variance. The variance is further reduced by the technique of control variate. The efficacy of the method has been justified both theoretically and empirically.}
}


@InProceedings{liu19b,
  title = 	 {Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers},
  author = 	 {Liu, Hong and Long, Mingsheng and Wang, Jianmin and Jordan, Michael},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4013--4022},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/liu19b/liu19b.pdf},
  url = 	 {http://proceedings.mlr.press/v97/liu19b.html},
  abstract = 	 {Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distributions of both domains. However, a theoretical prerequisite of domain adaptation is the adaptability measured by the expected risk of an ideal joint hypothesis over the source and target domains. In this respect, adversarial feature adaptation may potentially deteriorate the adaptability, since it distorts the original feature distributions when suppressing domain-specific variations. To this end, we propose Transferable Adversarial Training (TAT) to enable the adaptation of deep classifiers. The approach generates transferable examples to fill in the gap between the source and target domains, and adversarially trains the deep classifiers to make consistent predictions over the transferable examples. Without learning domain-invariant representations at the expense of distorting the feature distributions, the adaptability in the theoretical learning bound is algorithmically guaranteed. A series of experiments validate that our approach advances the state of the arts on a variety of domain adaptation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.}
}

@InProceedings{wu19f,
  title = 	 {Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment},
  author = 	 {Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6872--6881},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19f/wu19f.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wu19f.html},
  abstract = 	 {Domain adaptation addresses the common situation in which the target distribution generating our test data differs from the source distribution generating our training data. While absent assumptions, domain adaptation is impossible, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-adversarial approaches consist of aligning source and target encodings, an approach often motivated as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, a problem guaranteed to arise under shifting label distributions. We propose asymmetrically-relaxed distribution alignment, a new approach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize precise assumptions under which our algorithm is theoretically principled and demonstrate empirical benefits on both synthetic and real datasets.}
}

@inproceedings{li2018learning,
  title={Learning to generalize: Meta-learning for domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

% svhn
@inproceedings{SVHN,
  title={Reading Digits in Natural Images with Unsupervised Feature Learning},
  author={Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
  year={2011},
  booktitle={NeurIPS Workshop}
}

% imagenet
@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

% meta-learning
@InProceedings{finn19a,
  title = 	 {Online Meta-Learning},
  author = 	 {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1920--1930},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/finn19a/finn19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/finn19a.html},
  abstract = 	 {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.}
}

@InProceedings{zintgraf19a,
  title = 	 {Fast Context Adaptation via Meta-Learning},
  author = 	 {Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7693--7702},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zintgraf19a/zintgraf19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/zintgraf19a.html},
  abstract = 	 {We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

% MTL
@inproceedings{kokkinos2017ubernet,
  title={Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory},
  author={Kokkinos, Iasonas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6129--6138},
  year={2017}
}

@inproceedings{kobayashi2017sharing,
  title={Sharing convnet across heterogeneous tasks},
  author={Kobayashi, Takumi},
  booktitle={International Conference on Neural Information Processing},
  pages={343--353},
  year={2017},
  organization={Springer}
}

@inproceedings{zhang2014facial,
  title={Facial landmark detection by deep multi-task learning},
  author={Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  booktitle={European conference on computer vision},
  pages={94--108},
  year={2014},
  organization={Springer}
}

@article{Long2015LearningMT,
  title={Learning Multiple Tasks with Deep Relationship Networks},
  author={Mingsheng Long and Jianmin Wang},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.02117}
}

@article{Yang2016TraceNR,
  title={Trace Norm Regularised Deep Multi-Task Learning},
  author={Yongxin Yang and Timothy M. Hospedales},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04038}
}

@inproceedings{sener2018multi,
  title={Multi-task learning as multi-objective optimization},
  author={Sener, Ozan and Koltun, Vladlen},
  booktitle=NIPS,
  pages={527--538},
  year={2018}
}

@inproceedings{bilen2016integrated,
  title={Integrated perception with recurrent multi-task neural networks},
  author={Bilen, Hakan and Vedaldi, Andrea},
  booktitle=NIPS,
  pages={235--243},
  year={2016}
}

@inproceedings{misra2016cross,
  title={Cross-stitch networks for multi-task learning},
  author={Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3994--4003},
  year={2016}
}

@inproceedings{zhang2014improving,
  title={Improving multiview face detection with multi-task deep convolutional neural networks},
  author={Zhang, Cha and Zhang, Zhengyou},
  booktitle={IEEE Winter Conference on Applications of Computer Vision},
  pages={1036--1041},
  year={2014},
  organization={IEEE}
}

% dissimilar datasets => bad MTL
@inproceedings{donahue2014decaf,
  title={Decaf: A deep convolutional activation feature for generic visual recognition},
  author={Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={647--655},
  year={2014}
}

@inproceedings{sharif2014cnn,
  title={CNN features off-the-shelf: an astounding baseline for recognition},
  author={Sharif Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={806--813},
  year={2014}
}

@inproceedings{zamir2018taskonomy,
  title={Taskonomy: Disentangling task transfer learning},
  author={Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3712--3722},
  year={2018}
}

@article{yamada2019shakedrop,
  title={Shakedrop Regularization for Deep Residual Learning},
  author={Yamada, Yoshihiro and Iwamura, Masakazu and Akiba, Takuya and Kise, Koichi},
  journal={IEEE Access},
  volume={7},
  pages={186126--186136},
  year={2019},
  publisher={IEEE}
}

@Misc{tinyimagenet,
  title        = "{Tiny ImageNet Visual Recognition Challenge}",
  howpublished = "\url{https://tinyimagenet.herokuapp.com}"
}

@inproceedings{han2017deep,
  title={Deep pyramidal residual networks},
  author={Han, Dongyoon and Kim, Jiwhan and Kim, Junmo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5927--5935},
  year={2017}
}

@inproceedings{shakedrop,
  title={ShakeDrop regularization},
  author={Yoshihiro Yamada and Masakazu Iwamura and Koichi Kise},
  year={2018},
  booktitle={International Conference on Learning Representations (ICLR) Workshop}
}


@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@article{nesterov1998introductory,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  journal={Lecture notes},
  volume={3},
  number={4},
  pages={5},
  year={1998}
}

@inproceedings{
zhang2018mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@inproceedings{zhong2017random,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{cubuk2019autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={113--123},
  year={2019}
}

@Misc{metalearningtutorial,
  title        = "{Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning}",
  author={Finn, Chelsea and Levine, Sergey},
  howpublished = "\url{https://drive.google.com/file/d/1DuHyotdwEAEhmuHQWwRosdiVBVGm8uYx/view}"
}

@article{Yu2020GradientSF,
  title={Gradient Surgery for Multi-Task Learning},
  author={Tianhe Yu and Saurabh Kumar and Abhishek Gupta and Sergey Levine and Karol Hausman and Chelsea Finn},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.06782}
}

% STL-10
@inproceedings{coates2011analysis,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={215--223},
  year={2011}
}

% ResNet-50 v1
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% ResNet-50 v2
@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}