\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ablin et~al.(2020)Ablin, Peyré, and Moreau]{seff}
Ablin, P., Peyré, G., and Moreau, T.
\newblock Super-efficiency of automatic differentiation for functions defined
  as a minimum, 2020.

\bibitem[Balcan et~al.(2019)Balcan, Khodak, and Talwalkar]{provable_guarantees}
Balcan, M., Khodak, M., and Talwalkar, A.
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, pp.\
  424--433, 2019.

\bibitem[Beatson \& Adams(2019)Beatson and Adams]{randtelsums}
Beatson, A. and Adams, R.~P.
\newblock Efficient optimization of loops and limits with randomized
  telescoping sums.
\newblock volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  534--543, Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/beatson19a.html}.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chen et~al.(2018)Chen, Jiang, Dai, and Zhao]{advrobbilevel}
Chen, Z., Jiang, H., Dai, B., and Zhao, T.
\newblock Learning to defense by learning to attack.
\newblock \emph{CoRR}, abs/1811.01213, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.01213}.

\bibitem[Christianson(1992)]{hessian}
Christianson, B.
\newblock {Automatic Hessians by reverse accumulation}.
\newblock \emph{IMA Journal of Numerical Analysis}, 12\penalty0 (2):\penalty0
  135--150, 04 1992.
\newblock ISSN 0272-4979.
\newblock \doi{10.1093/imanum/12.2.135}.
\newblock URL \url{https://doi.org/10.1093/imanum/12.2.135}.

\bibitem[Fallah et~al.(2019)Fallah, Mokhtari, and Ozdaglar]{convergence_theory}
Fallah, A., Mokhtari, A., and Ozdaglar, A.~E.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock \emph{CoRR}, abs/1908.10400, 2019.
\newblock URL \url{http://arxiv.org/abs/1908.10400}.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{maml}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1126--1135. JMLR. org, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and
  Levine]{online_metalearning}
Finn, C., Rajeswaran, A., Kakade, S.~M., and Levine, S.
\newblock Online meta-learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, pp.\
  1920--1930, 2019.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{fordiff}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1165--1173, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/franceschi17a.html}.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{hyperopt}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1568--1577, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/franceschi18a.html}.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{concrete}
Gal, Y., Hron, J., and Kendall, A.
\newblock Concrete dropout.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3581--3590, 2017.

\bibitem[Gould et~al.(2016)Gould, Fernando, Cherian, Anderson, Cruz, and
  Guo]{impl2}
Gould, S., Fernando, B., Cherian, A., Anderson, P., Cruz, R.~S., and Guo, E.
\newblock On differentiating parameterized argmin and argmax problems with
  application to bi-level optimization.
\newblock \emph{CoRR}, abs/1607.05447, 2016.
\newblock URL \url{http://arxiv.org/abs/1607.05447}.

\bibitem[Griewank(1992)]{logckpt}
Griewank, A.
\newblock Achieving logarithmic growth of temporal and spatial complexity in
  reverse automatic differentiation.
\newblock \emph{Optimization Methods and Software}, 1\penalty0 (1):\penalty0
  35--54, 1992.
\newblock \doi{10.1080/10556789208805505}.
\newblock URL \url{https://doi.org/10.1080/10556789208805505}.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{autograd}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating Derivatives: Principles and Techniques of
  Algorithmic Differentiation, Second Edition}.
\newblock Other Titles in Applied Mathematics. Society for Industrial and
  Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA
  19104), 2008.
\newblock ISBN 9780898717761.
\newblock URL \url{https://books.google.co.uk/books?id=xoiiLaRxcbEC}.

\bibitem[Gu et~al.(2016)Gu, Levine, Sutskever, and Mnih]{muprop}
Gu, S., Levine, S., Sutskever, I., and Mnih, A.
\newblock Muprop: Unbiased backpropagation for stochastic neural networks.
\newblock In \emph{ICLR (Poster)}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.05176}.

\bibitem[Hascoet \& Araya-Polo(2006)Hascoet and Araya-Polo]{ckpt}
Hascoet, L. and Araya-Polo, M.
\newblock Enabling user-driven checkpointing strategies in reverse-mode
  automatic differentiation.
\newblock \emph{arXiv preprint cs/0606042}, 2006.

\bibitem[Hazan(2019)]{oco_book}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{CoRR}, abs/1909.05207, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.05207}.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{gumbel}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with {G}umbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Ji et~al.(2020)Ji, Yang, and Liang]{gcsmaml}
Ji, K., Yang, J., and Liang, Y.
\newblock Multi-step model-agnostic meta-learning: Convergence and improved
  algorithms.
\newblock \emph{arXiv preprint arXiv:2002.07836}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{autoencoder}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {B}ayes.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6114}.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{vardrop}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2575--2583, 2015.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and Hinton]{cifar100}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-100 (canadian institute for advanced research).
\newblock 2009.
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and Tenenbaum]{omniglot}
Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database. 2010.
\newblock \emph{URL http://yann. lecun. com/exdb/mnist}, 7:\penalty0 23, 2010.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS}: Differentiable architecture search.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1eYHoC5FX}.

\bibitem[Lorraine et~al.(2019)Lorraine, Vicol, and Duvenaud]{hyperoptimpl}
Lorraine, J., Vicol, P., and Duvenaud, D.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock \emph{arXiv preprint arXiv:1911.02590}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{revopt}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2113--2122, 2015.

\bibitem[Maheswaranathan et~al.(2019)Maheswaranathan, Metz, Tucker, Choi, and
  Sohl-Dickstein]{esblo}
Maheswaranathan, N., Metz, L., Tucker, G., Choi, D., and Sohl-Dickstein, J.
\newblock Guided evolutionary strategies: augmenting random search with
  surrogate gradients.
\newblock volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  4264--4273, Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/maheswaranathan19a.html}.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{reptile}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{CoRR}, abs/1803.02999, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.02999}.

\bibitem[Pedregosa(2016)]{impl1}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML’16, pp.\
  737–746. JMLR.org, 2016.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and Levine]{imaml}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  113--124. 2019.

\bibitem[Shaban et~al.(2018)Shaban, Cheng, Hatch, and Boots]{truncated}
Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:1810.10667}, 2018.

\bibitem[Song et~al.(2020)Song, Gao, Yang, Choromanski, Pacchiano, and
  Tang]{esmaml}
Song, X., Gao, W., Yang, Y., Choromanski, K., Pacchiano, A., and Tang, Y.
\newblock {ES-MAML}: Simple hessian-free meta learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1exA2NtDB}.

\bibitem[Tucker et~al.(2017)Tucker, Mnih, Maddison, Lawson, and
  Sohl-Dickstein]{rebar}
Tucker, G., Mnih, A., Maddison, C.~J., Lawson, J., and Sohl-Dickstein, J.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2627--2636, 2017.

\bibitem[Wierstra et~al.(2014)Wierstra, Schaul, Glasmachers, Sun, Peters, and
  Schmidhuber]{nes}
Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and
  Schmidhuber, J.
\newblock Natural evolution strategies.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 949--980, 2014.

\bibitem[Williams(1992)]{reinforce}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Zügner \& Günnemann(2019)Zügner and Günnemann]{advrobgraphs}
Zügner, D. and Günnemann, S.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bylnx209YX}.

\end{thebibliography}
