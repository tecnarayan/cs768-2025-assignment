@inproceedings{Berthelot_NeurIPS_2019_mixmatch,
	title={Mixmatch: A holistic approach to semi-supervised learning},
	author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5049--5059},
	year={2019}
}

@article{Beyer_arXiv_2020_done_ImageNet,
	title={Are we done with ImageNet?},
	author={Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
	journal={arXiv preprint arXiv:2006.07159},
	year={2020}
}

@article{Brown_arXiv_2020_GPT3,
	title={Language models are few-shot learners},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={arXiv preprint arXiv:2005.14165},
	year={2020}
}

@inproceedings{Chollet_CVPR_2017_JFT,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}

@inproceedings{Ghosh_AAAI_2017_MAE,
	title={Robust loss functions under label noise for deep neural networks},
	author={Ghosh, Aritra and Kumar, Himanshu and Sastry, PS},
	booktitle={Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
	pages={1919--1925},
	year={2017}
}

@inproceedings{Han_NeurIPS_2018,
	title={Masking: A new perspective of noisy supervision},
	author={Han, Bo and Yao, Jiangchao and Niu, Gang and Zhou, Mingyuan and Tsang, Ivor and Zhang, Ya and Sugiyama, Masashi},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5836--5846},
	year={2018}
}

@inproceedings{Iscen_ECCV_2020,
	title={Graph convolutional networks for learning with few clean and many noisy labels},
	author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondrej and Schmid, Cordelia},
	booktitle={Proceedings of the European Conference on Computer Vision},
	year={2020}
}

@inproceedings{Kolesnikov_ECCV_2020_BiT,
	title={Big transfer (BiT): General visual representation learning},
	author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  	booktitle={Proceedings of the European Conference on Computer Vision},
  	year={2020}
}

@inproceedings{Krizhevsky_NIPS_2012_AlexNet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}

@inproceedings{Li_AIStats_2020_DeepLearningRobust,
	title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
	author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={4313--4324},
	year={2020},
	organization={PMLR}
}

@inproceedings{Li_ICLR_2020_dividemix,
	title={Dividemix: Learning with noisy labels as semi-supervised learning},
	author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
	booktitle={International Conference on Learning Representation},
	year={2020}
}

@article{Lin_TIT_1991_JS_Divergence,
	title={Divergence measures based on the Shannon entropy},
	author={Lin, Jianhua},
	journal={IEEE Transactions on Information theory},
	volume={37},
	number={1},
	pages={145--151},
	year={1991},
	publisher={IEEE}
}

@InProceedings{Liu_ICML_2020_Peer_Loss,
  title = 	 {Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates},
  author =       {Liu, Yang and Guo, Hongyi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6226--6236},
  year = 	 {2020},
  editor = 	 {Hal DaumÃ© III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}


@inproceedings{Lukasik_ICML_2020_label_smoothing_label_noisy,
	title={Does label smoothing mitigate label noise?},
	author={Lukasik, Michal and Bhojanapalli, Srinadh and Menon, Aditya Krishna and Kumar, Sanjiv},
	booktitle={International Conference on Machine Learning},
	year={2020}
}

@misc{Ma_ICML_2020_Normalized_Loss,
      title={Normalized Loss Functions for Deep Learning with Noisy Labels}, 
      author={Xingjun Ma and Hanxun Huang and Yisen Wang and Simone Romano and Sarah Erfani and James Bailey},
      year={2020},
      eprint={2006.13554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Miyato_PAMI_2018_VAT,
	title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
	author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={41},
	number={8},
	pages={1979--1993},
	year={2018},
	publisher={IEEE}
}

@inproceedings{Nguyen_ICLR_2020_self_ensemble,
	title={Self: Learning to filter noisy labels with self-ensembling},
	author={Nguyen, Duc Tam and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Beggel, Laura and Brox, Thomas},
	booktitle={International Conference on Learning Representation},
	year={2019}
}

@inproceedings{Natarajan_NIPS_2013,
	title={Learning with noisy labels},
	author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	booktitle={Advances in neural information processing systems},
	pages={1196--1204},
	year={2013}
}

@article{Northcutt_arXiv_2017,
	title={Learning with confident examples: Rank pruning for robust classification with noisy labels},
	author={Northcutt, Curtis G and Wu, Tailin and Chuang, Isaac L},
	journal={arXiv preprint arXiv:1705.01936},
	year={2017}
}

@article{Oliver_arXiv_2018_Realistic_Eval_SSL,
  title={Realistic evaluation of deep semi-supervised learning algorithms},
  author={Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D and Goodfellow, Ian J},
  journal={arXiv preprint arXiv:1804.09170},
  year={2018}
}

@inproceedings{Patrini_CVPR_2017,
	title={Making deep neural networks robust to label noise: A loss correction approach},
	author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={1944--1952},
	year={2017}
}

@article{Reed_arXiv_2014_bootstrapping,
	title={Training deep neural networks on noisy labels with bootstrapping},
	author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	journal={arXiv preprint arXiv:1412.6596},
	year={2014}
}

@inproceedings{Seo_NeurIPS_2019,
	title={Combinatorial Inference against Label Noise},
	author={Seo, Paul Hongsuck and Kim, Geeho and Han, Bohyung},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1173--1183},
	year={2019}
}

@inproceedings{Sukhbaatar_ICLR_2015_confusion_matrix,
	title={Training convolutional networks with noisy labels},
	author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
	booktitle={Proceedings of the international conference on learning representation},
	year={2015}
}

@inproceedings{Sun_CVPR_2017_Unreasonable_Effectiveness_Data,
	title={Revisiting unreasonable effectiveness of data in deep learning era},
	author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={843--852},
	year={2017}
}

@inproceedings{Szegedy_CVPR_2016_inception_label_smoothing,
	title={Rethinking the inception architecture for computer vision},
	author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2818--2826},
	year={2016}
}

@inproceedings{Tanaka_CVPR_2018_Joint_Optimization,
	title={Joint optimization framework for learning with noisy labels},
	author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={5552--5560},
	year={2018}
}

@inproceedings{Tarvainen_NIPS_2017_mean_teacher,
	title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
	author={Tarvainen, Antti and Valpola, Harri},
	booktitle={Advances in neural information processing systems},
	pages={1195--1204},
	year={2017}
}

@inproceedings{Vahdat_NeurIPS_2017_CRF,
	title={Toward robustness against label noise in training deep discriminative neural networks},
	author={Vahdat, Arash},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5596--5605},
	year={2017}
}

@inproceedings{Wang_ICCV_2019_Symmetric_CE,
	title={Symmetric cross entropy for robust learning with noisy labels},
	author={Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
	booktitle={Proceedings of the IEEE International Conference on Computer Vision},
	pages={322--330},
	year={2019}
}

@inproceedings{Wei_ICLR_2021_f_Divergence,
	title={When Optimizing f-Divergence is Robust with Label Noise},
	author={Wei, Jiaheng and Liu, Yang},
	booktitle={International Conference on Learning Representation},
	year={2021}
}

@misc{devries2017cutout,
      title={Improved Regularization of Convolutional Neural Networks with Cutout}, 
      author={Terrance DeVries and Graham W. Taylor},
      year={2017},
      eprint={1708.04552},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Xia_NeurIPS_2019,
	title={Are Anchor Points Really Indispensable in Label-Noise Learning?},
	author={Xia, Xiaobo and Liu, Tongliang and Wang, Nannan and Han, Bo and Gong, Chen and Niu, Gang and Sugiyama, Masashi},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6838--6849},
	year={2019}
}

@inproceedings{Xu_NeurIPS_2019_Information_Theoretic_Mutual_Info_Loss,
	title={L\_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise},
	author={Xu, Yilun and Cao, Peng and Kong, Yuqing and Wang, Yizhou},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6225--6236},
	year={2019}
}


@inproceedings{Zhang_NeurIPS_2018_Generalized_CE,
	title={Generalized cross entropy loss for training deep neural networks with noisy labels},
	author={Zhang, Zhilu and Sabuncu, Mert},
	booktitle={Advances in neural information processing systems},
	pages={8778--8788},
	year={2018}
}


@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}


@misc{cubuk2019randaugment,
      title={RandAugment: Practical automated data augmentation with a reduced search space}, 
      author={Ekin D. Cubuk and Barret Zoph and Jonathon Shlens and Quoc V. Le},
      year={2019},
      eprint={1909.13719},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{Zhang17RethinkingGeneralization,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      eprint={1611.03530},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Jiang18MentorNet,
title	= {MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
author	= {Lu Jiang and Zhenyuan Zhou and Thomas Leung and Jia Li and Fei-Fei Li},
year	= {2018},
booktitle	= {ICML}
}

@misc{patrini2017making,
      title={Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach}, 
      author={Giorgio Patrini and Alessandro Rozza and Aditya Menon and Richard Nock and Lizhen Qu},
      year={2017},
      eprint={1609.03683},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{li2017webvision,
      title={WebVision Database: Visual Learning and Understanding from Web Data}, 
      author={Wen Li and Limin Wang and Wei Li and Eirikur Agustsson and Luc Van Gool},
      year={2017},
      eprint={1708.02862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zheltonozhskii2021contrast,
    title={Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels},
    author={Evgenii Zheltonozhskii and Chaim Baskin and Avi Mendelson and Alex M. Bronstein and Or Litany},
    year={2021},
    eprint={2103.13646},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{hendrycks2020augmix,
      title={AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty}, 
      author={Dan Hendrycks and Norman Mu and Ekin D. Cubuk and Barret Zoph and Justin Gilmer and Balaji Lakshminarayanan},
	booktitle={International Conference on Learning Representation},
	year={2020}
}

@misc{li2019learning,
      title={Learning to Learn from Noisy Labeled Data}, 
      author={Junnan Li and Yongkang Wong and Qi Zhao and Mohan Kankanhalli},
      year={2019},
      eprint={1812.05214},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2020earlylearning,
      title={Early-Learning Regularization Prevents Memorization of Noisy Labels}, 
      author={Sheng Liu and Jonathan Niles-Weed and Narges Razavian and Carlos Fernandez-Granda},
      year={2020},
      eprint={2007.00151},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2021learning,
      title={Learning with Feature-Dependent Label Noise: A Progressive Approach}, 
      author={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},
      year={2021},
      eprint={2103.07756},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tack2021consistency,
      title={Consistency Regularization for Adversarial Robustness}, 
      author={Jihoon Tack and Sihyun Yu and Jongheon Jeong and Minseon Kim and Sung Ju Hwang and Jinwoo Shin},
      year={2021},
      eprint={2103.04623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{song2019selfie,
title={{SELFIE}: Refurbishing Unclean Samples for Robust Deep Learning},
author={Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
booktitle={ICML},
year={2019} }

@inproceedings{lee2017cleannet,
  title={CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise},
  author={Lee, Kuang-Huei and He, Xiaodong and Zhang, Lei and Yang, Linjun},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year={2018}
}

@article{csiszar1967fdiv,
author="CSISZAR, I.",
title="Information-type measures of difference of probability distributions and indirect observation",
journal="Studia Scientiarum Mathematicarum Hungarica",
ISSN="",
publisher="",
year="1967",
month="",
volume="2",
number="",
pages="229-318",
URL="https://ci.nii.ac.jp/naid/10028997448/en/",
DOI="",
}


@Article{nielsen2019JSMeans,
AUTHOR = {Nielsen, Frank},
TITLE = {On the Jensen–Shannon Symmetrization of Distances Relying on Abstract Means},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {485},
URL = {https://www.mdpi.com/1099-4300/21/5/485},
ISSN = {1099-4300},
ABSTRACT = {The Jensen&ndash;Shannon divergence is a renowned bounded symmetrization of the unbounded Kullback&ndash;Leibler divergence which measures the total Kullback&ndash;Leibler divergence to the average mixture distribution. However, the Jensen&ndash;Shannon divergence between Gaussian distributions is not available in closed form. To bypass this problem, we present a generalization of the Jensen&ndash;Shannon (JS) divergence using abstract means which yields closed-form expressions when the mean is chosen according to the parametric family of distributions. More generally, we define the JS-symmetrizations of any distance using parameter mixtures derived from abstract means. In particular, we first show that the geometric mean is well-suited for exponential families, and report two closed-form formula for (i) the geometric Jensen&ndash;Shannon divergence between probability densities of the same exponential family; and (ii) the geometric JS-symmetrization of the reverse Kullback&ndash;Leibler divergence between probability densities of the same exponential family. As a second illustrating example, we show that the harmonic mean is well-suited for the scale Cauchy distributions, and report a closed-form formula for the harmonic Jensen&ndash;Shannon divergence between scale Cauchy distributions. Applications to clustering with respect to these novel Jensen&ndash;Shannon divergences are touched upon.},
DOI = {10.3390/e21050485}
}


@Inbook{Amari1985,
author="Amari, Shun-ichi",
title="$\alpha$-Divergence and $\alpha$-Projection in Statistical Manifold",
bookTitle="Differential-Geometrical Methods in Statistics",
year="1985",
publisher="Springer New York",
address="New York, NY",
pages="66--103",
abstract="The present chapter treats more fundamental structures underlying differential geometry of statistical manifolds. A pair of dual affine connections together with a Riemannian metric play a fundamental role in the present theory. Such a structure has not so far been noticed in literature of differential geometry. The dualistic structure of the geometry is elucidated by using the $\alpha$-flat manifold will turn out to be an interesting generalization of the Euclidean space, admitting the Pythagorean relation with respect to the $\alpha$-divergence of two points.",
isbn="978-1-4612-5056-2",
doi="10.1007/978-1-4612-5056-2_3",
url="https://doi.org/10.1007/978-1-4612-5056-2_3"
}


@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2020}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


