@inproceedings{blalock2020survey,
  title={What is the State of Neural Network Pruning?},
  author={Blalock, D. and Ortiz, J. and Frankle, J. and Guttag, J.},
  booktitle={MLSys},
  year={2020}
}

@inproceedings{lecun1989damage,
  title={Optimal Brain Damage},
  author={LeCun, Y. and Denker, J. and Solla, S.},
  booktitle={NeurIPS},
  year={1989}
}

@inproceedings{hassibi1992surgeon,
  title={Second order derivatives for network pruning: {Optimal Brain Surgeon}},
  author={Hassibi, B. and Stork, D.},
  booktitle={NeurIPS},
  year={1992}
}

@inproceedings{hassibi1993surgeon,
  title={Optimal Brain Surgeon and General Network Pruning},
  author={Hassibi, B. and Stork, D. and Wolff, G.},
  booktitle={IEEE International Conference on Neural Networks},
  year={1993}
}

@inproceedings{lebedev2016braindamage,
  title={Fast {ConvNets} Using Group-wise Brain Damage},
  author={V. Lebedev and V. Lempitsky},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{molchanov2017taylor,
  title={Pruning Convolutional Neural Networks for Resource Efficient Inference},
  author={Molchanov, P. and Tyree, S. and Karras, T. and Aila, T. and Kautz, J.},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{dong2017layerwise,
  title={Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon},
  author={X. Dong and S. Chen and S. Pan},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{yu2018importance,
  title={{NISP}: Pruning Networks using Neuron Importance Score Propagation},
  author={R. Yu and A. Li and C. Chen and J. Lai and V. Morariu and X. Han and M. Gao and C. Lin and L. Davis},
  booktitle={CVPR},
  year={2018}
}

@unpublished{zeng2018mlprune,
  title={{MLPrune}: Multi-Layer Pruning for Automated Neural Network Compression},
  author={W. Zeng and R. Urtasun},
  year={2018}
}

@inproceedings{lee2019pretraining,
  title={{SNIP}: Single-shot Network Pruning based on Connection Sensitivity},
  author={Lee, N. and Ajanthan, T. and Torr, P.},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{wang2019eigendamage,
  title={{EigenDamage}: Structured Pruning in the {Kronecker}-Factored Eigenbasis},
  author={C. Wang and R. Grosse and S. Fidler and G. Zhang},
  booktitle={ICML},
  year={2019}
}

@inproceedings{lee2020pretraining,
  title={A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
  author={Lee, N. and Ajanthan, T. and Gould, S. and Torr, P.},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{frankle2021initialization,
  title={Pruning Neural Networks at Initialization: Why are We Missing the Mark?},
  author={J. Frankle and G. Dziugaite and D. Roy and M. Carbin},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{wang2020tickets,
  title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
  author={C. Wang and G. Zhang and R. Grosse},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{singh2020woodfisher,
  title={{WoodFisher}: Efficient Second-Order Approximation for Neural Network Compression},
  author={Sidak Pal Singh and Dan Alistarh},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{serra2020lossless,
  title={Lossless Compression of Deep Neural Networks},
  author={T. Serra and A. Kumar and S. Ramalingam},
  booktitle={CPAIOR},
  year={2020}
}

@inproceedings{sourek2021lossless,
  title={Lossless Compression of Structured Convolutional Models via Lifting},
  author={G. Sourek and F. Zelezny},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{serra2021compression,
  title={Scaling Up Exact Neural Network Compression by
{ReLU} Stability},
  author={T. Serra and X. Yu and A. Kumar and S. Ramalingam},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{iclr2022compression,
  title={Model Compression via Symmetries of the Parameter Space},
  author={Iordan Ganev and Robin Walters},
  url={https://openreview.net/forum?id=8MN_GH4Ckp4},
  year={2022}
}

@inproceedings{hanson1988minimal,
  title={Comparing biases for minimal network construction with back-propagation},
  author={S. Hanson and L. Pratt},
  booktitle={NeurIPS},
  year={1988}
}

@article{mozer1989relevance,
  title={Using Relevance to Reduce Network Size Automatically},
  author={Mozer, M. and Smolensky, P.},
  journal={Connection Science},
  year={1989}
}

@article{janowsky1989prunning,
  title={Pruning Versus Clipping in Neural Networks},
  author={Janowsky, S.},
  journal={Physical Review A},
  year={1989}
}

@inproceedings{han2015connections,
  title={Learning Both Weights and Connections for Efficient Neural Network},
  author={Han, S. and Pool, J. and Tran, J. and Dally, W.},
  booktitle={NeurIPS},
  year={2015}
}

@inproceedings{han2016deepcompression,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and {Huffman} Coding},
  author={S. Han and H. Mao and W. Dally},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{li2017cnn,
  title={Pruning Filters for Efficient ConvNets},
  author={Li, H. and Kadav, A. and Durdanovic, I. and Samet, H. and Graf, H.},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{frankle2019lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, J. and Carbin, M.},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{gordon2020bert,
  title={Compressing {BERT}: Studying the Effects of Weight Pruning on Transfer Learning},
  author={M. Gordon and K. Duh and N. Andrews},
  booktitle={Rep4NLP Workshop},
  year={2020}
}

@inproceedings{tanaka2020synapticflow,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={H. Tanaka and D. Kunin and D. Yamins and S. Ganguli},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{denil2013parameters,
  title={Predicting Parameters in Deep Learning},
  author={M. Denil and B. Shakibi and L. Dinh and M. Ranzato and N. Freitas},
  booktitle={NeurIPS},
  year={2013}
}

@article{hooker2019forget,
  title={What Do Compressed Deep Neural Networks Forget?},
  author={S. Hooker and A. Courville and G. Clark and Y. Dauphin and A. Frome},
  journal={arXiv},
  volume={1911.05248},
  year={2019}
}

@article{paganini2020responsibly,
  title={Prune Responsibly},
  author={M. Paganini},
  journal={arXiv},
  volume={2009.09936},
  year={2020}
}

@article{hooker2020bias,
  title={Characterising Bias in Compressed Models},
  author={S. Hooker and N. Moorosi and G. Clark and S. Bengio and E. Denton},
  journal={arXiv},
  volume={2010.03058},
  year={2020}
}

@inproceedings{liebenwein2021lost,
  title={Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy},
  author={L. Liebenwein and C. Baykal and B. Carter and D. Gifford and D. Rus},
  booktitle={MLSys},
  year={2021}
}

@inproceedings{he2017cnn,
  title={Channel Pruning for Accelerating Very Deep Neural Networks},
  author={He, Y. and Zhang, X. and Sun, J.},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{luo2017thinet,
  title={{ThiNet}: A Filter Level Pruning Method for Deep Neural Network Compression},
  author={J. Luo and J. Wu and W. Lin},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{aghasi2017nettrim,
  title={{Net-Trim}: Convex Pruning of Deep Neural Networks with Performance Guarantee},
  author={Aghasi, A. and Abdi, A. and Nguyen, N. and Romberg, J.},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{elaraby2020importance,
  title={Identifying Efficient Sub-networks using Mixed Integer Programming},
  author={M. ElAraby and G. Wolf and M. Carvalho},
  booktitle={OPT Workshop},
  year={2020}
}

@inproceedings{srinivas2015datafree,
  title={Data-free parameter pruning for Deep Neural Networks},
  author={S. Srinivas and R. Venkatesh Babu},
  booktitle={BMVC},
  year={2015}
}

@inproceedings{mariet2016diversity,
  title={Diversity Networks: Neural Network Compression Using Determinantal Point Processes},
  author={Z. Mariet and S. Sra},
  booktitle={ICLR},
  year={2016}
}


@inproceedings{suau2020distillation,
  title={Filter Distillation for Network Compression},
  author={X. Suau and L. Zappella and N. Apostoloff},
  booktitle={WACV},
  year={2020}
}

@inproceedings{suzuki2020spectral,
  title={Spectral-pruning: Compressing Deep Neural Network via Spectral Analysis},
  author={Suzuki, T. and Abe, H. and Murata, T. and Horiuchi, S. and Ito, K. and Wachi, T. and Hirai, S. and Yukishima, M. and Nishimura,T.},
  booktitle={IJCAI},
  year={2020}
}

@inproceedings{jaderberg2014lowrank,
  title={Speeding up Convolutional Neural Networks with Low Rank Expansions},
  author={M. Jaderberg and A. Vedaldi and A. Zisserman},
  booktitle={BMVC},
  year={2014}
}

@inproceedings{denton2014linear,
  title={Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
  author={E. Denton and W. Zaremba and J. Bruna and Y. LeCun and R. Fergus},
  booktitle={NeurIPS},
  year={2014}
}

@inproceedings{lebedev2015decomposition,
  title={Speeding-up Convolutional Neural Networks Using Fine-tuned {CP}-Decomposition},
  author={V. Lebedev and Y. Ganin and M. Rakhuba and I. Oseledets and V. Lempitsky},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{arora2018bounds,
  title={Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author={Arora, S. and Ge, R. and Neyshabur, B. and Zhang, Y.},
  booktitle={ICML},
  year={2018}
}

@misc{wang2018wide,
      title={Wide Compression: Tensor Ring Nets}, 
      author={W. Wang and Y. Sun and B. Eriksson and W. Wang and V. Aggarwal},
      year={2018},
      eprint={1802.09052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{su2018tensorial,
      title={Tensorial Neural Networks: Generalization of Neural Networks and Application to Model Compression}, 
      author={J. Su and J. Li and B. Bhattacharjee and F. Huang},
      year={2018},
      eprint={1805.10352},
      archivePrefix={arXiv}
}

@inproceedings{suzuki2020bounds,
  title={Compression Based Bound for Non-compressed Network: Unified Generalization Error Analysis of Large Compressible Deep Neural Network},
  author={Suzuki, T. and Abe, H. and Nishimura, T.},
  booktitle={ICLR},
  year={2020}
}

@misc{li2020understanding,
      title={Understanding Generalization in Deep Learning via Tensor Methods}, 
      author={J. Li and Y. Sun and J. Su and T. Suzuki and F. Huang},
      year={2020},
      eprint={2001.05070},
      archivePrefix={arXiv},
}

@inproceedings{xing2020lossless,
  title={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},
  author={Xing, X. and Sha, L. and Hong, P. and Shang, Z. and Liu, J.},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{daiki2021randomization,
  title={Pruning Randomly Initialized Neural Networks with Iterative Randomization},
  author={Daiki Chijiwa and Shin'ya Yamaguchi and Yasutoshi Ida and Kenji Umakoshi and Tomohiro Inoue},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{ramanujan2020,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Vivek Ramanujan and Mitchell Wortsman and Aniruddha Kembhavi and Ali Farhadi and Mohammad Rastegari},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{wu2021adversarial,
  title={Adversarial Neuron Pruning Purifies Backdoored Deep Models},
  author={Dongxian Wu and Yisen Wang},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{liu2021sparse,
  title={Sparse Training via Boosting Pruning Plasticity with Neuroregeneration},
  author={Shiwei Liu and Tianlong Chen and Xiaohan Chen and Zahra Atashgahi and Lu Yin and Huanyu Kou and Li Shen and Mykola Pechenizkiy and Zhangyang Wang and Decebal Constantin Mocanu},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{huang2021rethinking,
  title={Rethinking the Pruning Criteria for Convolutional Neural Network},
  author={Zhongzhan Huang and Wenqi Shao and Xinjiang Wang and Liang Lin and Ping Luo},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{chen2021once,
  title={Only Train Once: A One-Shot Neural Network Training And Pruning Framework},
  author={Tianyi Chen and Bo Ji and Tianyu Ding and Biyi Fang and Guanyi Wang and Zhihui Zhu and Luming Liang and Yixin Shi and Sheng Yi and Xiao Tu},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{zhou2019lth,
  title={Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  author={Hattie Zhou and Janice Lan and Rosanne Liu and Jason Yosinski},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{malach2020lth,
  title={Proving the Lottery Ticket Hypothesis: Pruning is All You Need},
  author={Eran Malach and Gilad Yehudai and Shai Shalev-Shwartz and Ohad Shamir},
  booktitle={ICML},
  year={2020}
}

@inproceedings{pensia2020lth,
  title={Optimal Lottery Tickets via {SubsetSum}: Logarithmic Over-Parameterization is Sufficient},
  author={Ankit Pensia and Shashank Rajput and Alliot Nagle and Harit Vishwakarma and Dimitris Papailiopoulos},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{orseau2020lth,
  title={Logarithmic Pruning is All You Need},
  author={Laurent Orseau and Marcus Hutter and Omar Rivasplata},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{qian2021lth,
  title={A Probabilistic Approach to Neural Network Pruning},
  author={Xin Qian and Diego Klabjan},
  booktitle={ICML},
  year={2021}
}

@misc{elesedy2020lth,
      title={Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning}, 
      author={Bryn Elesedy and Varun Kanade and Yee Whye Teh},
      year={2020},
      eprint={2007.08243},
      archivePrefix={arXiv},
}

@inproceedings{ye2020forward,
  title={Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection},
  author={Mao Ye and Chengyue Gong and Lizhen Nie and Denny Zhou and Adam Klivans and Qiang Liu},
  booktitle={ICML},
  year={2020}
}

@inproceedings{baykal2019coresets,
  title={Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds},
  author={Cenk Baykal and Lucas Liebenwein and Igor Gilitschenski and Dan Feldman and Daniela Rus},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{liebenwein2020provable,
  title={Provable Filter Pruning for Efficient Neural Networks},
  author={Lucas Liebenwein and Cenk Baykal and Harry Lang and Dan Feldman and Daniela Rus},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{zhang2021lessdata,
  title={Efficient Lottery Ticket Finding: Less Data is More},
  author={Zhenyu Zhang and Xuxi Chen and Tianlong Chen and Zhangyang Wang},
  booktitle={ICML},
  year={2021}
}

@inproceedings{liu2021group,
  title={Group Fisher Pruning for Practical Network Compression},
  author={Liyang Liu and Shilong Zhang and Zhanghui Kuang and Aojun Zhou and Jing-Hao Xue and Xinjiang Wang and Yimin Chen and Wenming Yang and Qingmin Liao and Wayne Zhang},
  booktitle={ICML},
  year={2021}
}

@inproceedings{verma2021subdifferential,
  title={Sparsifying Networks via Subdifferential Inclusion},
  author={Sagar Verma and Jean-Christophe Pesquet},
  booktitle={ICML},
  year={2021}
}

@misc{ebrahimi2021kfac,
      title={Neuron-based Pruning of Deep Neural Networks with Better Generalization using Kronecker Factored Curvature Approximation}, 
      author={Abdolghani Ebrahimi and Diego Klabjan},
      year={2021},
      eprint={2111.08577},
      archivePrefix={arXiv}
}

@inproceedings{liu2019structure,
  title={Rethinking the Value of Network Pruning},
  author={Liu, Z. and Sun, M. and Zhou, T. and Huang, G. and Darrell, T.},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{renda2020retraining,
  title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
  author={A. Renda and J. Frankle and M. Carbin},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{zhang2017rethinking,
  title={Understanding deep learning requires rethinking generalization},
  author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  booktitle={ICLR},
  year={2017}
}

@ARTICLE{belkin2019nas,
  author={Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
  journal={PNAS}, 
  title={Reconciling modern machine learning practice and the bias-variance trade-off}, 
  year={2019},
  volume={116},
  number={32},
  pages={15849--15854}
}

@inproceedings{li2018landscape,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
  booktitle={NeurIPS},
  year={2018}
}

@ARTICLE{ruoyu2020landscape,
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine}, 
  title={The Global Landscape of Neural Networks: An Overview},
  year={2020},
  volume={37},
  number={5},
  pages={95--108}
}

@inproceedings{pascanu2013on,
  title={On the number of response regions of deep feedforward networks with piecewise linear activations},
  author={R. Pascanu and G. Mont\'{u}far and Y. Bengio},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{montufar2014on,
  title={On the Number of Linear Regions of Deep Neural Networks},
  author={G. Mont\'{u}far and R. Pascanu and K. Cho and Y. Bengio},
  booktitle={NeurIPS},
  year={2014}
} 

@inproceedings{montufar2017notes,
  title={Notes on the number of linear regions of deep neural networks},
  author={G. Mont\'{u}far},
  booktitle={SampTA},
  year={2017}
} 

@inproceedings{arora2018understanding,
  title={Understanding Deep Neural Networks with Rectified Linear Units},
  author={R. Arora and A. Basu and P. Mianjy and A. Mukherjee},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{serra2018bounding,
  title={Bounding and Counting Linear Regions of Deep Neural Networks},
  author={T. Serra and C. Tjandraatmadja and S. Ramalingam},
  booktitle={ICML},
  year={2018}
}

@inproceedings{hanin2019complexity,
  title={Complexity of Linear Regions in Deep Networks},
  author={Hanin, Boris and Rolnick, David},
  booktitle={ICML},
  year={2019}
}

@inproceedings{hanin2019deep,
  title={Deep ReLU Networks Have Surprisingly Few Activation Patterns},
  author={Hanin, Boris and Rolnick, David},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{serra2020empirical,
  title={Empirical Bounds on Linear Regions of Deep Rectifier Networks},
  author={T. Serra and S. Ramalingam},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{xiong2020cnn,
  title={On the Number of Linear Regions of Convolutional Neural Networks},
  author={H. Xiong and L. Huang and M. Yu and L. Liu and F. Zhu and L. Shao},
  booktitle={ICML},
  year={2020}
}

@article{Telgarsky2015,
  title={Representation Benefits of Deep Feedforward Networks},
  author={M. Telgarsky},
  eprint={1509.08101},
  archivePrefix={arXiv},
  year={2015}
}

@inproceedings{tseran2021expected,
  title={On the Expected Complexity of Maxout Networks},
  author={Hanna Tseran and Guido Mont\'{u}far},
  booktitle={NeurIPS},
  year={2021}
}

@misc{montufar2021maxout,
  title={Sharp bounds for the number of regions of maxout networks and vertices of {Minkowski} sums},
  author={Guido Mont\'{u}far and Yue Ren and Leon Zhang},
  eprint={2104.08135},
  archivePrefix={arXiv},
  year={2021}
}

@article{hart1987semigreedy,
  title={Semi-greedy heuristics: An empirical study},
  author={Hart, J. P. and Shogan, A. W.},
  journal={Operations Research Letters},
  volume = {6},
  number = {3},
  pages = {107--114},
  year={1987}
}

@article{feo1989setcovering,
  title={A probabilistic heuristic for a computationally difficult set covering problem},
  author={Feo, Thomas A. and Resende, Mauricio G. C.},
  journal={Operations Research Letters},
  volume = {8},
  number = {2},
  pages = {67--71},
  year={1989}
}


@article{feo1995grasp,
  title={Greedy Randomized Adaptive Search Procedures},
  author={Feo, Thomas A. and Resende, Mauricio G. C.},
  journal={Journal of Global Optimization},
  volume = {6},
  number = {2},
  pages = {109--133},
  year={1995}
}

@inproceedings{bixby2012history,
  title={A brief history of linear and mixed-integer programming computation},
  author={R.E. Bixby},
  booktitle={Documenta Mathematica, Extra Volume: Optimization Stories},
  pages={107--121},
  year={2012}
}

@article{benders1962,
  title={Partitioning procedures for solving mixed-variables programming problems},
  author={J.F. Benders},
  journal={Numerische Mathematik},
  volume = {4},
  pages = {238--252},
  year={1962}
}

@article{hooker2003lbbd,
  title={Logic-based {Benders} decomposition},
  author={J.N. Hooker and G. Ottosson},
  journal={Mathematical Programming},
  volume = {96},
  pages = {33--60},
  year={2003}
}

@article{padberg1989,
  title={The boolean quadric polytope: Some characteristics, facets and relatives},
  author={Manfred Padberg},
  journal={Mathematical Programming},
  volume = {45},
  pages = {139--172},
  year={1989}
}

@article{croes1958inversion,
  title={A method for solving traveling-salesman problems},
  author={G.A. Croes},
  journal={Operations Research},
  volume = {6},
  pages = {791--812},
  year={1958}
}

@article{flood1956tsp,
  title={The traveling-salesman problem},
  author={M.M. Flood},
  journal={Operations Research},
  volume = {4},
  pages = {61--75},
  year={1956}
}

@article{lin1965tsp,
  title={Computer solutions of the traveling salesman problem},
  author={S. Lin},
  journal={The Bell System Technical Journal},
  volume = {44},
  pages = {2245--2269},
  year={1965}
}

@book{tsp2006book,
  title={The Traveling Salesman Problem: A Computational Study},
  author={D.L. Applegate and R.E. Bibxy and V. Chv\'atal and W.J. Cook},
  year={2006},
  publisher={Princeton University Press}
}

@book{ls1996book,
  title={Local Search in Combinatorial Optimization},
  author={E. Aarts and J.K. Lenstra},
  year={1996},
  publisher={Wiley}
}

@book{ccz2014book,
  title={Integer Programming},
  author={M. Conforti and G. Cornu\'ejols and G. Zambelli},
  year={2014},
  publisher={Springer}
}


@misc{flounder1996,
  title = {Optimization: Your Worst Enemy},
  author = {Joseph M. Newcomer},
  howpublished = {\url{http://flounder.com/optimization.htm}},
  note = {Accessed: 2022-01-28},
  year={1996}
}

@article{mobilenets,
  title={{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={A.G. Howard and M. Zhu and B. Chen and D. Kalenichenko and W. Wang and T. Weyand and M. Andreetto and H. Adam},
  eprint={1704.04861},
  archivePrefix={arXiv},
  year={2017}
}

@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={Y. LeCun and L. Bottou and Y. Bengio and P. Haffner},
  journal={Proceedings of the IEEE},
  year={1998}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, A. and others},
  year={2009}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{imagenet,
  title={{ImageNet}: A Large-Scale Hierarchical
Image Database},
  author={J. Deng and W. Dong and R. Socher and L.-J. Li and K. Li and L. Fei-Fei},
  booktitle={CVPR},
  year={2009}
}

@article{hu2016trimming,
  title={Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures},
  author={Hengyuan Hu and Rui Peng and Yu-Wing Tai and Chi-Keung Tang},
  eprint={1607.03250},
  archivePrefix={arXiv},
  year={2016}
}

@article{good2022recall,
  title={Recall Distortion in Neural Network Pruning and the Undecayed Pruning Algorithm},
  author={Aidan Good and Jiaqi Lin and Hannah Sieg and Mikey Ferguson and Xin Yu and Shandian Zhe and Jerzy Wieczorek and Thiago Serra},
  eprint={2206.02976},
  archivePrefix={arXiv},
  year={2022}
}


@article{ramalingam2021less,
  title={Less data is more: Selecting informative and diverse subsets with balancing constraints},
  author={Ramalingam, Srikumar and Glasner, Daniel and Patel, Kaushal and Vemulapalli, Raviteja and Jayasumana, Sadeep and Kumar, Sanjiv},
  journal   = {CoRR},
  volume    = {abs/2104.12835},
  year={2021}
}

@article{roh2021sample,
  title={Sample selection for fair and robust training},
  author={Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={815--827},
  year={2021}
}

@article{nguyen2021dataset,
  title={Dataset distillation with infinitely wide convolutional networks},
  author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5186--5198},
  year={2021}
}




