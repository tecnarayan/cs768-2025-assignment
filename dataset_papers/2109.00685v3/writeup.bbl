% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{ynt/global//global/global}
    \entry{Boyd2004-ey}{book}{}
      \name{author}{2}{}{%
        {{hash=ec94d94cc487dd71939a90cbeaaf47d0}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=2608d599a0bc2a784c301329a8da5f7b}{%
           family={Vandenberghe},
           familyi={V\bibinitperiod},
           given={Lieven},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{fullhash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{bibnamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authorbibnamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authornamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authorfullhash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.}
      \field{month}{3}
      \field{title}{Convex Optimization}
      \field{year}{2004}
      \true{nocite}
    \endentry
    \entry{lecun-mnisthandwrittendigit-2010}{article}{}
      \name{author}{2}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=17acda211a651e90e228f1776ee07818}{%
           family={Cortes},
           familyi={C\bibinitperiod},
           given={Corinna},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \strng{fullhash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \strng{bibnamehash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \strng{authorbibnamehash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \strng{authornamehash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \strng{authorfullhash}{9efbfd2a4a8416bfe3ef496e12b597e0}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{howpublished}{http://yann.lecun.com/exdb/mnist/}
      \field{title}{{MNIST} handwritten digit database}
      \field{year}{2010}
      \verb{urlraw}
      \verb http://yann.lecun.com/exdb/mnist/
      \endverb
      \verb{url}
      \verb http://yann.lecun.com/exdb/mnist/
      \endverb
      \keyw{MSc _checked character_recognition mnist network neural}
    \endentry
    \entry{Daniely2012-pw}{article}{}
      \name{author}{3}{}{%
        {{hash=3e3cb694117f9794594290f446d2dd8f}{%
           family={Daniely},
           familyi={D\bibinitperiod},
           given={Amit},
           giveni={A\bibinitperiod}}}%
        {{hash=0dafa6bad2e178837ff918538e24226a}{%
           family={Sabato},
           familyi={S\bibinitperiod},
           given={Sivan},
           giveni={S\bibinitperiod}}}%
        {{hash=c2f9b7868dce4b6a6ed821be1f7ee4fc}{%
           family={Shwartz},
           familyi={S\bibinitperiod},
           given={Shai\bibnamedelima Shalev},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{73d54613dfbbc55d099103a636966077}
      \strng{fullhash}{73d54613dfbbc55d099103a636966077}
      \strng{bibnamehash}{73d54613dfbbc55d099103a636966077}
      \strng{authorbibnamehash}{73d54613dfbbc55d099103a636966077}
      \strng{authornamehash}{73d54613dfbbc55d099103a636966077}
      \strng{authorfullhash}{73d54613dfbbc55d099103a636966077}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \textbackslashemph\{approximation error\} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{title}{Multiclass Learning Approaches: A Theoretical Comparison with Implications}
      \field{year}{2012}
      \true{nocite}
      \verb{eprint}
      \verb 1205.6432
      \endverb
    \endentry
    \entry{Hardt2012-xk}{misc}{}
      \name{author}{2}{}{%
        {{hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod}}}%
        {{hash=d3a533fb3a2bb9d053af2a25892a7315}{%
           family={Moitra},
           familyi={M\bibinitperiod},
           given={Ankur},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{ce7ac90f9166d1aaf7915f913985df39}
      \strng{fullhash}{ce7ac90f9166d1aaf7915f913985df39}
      \strng{bibnamehash}{ce7ac90f9166d1aaf7915f913985df39}
      \strng{authorbibnamehash}{ce7ac90f9166d1aaf7915f913985df39}
      \strng{authornamehash}{ce7ac90f9166d1aaf7915f913985df39}
      \strng{authorfullhash}{ce7ac90f9166d1aaf7915f913985df39}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider a fundamental problem in unsupervised learning called \textbackslashemph\{subspace recovery\}: given a collection of $m$ points in $\mathbb\{R\}^n$, if many but not necessarily all of these points are contained in a $d$-dimensional subspace $T$ can we find it? The points contained in $T$ are called \{\textbackslashem inliers\} and the remaining points are \{\textbackslashem outliers\}. This problem has received considerable attention in computer science and in statistics. Yet efficient algorithms from computer science are not robust to \{\textbackslashem adversarial\} outliers, and the estimators from robust statistics are hard to compute in high dimensions. Are there algorithms for subspace recovery that are both robust to outliers and efficient? We give an algorithm that finds $T$ when it contains more than a $\frac\{d\}\{n\}$ fraction of the points. Hence, for say $d = n/2$ this estimator is both easy to compute and well-behaved when there are a constant fraction of outliers. We prove that it is Small Set Expansion hard to find $T$ when the fraction of errors is any larger, thus giving evidence that our estimator is an \{\textbackslashem optimal\} compromise between efficiency and robustness. As it turns out, this basic problem has a surprising number of connections to other areas including small set expansion, matroid theory and functional analysis that we make use of here.}
      \field{eprintclass}{cs.CC}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{Algorithms and Hardness for Robust Subspace Recovery}
      \field{year}{2012}
      \true{nocite}
      \verb{eprint}
      \verb 1211.1041
      \endverb
    \endentry
    \entry{Shalev-Shwartz2014-oj}{book}{}
      \name{author}{2}{}{%
        {{hash=a6c5827676b0a2058e5b904c3dd7f878}{%
           family={Shalev-Shwartz},
           familyi={S\bibinithyphendelim S\bibinitperiod},
           given={Shai},
           giveni={S\bibinitperiod}}}%
        {{hash=509866d90ae18bfca30c6f4cdc846529}{%
           family={Ben-David},
           familyi={B\bibinithyphendelim D\bibinitperiod},
           given={Shai},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{38fabf8438a7aae182a9d192e7aa2d98}
      \strng{fullhash}{38fabf8438a7aae182a9d192e7aa2d98}
      \strng{bibnamehash}{38fabf8438a7aae182a9d192e7aa2d98}
      \strng{authorbibnamehash}{38fabf8438a7aae182a9d192e7aa2d98}
      \strng{authornamehash}{38fabf8438a7aae182a9d192e7aa2d98}
      \strng{authorfullhash}{38fabf8438a7aae182a9d192e7aa2d98}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.}
      \field{month}{5}
      \field{title}{Understanding Machine Learning: From Theory to Algorithms}
      \field{year}{2014}
      \true{nocite}
    \endentry
    \entry{Ribeiro2016-ig}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9203dc724f298030e81ba470feb309af}{%
           family={Ribeiro},
           familyi={R\bibinitperiod},
           given={Marco\bibnamedelima Tulio},
           giveni={M\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=0b9694812f0dc77974d99bb875a76d48}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Sameer},
           giveni={S\bibinitperiod}}}%
        {{hash=c9ab8ac486d7f85ac914b1a8e51e512b}{%
           family={Guestrin},
           familyi={G\bibinitperiod},
           given={Carlos},
           giveni={C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {San Francisco, California, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{fullhash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{bibnamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authorbibnamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authornamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authorfullhash}{70906198418f30a779bc9bda0f6ae1f1}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.}
      \field{booktitle}{Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
      \field{month}{8}
      \field{series}{KDD '16}
      \field{title}{``Why Should {I} Trust You?'': Explaining the Predictions of Any Classifier}
      \field{year}{2016}
      \field{pages}{1135\bibrangedash 1144}
      \range{pages}{10}
      \keyw{explaining machine learning,interpretability,black box classifier,interpretable machine learning}
    \endentry
    \entry{Zhang2016-mh}{article}{}
      \name{author}{5}{}{%
        {{hash=02582b039d752b000c75f8d7d38a36f4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Chiyuan},
           giveni={C\bibinitperiod}}}%
        {{hash=02404a92b0be3f52ec5ac08e41c13445}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Samy},
           giveni={S\bibinitperiod}}}%
        {{hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod}}}%
        {{hash=3503059e1c0c778913607c87d8c5173a}{%
           family={Recht},
           familyi={R\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
      }
      \strng{namehash}{aefa5e3824e928a3f030ca71f5599336}
      \strng{fullhash}{5f9fd616ce70a721a8ba93854c0c517e}
      \strng{bibnamehash}{aefa5e3824e928a3f030ca71f5599336}
      \strng{authorbibnamehash}{aefa5e3824e928a3f030ca71f5599336}
      \strng{authornamehash}{aefa5e3824e928a3f030ca71f5599336}
      \strng{authorfullhash}{5f9fd616ce70a721a8ba93854c0c517e}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{Understanding deep learning requires rethinking generalization}
      \field{year}{2016}
      \verb{eprint}
      \verb 1611.03530
      \endverb
    \endentry
    \entry{Arpit2017-yz}{inproceedings}{}
      \name{author}{11}{}{%
        {{hash=0f0c297dd56ddda3c4590124b80d0472}{%
           family={Arpit},
           familyi={A\bibinitperiod},
           given={Devansh},
           giveni={D\bibinitperiod}}}%
        {{hash=024bdc529e71ff3969ee6225753cd5e0}{%
           family={Jastrz{ę}bski},
           familyi={J\bibinitperiod},
           given={Stanis{ł}aw},
           giveni={S\bibinitperiod}}}%
        {{hash=088c8a878ffb450444f0e6a8aa932e4b}{%
           family={Ballas},
           familyi={B\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
        {{hash=e7f73df1af7c68ac1fa70ee32d1da619}{%
           family={Krueger},
           familyi={K\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=a0d376ad8a651c5596443bd634847aae}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Emmanuel},
           giveni={E\bibinitperiod}}}%
        {{hash=112bb02df237d4bd02eaaa276df96a97}{%
           family={Kanwal},
           familyi={K\bibinitperiod},
           given={Maxinder\bibnamedelima S},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=50009aac81e7dcef9f5240d07bd3d52e}{%
           family={Maharaj},
           familyi={M\bibinitperiod},
           given={Tegan},
           giveni={T\bibinitperiod}}}%
        {{hash=824f2dd58979463cc1c555dcfd563d9f}{%
           family={Fischer},
           familyi={F\bibinitperiod},
           given={Asja},
           giveni={A\bibinitperiod}}}%
        {{hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=19ce069c35db05c86f9cdb9d9e6e3ce7}{%
           family={Lacoste-Julien},
           familyi={L\bibinithyphendelim J\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=4428b76e1301b2db58587fb18bb59a38}{%
           family={Precup},
           familyi={P\bibinitperiod},
           given={Doina},
           giveni={D\bibinitperiod}}}%
        {{hash=a71fae003da84f44e31d26e868859945}{%
           family={Teh},
           familyi={T\bibinitperiod},
           given={Yee\bibnamedelima Whye},
           giveni={Y\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{0e6427ce0c801277ccf7bc2d00582f0c}
      \strng{fullhash}{06e9d8ec1cc5b411c3e22c27a4030892}
      \strng{bibnamehash}{0e6427ce0c801277ccf7bc2d00582f0c}
      \strng{authorbibnamehash}{0e6427ce0c801277ccf7bc2d00582f0c}
      \strng{authornamehash}{0e6427ce0c801277ccf7bc2d00582f0c}
      \strng{authorfullhash}{06e9d8ec1cc5b411c3e22c27a4030892}
      \strng{editorbibnamehash}{ee1b5a05452f2afba9cd287be3ce0338}
      \strng{editornamehash}{ee1b5a05452f2afba9cd287be3ce0338}
      \strng{editorfullhash}{ee1b5a05452f2afba9cd287be3ce0338}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.}
      \field{booktitle}{Proceedings of the 34th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{A Closer Look at Memorization in Deep Networks}
      \field{volume}{70}
      \field{year}{2017}
      \field{pages}{233\bibrangedash 242}
      \range{pages}{10}
    \endentry
    \entry{Chen2017-kq}{misc}{}
      \name{author}{5}{}{%
        {{hash=2f993628f01d1c59a05a60f5687d3c03}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Xinyun},
           giveni={X\bibinitperiod}}}%
        {{hash=9d9ac6815e27331ba6620def723e357f}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Chang},
           giveni={C\bibinitperiod}}}%
        {{hash=a9c8728e355996d027a7e56b60820a18}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=d580ce8281a35acc5ad81adff1aeccbf}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Kimberly},
           giveni={K\bibinitperiod}}}%
        {{hash=d458d5f9b64652055bd81d57e4777b22}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Dawn},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{5c906860a957d517f9a85bca6c49bcdf}
      \strng{fullhash}{d48740dbefad1e60594cde5c16703235}
      \strng{bibnamehash}{5c906860a957d517f9a85bca6c49bcdf}
      \strng{authorbibnamehash}{5c906860a957d517f9a85bca6c49bcdf}
      \strng{authornamehash}{5c906860a957d517f9a85bca6c49bcdf}
      \strng{authorfullhash}{d48740dbefad1e60594cde5c16703235}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.CR}
      \field{eprinttype}{arXiv}
      \field{title}{Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning}
      \field{year}{2017}
      \verb{eprint}
      \verb 1712.05526
      \endverb
    \endentry
    \entry{Gu2017-rj}{article}{}
      \name{author}{3}{}{%
        {{hash=ae9f0f329631de4f39a9e2f22d620b7c}{%
           family={Gu},
           familyi={G\bibinitperiod},
           given={Tianyu},
           giveni={T\bibinitperiod}}}%
        {{hash=f6ed6e99ecdc816b5951ab17e3336a49}{%
           family={Dolan-Gavitt},
           familyi={D\bibinithyphendelim G\bibinitperiod},
           given={Brendan},
           giveni={B\bibinitperiod}}}%
        {{hash=110b9aee1eecd64f7ef3e8561b34467c}{%
           family={Garg},
           familyi={G\bibinitperiod},
           given={Siddharth},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \strng{fullhash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \strng{bibnamehash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \strng{authorbibnamehash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \strng{authornamehash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \strng{authorfullhash}{74b9b84b5e31cfbae6f54052951b2f0b}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \textbackslashemph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.}
      \field{eprintclass}{cs.CR}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{title}{{BadNets}: Identifying Vulnerabilities in the Machine Learning Model Supply Chain}
      \field{year}{2017}
      \verb{eprint}
      \verb 1708.06733
      \endverb
    \endentry
    \entry{Madry2017-ep}{misc}{}
      \name{author}{5}{}{%
        {{hash=155359c985c7d112498d27d32ad89bfe}{%
           family={Madry},
           familyi={M\bibinitperiod},
           given={Aleksander},
           giveni={A\bibinitperiod}}}%
        {{hash=0cbd9bf3ec2c5306948fe3da351748fc}{%
           family={Makelov},
           familyi={M\bibinitperiod},
           given={Aleksandar},
           giveni={A\bibinitperiod}}}%
        {{hash=bca2cf46e9eed1a3bcc54f4286b4e013}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Ludwig},
           giveni={L\bibinitperiod}}}%
        {{hash=240422b4c9d0d047da2b6f51d9220b59}{%
           family={Tsipras},
           familyi={T\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod}}}%
        {{hash=097451a2aff1de6e5880f0cb1d8b6030}{%
           family={Vladu},
           familyi={V\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{4530e2a23ee0240ba27644964b0d760f}
      \strng{fullhash}{e33786bf52938f374c7e70f07a5fd7fb}
      \strng{bibnamehash}{4530e2a23ee0240ba27644964b0d760f}
      \strng{authorbibnamehash}{4530e2a23ee0240ba27644964b0d760f}
      \strng{authornamehash}{4530e2a23ee0240ba27644964b0d760f}
      \strng{authorfullhash}{e33786bf52938f374c7e70f07a5fd7fb}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.}
      \field{eprintclass}{stat.ML}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{title}{Towards Deep Learning Models Resistant to Adversarial Attacks}
      \field{year}{2017}
      \verb{eprint}
      \verb 1706.06083
      \endverb
    \endentry
    \entry{Neyshabur2017-mk}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=e5f9e207dc41c0bc364d3e1d7be37cf1}{%
           family={Neyshabur},
           familyi={N\bibinitperiod},
           given={Behnam},
           giveni={B\bibinitperiod}}}%
        {{hash=666205762228d3cb67197dabe1b1f6e6}{%
           family={Bhojanapalli},
           familyi={B\bibinitperiod},
           given={Srinadh},
           giveni={S\bibinitperiod}}}%
        {{hash=ee109df10cc31a2d388d82e8b0c4257d}{%
           family={Mcallester},
           familyi={M\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod}}}%
      }
      \name{editor}{7}{}{%
        {{hash=e7e74de725116358b68a6e890c026145}{%
           family={Guyon},
           familyi={G\bibinitperiod},
           given={I},
           giveni={I\bibinitperiod}}}%
        {{hash=56e73897155d476124481b099f125669}{%
           family={Luxburg},
           familyi={L\bibinitperiod},
           given={U\bibnamedelima V},
           giveni={U\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=9b411b6c9cefdde16ffea77ecf612142}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=1444141e07dd9549dbb4b8530fe4ec15}{%
           family={Wallach},
           familyi={W\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=f3857e15544199442f3d4fb2cf6645b4}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
        {{hash=c73f06e2fc98c9b0bf52eb5fdce61943}{%
           family={Vishwanathan},
           familyi={V\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{74b1355b2778d7ef09e52798d8dbe5c4}
      \strng{fullhash}{5628b6a5c501506b8a5594a67abe293d}
      \strng{bibnamehash}{74b1355b2778d7ef09e52798d8dbe5c4}
      \strng{authorbibnamehash}{74b1355b2778d7ef09e52798d8dbe5c4}
      \strng{authornamehash}{74b1355b2778d7ef09e52798d8dbe5c4}
      \strng{authorfullhash}{5628b6a5c501506b8a5594a67abe293d}
      \strng{editorbibnamehash}{7cffda9a379be8eb9063eee4eb0f58b3}
      \strng{editornamehash}{7cffda9a379be8eb9063eee4eb0f58b3}
      \strng{editorfullhash}{d19b63f60b76ff116a8eb515123b5698}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Exploring Generalization in Deep Learning}
      \field{volume}{30}
      \field{year}{2017}
    \endentry
    \entry{Adi2018-fz}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=b524686c039bdbe0e8cca661fca6b56c}{%
           family={Adi},
           familyi={A\bibinitperiod},
           given={Yossi},
           giveni={Y\bibinitperiod}}}%
        {{hash=ce26bbb3e7787f40d77fcf6d09e32698}{%
           family={Baum},
           familyi={B\bibinitperiod},
           given={Carsten},
           giveni={C\bibinitperiod}}}%
        {{hash=c55edd870afd5174d08d32e8560235f0}{%
           family={Cisse},
           familyi={C\bibinitperiod},
           given={Moustapha},
           giveni={M\bibinitperiod}}}%
        {{hash=17a7ab804e95aee768da68b3797214a5}{%
           family={Pinkas},
           familyi={P\bibinitperiod},
           given={Benny},
           giveni={B\bibinitperiod}}}%
        {{hash=621fe59f9f3da150e86d3027269858a4}{%
           family={Keshet},
           familyi={K\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Baltimore, MD, USA}%
      }
      \list{publisher}{1}{%
        {USENIX Association}%
      }
      \strng{namehash}{a0aa4112f06ce4b8898a7fc912029960}
      \strng{fullhash}{1fe58fe3c81e003b8e32e22966a5bcd9}
      \strng{bibnamehash}{a0aa4112f06ce4b8898a7fc912029960}
      \strng{authorbibnamehash}{a0aa4112f06ce4b8898a7fc912029960}
      \strng{authornamehash}{a0aa4112f06ce4b8898a7fc912029960}
      \strng{authorfullhash}{1fe58fe3c81e003b8e32e22966a5bcd9}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep Neural Networks have recently gained lots of success after enabling several breakthroughs in notoriously challenging problems. Training these networks is computationally expensive and requires vast amounts of training data. Selling such pre-trained models can, therefore, be a lucrative business model. Unfortunately, once the models are sold they can be easily copied and redistributed. To avoid this, a tracking mechanism to identify models as the intellectual property of a particular vendor is necessary.In this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Our scheme works for general classification tasks and can easily be combined with current learning algorithms. We show experimentally that such a watermark has no noticeable impact on the primary task that the model is designed for and evaluate the robustness of our proposal against a multitude of practical attacks. Moreover, we provide a theoretical analysis, relating our approach to previous work on backdooring.}
      \field{booktitle}{Proceedings of the 27th {USENIX} Conference on Security Symposium}
      \field{month}{8}
      \field{series}{SEC'18}
      \field{title}{Turning your weakness into a strength: watermarking deep neural networks by backdooring}
      \field{year}{2018}
      \field{pages}{1615\bibrangedash 1631}
      \range{pages}{17}
    \endentry
    \entry{Cullina2018-os}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=7348f23e8e8558781fa286c8d0f048b8}{%
           family={Cullina},
           familyi={C\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=19e15437e6824ecd330ce35d0bea8c0c}{%
           family={Bhagoji},
           familyi={B\bibinitperiod},
           given={Arjun\bibnamedelima Nitin},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=34858990bc441f232fe15809281e1ee0}{%
           family={Mittal},
           familyi={M\bibinitperiod},
           given={Prateek},
           giveni={P\bibinitperiod}}}%
      }
      \name{editor}{6}{}{%
        {{hash=9b411b6c9cefdde16ffea77ecf612142}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=1444141e07dd9549dbb4b8530fe4ec15}{%
           family={Wallach},
           familyi={W\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=6c15b92d4ebdbfae091f6548dc7543ae}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=4f2317ca7fb46f798c6c220e39e332a0}{%
           family={Grauman},
           familyi={G\bibinitperiod},
           given={K},
           giveni={K\bibinitperiod}}}%
        {{hash=9bc86e7523b9aaf7183c5f7e7634048f}{%
           family={Cesa-Bianchi},
           familyi={C\bibinithyphendelim B\bibinitperiod},
           given={N},
           giveni={N\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{fullhash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{bibnamehash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{authorbibnamehash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{authornamehash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{authorfullhash}{74dc2b183bdac288dca38f985270c3f0}
      \strng{editorbibnamehash}{233f4c9ba2f85af3db1eed171136508c}
      \strng{editornamehash}{233f4c9ba2f85af3db1eed171136508c}
      \strng{editorfullhash}{f08403ac24c2e0a0ae72822c0c0afda6}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{{PAC-learning} in the presence of adversaries}
      \field{volume}{31}
      \field{year}{2018}
    \endentry
    \entry{Koh2018-bz}{misc}{}
      \name{author}{3}{}{%
        {{hash=b7309d5979903f9e56ec43ebf0d594ad}{%
           family={Koh},
           familyi={K\bibinitperiod},
           given={Pang\bibnamedelima Wei},
           giveni={P\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=fe7a7e80c1857d185d3ab01f15fe584d}{%
           family={Steinhardt},
           familyi={S\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod}}}%
        {{hash=86c032bbc45c3b616f0a1170befc0e82}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{dfa98671cb25c2641a3a3d2d567e327f}
      \strng{fullhash}{dfa98671cb25c2641a3a3d2d567e327f}
      \strng{bibnamehash}{dfa98671cb25c2641a3a3d2d567e327f}
      \strng{authorbibnamehash}{dfa98671cb25c2641a3a3d2d567e327f}
      \strng{authornamehash}{dfa98671cb25c2641a3a3d2d567e327f}
      \strng{authorfullhash}{dfa98671cb25c2641a3a3d2d567e327f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models' training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. Can data poisoning attacks break data sanitization defenses? In this paper, we develop three new attacks that can all bypass a broad range of data sanitization defenses, including commonly-used anomaly detectors based on nearest neighbors, training loss, and singular-value decomposition. For example, our attacks successfully increase the test error on the Enron spam detection dataset from 3\% to 24\% and on the IMDB sentiment classification dataset from 12\% to 29\% by adding just 3\% poisoned data. In contrast, many existing attacks from the literature do not explicitly consider defenses, and we show that those attacks are ineffective in the presence of the defenses we consider. Our attacks are based on two ideas: (i) we coordinate our attacks to place poisoned points near one another, which fools some anomaly detectors, and (ii) we formulate each attack as a constrained optimization problem, with constraints designed to ensure that the poisoned points evade detection. While this optimization involves solving an expensive bilevel problem, we explore and develop three efficient approximations to this problem based on influence functions; minimax duality; and the Karush-Kuhn-Tucker (KKT) conditions. Our results underscore the urgent need to develop more sophisticated and robust defenses against data poisoning attacks.}
      \field{eprintclass}{stat.ML}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{Stronger Data Poisoning Attacks Break Data Sanitization Defenses}
      \field{year}{2018}
      \true{nocite}
      \verb{eprint}
      \verb 1811.00741
      \endverb
    \endentry
    \entry{Shafahi2018-ns}{misc}{}
      \name{author}{7}{}{%
        {{hash=6670caf6b6cca50acb7430554817db1d}{%
           family={Shafahi},
           familyi={S\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
        {{hash=ae2bcc3ba4c6211a676269852ae01180}{%
           family={Ronny\bibnamedelima Huang},
           familyi={R\bibinitperiod\bibinitdelim H\bibinitperiod},
           given={W},
           giveni={W\bibinitperiod}}}%
        {{hash=87cde9972fb3ceaaada8ee6319d77832}{%
           family={Najibi},
           familyi={N\bibinitperiod},
           given={Mahyar},
           giveni={M\bibinitperiod}}}%
        {{hash=88b699cb33b744b2ba99665267f335f8}{%
           family={Suciu},
           familyi={S\bibinitperiod},
           given={Octavian},
           giveni={O\bibinitperiod}}}%
        {{hash=45fe5ab966df2ae2a98ac0437e84e46f}{%
           family={Studer},
           familyi={S\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod}}}%
        {{hash=f527bef12a2d794d6918be00878b605b}{%
           family={Dumitras},
           familyi={D\bibinitperiod},
           given={Tudor},
           giveni={T\bibinitperiod}}}%
        {{hash=3b1fea9d27f10122cd27336b3dc68ea7}{%
           family={Goldstein},
           familyi={G\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{425b7e23880a3a95ce6005b93f98c2b1}
      \strng{fullhash}{d0fdcc1b570a364b3307611c770c219f}
      \strng{bibnamehash}{425b7e23880a3a95ce6005b93f98c2b1}
      \strng{authorbibnamehash}{425b7e23880a3a95ce6005b93f98c2b1}
      \strng{authornamehash}{425b7e23880a3a95ce6005b93f98c2b1}
      \strng{authorfullhash}{d0fdcc1b570a364b3307611c770c219f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use ``clean-labels''; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\textit\{specific\}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a ``watermarking'' strategy that makes poisoning reliable using multiple ($\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{title}{Poison Frogs! Targeted {Clean-Label} Poisoning Attacks on Neural Networks}
      \field{year}{2018}
      \true{nocite}
      \verb{eprint}
      \verb 1804.00792
      \endverb
    \endentry
    \entry{Tran2018-bf}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=6dfbfd86b5e4b1c35eaece5fa58840ec}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Brandon},
           giveni={B\bibinitperiod}}}%
        {{hash=3b3012e4ca17639feb14b7bd7910c117}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jerry},
           giveni={J\bibinitperiod}}}%
        {{hash=155359c985c7d112498d27d32ad89bfe}{%
           family={Madry},
           familyi={M\bibinitperiod},
           given={Aleksander},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{6}{}{%
        {{hash=9b411b6c9cefdde16ffea77ecf612142}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=1444141e07dd9549dbb4b8530fe4ec15}{%
           family={Wallach},
           familyi={W\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=6c15b92d4ebdbfae091f6548dc7543ae}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=4f2317ca7fb46f798c6c220e39e332a0}{%
           family={Grauman},
           familyi={G\bibinitperiod},
           given={K},
           giveni={K\bibinitperiod}}}%
        {{hash=9bc86e7523b9aaf7183c5f7e7634048f}{%
           family={Cesa-Bianchi},
           familyi={C\bibinithyphendelim B\bibinitperiod},
           given={N},
           giveni={N\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{fullhash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{bibnamehash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{authorbibnamehash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{authornamehash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{authorfullhash}{8bf1fd72c29f09d57fe5eeea0133fe58}
      \strng{editorbibnamehash}{233f4c9ba2f85af3db1eed171136508c}
      \strng{editornamehash}{233f4c9ba2f85af3db1eed171136508c}
      \strng{editorfullhash}{f08403ac24c2e0a0ae72822c0c0afda6}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Spectral Signatures in Backdoor Attacks}
      \field{volume}{31}
      \field{year}{2018}
    \endentry
    \entry{Vershynin2018-xn}{book}{}
      \name{author}{1}{}{%
        {{hash=711c5aa2fe76a4ff674a2c6e2c7a2d91}{%
           family={Vershynin},
           familyi={V\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{fullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{bibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorbibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authornamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorfullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.}
      \field{month}{9}
      \field{title}{{High-Dimensional} Probability: An Introduction with Applications in Data Science}
      \field{year}{2018}
      \true{nocite}
    \endentry
    \entry{Gu2019-ip}{article}{}
      \name{author}{4}{}{%
        {{hash=ae9f0f329631de4f39a9e2f22d620b7c}{%
           family={Gu},
           familyi={G\bibinitperiod},
           given={Tianyu},
           giveni={T\bibinitperiod}}}%
        {{hash=d6360bc8f5ef2b9fdccd337e0cb18b84}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Kang},
           giveni={K\bibinitperiod}}}%
        {{hash=f6ed6e99ecdc816b5951ab17e3336a49}{%
           family={Dolan-Gavitt},
           familyi={D\bibinithyphendelim G\bibinitperiod},
           given={Brendan},
           giveni={B\bibinitperiod}}}%
        {{hash=110b9aee1eecd64f7ef3e8561b34467c}{%
           family={Garg},
           familyi={G\bibinitperiod},
           given={Siddharth},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{836c3448c24af0aa195da1cdfbc9fb1c}
      \strng{fullhash}{c271cadb388de35faeaa5f3c8a8da54f}
      \strng{bibnamehash}{836c3448c24af0aa195da1cdfbc9fb1c}
      \strng{authorbibnamehash}{836c3448c24af0aa195da1cdfbc9fb1c}
      \strng{authornamehash}{836c3448c24af0aa195da1cdfbc9fb1c}
      \strng{authorfullhash}{c271cadb388de35faeaa5f3c8a8da54f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper, we show that the outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has the state-of-the-art performance on the user's training and validation samples but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our U.S. street sign detector can persist even if the network is later retrained for another task and cause a drop in an accuracy of 25\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and-because the behavior of neural networks is difficult to explicate-stealthy. This paper provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.}
      \field{journaltitle}{IEEE Access}
      \field{title}{{BadNets}: Evaluating Backdooring Attacks on Deep Neural Networks}
      \field{volume}{7}
      \field{year}{2019}
      \field{pages}{47230\bibrangedash 47244}
      \range{pages}{15}
      \keyw{Training;Machine learning;Perturbation methods;Computational modeling;Biological neural networks;Security;Computer security;machine learning;neural networks}
    \endentry
    \entry{Ilyas2019-ot}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=924b414be21dcfc4f4f9f08196520746}{%
           family={Ilyas},
           familyi={I\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=bb66a5fcd7966034f5477f12cf059156}{%
           family={Santurkar},
           familyi={S\bibinitperiod},
           given={Shibani},
           giveni={S\bibinitperiod}}}%
        {{hash=240422b4c9d0d047da2b6f51d9220b59}{%
           family={Tsipras},
           familyi={T\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod}}}%
        {{hash=52a9eb75d8ed34150ce01eb9e4c43027}{%
           family={Engstrom},
           familyi={E\bibinitperiod},
           given={Logan},
           giveni={L\bibinitperiod}}}%
        {{hash=6dfbfd86b5e4b1c35eaece5fa58840ec}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Brandon},
           giveni={B\bibinitperiod}}}%
        {{hash=155359c985c7d112498d27d32ad89bfe}{%
           family={Madry},
           familyi={M\bibinitperiod},
           given={Aleksander},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{6}{}{%
        {{hash=1444141e07dd9549dbb4b8530fe4ec15}{%
           family={Wallach},
           familyi={W\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=6c15b92d4ebdbfae091f6548dc7543ae}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=02854da982edf66ffa27f4fc1c738779}{%
           family={Beygelzimer},
           familyi={B\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod}}}%
        {{hash=1611b9e0ccbac8352fbf5e74fc00e7a3}{%
           family={Alch{é}-Buc},
           familyi={A\bibinithyphendelim B\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod},
           prefix={d'},
           prefixi={d\bibinitperiod}}}%
        {{hash=f81d803635cc159bfc9d9145ebd34a14}{%
           family={Fox},
           familyi={F\bibinitperiod},
           given={E},
           giveni={E\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{f813698d8f340f336c185e9057a3cbc4}
      \strng{fullhash}{118ebda35586fb69d6c927e16253d27c}
      \strng{bibnamehash}{f813698d8f340f336c185e9057a3cbc4}
      \strng{authorbibnamehash}{f813698d8f340f336c185e9057a3cbc4}
      \strng{authornamehash}{f813698d8f340f336c185e9057a3cbc4}
      \strng{authorfullhash}{118ebda35586fb69d6c927e16253d27c}
      \strng{editorbibnamehash}{d8a1689cbb7ff26be8b55bdda56fab83}
      \strng{editornamehash}{d8a1689cbb7ff26be8b55bdda56fab83}
      \strng{editorfullhash}{546b9366e080b1a208dbe4afe4651079}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Adversarial Examples Are Not Bugs, They Are Features}
      \field{volume}{32}
      \field{year}{2019}
    \endentry
    \entry{Montasser2019-ro}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=eda91e1fcb66547936ed73c54fe4bbae}{%
           family={Montasser},
           familyi={M\bibinitperiod},
           given={Omar},
           giveni={O\bibinitperiod}}}%
        {{hash=ef7c5042fa12097a807dc1a551d0c049}{%
           family={Hanneke},
           familyi={H\bibinitperiod},
           given={Steve},
           giveni={S\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=c8729ce1a5dd9cb59f67b9958da9bc47}{%
           family={Beygelzimer},
           familyi={B\bibinitperiod},
           given={Alina},
           giveni={A\bibinitperiod}}}%
        {{hash=97d68bdafb2fefd550bc00375aab158e}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Phoenix, USA}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{fullhash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{bibnamehash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{authorbibnamehash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{authornamehash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{authorfullhash}{0fc6745e4547bd08f92d386d37c98fb4}
      \strng{editorbibnamehash}{b913414daca28190b8a4dfa9211acf38}
      \strng{editornamehash}{b913414daca28190b8a4dfa9211acf38}
      \strng{editorfullhash}{b913414daca28190b8a4dfa9211acf38}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the question of learning an adversarially robust predictor. We show that any hypothesis class $\mathcalH$ with finite VC dimension is robustly PAC learnable with an \textbackslashemphimproper learning rule. The requirement of being improper is necessary as we exhibit examples of hypothesis classes $\mathcalH$ with finite VC dimension that are \textbackslashemphnot robustly PAC learnable with any \textbackslashemphproper learning rule.}
      \field{booktitle}{Proceedings of the {Thirty-Second} Conference on Learning Theory}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{{VC} Classes are Adversarially Robustly Learnable, but Only Improperly}
      \field{volume}{99}
      \field{year}{2019}
      \field{pages}{2512\bibrangedash 2530}
      \range{pages}{19}
    \endentry
    \entry{Shen2018-jx}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=a212df60dfdca250c3a53d232877a470}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Yanyao},
           giveni={Y\bibinitperiod}}}%
        {{hash=78484c7f7f265bb7ea1d73937bb42d31}{%
           family={Sanghavi},
           familyi={S\bibinitperiod},
           given={Sujay},
           giveni={S\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=050c0df95e5eb5c3218fdd148d0e17a4}{%
           family={Chaudhuri},
           familyi={C\bibinitperiod},
           given={Kamalika},
           giveni={K\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{2b199417b7000d6548150fd7bc62387b}
      \strng{fullhash}{2b199417b7000d6548150fd7bc62387b}
      \strng{bibnamehash}{2b199417b7000d6548150fd7bc62387b}
      \strng{authorbibnamehash}{2b199417b7000d6548150fd7bc62387b}
      \strng{authornamehash}{2b199417b7000d6548150fd7bc62387b}
      \strng{authorfullhash}{2b199417b7000d6548150fd7bc62387b}
      \strng{editorbibnamehash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \strng{editornamehash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \strng{editorfullhash}{ddb716c8c736fb868f09f38a0f4dd1ed}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we study a simple and generic framework to tackle the problem of learning model parameters when a fraction of the training samples are corrupted. Our approach is motivated by a simple observation: in a variety of such settings, the evolution of training accuracy (as a function of training epochs) is different for clean samples and bad samples. We propose to iteratively minimize the trimmed loss, by alternating between (a) selecting samples with lowest current loss, and (b) retraining a model on only these samples. Analytically, we characterize the statistical performance and convergence rate of the algorithm for simple and natural linear and non-linear models. Experimentally, we demonstrate its effectiveness in three settings: (a) deep image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) deep image classifiers with adversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves state-of-the-art performance without having access to any a-priori guaranteed clean samples.}
      \field{booktitle}{Proceedings of the 36th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Learning with Bad Training Data via Iterative Trimmed Loss Minimization}
      \field{volume}{97}
      \field{year}{2019}
      \field{pages}{5739\bibrangedash 5748}
      \range{pages}{10}
    \endentry
    \entry{Turner2019-jc}{misc}{}
      \name{author}{3}{}{%
        {{hash=78f050f126d97fdc69dd55aea7678eaa}{%
           family={Turner},
           familyi={T\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=240422b4c9d0d047da2b6f51d9220b59}{%
           family={Tsipras},
           familyi={T\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod}}}%
        {{hash=155359c985c7d112498d27d32ad89bfe}{%
           family={Madry},
           familyi={M\bibinitperiod},
           given={Aleksander},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{dc82761c2ffe65baa9f09df448716b31}
      \strng{fullhash}{dc82761c2ffe65baa9f09df448716b31}
      \strng{bibnamehash}{dc82761c2ffe65baa9f09df448716b31}
      \strng{authorbibnamehash}{dc82761c2ffe65baa9f09df448716b31}
      \strng{authornamehash}{dc82761c2ffe65baa9f09df448716b31}
      \strng{authorfullhash}{dc82761c2ffe65baa9f09df448716b31}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.}
      \field{eprintclass}{stat.ML}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{title}{{Label-Consistent} Backdoor Attacks}
      \field{year}{2019}
      \verb{eprint}
      \verb 1912.02771
      \endverb
    \endentry
    \entry{Feldman2020-ex}{article}{}
      \name{author}{2}{}{%
        {{hash=2cdf1f22d9f874d0aee7260732af92c3}{%
           family={Feldman},
           familyi={F\bibinitperiod},
           given={Vitaly},
           giveni={V\bibinitperiod}}}%
        {{hash=02582b039d752b000c75f8d7d38a36f4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Chiyuan},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{c35626e45d0c994abf052eac53d23a2f}
      \strng{fullhash}{c35626e45d0c994abf052eac53d23a2f}
      \strng{bibnamehash}{c35626e45d0c994abf052eac53d23a2f}
      \strng{authorbibnamehash}{c35626e45d0c994abf052eac53d23a2f}
      \strng{authornamehash}{c35626e45d0c994abf052eac53d23a2f}
      \strng{authorfullhash}{c35626e45d0c994abf052eac53d23a2f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman (2019) proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given. In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in (Feldman, 2019).}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{title}{What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation}
      \field{year}{2020}
      \verb{eprint}
      \verb 2008.03703
      \endverb
    \endentry
    \entry{Li2020-my}{misc}{}
      \name{author}{5}{}{%
        {{hash=d4cf877c30d772fd737b1b5203aae766}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yiming},
           giveni={Y\bibinitperiod}}}%
        {{hash=9bb073fd1afe1c30cc207afbb798d3a3}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Baoyuan},
           giveni={B\bibinitperiod}}}%
        {{hash=b4e3e3950abd8d89bd6dd1087a0c875b}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Yong},
           giveni={Y\bibinitperiod}}}%
        {{hash=828a5b63771fe4db7767e6bd65ec78ce}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=1b08f92ef156dae0b665646b25c5f1c4}{%
           family={Xia},
           familyi={X\bibinitperiod},
           given={Shu-Tao},
           giveni={S\bibinithyphendelim T\bibinitperiod}}}%
      }
      \strng{namehash}{b1e130ca2c9ad9e777ebb6bcc0f94709}
      \strng{fullhash}{5834b34a8612db07453f8a6b66ed8f3f}
      \strng{bibnamehash}{b1e130ca2c9ad9e777ebb6bcc0f94709}
      \strng{authorbibnamehash}{b1e130ca2c9ad9e777ebb6bcc0f94709}
      \strng{authornamehash}{b1e130ca2c9ad9e777ebb6bcc0f94709}
      \strng{authorfullhash}{5834b34a8612db07453f8a6b66ed8f3f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at \textbackslashurl\{https://github.com/THUYimingLi/backdoor-learning-resources\}.}
      \field{eprintclass}{cs.CR}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{title}{Backdoor Learning: A Survey}
      \field{year}{2020}
      \verb{eprint}
      \verb 2007.08745
      \endverb
    \endentry
    \entry{Montasser2020-ir}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=eda91e1fcb66547936ed73c54fe4bbae}{%
           family={Montasser},
           familyi={M\bibinitperiod},
           given={Omar},
           giveni={O\bibinitperiod}}}%
        {{hash=986aaf119fc46098a46a164fd5598026}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Surbhi},
           giveni={S\bibinitperiod}}}%
        {{hash=e7ee044190b525d3659eaae1cce3086f}{%
           family={Diakonikolas},
           familyi={D\bibinitperiod},
           given={Ilias},
           giveni={I\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=2dc8d8eba3b07ccb6bda6a754b49b05a}{%
           family={Iii},
           familyi={I\bibinitperiod},
           given={Hal\bibnamedelima Daum{é}},
           giveni={H\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=d77c73a14b0f048484c1d34aba3071d4}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Aarti},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{4a43a56ce9d05731abb5ee9b766d6519}
      \strng{fullhash}{e39b16815ef4e28845b0ab1f92a3fc1d}
      \strng{bibnamehash}{4a43a56ce9d05731abb5ee9b766d6519}
      \strng{authorbibnamehash}{4a43a56ce9d05731abb5ee9b766d6519}
      \strng{authornamehash}{4a43a56ce9d05731abb5ee9b766d6519}
      \strng{authorfullhash}{e39b16815ef4e28845b0ab1f92a3fc1d}
      \strng{editorbibnamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editornamehash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \strng{editorfullhash}{82e5ccb4ae18bc710b7a16fa1219872b}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the problem of learning adversarially robust halfspaces in the distribution-independent setting. In the realizable setting, we provide necessary and sufficient conditions on the adversarial perturbation sets under which halfspaces are efficiently robustly learnable. In the presence of random label noise, we give a simple computationally efficient algorithm for this problem with respect to any $\ell_p$-perturbation.}
      \field{booktitle}{Proceedings of the 37th International Conference on Machine Learning}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Efficiently Learning Adversarially Robust Halfspaces with Noise}
      \field{volume}{119}
      \field{year}{2020}
      \field{pages}{7010\bibrangedash 7021}
      \range{pages}{12}
    \endentry
    \entry{Saha2019-ce}{article}{}
      \name{author}{3}{}{%
        {{hash=4407388da277b330096a51bf433603ae}{%
           family={Saha},
           familyi={S\bibinitperiod},
           given={Aniruddha},
           giveni={A\bibinitperiod}}}%
        {{hash=ffc76124065359213e7c3827e1bdb9e0}{%
           family={Subramanya},
           familyi={S\bibinitperiod},
           given={Akshayvarun},
           giveni={A\bibinitperiod}}}%
        {{hash=3243eb7133d2c0f3de1d3d53cace9afd}{%
           family={Pirsiavash},
           familyi={P\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \strng{fullhash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \strng{bibnamehash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \strng{authorbibnamehash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \strng{authornamehash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \strng{authorfullhash}{9d4a3a56e479e020d6ba92c7eeb3cd31}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.}
      \field{journaltitle}{AAAI}
      \field{month}{4}
      \field{number}{07}
      \field{title}{Hidden Trigger Backdoor Attacks}
      \field{volume}{34}
      \field{year}{2020}
      \field{pages}{11957\bibrangedash 11965}
      \range{pages}{9}
    \endentry
    \entry{Shan2020-dr}{inproceedings}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{5}{}{%
        {{hash=a7492c500a6dde75d6174384a596a479}{%
           family={Shan},
           familyi={S\bibinitperiod},
           given={S},
           giveni={S\bibinitperiod}}}%
        {{hash=78db5c82d387c33162c6263e4efaa60b}{%
           family={Wenger},
           familyi={W\bibinitperiod},
           given={E},
           giveni={E\bibinitperiod}}}%
        {{hash=8afab9bdc9b292debbeb136dfd97036c}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={J},
           giveni={J\bibinitperiod}}}%
        {{hash=e2e65bc31792929cfaa15cecf7f23965}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
        {{hash=1b2ced9ee7249adaa22940124d959652}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={H},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {usenix.org}%
      }
      \strng{namehash}{6cbe65d9e03d33f1d474c38527275509}
      \strng{fullhash}{405c86428ebd927f0718a5b80ad0c465}
      \strng{bibnamehash}{6cbe65d9e03d33f1d474c38527275509}
      \strng{authorbibnamehash}{6cbe65d9e03d33f1d474c38527275509}
      \strng{authornamehash}{6cbe65d9e03d33f1d474c38527275509}
      \strng{authorfullhash}{405c86428ebd927f0718a5b80ad0c465}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Today's proliferation of powerful facial recognition systems poses a real threat to personal privacy. As Clearview. ai demonstrated, anyone can canvas the Internet for data and train highly accurate facial recognition models of individuals without their knowledge. We need …}
      \field{booktitle}{Proceedings of the {Twenty-Ninth} USENIX Security Symposium}
      \field{journaltitle}{29th \{USENIX\} Security}
      \field{title}{Fawkes: Protecting privacy against unauthorized deep learning models}
      \field{year}{2020}
    \endentry
    \entry{Truong2020-dk}{misc}{}
      \name{author}{8}{}{%
        {{hash=56511a401b81896d6daef986765d1f11}{%
           family={Truong},
           familyi={T\bibinitperiod},
           given={Loc},
           giveni={L\bibinitperiod}}}%
        {{hash=ef01191c44cf656448f4068c73255861}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Chace},
           giveni={C\bibinitperiod}}}%
        {{hash=e64dc4aed7850253078e3d03618e6e52}{%
           family={Hutchinson},
           familyi={H\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=c8654120869a18eb9dfe01e4987e401d}{%
           family={August},
           familyi={A\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=8ed9880d30da2641e7437c76e46d8f6d}{%
           family={Praggastis},
           familyi={P\bibinitperiod},
           given={Brenda},
           giveni={B\bibinitperiod}}}%
        {{hash=85c8a5c70e10970bb1361b55352fbbae}{%
           family={Jasper},
           familyi={J\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=33f4ca413a99c66ee4add305b7cabbc1}{%
           family={Nichols},
           familyi={N\bibinitperiod},
           given={Nicole},
           giveni={N\bibinitperiod}}}%
        {{hash=76d3af8b0519f637be5a42436b1938f5}{%
           family={Tuor},
           familyi={T\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{980ebaa02b5d227de3508267a879cb34}
      \strng{fullhash}{b153352ad45f3e6e6bfd1937c171676b}
      \strng{bibnamehash}{980ebaa02b5d227de3508267a879cb34}
      \strng{authorbibnamehash}{980ebaa02b5d227de3508267a879cb34}
      \strng{authornamehash}{980ebaa02b5d227de3508267a879cb34}
      \strng{authorfullhash}{b153352ad45f3e6e6bfd1937c171676b}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Backdoor data poisoning attacks have recently been demonstrated in computer vision research as a potential safety risk for machine learning (ML) systems. Traditional data poisoning attacks manipulate training data to induce unreliability of an ML model, whereas backdoor data poisoning attacks maintain system performance unless the ML model is presented with an input containing an embedded ``trigger'' that provides a predetermined response advantageous to the adversary. Our work builds upon prior backdoor data-poisoning research for ML image classifiers and systematically assesses different experimental conditions including types of trigger patterns, persistence of trigger patterns during retraining, poisoning strategies, architectures (ResNet-50, NasNet, NasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive regularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup, Soft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the success rate of backdoor poisoning attacks varies widely, depending on several factors, including model architecture, trigger pattern and regularization technique. Second, we find that poisoned models are hard to detect through performance inspection alone. Third, regularization typically reduces backdoor success rate, although it can have no effect or even slightly increase it, depending on the form of regularization. Finally, backdoors inserted through data poisoning can be rendered ineffective after just a few epochs of additional training on a small set of clean data without affecting the model's performance.}
      \field{eprintclass}{cs.CV}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{title}{Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers}
      \field{year}{2020}
      \verb{eprint}
      \verb 2004.11514
      \endverb
    \endentry
    \entry{Wang2020-yt}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=0b620b05ea1f5b470cdf9c505a1f191f}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Hongyi},
           giveni={H\bibinitperiod}}}%
        {{hash=6bdf8094c7dd8d3ef5b0edcc46de2785}{%
           family={Sreenivasan},
           familyi={S\bibinitperiod},
           given={Kartik},
           giveni={K\bibinitperiod}}}%
        {{hash=b30c300302a7f35b8f8cff6edaab64bf}{%
           family={Rajput},
           familyi={R\bibinitperiod},
           given={Shashank},
           giveni={S\bibinitperiod}}}%
        {{hash=cdd0516f236599c44e46e22b10966164}{%
           family={Vishwakarma},
           familyi={V\bibinitperiod},
           given={Harit},
           giveni={H\bibinitperiod}}}%
        {{hash=0cca37e5345b360b8549d501a93f8816}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Saurabh},
           giveni={S\bibinitperiod}}}%
        {{hash=c82fc22cd06646542e04a5abbfa161ed}{%
           family={Sohn},
           familyi={S\bibinitperiod},
           given={Jy-yong},
           giveni={J\bibinithyphendelim y\bibinitperiod}}}%
        {{hash=3de3c2300042d3868ce512b74ee27993}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kangwook},
           giveni={K\bibinitperiod}}}%
        {{hash=ab0562ce82f5b7a093c659f376824faf}{%
           family={Papailiopoulos},
           familyi={P\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=6c15b92d4ebdbfae091f6548dc7543ae}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod}}}%
        {{hash=6cb5af0e649387de5bbfdb150cc7a786}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=ae95119be0d9d86ffdec9f3142af353e}{%
           family={Hadsell},
           familyi={H\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
        {{hash=20f36d8aae4f5467e0c03ccdaee4076a}{%
           family={Balcan},
           familyi={B\bibinitperiod},
           given={M.\bibnamedelimi F.},
           giveni={M\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=006d527428ae3b86c2093d6d28e16072}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{90ce9fa73a042d98bc3426db97155a94}
      \strng{fullhash}{a2978c24d8c2701f764b6290006c2963}
      \strng{bibnamehash}{90ce9fa73a042d98bc3426db97155a94}
      \strng{authorbibnamehash}{90ce9fa73a042d98bc3426db97155a94}
      \strng{authornamehash}{90ce9fa73a042d98bc3426db97155a94}
      \strng{authorfullhash}{a2978c24d8c2701f764b6290006c2963}
      \strng{editorbibnamehash}{1bb757b97fffae621331dd700b5a7ca9}
      \strng{editornamehash}{1bb757b97fffae621331dd700b5a7ca9}
      \strng{editorfullhash}{01684a8bcecde64514068030e781dc4f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Attack of the Tails: Yes, You Really Can Backdoor Federated Learning}
      \field{volume}{33}
      \field{year}{2020}
      \field{pages}{16070\bibrangedash 16084}
      \range{pages}{15}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf
      \endverb
    \endentry
    \entry{Borgnia2021-vk}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=52e600d2319fbc2d92714e15b6389e7f}{%
           family={Borgnia},
           familyi={B\bibinitperiod},
           given={Eitan},
           giveni={E\bibinitperiod}}}%
        {{hash=8443bb3211fe3ace8f21821122a81a8d}{%
           family={Cherepanova},
           familyi={C\bibinitperiod},
           given={Valeriia},
           giveni={V\bibinitperiod}}}%
        {{hash=8095426357328937e61000a041f71d41}{%
           family={Fowl},
           familyi={F\bibinitperiod},
           given={Liam},
           giveni={L\bibinitperiod}}}%
        {{hash=1cf7e4a968493e3c27f3b9886f8c8767}{%
           family={Ghiasi},
           familyi={G\bibinitperiod},
           given={Amin},
           giveni={A\bibinitperiod}}}%
        {{hash=e2a6fb7ce325e06d63c45f7a73a65438}{%
           family={Geiping},
           familyi={G\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=60a7228eec51bdb9b0688a11bd6c8e6c}{%
           family={Goldblum},
           familyi={G\bibinitperiod},
           given={Micah},
           giveni={M\bibinitperiod}}}%
        {{hash=3b1fea9d27f10122cd27336b3dc68ea7}{%
           family={Goldstein},
           familyi={G\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=b5e41df848354543d1901e8689521c2b}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Arjun},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{500c2a18c7f990b7609370da2e4fb8e4}
      \strng{fullhash}{692d08beabb9182af6d01d9f5d3d763c}
      \strng{bibnamehash}{500c2a18c7f990b7609370da2e4fb8e4}
      \strng{authorbibnamehash}{500c2a18c7f990b7609370da2e4fb8e4}
      \strng{authornamehash}{500c2a18c7f990b7609370da2e4fb8e4}
      \strng{authorfullhash}{692d08beabb9182af6d01d9f5d3d763c}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9\%.}
      \field{booktitle}{{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})}
      \field{month}{6}
      \field{title}{Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff}
      \field{year}{2021}
      \field{pages}{3855\bibrangedash 3859}
      \range{pages}{5}
      \keyw{Industries;Toxicology;Conferences;Training data;Machine learning;Signal processing;Data models;Data Poisoning;Backdoor Attacks;Adversarial Attacks;Differential Privacy;Data Augmentation}
    \endentry
    \entry{Xiao2020-ju}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=2f53cdd34ede3c8786fc9fe7ec0d2da4}{%
           family={Xiao},
           familyi={X\bibinitperiod},
           given={Kai\bibnamedelima Yuanqing},
           giveni={K\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=52a9eb75d8ed34150ce01eb9e4c43027}{%
           family={Engstrom},
           familyi={E\bibinitperiod},
           given={Logan},
           giveni={L\bibinitperiod}}}%
        {{hash=924b414be21dcfc4f4f9f08196520746}{%
           family={Ilyas},
           familyi={I\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=155359c985c7d112498d27d32ad89bfe}{%
           family={Madry},
           familyi={M\bibinitperiod},
           given={Aleksander},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{ad1473e65c3a381d756672bab2faa075}
      \strng{fullhash}{3a9f42d0aa9950982475d93bc96cd31a}
      \strng{bibnamehash}{ad1473e65c3a381d756672bab2faa075}
      \strng{authorbibnamehash}{ad1473e65c3a381d756672bab2faa075}
      \strng{authornamehash}{ad1473e65c3a381d756672bab2faa075}
      \strng{authorfullhash}{3a9f42d0aa9950982475d93bc96cd31a}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Learning Representations}
      \field{title}{Noise or Signal: The Role of Image Backgrounds in Object Recognition}
      \field{year}{2021}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=gl3D-xY7wLq
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=gl3D-xY7wLq
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

