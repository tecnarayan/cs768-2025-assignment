\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332, 2019.

\bibitem[Barthe et~al.(2005)Barthe, Gu{\'e}don, Mendelson, and
  Naor]{barthe2005probabilistic}
Barthe, F., Gu{\'e}don, O., Mendelson, S., and Naor, A.
\newblock A probabilistic approach to the geometry of the $\ell_p^n$-ball.
\newblock \emph{The Annals of Probability}, 33\penalty0 (2):\penalty0 480--513,
  2005.

\bibitem[Beck \& Hallak(2020)Beck and Hallak]{beck2020convergence}
Beck, A. and Hallak, N.
\newblock On the convergence to stationary points of deterministic and
  randomized feasible descent directions methods.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  56--79, 2020.

\bibitem[Bedna{\v{r}}{\'\i}k \& Pastor(2013)Bedna{\v{r}}{\'\i}k and
  Pastor]{bednavrik2013lipschitz}
Bedna{\v{r}}{\'\i}k, D. and Pastor, K.
\newblock On lipschitz behaviour of some generalized derivatives.
\newblock \emph{Mathematica Slovaca}, 63\penalty0 (3):\penalty0 587--594, 2013.

\bibitem[Bena{\"\i}m et~al.(2005)Bena{\"\i}m, Hofbauer, and
  Sorin]{benaim2005stochastic}
Bena{\"\i}m, M., Hofbauer, J., and Sorin, S.
\newblock Stochastic approximations and differential inclusions.
\newblock \emph{SIAM Journal on Control and Optimization}, 44\penalty0
  (1):\penalty0 328--348, 2005.

\bibitem[Bolte et~al.(2018)Bolte, Sabach, Teboulle, and
  Vaisbourd]{bolte2018first}
Bolte, J., Sabach, S., Teboulle, M., and Vaisbourd, Y.
\newblock First order methods beyond convexity and {L}ipschitz gradient
  continuity with applications to quadratic inverse problems.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (3):\penalty0
  2131--2151, 2018.

\bibitem[Brooks(2011)]{brooks2011support}
Brooks, J.~P.
\newblock Support vector machines with the ramp loss and the hard margin loss.
\newblock \emph{Operations Research}, 59\penalty0 (2):\penalty0 467--479, 2011.

\bibitem[Burke et~al.(2002)Burke, Lewis, and Overton]{burke2002approximating}
Burke, J.~V., Lewis, A.~S., and Overton, M.~L.
\newblock Approximating subdifferentials by random sampling of gradients.
\newblock \emph{Mathematics of Operations Research}, 27\penalty0 (3):\penalty0
  567--584, 2002.

\bibitem[Burke et~al.(2005)Burke, Lewis, and Overton]{burke2005robust}
Burke, J.~V., Lewis, A.~S., and Overton, M.~L.
\newblock A robust gradient sampling algorithm for nonsmooth, nonconvex
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (3):\penalty0
  751--779, 2005.

\bibitem[Burke et~al.(2020)Burke, Curtis, Lewis, Overton, and
  Sim{\~o}es]{burke2020gradient}
Burke, J.~V., Curtis, F.~E., Lewis, A.~S., Overton, M.~L., and Sim{\~o}es,
  L.~E.
\newblock Gradient sampling methods for nonsmooth optimization.
\newblock \emph{Numerical Nonsmooth Optimization: State of the Art Algorithms},
  pp.\  201--225, 2020.

\bibitem[Carrizosa et~al.(2014)Carrizosa, Nogales-G{\'o}mez, and
  Morales]{carrizosa2014heuristic}
Carrizosa, E., Nogales-G{\'o}mez, A., and Morales, D.~R.
\newblock Heuristic approaches for support vector machines with the ramp loss.
\newblock \emph{Optimization Letters}, 8\penalty0 (3):\penalty0 1125--1135,
  2014.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2018lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 2937--2947, 2019.

\bibitem[Clarke(1990)]{clarke1990optimization}
Clarke, F.~H.
\newblock \emph{Optimization and Nonsmooth Analysis}.
\newblock SIAM, 1990.

\bibitem[Collobert et~al.(2006{\natexlab{a}})Collobert, Sinz, Weston, and
  Bottou]{collobert2006trading}
Collobert, R., Sinz, F., Weston, J., and Bottou, L.
\newblock Trading convexity for scalability.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  201--208, 2006{\natexlab{a}}.

\bibitem[Collobert et~al.(2006{\natexlab{b}})Collobert, Sinz, Weston, Bottou,
  and Joachims]{collobert2006large}
Collobert, R., Sinz, F., Weston, J., Bottou, L., and Joachims, T.
\newblock Large scale transductive {SVM}s.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0 (8),
  2006{\natexlab{b}}.

\bibitem[Daniilidis \& Drusvyatskiy(2020)Daniilidis and
  Drusvyatskiy]{daniilidis2020pathological}
Daniilidis, A. and Drusvyatskiy, D.
\newblock Pathological subgradient dynamics.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (2):\penalty0
  1327--1338, 2020.

\bibitem[Davis \& Drusvyatskiy(2019)Davis and
  Drusvyatskiy]{davis2019stochastic}
Davis, D. and Drusvyatskiy, D.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Davis \& Drusvyatskiy(2020)Davis and Drusvyatskiy]{davis19opt}
Davis, D. and Drusvyatskiy, D.
\newblock Subgradient methods under weak convexity and tame geometry.
\newblock \emph{SIAG/OPT Views and News}, 28\penalty0 (1), 2020.

\bibitem[Davis \& Grimmer(2019)Davis and Grimmer]{davis2019proximally}
Davis, D. and Grimmer, B.
\newblock Proximally guided stochastic subgradient method for nonsmooth,
  nonconvex problems.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (3):\penalty0
  1908--1930, 2019.

\bibitem[Davis et~al.(2020)Davis, Drusvyatskiy, Kakade, and
  Lee]{davis2020stochastic}
Davis, D., Drusvyatskiy, D., Kakade, S., and Lee, J.~D.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (1):\penalty0 119--154, 2020.

\bibitem[Davis et~al.(2021)Davis, Drusvyatskiy, Lee, Padmanabhan, and
  Ye]{davis2021gradient}
Davis, D., Drusvyatskiy, D., Lee, Y.~T., Padmanabhan, S., and Ye, G.
\newblock A gradient sampling method with complexity guarantees for lipschitz
  functions in high and low dimensions.
\newblock \emph{arXiv preprint arXiv:2112.06969}, 2021.

\bibitem[Dontchev \& Rockafellar(2009)Dontchev and
  Rockafellar]{dontchev2009implicit}
Dontchev, A.~L. and Rockafellar, R.~T.
\newblock \emph{Implicit Functions and Solution Mappings}, volume 543.
\newblock Springer, 2009.

\bibitem[Drusvyatskiy \& Paquette(2019)Drusvyatskiy and
  Paquette]{drusvyatskiy2019efficiency}
Drusvyatskiy, D. and Paquette, C.
\newblock Efficiency of minimizing compositions of convex functions and smooth
  maps.
\newblock \emph{Mathematical Programming}, 178\penalty0 (1):\penalty0 503--558,
  2019.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Duchi \& Ruan(2018)Duchi and Ruan]{duchi2018stochastic}
Duchi, J.~C. and Ruan, F.
\newblock Stochastic methods for composite and weakly convex optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  3229--3259, 2018.

\bibitem[Eberle et~al.(2021)Eberle, Jentzen, Riekert, and
  Weiss]{eberle2021existence}
Eberle, S., Jentzen, A., Riekert, A., and Weiss, G.~S.
\newblock Existence, uniqueness, and convergence rates for gradient flows in
  the training of artificial neural networks with {ReLU} activation.
\newblock \emph{arXiv preprint arXiv:2108.08106}, 2021.

\bibitem[Ertekin et~al.(2010)Ertekin, Bottou, and Giles]{ertekin2010nonconvex}
Ertekin, S., Bottou, L., and Giles, C.~L.
\newblock Nonconvex online support vector machines.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 33\penalty0 (2):\penalty0 368--381, 2010.

\bibitem[Goldstein(1977)]{goldstein1977optimization}
Goldstein, A.
\newblock Optimization of {L}ipschitz continuous functions.
\newblock \emph{Mathematical Programming}, 13\penalty0 (1):\penalty0 14--22,
  1977.

\bibitem[Huang et~al.(2014)Huang, Shi, and Suykens]{huang2014ramp}
Huang, X., Shi, L., and Suykens, J.~A.
\newblock Ramp loss linear programming support vector machine.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2185--2211, 2014.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8580--8589, 2018.

\bibitem[Jentzen \& Riekert(2021)Jentzen and Riekert]{jentzen2021existence}
Jentzen, A. and Riekert, A.
\newblock On the existence of global minima and convergence analyses for
  gradient descent methods in the training of deep neural networks.
\newblock \emph{arXiv preprint arXiv:2112.09684}, 2021.

\bibitem[Kakade \& Lee(2018)Kakade and Lee]{kakade2018provably}
Kakade, S.~M. and Lee, J.~D.
\newblock Provably correct automatic subdifferentiation for qualified programs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7125--7135, 2018.

\bibitem[Keshet \& McAllester(2011)Keshet and
  McAllester]{david2011generalization}
Keshet, J. and McAllester, D.
\newblock Generalization bounds and consistency for latent structural probit
  and ramp loss.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~24, 2011.

\bibitem[Khan \& Barton(2013)Khan and Barton]{khan2013evaluating}
Khan, K.~A. and Barton, P.~I.
\newblock Evaluating an element of the clarke generalized jacobian of a
  composite piecewise differentiable function.
\newblock \emph{ACM Transactions on Mathematical Software}, 39\penalty0
  (4):\penalty0 1--28, 2013.

\bibitem[Kiwiel(2007)]{kiwiel2007convergence}
Kiwiel, K.~C.
\newblock Convergence of the gradient sampling algorithm for nonsmooth
  nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (2):\penalty0
  379--388, 2007.

\bibitem[Kiwiel(2010)]{kiwiel2010nonderivative}
Kiwiel, K.~C.
\newblock A nonderivative version of the gradient sampling algorithm for
  nonsmooth nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (4):\penalty0
  1983--1994, 2010.

\bibitem[Kornowski \& Shamir(2021)Kornowski and Shamir]{kornowski2021oracle}
Kornowski, G. and Shamir, O.
\newblock Oracle complexity in nonsmooth nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  324--334, 2021.

\bibitem[Lewis \& Pang(2010)Lewis and Pang]{lewis2010lipschitz}
Lewis, A.~S. and Pang, C.~J.
\newblock Lipschitz behavior of the robust regularization.
\newblock \emph{SIAM Journal on Control and Optimization}, 48\penalty0
  (5):\penalty0 3080--3104, 2010.

\bibitem[Liu et~al.(2005)Liu, Shen, and Doss]{liu2005multicategory}
Liu, Y., Shen, X., and Doss, H.
\newblock Multicategory $\psi$-learning and support vector machine:
  computational tools.
\newblock \emph{Journal of Computational and Graphical Statistics}, 14\penalty0
  (1):\penalty0 219--236, 2005.

\bibitem[Maibing \& Igel(2015)Maibing and Igel]{maibing2015computational}
Maibing, S.~F. and Igel, C.
\newblock Computational complexity of linear large margin classification with
  ramp loss.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  259--267. PMLR, 2015.

\bibitem[Majewski et~al.(2018)Majewski, Miasojedow, and
  Moulines]{majewski2018analysis}
Majewski, S., Miasojedow, B., and Moulines, E.
\newblock Analysis of nonsmooth stochastic approximation: The differential
  inclusion approach.
\newblock \emph{arXiv preprint arXiv:1805.01916}, 2018.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of Machine Learning}.
\newblock MIT press, 2018.

\bibitem[Munkres(1974)]{munkres1974topology}
Munkres, J.~R.
\newblock \emph{Topology: A First Course}.
\newblock Prentice-Hall, 1974.

\bibitem[Nemirovskij \& Yudin(1983)Nemirovskij and
  Yudin]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock Wiley-Interscience, 1983.

\bibitem[Nesterov(2005)]{nesterov2005lexicographic}
Nesterov, {\relax Yu}.
\newblock Lexicographic differentiation of nonsmooth functions.
\newblock \emph{Mathematical Programming}, 104\penalty0 (2):\penalty0 669--700,
  2005.

\bibitem[Robinson(1981)]{robinson1981some}
Robinson, S.~M.
\newblock Some continuity properties of polyhedral multifunctions.
\newblock In \emph{Mathematical Programming at Oberwolfach}, pp.\  206--214.
  Springer, 1981.

\bibitem[Rockafellar \& Wets(2009)Rockafellar and
  Wets]{rockafellar2009variational}
Rockafellar, R.~T. and Wets, R. J.-B.
\newblock \emph{Variational Analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Shen et~al.(2003)Shen, Tseng, Zhang, and Wong]{shen2003psi}
Shen, X., Tseng, G.~C., Zhang, X., and Wong, W.~H.
\newblock On $\psi$-learning.
\newblock \emph{Journal of the American Statistical Association}, 98\penalty0
  (463):\penalty0 724--734, 2003.

\bibitem[Suzumura et~al.(2017)Suzumura, Ogawa, Sugiyama, Karasuyama, and
  Takeuchi]{suzumura2017homotopy}
Suzumura, S., Ogawa, K., Sugiyama, M., Karasuyama, M., and Takeuchi, I.
\newblock Homotopy continuation approaches for robust {SV} classification and
  regression.
\newblock \emph{Machine Learning}, 106\penalty0 (7):\penalty0 1009--1038, 2017.

\bibitem[Tian \& So(2021)Tian and So]{tian2021hardness}
Tian, L. and So, A. M.-C.
\newblock On the hardness of computing near-approximate stationary points of
  {C}larke regular nonsmooth nonconvex problems and certain {DC} programs.
\newblock \emph{ICML Workshop on Beyond First-Order Methods in ML Systems},
  2021.

\bibitem[Tian \& So(2022)Tian and So]{tian2022computing}
Tian, L. and So, A. M.-C.
\newblock Computing d-stationary points of $\rho$-margin loss {SVM}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3772--3793. PMLR, 2022.

\bibitem[Vial(1983)]{vial1983strong}
Vial, J.-P.
\newblock Strong and weak convexity of sets and functions.
\newblock \emph{Mathematics of Operations Research}, 8\penalty0 (2):\penalty0
  231--259, 1983.

\bibitem[Wang et~al.(2021)Wang, Shao, and Xiu]{wang2021proximal}
Wang, H., Shao, Y., and Xiu, N.
\newblock Proximal operator and optimality conditions for ramp loss {SVM}.
\newblock \emph{Optimization Letters}, pp.\  1--16, 2021.

\bibitem[Wolfe(1975)]{wolfe1975method}
Wolfe, P.
\newblock A method of conjugate subgradients for minimizing nondifferentiable
  functions.
\newblock In \emph{Nondifferentiable Optimization}, pp.\  145--173. Springer,
  1975.

\bibitem[Wu \& Liu(2007)Wu and Liu]{wu2007robust}
Wu, Y. and Liu, Y.
\newblock Robust truncated hinge loss support vector machines.
\newblock \emph{Journal of the American Statistical Association}, 102\penalty0
  (479):\penalty0 974--983, 2007.

\bibitem[Zhang et~al.(2020)Zhang, Lin, Jegelka, Jadbabaie, and
  Sra]{zhang2020complexity}
Zhang, J., Lin, H., Jegelka, S., Jadbabaie, A., and Sra, S.
\newblock Complexity of finding stationary points of nonsmooth nonconvex
  functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11173--11182, 2020.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
