\begin{thebibliography}{10}

\bibitem{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In {\em Proceedings of the 2016 ACM SIGSAC conference on computer and
  communications security}, pages 308--318, 2016.

\bibitem{allen2018natasha}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{andrew2021differentially}
Galen Andrew, Om~Thakkar, Brendan McMahan, and Swaroop Ramaswamy.
\newblock Differentially private learning with adaptive clipping.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{banarjee2005}
Satanjeev Banerjee and Alon Lavie.
\newblock {METEOR}: An automatic metric for {MT} evaluation with improved
  correlation with human judgments.
\newblock In {\em Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic
  Evaluation Measures for Machine Translation and/or Summarization}, pages
  65--72, Ann Arbor, Michigan, June 2005. Association for Computational
  Linguistics.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em Siam Review}, 60(2):223--311, 2018.

\bibitem{bu2020deep}
Zhiqi Bu, Jinshuo Dong, Qi~Long, and Weijie~J Su.
\newblock Deep learning with gaussian differential privacy.
\newblock {\em Harvard data science review}, 2020(23), 2020.

\bibitem{bu2021fast}
Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin~Tat Lee, Hanwen Shen, and
  Uthaipon Tantipongpipat.
\newblock Fast and memory efficient differentially private-sgd via jl
  projections.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{bu2021convergence}
Zhiqi Bu, Hua Wang, and Qi~Long.
\newblock On the convergence and calibration of deep learning with differential
  privacy.
\newblock {\em arXiv preprint arXiv:2106.07830}, 2021.

\bibitem{bun2018composable}
Mark Bun, Cynthia Dwork, Guy~N Rothblum, and Thomas Steinke.
\newblock Composable and versatile privacy via truncated cdp.
\newblock In {\em Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 74--86, 2018.

\bibitem{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In {\em 30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650, 2021.

\bibitem{chaudhari2018stochastic}
Pratik Chaudhari and Stefano Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In {\em 2018 Information Theory and Applications Workshop (ITA)},
  pages 1--10. IEEE, 2018.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{chen2020understanding}
Xiangyi Chen, Steven~Z Wu, and Mingyi Hong.
\newblock Understanding gradient clipping in private sgd: A geometric
  perspective.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13773--13782, 2020.

\bibitem{cutkosky2020momentum}
Ashok Cutkosky and Harsh Mehta.
\newblock Momentum improves normalized sgd.
\newblock In {\em International conference on machine learning}, pages
  2260--2268. PMLR, 2020.

\bibitem{das2021convergence}
Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit~S Dhillon.
\newblock On the convergence of differentially private federated learning on
  non-lipschitz objectives, and with normalized client updates.
\newblock {\em arXiv preprint arXiv:2106.07094}, 2021.

\bibitem{de2022unlocking}
Soham De, Leonard Berrada, Jamie Hayes, Samuel~L Smith, and Borja Balle.
\newblock Unlocking high-accuracy differentially private image classification
  through scale.
\newblock {\em arXiv preprint arXiv:2204.13650}, 2022.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dong2019gaussian}
Jinshuo Dong, Aaron Roth, and Weijie~J Su.
\newblock Gaussian differential privacy.
\newblock {\em Journal of the Royal Statistical Society Series B}, 84(1):3--37,
  2022.

\bibitem{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem{du2021dp}
Jian Du and Haitao Mi.
\newblock Dp-fp: Differentially private forward propagation for large models.
\newblock {\em arXiv preprint arXiv:2112.14430}, 2021.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem{dusek.etal2020:csl}
Ondrej Dusek, Jekaterina Novikova, and Verena Rieser.
\newblock Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural
  Language Generation}}: {{The E2E NLG Challenge}}.
\newblock {\em Computer Speech \& Language}, 59:123--156, January 2020.

\bibitem{dwork2008differential}
Cynthia Dwork.
\newblock Differential privacy: A survey of results.
\newblock In {\em International conference on theory and applications of models
  of computation}, pages 1--19. Springer, 2008.

\bibitem{dwork2006calibrating}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In {\em Theory of cryptography conference}, pages 265--284. Springer,
  2006.

\bibitem{dwork2014algorithmic}
Cynthia Dwork, Aaron Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Found. Trends Theor. Comput. Sci.}, 9(3-4):211--407, 2014.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{golatkar2022mixed}
Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns,
  and Stefano Soatto.
\newblock Mixed differential privacy in computer vision.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8376--8386, 2022.

\bibitem{goodfellow2014explaining}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{gopi2021numerical}
Sivakanth Gopi, Yin~Tat Lee, and Lukas Wutschitz.
\newblock Numerical composition of differential privacy.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{he2022exploring}
Jiyan He, Xuechen Li, Da~Yu, Huishuai Zhang, Janardhan Kulkarni, Yin~Tat Lee,
  Arturs Backurs, Nenghai Yu, and Jiang Bian.
\newblock Exploring the limits of differentially private deep learning with
  group-wise clipping.
\newblock {\em arXiv preprint arXiv:2212.01539}, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang,
  Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{WinNT}
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai.
\newblock First quora dataset release: Question pairs, 2017.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em International Conference on Learning Representations}, 12 2014.

\bibitem{klause2022differentially}
Helena Klause, Alexander Ziller, Daniel Rueckert, Kerstin Hammernik, and
  Georgios Kaissis.
\newblock Differentially private training of residual networks with scale
  normalisation.
\newblock {\em arXiv preprint arXiv:2203.00324}, 2022.

\bibitem{koskela2020computing}
Antti Koskela, Joonas J{\"a}lk{\"o}, and Antti Honkela.
\newblock Computing tight differential privacy guarantees using fft.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2560--2569. PMLR, 2020.

\bibitem{kurakin2022toward}
Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and
  Abhradeep Thakurta.
\newblock Toward training at imagenet scale with differential privacy.
\newblock {\em arXiv preprint arXiv:2201.12328}, 2022.

\bibitem{lhoest-etal-2021-datasets}
Quentin Lhoest, Albert Villanova~del Moral, Yacine Jernite, Abhishek Thakur,
  Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu,
  Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik,
  Simon Brandeis, Teven Le~Scao, Victor Sanh, Canwen Xu, Nicolas Patry,
  Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl{\'e}ment
  Delangue, Th{\'e}o Matussi{\`e}re, Lysandre Debut, Stas Bekman, Pierric
  Cistac, Thibault Goehringer, Victor Mustar, Fran{\c{c}}ois Lagunas, Alexander
  Rush, and Thomas Wolf.
\newblock Datasets: A community library for natural language processing.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 175--184, Online
  and Punta Cana, Dominican Republic, November 2021. Association for
  Computational Linguistics.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, 2021.

\bibitem{li2021large}
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{lin-2004-rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In {\em Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain, July 2004. Association for Computational Linguistics.

\bibitem{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mandic2004generalized}
Danilo~P Mandic.
\newblock A generalized normalized gradient descent algorithm.
\newblock {\em IEEE signal processing letters}, 11(2):115--118, 2004.

\bibitem{mandt2017stochastic}
Stephan Mandt, Matthew~D Hoffman, and David~M Blei.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock {\em Journal of Machine Learning Research}, 18:1--35, 2017.

\bibitem{mcmahan2017learning}
H~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mehta2022large}
Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky.
\newblock Large scale transfer learning for differentially private image
  classification.
\newblock {\em arXiv preprint arXiv:2205.02973}, 2022.

\bibitem{mironov2017renyi}
Ilya Mironov.
\newblock R{\'e}nyi differential privacy.
\newblock In {\em 2017 IEEE 30th computer security foundations symposium
  (CSF)}, pages 263--275. IEEE, 2017.

\bibitem{misra2019mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function.
\newblock {\em BMVC 2020}, 2019.

\bibitem{murray2019revisiting}
Ryan Murray, Brian Swenson, and Soummya Kar.
\newblock Revisiting normalized gradient descent: Fast evasion of saddle
  points.
\newblock {\em IEEE Transactions on Automatic Control}, 64(11):4818--4824,
  2019.

\bibitem{nesterov1983method}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem{papernot2021hyperparameter}
Nicolas Papernot and Thomas Steinke.
\newblock Hyperparameter tuning with renyi differential privacy.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{papernot2020tempered}
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and {\'U}lfar
  Erlingsson.
\newblock Tempered sigmoid activations for deep learning with differential
  privacy.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9312--9321, 2021.

\bibitem{Papineni02bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: A method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th Annual Meeting on Association for
  Computational Linguistics}, ACL '02, page 311–318, USA, 2002. Association
  for Computational Linguistics.

\bibitem{pichapati2019adaclip}
Venkatadheeraj Pichapati, Ananda~Theertha Suresh, Felix~X Yu, Sashank~J Reddi,
  and Sanjiv Kumar.
\newblock Adaclip: Adaptive clipping for private sgd.
\newblock {\em arXiv preprint arXiv:1908.07643}, 2019.

\bibitem{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em Ussr computational mathematics and mathematical physics},
  4(5):1--17, 1964.

\bibitem{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855, 1992.

\bibitem{qiao2019micro}
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille.
\newblock Micro-batch training with batch-channel normalization and weight
  standardization.
\newblock {\em arXiv preprint arXiv:1903.10520}, 2019.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock In {\em EMNLP}, 2016.

\bibitem{sadjadi20182017}
Seyed~Omid Sadjadi, Timothee Kheyrkhah, Audrey Tong, Craig~S Greenberg,
  Douglas~A Reynolds, Elliot Singer, Lisa~P Mason, Jaime Hernandez-Cordero,
  et~al.
\newblock The 2017 nist language recognition evaluation.
\newblock In {\em Odyssey}, pages 82--89, 2018.

\bibitem{shamsabadi2021losing}
Ali~Shahin Shamsabadi and Nicolas Papernot.
\newblock Losing less: A loss for differentially private deep learning.
\newblock 2021.

\bibitem{shokri2017membership}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In {\em 2017 IEEE symposium on security and privacy (SP)}, pages
  3--18. IEEE, 2017.

\bibitem{smith2018don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem{tramer2020differentially}
Florian Tramer and Dan Boneh.
\newblock Differentially private learning needs better features (or much more
  data).
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{vedantam2015cider}
Ramakrishna Vedantam, C~Lawrence~Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4566--4575, 2015.

\bibitem{wang2019subsampled}
Yu-Xiang Wang, Borja Balle, and Shiva~Prasad Kasiviswanathan.
\newblock Subsampled r{\'e}nyi differential privacy and analytical moments
  accountant.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1226--1235. PMLR, 2019.

\bibitem{N18-1101}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122. Association for
  Computational Linguistics, 2018.

\bibitem{xie2020diffusion}
Zeke Xie, Issei Sato, and Masashi Sugiyama.
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient
  descent exponentially favors flat minima.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yang2022normalized}
Xiaodong Yang, Huishuai Zhang, Wei Chen, and Tie-Yan Liu.
\newblock Normalized/clipped sgd with perturbation for differentially private
  non-convex optimization.
\newblock {\em arXiv preprint arXiv:2206.13033}, 2022.

\bibitem{you2017scaling}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling sgd batch size to 32k for imagenet training.
\newblock {\em arXiv preprint arXiv:1708.03888}, 6(12):6, 2017.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{opacus}
Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine,
  Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj,
  Jessica Zhao, Graham Cormode, and Ilya Mironov.
\newblock Opacus: {U}ser-friendly differential privacy library in {PyTorch}.
\newblock {\em arXiv preprint arXiv:2109.12298}, 2021.

\bibitem{yu2021large}
Da~Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.
\newblock Large scale private learning via low-rank reparametrization.
\newblock In {\em International Conference on Machine Learning}, pages
  12208--12218. PMLR, 2021.

\bibitem{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\bibitem{zhao2020stochastic}
Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li.
\newblock Stochastic normalized gradient descent with momentum for large batch
  training.
\newblock {\em arXiv preprint arXiv:2007.13985}, 2020.

\bibitem{zhao2021convergence}
Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li.
\newblock On the convergence and improvement of stochastic normalized gradient
  descent.
\newblock {\em Science China Information Sciences}, 64:1--13, 2021.

\bibitem{zhu2021optimal}
Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang.
\newblock Optimal accounting of differential privacy via characteristic
  function.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4782--4817. PMLR, 2022.

\end{thebibliography}
