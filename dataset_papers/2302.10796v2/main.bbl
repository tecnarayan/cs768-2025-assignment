\newcommand{\arxiv}[1]{arXiv:\href{https://arxiv.org/abs/#1}{\ttfamily{#1}}\?}\newcommand{\arXiv}[1]{arXiv:\href{https://arxiv.org/abs/#1}{\ttfamily{#1}}\?}\def\?#1{\if.#1{}\else#1\fi}
\begin{thebibliography}{69}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Arunachalam and de~Wolf(2017)}]{arunachalam2017guest}
\text{Arunachalam, S.} and \text{de~Wolf, R.} (2017).
\newblock Guest column: a survey of quantum learning theory.
\newblock \textit{ACM SIGACT News}, \textbf{48} 41--67.
\newblock \arxiv{1701.06806}.

\bibitem[{Auer et~al.(2008)Auer, Jaksch and Ortner}]{auer2008near}
\text{Auer, P.}, \text{Jaksch, T.} and \text{Ortner, R.} (2008).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Advances in Neural Information Processing systems},
  \textbf{21}.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\text{Ayoub, A.}, \text{Jia, Z.}, \text{Szepesvari, C.}, \text{Wang, M.} and
  \text{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{2006.01107}.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\text{Azar, M.~G.}, \text{Osband, I.} and \text{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1703.05449}.

\bibitem[{Barenco et~al.(1995)Barenco, Bennett, Cleve, DiVincenzo, Margolus,
  Shor, Sleator, Smolin and Weinfurter}]{barenco1995elementary}
\text{Barenco, A.}, \text{Bennett, C.~H.}, \text{Cleve, R.}, \text{DiVincenzo,
  D.~P.}, \text{Margolus, N.}, \text{Shor, P.}, \text{Sleator, T.},
  \text{Smolin, J.~A.} and \text{Weinfurter, H.} (1995).
\newblock Elementary gates for quantum computation.
\newblock \textit{Physical Review A}, \textbf{52} 3457.
\newblock \arxiv{quant-ph/9503016}.

\bibitem[{Biamonte et~al.(2017)Biamonte, Wittek, Pancotti, Rebentrost, Wiebe
  and Lloyd}]{biamonte2017quantum}
\text{Biamonte, J.}, \text{Wittek, P.}, \text{Pancotti, N.}, \text{Rebentrost,
  P.}, \text{Wiebe, N.} and \text{Lloyd, S.} (2017).
\newblock Quantum machine learning.
\newblock \textit{Nature}, \textbf{549} 195.
\newblock \arxiv{1611.09347}.

\bibitem[{Brassard et~al.(2002)Brassard, H{\o}yer, Mosca and
  Tapp}]{brassard2002amplitude}
\text{Brassard, G.}, \text{H{\o}yer, P.}, \text{Mosca, M.} and \text{Tapp, A.}
  (2002).
\newblock Quantum amplitude amplification and estimation.
\newblock \textit{Contemporary Mathematics}, \textbf{305} 53--74.
\newblock \arxiv{quant-ph/0005055}.

\bibitem[{Cai et~al.(2020)Cai, Yang, Jin and Wang}]{cai2020provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1912.05830}.

\bibitem[{Casal{\'e} et~al.(2020)Casal{\'e}, Di~Molfetta, Kadri and
  Ralaivola}]{casale2020quantum}
\text{Casal{\'e}, B.}, \text{Di~Molfetta, G.}, \text{Kadri, H.} and
  \text{Ralaivola, L.} (2020).
\newblock Quantum bandits.
\newblock \textit{Quantum Machine Intelligence}, \textbf{2} 1--7.
\newblock \arxiv{2002.06395}.

\bibitem[{Chen et~al.(2022)Chen, Li, Yuan, Gu and Jordan}]{chen2022general}
\text{Chen, Z.}, \text{Li, C.~J.}, \text{Yuan, A.}, \text{Gu, Q.} and
  \text{Jordan, M.~I.} (2022).
\newblock A general framework for sample-efficient function approximation in
  reinforcement learning.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2209.15634}.

\bibitem[{Cherrat et~al.(2022)Cherrat, Kerenidis and
  Prakash}]{cherrat2022quantum}
\text{Cherrat, E.~A.}, \text{Kerenidis, I.} and \text{Prakash, A.} (2022).
\newblock Quantum reinforcement learning via policy iteration.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2203.01889}.

\bibitem[{Cornelissen(2018)}]{cornelissen2018quantum}
\text{Cornelissen, A.} (2018).
\newblock \textit{Quantum gradient estimation and its application to quantum
  reinforcement learning}.
\newblock Ph.D. thesis, Delft University of Technology.

\bibitem[{Cornelissen et~al.(2022)Cornelissen, Hamoudi and
  Jerbi}]{cornelissen2022near}
\text{Cornelissen, A.}, \text{Hamoudi, Y.} and \text{Jerbi, S.} (2022).
\newblock Near-optimal quantum algorithms for multivariate mean estimation.
\newblock In \textit{Proceedings of the 54th Annual ACM SIGACT Symposium on
  Theory of Computing}.
\newblock \arxiv{2111.09787}.

\bibitem[{Dann et~al.(2019)Dann, Li, Wei and Brunskill}]{dann2019policy}
\text{Dann, C.}, \text{Li, L.}, \text{Wei, W.} and \text{Brunskill, E.} (2019).
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1811.03056}.

\bibitem[{Dong and Ma(2022)}]{dong2022asymptotic}
\text{Dong, K.} and \text{Ma, T.} (2022).
\newblock Asymptotic instance-optimal algorithms for interactive decision
  making.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2206.02326}.

\bibitem[{Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun and
  Wang}]{du2021bilinear}
\text{Du, S.}, \text{Kakade, S.}, \text{Lee, J.}, \text{Lovett, S.},
  \text{Mahajan, G.}, \text{Sun, W.} and \text{Wang, R.} (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in {RL}.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{2103.10897}.

\bibitem[{Dunjko and Briegel(2018)}]{dunjko2018machine}
\text{Dunjko, V.} and \text{Briegel, H.~J.} (2018).
\newblock Machine learning \& artificial intelligence in the quantum domain: a
  review of recent progress.
\newblock \textit{Reports on Progress in Physics}, \textbf{81} 074001.
\newblock \arxiv{1709.02779}.

\bibitem[{Dunjko et~al.(2015)Dunjko, Taylor and Briegel}]{dunjko2015framework}
\text{Dunjko, V.}, \text{Taylor, J.~M.} and \text{Briegel, H.~J.} (2015).
\newblock Framework for learning agents in quantum environments.
\newblock \arxiv{1507.08482}.

\bibitem[{Dunjko et~al.(2016)Dunjko, Taylor and Briegel}]{dunjko2016quantum}
\text{Dunjko, V.}, \text{Taylor, J.~M.} and \text{Briegel, H.~J.} (2016).
\newblock Quantum-enhanced machine learning.
\newblock \textit{Physical Review Letters}, \textbf{117} 130501.
\newblock \arxiv{1610.08251}.

\bibitem[{Dunjko et~al.(2017)Dunjko, Taylor and Briegel}]{dunjko2017advances}
\text{Dunjko, V.}, \text{Taylor, J.~M.} and \text{Briegel, H.~J.} (2017).
\newblock Advances in quantum reinforcement learning.
\newblock In \textit{2017 IEEE International Conference on Systems, Man, and
  Cybernetics}. IEEE.
\newblock \arxiv{1811.08676}.

\bibitem[{Foster et~al.(2021)Foster, Kakade, Qian and
  Rakhlin}]{foster2021statistical}
\text{Foster, D.~J.}, \text{Kakade, S.~M.}, \text{Qian, J.} and \text{Rakhlin,
  A.} (2021).
\newblock The statistical complexity of interactive decision making.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2112.13487}.

\bibitem[{Ganguly et~al.(2023)Ganguly, Wu, Wang and
  Aggarwal}]{ganguly2023quantum}
\text{Ganguly, B.}, \text{Wu, Y.}, \text{Wang, D.} and \text{Aggarwal, V.}
  (2023).
\newblock Quantum computing provides exponential regret improvement in episodic
  reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2302.08617}.

\bibitem[{Gily{\'e}n and Li(2020)}]{gilyen2020distributional}
\text{Gily{\'e}n, A.} and \text{Li, T.} (2020).
\newblock Distributional property testing in a quantum world.
\newblock In \textit{Proceedings of the 11th Innovations in Theoretical
  Computer Science Conference}, vol. 151 of \textit{Leibniz International
  Proceedings in Informatics (LIPIcs)}. Schloss Dagstuhl-Leibniz-Zentrum
  f{\"u}r Informatik.
\newblock \arxiv{1902.00814}.

\bibitem[{Grover and Rudolph(2002)}]{grover2002creating}
\text{Grover, L.} and \text{Rudolph, T.} (2002).
\newblock Creating superpositions that correspond to efficiently integrable
  probability distributions.
\newblock \arxiv{quant-ph/0208112}.

\bibitem[{Grover(1997)}]{grover1997quantum}
\text{Grover, L.~K.} (1997).
\newblock Quantum mechanics helps in searching for a needle in a haystack.
\newblock \textit{Physical Review Letters}, \textbf{79} 325.
\newblock \arxiv{quant-ph/9706033}.

\bibitem[{Hamann et~al.(2021)Hamann, Dunjko and W{\"o}lk}]{hamann2021quantum}
\text{Hamann, A.}, \text{Dunjko, V.} and \text{W{\"o}lk, S.} (2021).
\newblock Quantum-accessible reinforcement learning beyond strictly epochal
  environments.
\newblock \textit{Quantum Machine Intelligence}, \textbf{3} 1--18.
\newblock \arxiv{2008.01481}.

\bibitem[{Hamann and W{\"o}lk(2022)}]{hamann2022performance}
\text{Hamann, A.} and \text{W{\"o}lk, S.} (2022).
\newblock Performance analysis of a hybrid agent for quantum-accessible
  reinforcement learning.
\newblock \textit{New Journal of Physics}, \textbf{24} 033044.
\newblock \arxiv{2107.14001}.

\bibitem[{Hamoudi(2021)}]{hamoudi2021subgaussian}
\text{Hamoudi, Y.} (2021).
\newblock Quantum sub-{G}aussian mean estimator.
\newblock In \textit{Proceedings of the 29th Annual European Symposium on
  Algorithms}, vol. 204 of \textit{Leibniz International Proceedings in
  Informatics}. Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik.
\newblock \arxiv{2108.12172}.

\bibitem[{Hamoudi and Magniez(2019)}]{hamoudi2019Chebyshev}
\text{Hamoudi, Y.} and \text{Magniez, F.} (2019).
\newblock Quantum {C}hebyshev's inequality and applications.
\newblock In \textit{Proceedings of the 46th International Colloquium on
  Automata, Languages, and Programming}, vol. 132 of \textit{Leibniz
  International Proceedings in Informatics}.
\newblock \arxiv{1807.06456}.

\bibitem[{He et~al.(2021)He, Zhou and Gu}]{he2021logarithmic}
\text{He, J.}, \text{Zhou, D.} and \text{Gu, Q.} (2021).
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{2011.11566}.

\bibitem[{Ishfaq et~al.(2021)Ishfaq, Cui, Nguyen, Ayoub, Yang, Wang, Precup and
  Yang}]{ishfaq2021randomized}
\text{Ishfaq, H.}, \text{Cui, Q.}, \text{Nguyen, V.}, \text{Ayoub, A.},
  \text{Yang, Z.}, \text{Wang, Z.}, \text{Precup, D.} and \text{Yang, L.}
  (2021).
\newblock Randomized exploration in reinforcement learning with general value
  function approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\text{Jaksch, T.}, \text{Ortner, R.} and \text{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11}
  1563--1600.

\bibitem[{Jerbi et~al.(2022)Jerbi, Cornelissen, Ozols and
  Dunjko}]{jerbi2022quantum}
\text{Jerbi, S.}, \text{Cornelissen, A.}, \text{Ozols, M.} and \text{Dunjko,
  V.} (2022).
\newblock Quantum policy gradient algorithms.
\newblock \arxiv{2212.09328}.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}, \text{Langford,
  J.} and \text{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low bellman rank are
  {PAC}-learnable.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1610.09512}.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\text{Jin, C.}, \text{Allen-Zhu, Z.}, \text{Bubeck, S.} and \text{Jordan,
  M.~I.} (2018).
\newblock Is {Q}-learning provably efficient?
\newblock \textit{Advances in Neural Information Processing systems},
  \textbf{31}.
\newblock \arxiv{1807.03765}.

\bibitem[{Jin et~al.(2021)Jin, Liu and Miryoosefi}]{jin2021bellman}
\text{Jin, C.}, \text{Liu, Q.} and \text{Miryoosefi, S.} (2021).
\newblock Bellman eluder dimension: New rich classes of {RL} problems, and
  sample-efficient algorithms.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{34}.
\newblock \arxiv{2102.00815}.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{1907.05388}.

\bibitem[{Kakade(2003)}]{kakade2003sample}
\text{Kakade, S.~M.} (2003).
\newblock \textit{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom).

\bibitem[{Kearns and Singh(1998)}]{kearns1998finite}
\text{Kearns, M.} and \text{Singh, S.} (1998).
\newblock Finite-sample convergence rates for {Q}-learning and indirect
  algorithms.
\newblock In \textit{Advances in Neural Information Processing systems},
  vol.~11.

\bibitem[{Li and Wu(2019)}]{li2019entropy}
\text{Li, T.} and \text{Wu, X.} (2019).
\newblock Quantum query complexity of entropy estimation.
\newblock \textit{IEEE Transactions on Information Theory}, \textbf{65}
  2899--2921.
\newblock \arxiv{1710.06025}.

\bibitem[{Li and Zhang(2022)}]{li2022approx}
\text{Li, T.} and \text{Zhang, R.} (2022).
\newblock Quantum speedups of optimizing approximately convex functions with
  applications to logarithmic regret stochastic convex bandits.
\newblock In \textit{Advances in Neural Information Processing Systems},
  vol.~35.
\newblock \arxiv{2209.12897}.

\bibitem[{Lumbreras et~al.(2022)Lumbreras, Haapasalo and
  Tomamichel}]{lumbreras2022multi}
\text{Lumbreras, J.}, \text{Haapasalo, E.} and \text{Tomamichel, M.} (2022).
\newblock Multi-armed quantum bandits: Exploration versus exploitation when
  learning properties of quantum states.
\newblock \textit{Quantum}, \textbf{6} 749.
\newblock \arxiv{2108.13050}.

\bibitem[{Meyer et~al.(2022)Meyer, Ufrecht, Periyasamy, Scherer, Plinge and
  Mutschler}]{meyer2022survey}
\text{Meyer, N.}, \text{Ufrecht, C.}, \text{Periyasamy, M.}, \text{Scherer,
  D.~D.}, \text{Plinge, A.} and \text{Mutschler, C.} (2022).
\newblock A survey on quantum reinforcement learning.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2211.03464}.

\bibitem[{Modi et~al.(2020)Modi, Jiang, Tewari and Singh}]{modi2020sample}
\text{Modi, A.}, \text{Jiang, N.}, \text{Tewari, A.} and \text{Singh, S.}
  (2020).
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.
\newblock \arxiv{1910.10597}.

\bibitem[{Montanaro(2015)}]{montanaro2015quantum}
\text{Montanaro, A.} (2015).
\newblock Quantum speedup of {M}onte {C}arlo methods.
\newblock \textit{Proceedings of the Royal Society A: Mathematical, Physical
  and Engineering Sciences}, \textbf{471} 20150301.
\newblock \arxiv{1504.06987}.

\bibitem[{Nayak and Wu(1999)}]{nayak1999quantum}
\text{Nayak, A.} and \text{Wu, F.} (1999).
\newblock The quantum query complexity of approximating the median and related
  statistics.
\newblock In \textit{Proceedings of the 31st Annual ACM Symposium on Theory of
  Computing}.
\newblock \arxiv{quant-ph/9804066}.

\bibitem[{Nielsen and Chuang(2002)}]{nielsen2002quantum}
\text{Nielsen, M.~A.} and \text{Chuang, I.} (2002).
\newblock Quantum computation and quantum information.

\bibitem[{Saggio et~al.(2021)Saggio, Asenbeck, Hamann, Str{\"o}mberg,
  Schiansky, Dunjko, Friis, Harris, Hochberg, Englund, W\"{0}lk, Briegel and
  Walther}]{saggio2021experimental}
\text{Saggio, V.}, \text{Asenbeck, B.~E.}, \text{Hamann, A.},
  \text{Str{\"o}mberg, T.}, \text{Schiansky, P.}, \text{Dunjko, V.},
  \text{Friis, N.}, \text{Harris, N.~C.}, \text{Hochberg, M.}, \text{Englund,
  D.}, \text{W\"{0}lk, S.}, \text{Briegel, H.~J.} and \text{Walther, P.}
  (2021).
\newblock Experimental quantum speed-up in reinforcement learning agents.
\newblock \textit{Nature}, \textbf{591} 229--233.

\bibitem[{Schuld et~al.(2015)Schuld, Sinayskiy and
  Petruccione}]{schuld2015introduction}
\text{Schuld, M.}, \text{Sinayskiy, I.} and \text{Petruccione, F.} (2015).
\newblock An introduction to quantum machine learning.
\newblock \textit{Contemporary Physics}, \textbf{56} 172--185.
\newblock \arxiv{1409.3097}.

\bibitem[{Sidford et~al.(2018)Sidford, Wang, Wu, Yang and Ye}]{sidford2018near}
\text{Sidford, A.}, \text{Wang, M.}, \text{Wu, X.}, \text{Yang, L.} and
  \text{Ye, Y.} (2018).
\newblock Near-optimal time and sample complexities for solving {M}arkov
  decision processes with a generative model.
\newblock In \textit{Advances in Neural Information Processing Systems},
  vol.~31.

\bibitem[{Simchowitz and Jamieson(2019)}]{simchowitz2019non}
\text{Simchowitz, M.} and \text{Jamieson, K.~G.} (2019).
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{32}.
\newblock \arxiv{1905.03814}.

\bibitem[{Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal and
  Langford}]{sun2019model}
\text{Sun, W.}, \text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}
  and \text{Langford, J.} (2019).
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{1811.08540}.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT Press.

\bibitem[{van Apeldoorn(2021)}]{van2021quantum}
\text{van Apeldoorn, J.} (2021).
\newblock Quantum probability oracles \& multidimensional amplitude estimation.
\newblock In \textit{16th Conference on the Theory of Quantum Computation,
  Communication and Cryptography (TQC 2021)}. Schloss Dagstuhl-Leibniz-Zentrum
  f{\"u}r Informatik.

\bibitem[{Wan et~al.(2022)Wan, Zhang, Li, Zhang and Sun}]{wan2022quantum}
\text{Wan, Z.}, \text{Zhang, Z.}, \text{Li, T.}, \text{Zhang, J.} and
  \text{Sun, X.} (2022).
\newblock Quantum multi-armed bandits and stochastic linear bandits enjoy
  logarithmic regrets.
\newblock In \textit{To Appear in the Proceedings of the 37th AAAI Conference
  on Artificial Intelligence}.
\newblock \arxiv{2205.14988}.

\bibitem[{Wang et~al.(2021{\natexlab{a}})Wang, Sundaram, Kothari, Kapoor and
  Roetteler}]{wang2021RL}
\text{Wang, D.}, \text{Sundaram, A.}, \text{Kothari, R.}, \text{Kapoor, A.} and
  \text{Roetteler, M.} (2021{\natexlab{a}}).
\newblock Quantum algorithms for reinforcement learning with a generative
  model.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{2112.08451}.

\bibitem[{Wang et~al.(2021{\natexlab{b}})Wang, You, Li and
  Childs}]{wang2021quantum}
\text{Wang, D.}, \text{You, X.}, \text{Li, T.} and \text{Childs, A.~M.}
  (2021{\natexlab{b}}).
\newblock Quantum exploration algorithms for multi-armed bandits.
\newblock In \textit{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence}, vol.~35.
\newblock \arxiv{2007.07049}.

\bibitem[{Wang et~al.(2020)Wang, Salakhutdinov and
  Yang}]{wang2020reinforcement}
\text{Wang, R.}, \text{Salakhutdinov, R.~R.} and \text{Yang, L.} (2020).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 6123--6135.

\bibitem[{Wiedemann et~al.(2022)Wiedemann, Hein, Udluft and
  Mendl}]{wiedemann2022quantum}
\text{Wiedemann, S.}, \text{Hein, D.}, \text{Udluft, S.} and \text{Mendl, C.}
  (2022).
\newblock Quantum policy iteration via amplitude estimation and {G}rover
  search--towards quantum advantage for reinforcement learning.
\newblock \arxiv{2206.04741}.

\bibitem[{Wu et~al.(2022)Wu, Yang, Zhong, Wang, Du and Jiao}]{wu2022nearly}
\text{Wu, T.}, \text{Yang, Y.}, \text{Zhong, H.}, \text{Wang, L.}, \text{Du,
  S.} and \text{Jiao, J.} (2022).
\newblock Nearly optimal policy optimization with stable at any time guarantee.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{2112.10935}.

\bibitem[{Xu et~al.(2021)Xu, Ma and Du}]{xu2021fine}
\text{Xu, H.}, \text{Ma, T.} and \text{Du, S.} (2021).
\newblock Fine-grained gap-dependent bounds for tabular {MDP}s via adaptive
  multi-step bootstrap.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{2102.04692}.

\bibitem[{Yang et~al.(2021)Yang, Yang and Du}]{yang2021q}
\text{Yang, K.}, \text{Yang, L.} and \text{Du, S.} (2021).
\newblock Q-learning with logarithmic regret.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.
\newblock \arxiv{2006.09118}.

\bibitem[{Yang and Wang(2019)}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1902.04779}.

\bibitem[{Yang and Wang(2020)}]{yang2020reinforcement}
\text{Yang, L.} and \text{Wang, M.} (2020).
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1905.10389}.

\bibitem[{Zanette and Brunskill(2019)}]{zanette2019tighter}
\text{Zanette, A.} and \text{Brunskill, E.} (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \textit{International Conference on Machine Learning}. PMLR.
\newblock \arxiv{1901.00210}.

\bibitem[{Zhang et~al.(2021)Zhang, Ji and Du}]{zhang2021isreinforcement}
\text{Zhang, Z.}, \text{Ji, X.} and \text{Du, S.} (2021).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{2009.13503}.

\bibitem[{Zhang et~al.(2022)Zhang, Ji and Du}]{zhang2022horizon}
\text{Zhang, Z.}, \text{Ji, X.} and \text{Du, S.} (2022).
\newblock Horizon-free reinforcement learning in polynomial time: the power of
  stationary policies.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{2203.12922}.

\bibitem[{Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang and
  Zhang}]{zhong2022posterior}
\text{Zhong, H.}, \text{Xiong, W.}, \text{Zheng, S.}, \text{Wang, L.},
  \text{Wang, Z.}, \text{Yang, Z.} and \text{Zhang, T.} (2022).
\newblock Gec: A unified framework for interactive decision making in mdp,
  pomdp, and beyond.
\newblock \textit{arXiv preprint}.
\newblock \arxiv{2211.01962}.

\bibitem[{Zhou et~al.(2021)Zhou, Gu and Szepesvari}]{zhou2021nearly}
\text{Zhou, D.}, \text{Gu, Q.} and \text{Szepesvari, C.} (2021).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.
\newblock \arxiv{2212.06132}.

\end{thebibliography}
