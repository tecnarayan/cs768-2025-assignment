\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 1--8,
  2004.

\bibitem[Akrour et~al.(2012)Akrour, Schoenauer, and Sebag]{Akrour12}
Riad Akrour, Marc Schoenauer, and Michèle Sebag.
\newblock {April}: Active preference learning-based reinforcement learning.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 116--131, 2012.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Mané]{Amodei16}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Mané.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{Andrychowicz17}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI~Pieter Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5048--5058, 2017.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{Bellemare16}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1471--1479, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare13}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The {A}rcade {L}earning {E}nvironment: An evaluation platform for
  general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{Bellemare17}
Marc~G Bellemare, Will Dabney, and Rémi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  449--458, 2017.

\bibitem[Bradley and Terry(1952)]{Bradley52}
Ralph~A Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: {I. The} method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Chentanez et~al.(2005)Chentanez, Barto, and
  Singh]{chentanez2005intrinsically}
Nuttapong Chentanez, Andrew~G Barto, and Satinder~P Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1281--1288, 2005.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{Christiano17}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4302--4310, 2017.

\bibitem[Dabney et~al.(2017)Dabney, Rowland, Bellemare, and Munos]{Dabney17}
Will Dabney, Mark Rowland, Marc~G Bellemare, and Rémi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock \emph{arXiv preprint arXiv:1710.10044}, 2017.

\bibitem[Daniel et~al.(2015)Daniel, Kroemer, Viering, Metz, and
  Peters]{Daniel15}
Christian Daniel, Oliver Kroemer, Malte Viering, Jan Metz, and Jan Peters.
\newblock Active reward learning with a novel acquisition function.
\newblock \emph{Autonomous Robots}, 39\penalty0 (3):\penalty0 389--405, 2015.

\bibitem[El~Asri et~al.(2016)El~Asri, Piot, Geist, Laroche, and
  Pietquin]{ElAsri16}
Layla El~Asri, Bilal Piot, Matthieu Geist, Romain Laroche, and Olivier
  Pietquin.
\newblock Score-based inverse reinforcement learning.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, pages 457--465, 2016.

\bibitem[Elo(1978)]{Elo78}
Arpad Elo.
\newblock \emph{The Rating of Chessplayers, Past and Present}.
\newblock Arco Pub., 1978.

\bibitem[Everitt(2018)]{everitt2018}
Tom Everitt.
\newblock \emph{Towards Safe Artificial General Intelligence}.
\newblock PhD thesis, Australian National University, 2018.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{Eysenbach18}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and Szegedy]{Goodfellow14}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gregor et~al.(2016)Gregor, Rezende, and Wierstra]{Gregor16}
Karol Gregor, Danilo~Jimenez Rezende, and Daan Wierstra.
\newblock Variational intrinsic control.
\newblock \emph{arXiv preprint arXiv:1611.07507}, 2016.

\bibitem[Hasselt et~al.(2016)Hasselt, Guez, and Silver]{Hasselt16}
Hado~van Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{AAAI}, pages 2094--2100, 2016.

\bibitem[Hester et~al.(2018)Hester, Večerík, Pietquin, Lanctot, Schaul, Piot,
  Sendonaris, Dulac-Arnold, Osband, Agapiou, Leibo, and Gruslys]{Hester18}
Todd Hester, Matej Večerík, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou,
  Joel~Z Leibo, and Audrunas Gruslys.
\newblock Deep {Q}-learning from demonstrations.
\newblock In \emph{AAAI}, 2018.

\bibitem[Ho and Ermon(2016)]{Ho16}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4565--4573, 2016.

\bibitem[Ioffe and Szegedy(2015)]{Ioffe15}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  448--456, 2015.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{Jaderberg16}
Max Jaderberg, Volodymyr Mnih, Wojciech~Marian Czarnecki, Tom Schaul, Joel~Z
  Leibo, David Silver, and Koray Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kingma and Ba(2014)]{Kingma14}
Diederik~P Kingma and Jimmy Ba.
\newblock {Adam}: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Knox and Stone(2009)]{Knox09}
W~Bradley Knox and Peter Stone.
\newblock Interactively shaping agents via human reinforcement: The {TAMER}
  framework.
\newblock In \emph{International Conference on Knowledge Capture}, pages 9--16,
  2009.

\bibitem[Lehman et~al.(2018)Lehman, Clune, Misevic, Adami, Beaulieu, Bentley,
  Bernard, Belson, Bryson, Cheney, Cully, Doncieux, Dyer, Ellefsen, Feldt,
  Fischer, Forrest, Frénoy, Gagné, Le~Goff, Grabowski, Hodjat, Hutter,
  Keller, Knibbe, Krcah, Lenski, Lipson, MacCurdy, Maestre, Miikkulainen,
  Mitri, Moriarty, Mouret, Nguyen, Ofria, Parizeau, Parsons, Pennock, Punch,
  Ray, Schoenauer, Shulte, Sims, Stanley, Taddei, Tarapore, Thibault, Weimer,
  Watson, and Yosinski]{Lehman18}
Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Julie Beaulieu,
  Peter~J Bentley, Samuel Bernard, Guillaume Belson, David~M Bryson, Nick
  Cheney, Antoine Cully, Stephane Doncieux, Fred~C Dyer, Kai~Olav Ellefsen,
  Robert Feldt, Stephan Fischer, Stephanie Forrest, Antoine Frénoy, Christian
  Gagné, Leni Le~Goff, Laura~M Grabowski, Babak Hodjat, Frank Hutter, Laurent
  Keller, Carole Knibbe, Peter Krcah, Richard~E Lenski, Hod Lipson, Robert
  MacCurdy, Carlos Maestre, Risto Miikkulainen, Sara Mitri, David~E Moriarty,
  Jean-Baptiste Mouret, Anh Nguyen, Charles Ofria, Marc Parizeau, David
  Parsons, Robert~T Pennock, William~F Punch, Thomas~S Ray, Marc Schoenauer,
  Eric Shulte, Karl Sims, Kenneth~O Stanley, François Taddei, Danesh Tarapore,
  Simon Thibault, Westley Weimer, Richard Watson, and Jason Yosinski.
\newblock The surprising creativity of digital evolution: A collection of
  anecdotes from the evolutionary computation and artificial life research
  communities.
\newblock \emph{arXiv preprint arXiv:1803.03453}, 2018.

\bibitem[Lin et~al.(2017)Lin, Harrison, Keech, and Riedl]{Lin17}
Zhiyu Lin, Brent Harrison, Aaron Keech, and Mark~O Riedl.
\newblock Explore, exploit or listen: Combining human feedback and policy model
  to speed up deep reinforcement learning in {3D} worlds.
\newblock \emph{arXiv preprint arXiv:1709.03969}, 2017.

\bibitem[MacGlashan et~al.(2017)MacGlashan, Ho, Loftin, Peng, Roberts, Taylor,
  and Littman]{MacGlashan17}
James MacGlashan, Mark~K Ho, Robert Loftin, Bei Peng, David Roberts, Matthew~E
  Taylor, and Michael~L Littman.
\newblock Interactive learning from policy-dependent human feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  2285--2294, 2017.

\bibitem[Mathewson and Pilarski(2017)]{Mathewson17}
Kory Mathewson and Patrick Pilarski.
\newblock Actor-critic reinforcement learning with simultaneous human control
  and feedback.
\newblock \emph{arXiv preprint arXiv:1703.01274}, 2017.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Mnih13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing {Atari} with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih15}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{Mnih16}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1928--1937, 2016.

\bibitem[Mohamed and Rezende(2015)]{mohamed2015variational}
Shakir Mohamed and Danilo~Jimenez Rezende.
\newblock Variational information maximisation for intrinsically motivated
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2125--2133, 2015.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{Nair2017}
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter
  Abbeel.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{International Conference on Robotics and Automation}, pages
  6292--6299, 2018.

\bibitem[Ng and Russell(2000)]{Ng00}
Andrew~Y Ng and Stuart Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  663--670, 2000.

\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{orseau2013universal}
Laurent Orseau, Tor Lattimore, and Marcus Hutter.
\newblock Universal knowledge-seeking agents for stochastic environments.
\newblock In \emph{Algorithmic Learning Theory}, pages 158--172, 2013.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and Darrell]{Pathak17}
Deepak Pathak, Pulkit Agrawal, Alexei~A Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  2778--2787, 2017.

\bibitem[Pilarski et~al.(2011)Pilarski, Dawson, Degris, Fahimi, Carey, and
  Sutton]{Pilarski11}
Patrick~M Pilarski, Michael~R Dawson, Thomas Degris, Farbod Fahimi, Jason~P
  Carey, and Richard Sutton.
\newblock Online human training of a myoelectric prosthesis controller via
  actor-critic reinforcement learning.
\newblock In \emph{International Conference on Rehabilitation Robotics}, pages
  1--7, 2011.

\bibitem[Salge et~al.(2014)Salge, Glackin, and Polani]{salge2014empowerment}
Christoph Salge, Cornelius Glackin, and Daniel Polani.
\newblock Empowerment---an introduction.
\newblock In \emph{Guided Self-Organization: Inception}, pages 67--114.
  Springer, 2014.

\bibitem[Saunders et~al.(2018)Saunders, Sastry, Stuhlmueller, and
  Evans]{Saunders17}
William Saunders, Girish Sastry, Andreas Stuhlmueller, and Owain Evans.
\newblock Trial without error: Towards safe reinforcement learning via human
  intervention.
\newblock In \emph{International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 2067--2069, 2018.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and Silver]{Schaul15}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2015.

\bibitem[Schmidhuber(2006)]{schmidhuber2006developmental}
Jürgen Schmidhuber.
\newblock Developmental robotics, optimal artificial curiosity, creativity,
  music, and the fine arts.
\newblock \emph{Connection Science}, 18\penalty0 (2):\penalty0 173--187, 2006.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{Silver16}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava14}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Storck et~al.(1995)Storck, Hochreiter, and
  Schmidhuber]{storck1995reinforcement}
Jan Storck, Sepp Hochreiter, and Jürgen Schmidhuber.
\newblock Reinforcement driven information acquisition in non-deterministic
  environments.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 159--164, 1995.

\bibitem[Sutton and Barto(2018)]{Sutton98}
Richard Sutton and Andrew Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT press, 2nd edition, 2018.

\bibitem[Tarvainen and Valpola(2017)]{Tarvainen17}
Antti Tarvainen and Harri Valpola.
\newblock Weight-averaged consistency targets improve semi-supervised deep
  learning results.
\newblock \emph{arXiv preprint arXiv:1703.01780}, 2017.

\bibitem[Večerík et~al.(2017)Večerík, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Rothörl, Lampe, and Riedmiller]{Vevcerik17}
Matej Večerík, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin,
  Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin
  Riedmiller.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and
  Freitas]{Wang16}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando
  Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1995--2003, 2016.

\bibitem[Warnell et~al.(2017)Warnell, Waytowich, Lawhern, and Stone]{Warnell17}
Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone.
\newblock Deep {TAMER}: Interactive agent shaping in high-dimensional state
  spaces.
\newblock \emph{arXiv preprint arXiv:1709.10163}, 2017.

\bibitem[Wilson et~al.(2012)Wilson, Fern, and Tadepalli]{Wilson12}
Aaron Wilson, Alan Fern, and Prasad Tadepalli.
\newblock A {B}ayesian approach for policy learning from trajectory preference
  queries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1133--1141, 2012.

\bibitem[Wirth and Fürnkranz(2013)]{Wirth13}
Christian Wirth and Johannes Fürnkranz.
\newblock Preference-based reinforcement learning: A preliminary survey.
\newblock In \emph{ECML/PKDD Workshop on Reinforcement Learning from
  Generalized Feedback: Beyond Numeric Rewards}, 2013.

\bibitem[Wirth et~al.(2016)Wirth, Fürnkranz, and Neumann]{Wirth16}
Christian Wirth, J~Fürnkranz, and Gerhard Neumann.
\newblock Model-free preference-based reinforcement learning.
\newblock In \emph{AAAI}, pages 2222--2228, 2016.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{\"u}rnkranz]{Wirth2017}
Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F{\"u}rnkranz.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4945--4990, 2017.

\bibitem[Zhang and Ma(2018)]{Zhang18}
Xiaoqin Zhang and Huimin Ma.
\newblock Pretraining deep actor-critic reinforcement learning algorithms with
  expert demonstrations.
\newblock \emph{arXiv preprint arXiv:1801.10459}, 2018.

\bibitem[Zhu et~al.(2018)Zhu, Wang, Merel, Rusu, Erez, Cabi, Tunyasuvunakool,
  Kram{\'a}r, Hadsell, de~Freitas, and Heess]{Zhu18}
Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran
  Tunyasuvunakool, J{\'a}nos Kram{\'a}r, Raia Hadsell, Nando de~Freitas, and
  Nicolas Heess.
\newblock Reinforcement and imitation learning for diverse visuomotor skills.
\newblock \emph{arXiv preprint arXiv:1802.09564}, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and Dey]{Ziebart08}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI}, pages 1433--1438, 2008.

\end{thebibliography}
