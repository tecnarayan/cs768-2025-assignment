\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Beutel, Covington, Jain,
  Belletti, and Chi]{chen19}
Chen, M., Beutel, A., Covington, P., Jain, S., Belletti, F., and Chi, E.~H.
\newblock Top-k off-policy correction for a {REINFORCE} recommender system.
\newblock In \emph{Proceedings of the Twelfth {ACM} International Conference on
  Web Search and Data Mining, {WSDM} 2019, Melbourne, VIC, Australia, February
  11-15, 2019}, pp.\  456--464. {ACM}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Gummadi, Harris, and
  Schuurmans]{surrogate}
Chen, M., Gummadi, R., Harris, C., and Schuurmans, D.
\newblock Surrogate objectives for batch policy optimization in one-step
  decision making.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.
\newblock
  \href{https://papers.nips.cc/paper/9086-surrogate-objectives-for-batch-policy-optimization-in-one-step-decision-making}{URL}.

\bibitem[Dabney(2014)]{dabney14}
Dabney, W.
\newblock \emph{Adaptive step-sizes for reinforcement learning}.
\newblock PhD thesis, 2014.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{ArXiv:1802.01561}, 2018.
\newblock \href{https://arxiv.org/abs/1802.01561}{URL}.

\bibitem[Flet-Berliac et~al.(2021)Flet-Berliac, Ouhamma, Maillard, and
  Preux]{flet21}
Flet-Berliac, Y., Ouhamma, R., Maillard, O.-A., and Preux, P.
\newblock Learning value functions in deep policy gradients using residual
  variance.
\newblock In \emph{ICLR}, 2021.

\bibitem[Ghosh et~al.(2020)Ghosh, Machado, and Roux]{GMR20}
Ghosh, D., Machado, M.~C., and Roux, N.~L.
\newblock An operator view of policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Greensmith et~al.(2001)Greensmith, Bartlett, and Baxter]{baseline2}
Greensmith, E., Bartlett, P.~L., and Baxter, J.
\newblock Variance reduction techniques for gradient estimates in reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2001.
\newblock
  \href{http://papers.nips.cc/paper/2052-variance-reduction-techniques-for-gradient-estimates-in-reinforcement-learning.pdf}{URL}.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Ghahramani, Turner, and
  Levine]{actionbaseline2}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., and Levine, S.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock In \emph{ICLR}, 2017.
\newblock \href{https://arxiv.org/pdf/1611.02247.pdf}{URL}.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361. PMLR, 2017.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth{-}Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, Henderson, Novikov,
  Colmenarejo, Cabi, G{\"{u}}l{\c{c}}ehre, Paine, Cowie, Wang, Piot, and
  de~Freitas]{acme}
Hoffman, M., Shahriari, B., Aslanides, J., Barth{-}Maron, G., Behbahani, F.,
  Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., Henderson,
  S., Novikov, A., Colmenarejo, S.~G., Cabi, S., G{\"{u}}l{\c{c}}ehre,
  {\c{C}}., Paine, T.~L., Cowie, A., Wang, Z., Piot, B., and de~Freitas, N.
\newblock Acme: {A} research framework for distributed reinforcement learning.
\newblock \emph{ArXiv:2006.00979}, 2020.
\newblock \href{https://arxiv.org/abs/2006.00979}{URL}.

\bibitem[Huber(1964)]{huber64}
Huber, P.~J.
\newblock Robust estimation of a location parameter.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, 1964.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1008--1014. Citeseer, 2000.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{cql}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{ArXiv:2006.04779}, 2020.
\newblock \href{https://arxiv.org/pdf/2006.04779.pdf}{URL}.

\bibitem[Kumar et~al.(2019)Kumar, Dadashi, Ahmed, Schuurmans, and
  Bellemare]{gpi}
Kumar, S., Dadashi, R., Ahmed, Z., Schuurmans, D., and Bellemare, M.~G.
\newblock Generalized policy updates for policy optimization.
\newblock In \emph{NeurIPS 2019 Optimization Foundations for Reinforcement
  Learning Workshop}, 2019.
\newblock
  \href{https://optrl2019.github.io/assets/accepted_papers/68.pdf}{URL}.

\bibitem[Liang et~al.(2018)Liang, Norouzi, Berant, Le, and Lao]{mapo}
Liang, C., Norouzi, M., Berant, J., Le, Q.~V., and Lao, N.
\newblock Memory augmented policy optimization for program synthesis and
  semantic parsing.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.
\newblock \href{https://arxiv.org/pdf/1807.02322.pdf}{URL}.

\bibitem[Maei et~al.(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver, and
  Sutton]{maei2009convergent}
Maei, H.~R., Szepesvari, C., Bhatnagar, S., Precup, D., Silver, D., and Sutton,
  R.~S.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{NIPS}, pp.\  1204--1212, 2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and Schuurmans]{pcl}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.
\newblock \href{https://arxiv.org/pdf/1702.08892.pdf}{URL}.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{pgqcombine}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock Combining policy gradient and q-learning.
\newblock In \emph{ICLR}, 2017.
\newblock \href{https://openreview.net/forum?id=B1kJ6H9ex}{URL}.

\bibitem[Oh et~al.(2018)Oh, Guo, Singh, and Lee]{sil}
Oh, J., Guo, Y., Singh, S., and Lee, H.
\newblock Self-imitation learning.
\newblock In \emph{ICML}, 2018.
\newblock \href{https://arxiv.org/pdf/1806.05635.pdf}{URL}.

\bibitem[Parker-Holder et~al.(2022)Parker-Holder, Rajan, Song, Biedenkapp,
  Miao, Eimer, Zhang, Nguyen, Calandra, Faust, Hutter, and
  Lindauer]{parkerholder2022automated}
Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A., Miao, Y., Eimer, T.,
  Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F., and Lindauer, M.
\newblock Automated reinforcement learning (autorl): A survey and open
  problems, 2022.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008natural}
Peters, J. and Schaal, S.
\newblock Natural actor-critic.
\newblock \emph{Neurocomputing}, 71\penalty0 (7-9):\penalty0 1180--1190, 2008.

\bibitem[Precup(2000)]{precup2000temporal}
Precup, D.
\newblock \emph{Temporal abstraction in reinforcement learning}.
\newblock University of Massachusetts Amherst, 2000.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2016high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{ICLR}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv:1707.06347}, 2017.
\newblock \href{https://arxiv.org/pdf/1707.06347.pdf}{URL}.

\bibitem[Schulman et~al.(2018)Schulman, Chen, and Abbeel]{pgequiv}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{ArXiv:1704.06440}, 2018.
\newblock \href{https://arxiv.org/pdf/1704.06440.pdf}{URL}.

\bibitem[Serrano et~al.(2021)Serrano, Curi, Krause, and
  Neu]{serrano2021logistic}
Serrano, J.~B., Curi, S., Krause, A., and Neu, G.
\newblock Logistic q-learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3610--3618. PMLR, 2021.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{pgoriginal}
Sutton, R., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1999.
\newblock
  \href{https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}{URL}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tucker et~al.(2018)Tucker, Bhupatiraju, Gu, Turner, Ghahramani, and
  Levine]{actionbaseline3}
Tucker, G., Bhupatiraju, S., Gu, S., Turner, R.~E., Ghahramani, Z., and Levine,
  S.
\newblock The mirage of action-dependent baselines in reinforcement learning.
\newblock In \emph{ICML}, 2018.
\newblock \href{https://arxiv.org/abs/1802.10031}{URL}.

\bibitem[Vieillard et~al.(2020{\natexlab{a}})Vieillard, Kozuno, Scherrer,
  Pietquin, Munos, and Geist]{vieillard2020leverage}
Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M.
\newblock Leverage the average: an analysis of kl regularization in
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Vieillard et~al.(2020{\natexlab{b}})Vieillard, Scherrer, Pietquin, and
  Geist]{vieillard2020momentum}
Vieillard, N., Scherrer, B., Pietquin, O., and Geist, M.
\newblock Momentum in reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2529--2538. PMLR, 2020{\natexlab{b}}.

\bibitem[Weaver \& Tao(2001)Weaver and Tao]{baseline1}
Weaver, L. and Tao, N.
\newblock The optimal reward baseline for gradient-based reinforcement
  learning.
\newblock In \emph{UAI}, 2001.
\newblock \href{https://arxiv.org/pdf/1301.2315.pdf}{URL}.

\bibitem[Wen et~al.(2021)Wen, Kumar, Gummadi, and Schuurmans]{ours21}
Wen, J., Kumar, S., Gummadi, R., and Schuurmans, D.
\newblock Characterizing the gap between actor-critic and policy gradient.
\newblock In \emph{ICML}, 2021.

\bibitem[Wu et~al.(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade, Mordatch,
  and Abbeel]{actionbaseline1}
Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.~M.,
  Mordatch, I., and Abbeel, P.
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock In \emph{ICLR}, 2018.
\newblock \href{https://arxiv.org/pdf/1803.07246.pdf}{URL}.

\end{thebibliography}
