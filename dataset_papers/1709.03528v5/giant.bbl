\begin{thebibliography}{10}

\bibitem{achlioptas2003database}
Dimitris Achlioptas.
\newblock Database-friendly random projections: {J}ohnson-{L}indenstrauss with
  binary coins.
\newblock {\em Journal of computer and System Sciences}, 66(4):671--687, 2003.

\bibitem{bonawitz2017practical}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy preserving machine learning.
\newblock {\em IACR Cryptology ePrint Archive}, 2017:281, 2017.

\bibitem{clarkson2013low}
Kenneth~L. Clarkson and David~P. Woodruff.
\newblock Low rank approximation and regression in input sparsity time.
\newblock In {\em Annual ACM Symposium on theory of computing (STOC)}, 2013.

\bibitem{dean2008mapreduce}
Jeffrey Dean and Sanjay Ghemawat.
\newblock {MapReduce}: simplified data processing on large clusters.
\newblock {\em Communications of the ACM}, 51(1):107--113, 2008.

\bibitem{drineas2012fast}
Petros Drineas, Malik Magdon-Ismail, Michael~W. Mahoney, and David~P. Woodruff.
\newblock Fast approximation of matrix coherence and statistical leverage.
\newblock {\em Journal of Machine Learning Research}, 13:3441--3472, 2012.

\bibitem{drineas2016randnla}
Petros Drineas and Michael~W Mahoney.
\newblock {RandNLA}: randomized numerical linear algebra.
\newblock {\em Communications of the ACM}, 59(6):80--90, 2016.

\bibitem{drineas2006sampling}
Petros Drineas, Michael~W. Mahoney, and S.~Muthukrishnan.
\newblock Sampling algorithms for $\ell_2$ regression and applications.
\newblock In {\em Annual ACM-SIAM Symposium on Discrete Algorithm (SODA)},
  2006.

\bibitem{drineas2011faster}
Petros Drineas, Michael~W. Mahoney, S.~Muthukrishnan, and Tam{\'a}s Sarl{\'o}s.
\newblock Faster least squares approximation.
\newblock {\em Numerische Mathematik}, 117(2):219--249, 2011.

\bibitem{fercoq2016optimization}
Olivier Fercoq and Peter Richt{\'a}rik.
\newblock Optimization in high dimensions via accelerated, parallel, and
  proximal coordinate descent.
\newblock {\em SIAM Review}, 58(4):739--771, 2016.

\bibitem{golub2012matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix computations}, volume~3.
\newblock JHU Press, 2012.

\bibitem{jaggi2014communication}
Martin Jaggi, Virginia Smith, Martin Tak{\'a}c, Jonathan Terhorst, Sanjay
  Krishnan, Thomas Hofmann, and Michael~I Jordan.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem{johnson1984extensions}
William~B. Johnson and Joram Lindenstrauss.
\newblock Extensions of {L}ipschitz mappings into a {H}ilbert space.
\newblock {\em Contemporary mathematics}, 26(189-206), 1984.

\bibitem{konevcny2016federated}
Jakub Kone{{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{konevcny2016federated2}
Jakub Kone{{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{lee2015distributed}
Jason~D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang.
\newblock Distributed stochastic variance reduced gradient methods and a lower
  bound for communication complexity.
\newblock {\em arXiv preprint arXiv:1507.07595}, 2015.

\bibitem{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em USENIX Symposium on Operating Systems Design and
  Implementation (OSDI)}, 2014.

\bibitem{liu1989limited}
Dong~C. Liu and Jorge Nocedal.
\newblock On the limited memory {BFGS} method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1-3):503--528, 1989.

\bibitem{liu2015asynchronous}
Ji~Liu, Stephen~J Wright, Christopher R{\'e}, Victor Bittorf, and Srikrishna
  Sridhar.
\newblock An asynchronous parallel stochastic coordinate descent algorithm.
\newblock {\em Journal of Machine Learning Research}, 16(285-322):1--5, 2015.

\bibitem{low2012graphlab}
Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, and
  Joseph~M. Hellerstein.
\newblock Distributed {GraphLab}: A framework for machine learning and data
  mining in the cloud.
\newblock {\em Proceedings of the VLDB Endowment}, 2012.

\bibitem{lu2013faster}
Yichao Lu, Paramveer Dhillon, Dean~P Foster, and Lyle Ungar.
\newblock Faster ridge regression via the subsampled randomized {H}adamard
  transform.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem{ma2015adding}
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, and
  Martin Takac.
\newblock Adding vs. averaging in distributed primal-dual optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2015.

\bibitem{mahajan2013efficient}
Dhruv Mahajan, Nikunj Agrawal, S~Sathiya Keerthi, S~Sundararajan, and L{\'e}on
  Bottou.
\newblock An efficient distributed learning algorithm based on effective local
  functional approximations.
\newblock {\em arXiv preprint arXiv:1310.8418}, 2013.

\bibitem{mahajan2013parallel}
Dhruv Mahajan, S~Sathiya Keerthi, S~Sundararajan, and L{\'e}on Bottou.
\newblock A parallel {SGD} method with strong convergence.
\newblock {\em arXiv preprint arXiv:1311.0636}, 2013.

\bibitem{mahoney2011ramdomized}
Michael~W. Mahoney.
\newblock Randomized algorithms for matrices and data.
\newblock {\em Foundations and Trends in Machine Learning}, 3(2):123--224,
  2011.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2017.

\bibitem{meng2016mllib}
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman,
  Davies Liu, Jeremy Freeman, DB~Tsai, Manish Amde, Sean Owen, et~al.
\newblock {MLlib}: machine learning in {A}pache {S}park.
\newblock {\em Journal of Machine Learning Research}, 17(34):1--7, 2016.

\bibitem{meng2013low}
Xiangrui Meng and Michael~W Mahoney.
\newblock Low-distortion subspace embeddings in input-sparsity time and
  applications to robust linear regression.
\newblock In {\em Annual {ACM} Symposium on Theory of Computing (STOC)}, 2013.

\bibitem{necoara2016parallel}
Ion Necoara and Dragos Clipici.
\newblock Parallel random coordinate descent method for composite minimization:
  Convergence analysis and error bounds.
\newblock {\em SIAM Journal on Optimization}, 26(1):197--226, 2016.

\bibitem{nelson2013osnap}
John Nelson and Huy~L Nguy{\^e}n.
\newblock {OSNAP}: Faster numerical linear algebra algorithms via sparser
  subspace embeddings.
\newblock In {\em IEEE Annual Symposium on Foundations of Computer Science
  (FOCS)}, 2013.

\bibitem{nemirovskii1983problem}
A.S. Nemirovskii and D.B. Yudin.
\newblock {\em Problem Complexity and Method Efficiency in Optimization}.
\newblock A Wiley-Interscience publication. Wiley, 1983.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate $\mathcal{O} (1/k^{2})$.
\newblock In {\em Soviet Mathematics Doklady}, volume~27, pages 372--376, 1983.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{nocedal2006numerical}
Jorge Nocedal and Stephen Wright.
\newblock {\em Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{pilanci2017newton}
Mert Pilanci and Martin~J Wainwright.
\newblock Newton sketch: A near linear-time optimization algorithm with
  linear-quadratic convergence.
\newblock {\em SIAM Journal on Optimization}, 27(1):205--245, 2017.

\bibitem{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2007.

\bibitem{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock {Hogwild}: a lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2011.

\bibitem{reddi2015variance}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander~J
  Smola.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)}.
  2015.

\bibitem{reddi2016aide}
Sashank~J Reddi, Jakub Kone{{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock {AIDE}: {f}ast and communication efficient distributed optimization.
\newblock {\em arXiv preprint arXiv:1608.06879}, 2016.

\bibitem{richtarik2016distributed}
Peter Richt{\'a}rik and Martin Tak{\'a}{{c}}.
\newblock Distributed coordinate descent method for learning with big data.
\newblock {\em Journal of Machine Learning Research}, 17(1):2657--2681, 2016.

\bibitem{richtarik2016parallel}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock {\em Mathematical Programming}, 156(1-2):433--484, 2016.

\bibitem{roosta2016global}
Farbod Roosta-Khorasani and Michael~W Mahoney.
\newblock {Sub-sampled Newton methods I: globally convergent algorithms}.
\newblock {\em arXiv preprint arXiv:1601.04737}, 2016.

\bibitem{roosta2016sub}
Farbod Roosta-Khorasani and Michael~W Mahoney.
\newblock {Sub-sampled Newton methods II: Local convergence rates}.
\newblock {\em arXiv preprint arXiv:1601.04738}, 2016.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: from theory to algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{shamir2014distributed}
Ohad Shamir and Nathan Srebro.
\newblock Distributed stochastic optimization and learning.
\newblock In {\em Annual Allerton Conference on Communication, Control, and
  Computing}, 2014.

\bibitem{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In {\em International conference on machine learning (ICML)}, 2014.

\bibitem{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar.
\newblock Federated multi-task learning.
\newblock {\em arXiv preprint arXiv:1705.10467}, 2017.

\bibitem{smith2016cocoa}
Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael~I Jordan, and
  Martin Jaggi.
\newblock {CoCoA}: A general framework for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1611.02189}, 2016.

\bibitem{tropp2011improved}
Joel~A Tropp.
\newblock Improved analysis of the subsampled randomized hadamard transform.
\newblock {\em Advances in Adaptive Data Analysis}, 3(01n02):115--126, 2011.

\bibitem{wang2017sketched}
Shusen Wang, Alex Gittens, and Michael~W. Mahoney.
\newblock Sketched ridge regression: Optimization perspective, statistical
  perspective, and model averaging.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{wang2016spsd}
Shusen Wang, Luo Luo, and Zhihua Zhang.
\newblock {SPSD} matrix approximation vis column selection: Theories,
  algorithms, and extensions.
\newblock {\em Journal of Machine Learning Research}, 17(49):1--49, 2016.

\bibitem{woodruff2014sketching}
David~P Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10(1--2):1--157, 2014.

\bibitem{wright1999numerical}
Stephen Wright and Jorge Nocedal.
\newblock Numerical optimization.
\newblock {\em Springer Science}, 35:67--68, 1999.

\bibitem{xu2016sub}
Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher R{\'e}, and Michael~W
  Mahoney.
\newblock Sub-sampled {N}ewton methods with non-uniform sampling.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem{yang2013trading}
Tianbao Yang.
\newblock Trading computation for communication: distributed stochastic dual
  coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem{zaharia2010spark}
Matei Zaharia, Mosharaf Chowdhury, Michael~J Franklin, Scott Shenker, and Ion
  Stoica.
\newblock Spark: Cluster computing with working sets.
\newblock {\em HotCloud}, 10(10-10):95, 2010.

\bibitem{zhang2013communication}
Yuchen Zhang, John~C. Duchi, and Martin~J. Wainwright.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock {\em Journal of Machine Learning Research}, 14:3321--3363, 2013.

\bibitem{zhang2015disco}
Yuchen Zhang and Xiao Lin.
\newblock {DiSCO}: {d}istributed optimization for self-concordant empirical
  loss.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2015.

\bibitem{zheng2016general}
Shun Zheng, Fen Xia, Wei Xu, and Tong Zhang.
\newblock A general distributed dual coordinate optimization framework for
  regularized loss minimization.
\newblock {\em arXiv preprint arXiv:1604.03763}, 2016.

\bibitem{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2010.

\end{thebibliography}
