\begin{thebibliography}{52}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Ali et~al.(2020)Ali, Dobriban and Tibshirani}]{ali2020implicit}
\textsc{Ali, A.}, \textsc{Dobriban, E.} and \textsc{Tibshirani, R.} (2020).
\newblock The implicit regularization of stochastic gradient flow for least squares.
\newblock In \textit{8th International Conference on Learning Representations, {ICLR} 2020}. PMLR.

\bibitem[{Arora et~al.(2019)Arora, Cohen, Hu and Luo}]{arora2019implicit}
\textsc{Arora, S.}, \textsc{Cohen, N.}, \textsc{Hu, W.} and \textsc{Luo, Y.} (2019).
\newblock Implicit regularization in deep matrix factorization.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{32}.

\bibitem[{Balles and Hennig(2018)}]{DBLP:conf/icml/BallesH18}
\textsc{Balles, L.} and \textsc{Hennig, P.} (2018).
\newblock Dissecting adam: The sign, magnitude and variance of stochastic gradients.
\newblock In \textit{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018} (J.~G. Dy and A.~Krause, eds.), vol.~80 of \textit{Proceedings of Machine Learning Research}. {PMLR}.

\bibitem[{Balles et~al.(2020)Balles, Pedregosa and Roux}]{DBLP:journals/corr/abs-2002-08056}
\textsc{Balles, L.}, \textsc{Pedregosa, F.} and \textsc{Roux, N.~L.} (2020).
\newblock The geometry of sign gradient descent.
\newblock \textit{CoRR} \textbf{abs/2002.08056}.

\bibitem[{Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli and Anandkumar}]{DBLP:conf/icml/BernsteinWAA18}
\textsc{Bernstein, J.}, \textsc{Wang, Y.}, \textsc{Azizzadenesheli, K.} and \textsc{Anandkumar, A.} (2018).
\newblock {SIGNSGD:} compressed optimisation for non-convex problems.
\newblock In \textit{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018}, vol.~80 of \textit{Proceedings of Machine Learning Research}. {PMLR}.

\bibitem[{Bernstein et~al.(2019)Bernstein, Zhao, Azizzadenesheli and Anandkumar}]{DBLP:conf/iclr/BernsteinZAA19}
\textsc{Bernstein, J.}, \textsc{Zhao, J.}, \textsc{Azizzadenesheli, K.} and \textsc{Anandkumar, A.} (2019).
\newblock signsgd with majority vote is communication efficient and fault tolerant.
\newblock In \textit{7th International Conference on Learning Representations, {ICLR} 2019}. OpenReview.net.

\bibitem[{Cao et~al.(2023)Cao, Zou, Li and Gu}]{cao2023implicit}
\textsc{Cao, Y.}, \textsc{Zou, D.}, \textsc{Li, Y.} and \textsc{Gu, Q.} (2023).
\newblock The implicit bias of batch normalization in linear models and two-layer linear convolutional neural networks.
\newblock In \textit{The Thirty Sixth Annual Conference on Learning Theory}. PMLR.

\bibitem[{Chen et~al.(2023)Chen, Liu, Liang et~al.}]{chen2023lion}
\textsc{Chen, L.}, \textsc{Liu, B.}, \textsc{Liang, K.} \textsc{et~al.} (2023).
\newblock Lion secretly solves a constrained optimization: As lyapunov predicts.
\newblock In \textit{The Twelfth International Conference on Learning Representations}.

\bibitem[{Chen et~al.(2024)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong, Hsieh, Lu et~al.}]{chen2024symbolic}
\textsc{Chen, X.}, \textsc{Liang, C.}, \textsc{Huang, D.}, \textsc{Real, E.}, \textsc{Wang, K.}, \textsc{Pham, H.}, \textsc{Dong, X.}, \textsc{Luong, T.}, \textsc{Hsieh, C.-J.}, \textsc{Lu, Y.} \textsc{et~al.} (2024).
\newblock Symbolic discovery of optimization algorithms.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{36}.

\bibitem[{Chen et~al.(2019)Chen, Liu, Sun and Hong}]{chen2019convergence}
\textsc{Chen, X.}, \textsc{Liu, S.}, \textsc{Sun, R.} and \textsc{Hong, M.} (2019).
\newblock On the convergence of a class of adam-type algorithms for non-convex optimization.
\newblock In \textit{7th International Conference on Learning Representations, ICLR 2019}.

\bibitem[{Chizat and Bach(2020)}]{chizat2020implicit}
\textsc{Chizat, L.} and \textsc{Bach, F.} (2020).
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \textit{Conference on learning theory}. PMLR.

\bibitem[{De et~al.(2018)De, Mukherjee and Ullah}]{de2018convergence}
\textsc{De, S.}, \textsc{Mukherjee, A.} and \textsc{Ullah, E.} (2018).
\newblock Convergence guarantees for rmsprop and adam in non-convex optimization and an empirical comparison to nesterov acceleration.
\newblock \textit{arXiv preprint arXiv:1807.06766} .

\bibitem[{Frei et~al.(2022)Frei, Vardi, Bartlett, Srebro and Hu}]{frei2022implicit}
\textsc{Frei, S.}, \textsc{Vardi, G.}, \textsc{Bartlett, P.}, \textsc{Srebro, N.} and \textsc{Hu, W.} (2022).
\newblock Implicit bias in leaky relu networks trained on high-dimensional data.
\newblock In \textit{The Eleventh International Conference on Learning Representations}.

\bibitem[{Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry and Srebro}]{pmlr-v80-gunasekar18a}
\textsc{Gunasekar, S.}, \textsc{Lee, J.}, \textsc{Soudry, D.} and \textsc{Srebro, N.} (2018{\natexlab{a}}).
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \textit{Proceedings of the 35th International Conference on Machine Learning}, vol.~80 of \textit{Proceedings of Machine Learning Research}. PMLR.

\bibitem[{Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry and Srebro}]{gunasekar2018implicit}
\textsc{Gunasekar, S.}, \textsc{Lee, J.~D.}, \textsc{Soudry, D.} and \textsc{Srebro, N.} (2018{\natexlab{b}}).
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \textit{Advances in neural information processing systems} \textbf{31}.

\bibitem[{Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur and Srebro}]{gunasekar2017implicit}
\textsc{Gunasekar, S.}, \textsc{Woodworth, B.~E.}, \textsc{Bhojanapalli, S.}, \textsc{Neyshabur, B.} and \textsc{Srebro, N.} (2017).
\newblock Implicit regularization in matrix factorization.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Guo et~al.(2021)Guo, Xu, Yin, Jin and Yang}]{guo2021novel}
\textsc{Guo, Z.}, \textsc{Xu, Y.}, \textsc{Yin, W.}, \textsc{Jin, R.} and \textsc{Yang, T.} (2021).
\newblock A novel convergence analysis for algorithms of the adam family.
\newblock \textit{arXiv preprint arXiv:2112.03459} .

\bibitem[{Huang et~al.(2021)Huang, Li and Huang}]{huang2021super}
\textsc{Huang, F.}, \textsc{Li, J.} and \textsc{Huang, H.} (2021).
\newblock Super-adam: faster and universal framework of adaptive gradients.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{34} 9074--9085.

\bibitem[{Ji and Telgarsky(2019{\natexlab{a}})}]{ji2019gradient}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2019{\natexlab{a}}).
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \textit{7th International Conference on Learning Representations, ICLR 2019}.

\bibitem[{Ji and Telgarsky(2019{\natexlab{b}})}]{pmlr-v99-ji19a}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2019{\natexlab{b}}).
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \textit{Proceedings of the Thirty-Second Conference on Learning Theory}, vol.~99 of \textit{Proceedings of Machine Learning Research}. PMLR.

\bibitem[{Ji and Telgarsky(2020)}]{ji2020directional}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2020).
\newblock Directional convergence and alignment in deep learning.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{33} 17176--17186.

\bibitem[{Ji and Telgarsky(2021)}]{DBLP:conf/alt/JiT21}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2021).
\newblock Characterizing the implicit bias via a primal-dual analysis.
\newblock In \textit{Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide}, vol. 132 of \textit{Proceedings of Machine Learning Research}. {PMLR}.

\bibitem[{Jin and Mont{\'u}far(2023)}]{jin2023implicit}
\textsc{Jin, H.} and \textsc{Mont{\'u}far, G.} (2023).
\newblock Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks.
\newblock \textit{Journal of Machine Learning Research} \textbf{24} 1--97.

\bibitem[{Kingma and Ba(2015)}]{DBLP:journals/corr/KingmaB14}
\textsc{Kingma, D.~P.} and \textsc{Ba, J.} (2015).
\newblock Adam: {A} method for stochastic optimization.
\newblock In \textit{3rd International Conference on Learning Representations, {ICLR} 2015}.

\bibitem[{Kou et~al.(2024)Kou, Chen and Gu}]{kou2024implicit}
\textsc{Kou, Y.}, \textsc{Chen, Z.} and \textsc{Gu, Q.} (2024).
\newblock Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{36}.

\bibitem[{Kunstner et~al.(2023)Kunstner, Chen, Lavington and Schmidt}]{DBLP:conf/iclr/KunstnerCL023}
\textsc{Kunstner, F.}, \textsc{Chen, J.}, \textsc{Lavington, J.~W.} and \textsc{Schmidt, M.} (2023).
\newblock Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be.
\newblock In \textit{The Eleventh International Conference on Learning Representations, {ICLR} 2023}. OpenReview.net.

\bibitem[{Li et~al.(2018)Li, Ma and Zhang}]{li2018algorithmic}
\textsc{Li, Y.}, \textsc{Ma, T.} and \textsc{Zhang, H.} (2018).
\newblock Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations.
\newblock In \textit{Conference On Learning Theory}.

\bibitem[{Liu et~al.(2020)Liu, Zhang, Orabona and Yang}]{liu2020adam}
\textsc{Liu, M.}, \textsc{Zhang, W.}, \textsc{Orabona, F.} and \textsc{Yang, T.} (2020).
\newblock Adam+: A stochastic method with adaptive variance reduction.
\newblock \textit{arXiv preprint arXiv:2011.11985} .

\bibitem[{Lyu and Li(2019)}]{lyu2019gradient}
\textsc{Lyu, K.} and \textsc{Li, J.} (2019).
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \textit{7th International Conference on Learning Representations, {ICLR} 2019}.

\bibitem[{Nacson et~al.(2019)Nacson, Srebro and Soudry}]{DBLP:conf/aistats/NacsonSS19}
\textsc{Nacson, M.~S.}, \textsc{Srebro, N.} and \textsc{Soudry, D.} (2019).
\newblock Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate.
\newblock In \textit{The 22nd International Conference on Artificial Intelligence and Statistics, {AISTATS} 2019}, vol.~89 of \textit{Proceedings of Machine Learning Research}. {PMLR}.

\bibitem[{Neyshabur et~al.(2014)Neyshabur, Tomioka and Srebro}]{neyshabur2014search}
\textsc{Neyshabur, B.}, \textsc{Tomioka, R.} and \textsc{Srebro, N.} (2014).
\newblock In search of the real inductive bias: On the role of implicit regularization in deep learning.
\newblock \textit{arXiv preprint arXiv:1412.6614} .

\bibitem[{Pan and Li(2022)}]{pan2022toward}
\textsc{Pan, Y.} and \textsc{Li, Y.} (2022).
\newblock Toward understanding why adam converges faster than sgd for transformers.
\newblock In \textit{OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}.

\bibitem[{Phuong and Lampert(2020)}]{phuong2020inductive}
\textsc{Phuong, M.} and \textsc{Lampert, C.~H.} (2020).
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In \textit{8th International Conference on Learning Representations, {ICLR} 2020}.

\bibitem[{Qian and Qian(2019)}]{DBLP:conf/nips/QianQ19}
\textsc{Qian, Q.} and \textsc{Qian, X.} (2019).
\newblock The implicit bias of adagrad on separable data.
\newblock In \textit{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019}.

\bibitem[{Razin and Cohen(2020)}]{razin2020implicit}
\textsc{Razin, N.} and \textsc{Cohen, N.} (2020).
\newblock Implicit regularization in deep learning may not be explainable by norms.
\newblock \textit{Advances in neural information processing systems} \textbf{33} 21174--21187.

\bibitem[{Reddi et~al.(2018)Reddi, Kale and Kumar}]{reddi2018convergence}
\textsc{Reddi, S.~J.}, \textsc{Kale, S.} and \textsc{Kumar, S.} (2018).
\newblock On the convergence of adam and beyond.
\newblock In \textit{6th International Conference on Learning Representations, {ICLR} 2018}.

\bibitem[{Rosasco and Villa(2015)}]{rosasco2015learning}
\textsc{Rosasco, L.} and \textsc{Villa, S.} (2015).
\newblock Learning with incremental iterative regularization.
\newblock \textit{Advances in neural information processing systems} \textbf{28}.

\bibitem[{Shalev{-}Shwartz and Singer(2010)}]{DBLP:journals/ml/Shalev-ShwartzS10}
\textsc{Shalev{-}Shwartz, S.} and \textsc{Singer, Y.} (2010).
\newblock On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms.
\newblock \textit{Mach. Learn.} \textbf{80} 141--163.

\bibitem[{Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar and Srebro}]{DBLP:journals/jmlr/SoudryHNGS18}
\textsc{Soudry, D.}, \textsc{Hoffer, E.}, \textsc{Nacson, M.~S.}, \textsc{Gunasekar, S.} and \textsc{Srebro, N.} (2018).
\newblock The implicit bias of gradient descent on separable data.
\newblock \textit{J. Mach. Learn. Res.} \textbf{19} 70:1--70:57.

\bibitem[{Telgarsky(2013)}]{DBLP:conf/icml/Telgarsky13}
\textsc{Telgarsky, M.} (2013).
\newblock Margins, shrinkage, and boosting.
\newblock In \textit{Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013}, vol.~28 of \textit{{JMLR} Workshop and Conference Proceedings}. JMLR.org.

\bibitem[{Vardi et~al.(2022{\natexlab{a}})Vardi, Shamir and Srebro}]{vardi2022margin}
\textsc{Vardi, G.}, \textsc{Shamir, O.} and \textsc{Srebro, N.} (2022{\natexlab{a}}).
\newblock On margin maximization in linear and relu networks.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{35} 37024--37036.

\bibitem[{Vardi et~al.(2022{\natexlab{b}})Vardi, Yehudai and Shamir}]{vardi2022gradient}
\textsc{Vardi, G.}, \textsc{Yehudai, G.} and \textsc{Shamir, O.} (2022{\natexlab{b}}).
\newblock Gradient methods provably converge to non-robust networks.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{35} 20921--20932.

\bibitem[{Wang et~al.(2021)Wang, Meng, Chen and Liu}]{wang2021implicit}
\textsc{Wang, B.}, \textsc{Meng, Q.}, \textsc{Chen, W.} and \textsc{Liu, T.-Y.} (2021).
\newblock The implicit bias for adaptive optimization algorithms on homogeneous neural networks.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Wang et~al.(2022)Wang, Meng, Zhang, Sun, Chen, Ma and Liu}]{NEURIPS2022_ab3f6bbe}
\textsc{Wang, B.}, \textsc{Meng, Q.}, \textsc{Zhang, H.}, \textsc{Sun, R.}, \textsc{Chen, W.}, \textsc{Ma, Z.-M.} and \textsc{Liu, T.-Y.} (2022).
\newblock Does momentum change the implicit regularization on separable data?
\newblock In \textit{Advances in Neural Information Processing Systems}, vol.~35. Curran Associates, Inc.

\bibitem[{Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro and Recht}]{wilson2017marginal}
\textsc{Wilson, A.~C.}, \textsc{Roelofs, R.}, \textsc{Stern, M.}, \textsc{Srebro, N.} and \textsc{Recht, B.} (2017).
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \textit{Advances in neural information processing systems} \textbf{30}.

\bibitem[{Wu et~al.(2023)Wu, Braverman and Lee}]{DBLP:conf/nips/WuBL23}
\textsc{Wu, J.}, \textsc{Braverman, V.} and \textsc{Lee, J.~D.} (2023).
\newblock Implicit bias of gradient descent for logistic regression at the edge of stability.
\newblock In \textit{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023}.

\bibitem[{Wu et~al.(2020)Wu, Dobriban, Ren, Wu, Li, Gunasekar, Ward and Liu}]{wu2020implicit}
\textsc{Wu, X.}, \textsc{Dobriban, E.}, \textsc{Ren, T.}, \textsc{Wu, S.}, \textsc{Li, Z.}, \textsc{Gunasekar, S.}, \textsc{Ward, R.} and \textsc{Liu, Q.} (2020).
\newblock Implicit regularization and convergence for weight normalization.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{33} 2835--2847.

\bibitem[{Xie and Li(2024)}]{xie2024implicit}
\textsc{Xie, S.} and \textsc{Li, Z.} (2024).
\newblock Implicit bias of adamw: $\ell_\infty$ norm constrained optimization.

\bibitem[{Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar and Sra}]{zhang2020adaptive}
\textsc{Zhang, J.}, \textsc{Karimireddy, S.~P.}, \textsc{Veit, A.}, \textsc{Kim, S.}, \textsc{Reddi, S.}, \textsc{Kumar, S.} and \textsc{Sra, S.} (2020).
\newblock Why are adaptive methods good for attention models?
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{33} 15383--15393.

\bibitem[{Zhou et~al.(2024)Zhou, Chen, Cao, Tang, Yang and Gu}]{zhou2018convergence}
\textsc{Zhou, D.}, \textsc{Chen, J.}, \textsc{Cao, Y.}, \textsc{Tang, Y.}, \textsc{Yang, Z.} and \textsc{Gu, Q.} (2024).
\newblock On the convergence of adaptive gradient methods for nonconvex optimization.
\newblock \textit{Transactions on Machine Learning Research} .

\bibitem[{Zhou et~al.(2020)Zhou, Feng, Ma, Xiong, Hoi et~al.}]{zhou2020towards}
\textsc{Zhou, P.}, \textsc{Feng, J.}, \textsc{Ma, C.}, \textsc{Xiong, C.}, \textsc{Hoi, S. C.~H.} \textsc{et~al.} (2020).
\newblock Towards theoretically understanding why sgd generalizes better than adam in deep learning.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{33} 21285--21296.

\bibitem[{Zou et~al.(2023)Zou, Cao, Li and Gu}]{DBLP:conf/iclr/Zou0LG23}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Li, Y.} and \textsc{Gu, Q.} (2023).
\newblock Understanding the generalization of adam in learning neural networks with proper regularization.
\newblock In \textit{The Eleventh International Conference on Learning Representations, {ICLR} 2023}. OpenReview.net.

\end{thebibliography}
