\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2018{\natexlab{a}})]{howtomakethegradientssmall}
Allen-Zhu, Z.
\newblock How to make the gradients small stochastically.
\newblock \emph{CoRR}, abs/1801.02982, 2018{\natexlab{a}}.

\bibitem[Allen-Zhu(2018{\natexlab{b}})]{natasha2}
Allen-Zhu, Z.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  2680--2691, 2018{\natexlab{b}}.

\bibitem[Allen-Zhu \& Li(2018)Allen-Zhu and Li]{neon2}
Allen-Zhu, Z. and Li, Y.
\newblock Neon2: Finding local minima via first-order oracles.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  3720--3730, 2018.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and
  Zhang]{strongergeneralizationboundsfordeepnets}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{spectrally-normalizedmarginbounds}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{CoRR}, abs/1706.08498, 2017.

\bibitem[Cesa-bianchi et~al.(2002)Cesa-bianchi, Conconi, and
  Gentile]{onlinetobatch}
Cesa-bianchi, N., Conconi, A., and Gentile, C.
\newblock On the generalization ability of on-line learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 14}, pp.\
  359--366. MIT Press, 2002.

\bibitem[{Chaudhari} et~al.(2016){Chaudhari}, {Choromanska}, {Soatto}, {LeCun},
  {Baldassi}, {Borgs}, {Chayes}, {Sagun}, and {Zecchina}]{entropy-sgd}
{Chaudhari}, P., {Choromanska}, A., {Soatto}, S., {LeCun}, Y., {Baldassi}, C.,
  {Borgs}, C., {Chayes}, J., {Sagun}, L., and {Zecchina}, R.
\newblock {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}.
\newblock \emph{ArXiv e-prints}, November 2016.

\bibitem[Choromanska et~al.(2015)Choromanska, LeCun, and Arous]{open_prob}
Choromanska, A., LeCun, Y., and Arous, G.~B.
\newblock Open problem: The landscape of the loss surfaces of multilayer
  networks.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory,
  {COLT} 2015, Paris, France, July 3-6, 2015}, pp.\  1756--1760, 2015.

\bibitem[Cooper(2018)]{thelosslandscapeofoverparameterizednn}
Cooper, Y.
\newblock The loss landscape of overparameterized neural networks.
\newblock \emph{CoRR}, abs/1804.10200, 2018.

\bibitem[{Dinh} et~al.(2017){Dinh}, {Pascanu}, {Bengio}, and
  {Bengio}]{sharpminimacangenearlizefordeepnets}
{Dinh}, L., {Pascanu}, R., {Bengio}, S., and {Bengio}, Y.
\newblock {Sharp Minima Can Generalize For Deep Nets}.
\newblock \emph{ArXiv e-prints}, March 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{essentiallynobarriersin}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80, pp.\  1309--1318,
  10--15 Jul 2018.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{losssurfacesmodeconnectivity}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{CoRR}, abs/1802.10026, 2018.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{Ge2015}
Ge, R., Huang, F., Jin, C., and Yuan, Y.
\newblock Escaping from saddle points - online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT 2015}, volume~40, pp.\  797--842, 2015.

\bibitem[{Ge} et~al.(2017){Ge}, {Lee}, and {Ma}]{landscapedesign}
{Ge}, R., {Lee}, J.~D., and {Ma}, T.
\newblock {Learning One-hidden-layer Neural Networks with Landscape Design}.
\newblock \emph{ArXiv e-prints}, November 2017.

\bibitem[Goodfellow \& Vinyals(2014)Goodfellow and
  Vinyals]{qualitativelycharacterizing}
Goodfellow, I.~J. and Vinyals, O.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{CoRR}, abs/1412.6544, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{trainimagenetin1hour}
Goyal, P., Doll{\'{a}}r, P., Girshick, R.~B., Noordhuis, P., Wesolowski, L.,
  Kyrola, A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock \emph{CoRR}, abs/1706.02677, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{characterizingimplicitbias}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{PMLR}, volume~80, pp.\  1832--1841, 10--15 Jul 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pp.\  770--778, 2016.

\bibitem[Hochreiter \& Schmidhuber(1995)Hochreiter and
  Schmidhuber]{flat_minima_2}
Hochreiter, S. and Schmidhuber, J.
\newblock Simplifying neural nets by discovering flat minima.
\newblock In \emph{Advances in Neural Information Processing Systems 7}, pp.\
  529--536. MIT Press, 1995.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{trainlonger}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  1729--1739. Curran Associates, Inc., 2017.

\bibitem[{Huang} et~al.(2016){Huang}, {Liu}, {Weinberger}, and {van der
  Maaten}]{densenet}
{Huang}, G., {Liu}, Z., {Weinberger}, K.~Q., and {van der Maaten}, L.
\newblock {Densely Connected Convolutional Networks}.
\newblock \emph{ArXiv e-prints}, August 2016.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{snapshotensembles}
Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.~E., and Weinberger, K.~Q.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock In \emph{ICLR 2017}, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, pp.\  448--456, 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{swa}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{Uncertainty in Artificial Intelligence (UAI)}, 2018.

\bibitem[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{threefactorsinfluencingminima}
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y.,
  and Storkey, A.~J.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{CoRR}, abs/1711.04623, 2017.

\bibitem[Ji \& Telgarsky(2018)Ji and Telgarsky]{riskandparameterconvergence}
Ji, Z. and Telgarsky, M.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{CoRR}, abs/1803.07300, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{escapesaddlepointsefficiently}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock \emph{CoRR}, abs/1703.00887, 2017.

\bibitem[Jin et~al.(2018{\natexlab{a}})Jin, Liu, Ge, and
  Jordan]{onthelocalminimaoftheempriicalrisk}
Jin, C., Liu, L.~T., Ge, R., and Jordan, M.~I.
\newblock On the local minima of the empirical risk.
\newblock In \emph{NeurIPS}, pp.\  4901--4910, 2018{\natexlab{a}}.

\bibitem[Jin et~al.(2018{\natexlab{b}})Jin, Netrapalli, and
  Jordan]{acceleratedgradientdescentescapesaddlepoints}
Jin, C., Netrapalli, P., and Jordan, M.~I.
\newblock Accelerated gradient descent escapes saddle points faster than
  gradient descent.
\newblock In \emph{Conference On Learning Theory, {COLT} 2018, Stockholm,
  Sweden, 6-9 July 2018.}, pp.\  1042--1085, 2018{\natexlab{b}}.

\bibitem[{Kawaguchi} et~al.(2017){Kawaguchi}, {Pack Kaelbling}, and
  {Bengio}]{generalizationindeeplearning}
{Kawaguchi}, K., {Pack Kaelbling}, L., and {Bengio}, Y.
\newblock {Generalization in Deep Learning}.
\newblock \emph{ArXiv e-prints}, October 2017.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{largebatchtraining}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{ICLR 2017}, 2017.

\bibitem[{Kleinberg} et~al.(2018){Kleinberg}, {Li}, and
  {Yuan}]{analternativeview}
{Kleinberg}, R., {Li}, Y., and {Yuan}, Y.
\newblock {An Alternative View: When Does SGD Escape Local Minima?}
\newblock \emph{ArXiv e-prints}, February 2018.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{visualizingthelosslandscape}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  6391--6401. Curran Associates, Inc., 2018.

\bibitem[Masters \& Luschi(2018)Masters and
  Luschi]{revisitingsmallbatchtraining}
Masters, D. and Luschi, C.
\newblock Revisiting small batch training for deep neural networks.
\newblock \emph{CoRR}, abs/1804.07612, 2018.

\bibitem[Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{apacbayesianapproach}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{CoRR}, abs/1707.09564, 2017{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Bhojanapalli,
  Mcallester, and Srebro]{exploringgeneralizationindeeplearning}
Neyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  5947--5956. Curran Associates, Inc., 2017{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{towardsunderstandingtherole}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{CoRR}, abs/1805.12076, 2018.

\bibitem[Pennington \& Bahri(2017)Pennington and
  Bahri]{geometryofneuralnetworklosssurfaces}
Pennington, J. and Bahri, Y.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pp.\  2798--2806. PMLR, 2017.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyakaveraging}
Polyak, B.~T. and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM J. Control Optim.}, 30\penalty0 (4):\penalty0 838--855,
  July 1992.
\newblock ISSN 0363-0129.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and
  Sridharan]{makinggradientdescentoptimal}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{ICML}, 2012.

\bibitem[{Safran} \& {Shamir}(2017){Safran} and
  {Shamir}]{spuriouslocalminimaarecommon}
{Safran}, I. and {Shamir}, O.
\newblock {Spurious Local Minima are Common in Two-Layer ReLU Neural Networks}.
\newblock \emph{ArXiv e-prints}, December 2017.

\bibitem[Sagun et~al.(2017)Sagun, Evci, G{\"{u}}ney, Dauphin, and
  Bottou]{empiricalanalysisofhessianofoverparameter}
Sagun, L., Evci, U., G{\"{u}}ney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{CoRR}, abs/1706.04454, 2017.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Sridharan, and
  Srebro]{shalev2009stochastic}
Shalev-Shwartz, S., Shamir, O., Sridharan, K., and Srebro, N.
\newblock Stochastic convex optimization.
\newblock In \emph{Proceedings of The 22nd Conference on Learning Theory},
  2009.

\bibitem[{Smith} \& {Le}(2017){Smith} and
  {Le}]{abayesianperspectiveongeneralization}
{Smith}, S.~L. and {Le}, Q.~V.
\newblock {A Bayesian Perspective on Generalization and Stochastic Gradient
  Descent}.
\newblock \emph{ArXiv e-prints}, October 2017.

\bibitem[Smith et~al.(2017)Smith, Kindermans, and Le]{dontdecaythelearningrate}
Smith, S.~L., Kindermans, P., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{CoRR}, abs/1711.00489, 2017.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, and Srebro]{theimplicitbiasofgd}
Soudry, D., Hoffer, E., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1710.10345}, 2017.

\bibitem[Wu et~al.(2017)Wu, Zhu,
  et~al.]{towardsunderstandinggeneralizationofdeeplearning}
Wu, L., Zhu, Z., et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock \emph{arXiv preprint arXiv:1706.10239}, 2017.

\bibitem[Xu et~al.(2018)Xu, Rong, and
  Yang]{firstorderstochasticalgorithmsforescapingfrom}
Xu, Y., Rong, J., and Yang, T.
\newblock First-order stochastic algorithms for escaping from saddle points in
  almost linear time.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  5535--5545. Curran Associates, Inc., 2018.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{nonvacuousgeneralizationbound}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  {PAC}-bayesian compression approach.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
