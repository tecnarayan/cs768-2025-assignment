\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajalloeian and Stich(2020)]{ajalloeian2020analysis}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock Analysis of {SGD} with biased gradient estimators.
\newblock \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem[Anselmi et~al.(2016)Anselmi, Leibo, Rosasco, Mutch, Tacchetti, and Poggio]{anselmi2013unsupervised}
Fabio Anselmi, Joel~Z Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio.
\newblock Unsupervised learning of invariant representations.
\newblock \emph{Theoretical Computer Science}, 633:\penalty0 112--121, 2016.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Beck(2017)]{beck17firstorder}
Amir Beck.
\newblock \emph{First-Order Methods in Optimization}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA, 2017.

\bibitem[Braun et~al.(2017)Braun, Guzm{\'a}n, and Pokutta]{braun2017lower}
G{\'a}bor Braun, Crist{\'o}bal Guzm{\'a}n, and Sebastian Pokutta.
\newblock Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory.
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0 (7):\penalty0 4709--4724, 2017.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Bujok et~al.(2015)Bujok, Hambly, and Reisinger]{bujok2015multilevel}
Karolina Bujok, BM~Hambly, and Christoph Reisinger.
\newblock Multilevel simulation of functionals of bernoulli random variables with application to basket credit derivatives.
\newblock \emph{Methodology and Computing in Applied Probability}, 17\penalty0 (3):\penalty0 579--604, 2015.

\bibitem[Chen and Luss(2018)]{chen2018stochastic}
Jie Chen and Ronny Luss.
\newblock Stochastic gradient descent with biased but consistent gradient estimators.
\newblock \emph{arXiv preprint arXiv:1807.11880}, 2018.

\bibitem[Chen et~al.(2020)Chen, Sun, and Yin]{chen2020solving}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization.
\newblock \emph{arXiv preprint arXiv:2008.10847}, 2020.

\bibitem[Chen et~al.(2019)Chen, Yuan, Yi, Zhou, Chen, and Yang]{chen2018universal}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao Yang.
\newblock Universal stagewise learning for non-convex problems with convergence on averaged solutions.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Cover(1999)]{cover1999elements}
Thomas~M Cover.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Dai et~al.(2017)Dai, He, Pan, Boots, and Song]{pmlr-v54-dai17a}
Bo~Dai, Niao He, Yunpeng Pan, Byron Boots, and Le~Song.
\newblock Learning from conditional distributions via dual embeddings.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1458--1467, 2017.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and Song]{pmlr-v80-dai18c}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le~Song.
\newblock {SBEED}: Convergent reinforcement learning with nonlinear function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages 1125--1134, 2018.

\bibitem[Davis and Drusvyatskiy(2019)]{davis2019stochastic}
Damek Davis and Dmitriy Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0 207--239, 2019.

\bibitem[Davis et~al.(2020)Davis, Drusvyatskiy, Kakade, and Lee]{davis2020stochastic}
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason~D Lee.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of computational mathematics}, 20\penalty0 (1):\penalty0 119--154, 2020.

\bibitem[Drusvyatskiy(2017)]{drusvyatskiy2017proximal}
Dmitriy Drusvyatskiy.
\newblock The proximal point method revisited.
\newblock \emph{arXiv preprint arXiv:1712.06038}, 2017.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and Ozdaglar]{fallah2019convergence}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic meta-learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1082--1092, 2020.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 689--699, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem[Ghadimi et~al.(2020)Ghadimi, Ruszczynski, and Wang]{ghadimi2018single}
Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang.
\newblock A single timescale stochastic approximation method for nested stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0 960--979, 2020.

\bibitem[Giles(2008)]{giles2008multilevel}
Michael~B Giles.
\newblock Multilevel monte carlo path simulation.
\newblock \emph{Operations research}, 56\penalty0 (3):\penalty0 607--617, 2008.

\bibitem[Giles(2015)]{giles2015multilevel}
Michael~B Giles.
\newblock Multilevel monte carlo methods.
\newblock \emph{Acta Numerica}, 24:\penalty0 259, 2015.

\bibitem[Giles and Haji-Ali(2019)]{giles2019multilevel}
Michael~B Giles and Abdul-Lateef Haji-Ali.
\newblock Multilevel nested simulation for efficient risk estimation.
\newblock \emph{SIAM/ASA Journal on Uncertainty Quantification}, 7\penalty0 (2):\penalty0 497--525, 2019.

\bibitem[Gordy and Juneja(2010)]{gordy2010nested}
Michael~B Gordy and Sandeep Juneja.
\newblock Nested simulation in portfolio risk measurement.
\newblock \emph{Management Science}, 56\penalty0 (10):\penalty0 1833--1848, 2010.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Hong and Juneja(2009)]{hong2009estimating}
L~Jeff Hong and Sandeep Juneja.
\newblock Estimating the mean of a non-linear function of conditional expectation.
\newblock In \emph{Winter Simulation Conference}, pages 1223--1236. Winter Simulation Conference, 2009.

\bibitem[Hong et~al.(2017)Hong, Juneja, and Liu]{hong2017kernel}
L~Jeff Hong, Sandeep Juneja, and Guangwu Liu.
\newblock Kernel smoothing for nested estimation with application to portfolio risk measurement.
\newblock \emph{Operations Research}, 65\penalty0 (3):\penalty0 657--673, 2017.

\bibitem[Hu et~al.(2020{\natexlab{a}})Hu, Seiler, and Lessard]{hu2020analysis}
Bin Hu, Peter Seiler, and Laurent Lessard.
\newblock Analysis of biased stochastic gradient descent using sequential semidefinite programs.
\newblock \emph{Mathematical Programming}, pages 1--26, 2020{\natexlab{a}}.

\bibitem[Hu et~al.(2016)Hu, Prashanth, Gy{\"o}rgy, and Szepesv{\'a}ri]{hu2016bandit}
Xiaowei Hu, LA~Prashanth, Andr{\'a}s Gy{\"o}rgy, and Csaba Szepesv{\'a}ri.
\newblock (bandit) convex optimization with biased noisy gradient oracles.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 819--828, 2016.

\bibitem[Hu et~al.(2020{\natexlab{b}})Hu, Chen, and He]{hu2019sample}
Yifan Hu, Xin Chen, and Niao He.
\newblock Sample complexity of sample average approximation for conditional stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (3):\penalty0 2103--2133, 2020{\natexlab{b}}.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance reduction.
\newblock In \emph{Advances in neural information processing systems}, pages 315--323, 2013.

\bibitem[Karimi et~al.(2019)Karimi, Miasojedow, Moulines, and Wai]{karimi2019non}
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In \emph{Conference on Learning Theory}, pages 1944--1974, 2019.

\bibitem[Mroueh et~al.(2015)Mroueh, Voinea, and Poggio]{mroueh2015learning}
Youssef Mroueh, Stephen Voinea, and Tomaso~A Poggio.
\newblock Learning with group invariant features: A kernel perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1558--1566, 2015.

\bibitem[Muandet et~al.(2019)Muandet, Mehrjou, Lee, and Raj]{muandet2019dual}
Krikamol Muandet, Arash Mehrjou, Si~Kai Lee, and Anant Raj.
\newblock Dual {IV}: A single stage instrumental variable regression.
\newblock \emph{arXiv preprint arXiv:1910.12358}, 2019.

\bibitem[Nachum and Dai(2020)]{nachum2020}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0 1574--1609, 2009.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning-Volume 70}, pages 2613--2621. JMLR. org, 2017.

\bibitem[Pang et~al.(2017)Pang, Razaviyayn, and Alvarado]{pang2017computing}
Jong-Shi Pang, Meisam Razaviyayn, and Alberth Alvarado.
\newblock Computing b-stationary points of nonsmooth dc programs.
\newblock \emph{Mathematics of Operations Research}, 42\penalty0 (1):\penalty0 95--118, 2017.

\bibitem[Paquette et~al.(2018)Paquette, Lin, Drusvyatskiy, Mairal, and Harchaoui]{paquette2017catalyst}
Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, and Zaid Harchaoui.
\newblock Catalyst for gradient-based nonconvex optimization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research}, pages 613--622. PMLR, 2018.

\bibitem[Shamir and Zhang(2013)]{shamir2013stochastic}
Ohad Shamir and Tong Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes.
\newblock In \emph{International Conference on Machine Learning}, pages 71--79, 2013.

\bibitem[Singh et~al.(2019)Singh, Sahani, and Gretton]{singh2019kernel}
Rahul Singh, Maneesh Sahani, and Arthur Gretton.
\newblock Kernel instrumental variable regression.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages 4595--4607. Curran Associates, Inc., 2019.

\bibitem[Wang et~al.(2016)Wang, Liu, and Fang]{wang2016accelerating}
Mengdi Wang, Ji~Liu, and Ethan Fang.
\newblock Accelerating stochastic composition optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1714--1722, 2016.

\bibitem[Wang et~al.(2017)Wang, Fang, and Liu]{wang2017stochastic}
Mengdi Wang, Ethan~X Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1-2):\penalty0 419--449, 2017.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and Tarokh]{wang2018spiderboost}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 2406--2416, 2019.

\bibitem[Yang et~al.(2019)Yang, Wang, and Fang]{yang2019multilevel}
Shuoguang Yang, Mengdi Wang, and Ethan~X Fang.
\newblock Multilevel stochastic gradient methods for nested composition optimization.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0 616--659, 2019.

\bibitem[{Yao}(1977)]{yao1977}
A.~C. {Yao}.
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \emph{18th Annual Symposium on Foundations of Computer Science (sfcs 1977)}, pages 222--227, Oct 1977.

\bibitem[Zhang and Xiao(2019)]{zhang2019multi}
Junyu Zhang and Lin Xiao.
\newblock Multi-level composite stochastic optimization via nested variance reduction.
\newblock \emph{arXiv preprint arXiv:1908.11468}, 2019.

\bibitem[Zhang and He(2018)]{zhang2018convergence}
Siqi Zhang and Niao He.
\newblock On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1806.04781}, 2018.

\end{thebibliography}
