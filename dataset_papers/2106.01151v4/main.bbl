\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Machado, Castro, and
  Bellemare]{agarwal2021contrastive}
R.~Agarwal, M.~C. Machado, P.~S. Castro, and M.~G. Bellemare.
\newblock Contrastive behavioral similarity embeddings for generalization in
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2101.05265}, 2021.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski,
  et~al.]{andrychowicz2020matters}
M.~Andrychowicz, A.~Raichuk, P.~Sta{\'n}czyk, M.~Orsini, S.~Girgin,
  R.~Marinier, L.~Hussenot, M.~Geist, O.~Pietquin, M.~Michalski, et~al.
\newblock What matters in on-policy reinforcement learning? a large-scale
  empirical study.
\newblock \emph{arXiv preprint arXiv:2006.05990}, 2020.

\bibitem[Anthony et~al.(2020)Anthony, Eccles, Tacchetti, Kram{\'a}r, Gemp,
  Hudson, Porcel, Lanctot, P{\'e}rolat, Everett, et~al.]{anthony2020learning}
T.~Anthony, T.~Eccles, A.~Tacchetti, J.~Kram{\'a}r, I.~Gemp, T.~C. Hudson,
  N.~Porcel, M.~Lanctot, J.~P{\'e}rolat, R.~Everett, et~al.
\newblock Learning to play no-press diplomacy with best response policy
  iteration.
\newblock \emph{arXiv preprint arXiv:2006.04635}, 2020.

\bibitem[Argenson and Dulac-Arnold(2020)]{argenson2020model}
A.~Argenson and G.~Dulac-Arnold.
\newblock Model-based offline planning.
\newblock \emph{arXiv preprint arXiv:2008.05556}, 2020.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pages
  214--223. PMLR, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{bellemare13arcade}
M.~G. {Bellemare}, Y.~{Naddaf}, J.~{Veness}, and M.~{Bowling}.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, jun 2013.

\bibitem[Bjorck et~al.(2021)Bjorck, Chen, De~Sa, Gomes, and
  Weinberger]{bjorck2021low}
J.~Bjorck, X.~Chen, C.~De~Sa, C.~P. Gomes, and K.~Q. Weinberger.
\newblock Low-precision reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.13565}, 2021.

\bibitem[Brock et~al.(2016)Brock, Lim, Ritchie, and Weston]{brock2016neural}
A.~Brock, T.~Lim, J.~M. Ritchie, and N.~Weston.
\newblock Neural photo editing with introspective adversarial networks.
\newblock \emph{arXiv preprint arXiv:1609.07093}, 2016.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cobbe et~al.(2019)Cobbe, Klimov, Hesse, Kim, and
  Schulman]{cobbe2019quantifying}
K.~Cobbe, O.~Klimov, C.~Hesse, T.~Kim, and J.~Schulman.
\newblock Quantifying generalization in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1282--1289. PMLR, 2019.

\bibitem[Cui et~al.(2020)Cui, Chow, and Ghavamzadeh]{cui2020control}
B.~Cui, Y.~Chow, and M.~Ghavamzadeh.
\newblock Control-aware representations for model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.13408}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{engstrom2020implementation}
L.~Engstrom, A.~Ilyas, S.~Santurkar, D.~Tsipras, F.~Janoos, L.~Rudolph, and
  A.~Madry.
\newblock Implementation matters in deep policy gradients: A case study on ppo
  and trpo.
\newblock \emph{arXiv preprint arXiv:2005.12729}, 2020.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning}, pages
  1407--1416. PMLR, 2018.

\bibitem[Espeholt et~al.(2019)Espeholt, Marinier, Stanczyk, Wang, and
  Michalski]{espeholt2019seed}
L.~Espeholt, R.~Marinier, P.~Stanczyk, K.~Wang, and M.~Michalski.
\newblock Seed rl: Scalable and efficient deep-rl with accelerated central
  inference.
\newblock \emph{arXiv preprint arXiv:1910.06591}, 2019.

\bibitem[Farebrother et~al.(2018)Farebrother, Machado, and
  Bowling]{farebrother2018generalization}
J.~Farebrother, M.~C. Machado, and M.~Bowling.
\newblock Generalization and regularization in dqn.
\newblock \emph{arXiv preprint arXiv:1810.00123}, 2018.

\bibitem[Farnia et~al.(2018)Farnia, Zhang, and Tse]{farnia2018generalizable}
F.~Farnia, J.~M. Zhang, and D.~Tse.
\newblock Generalizable adversarial training via spectral normalization.
\newblock \emph{arXiv preprint arXiv:1811.07457}, 2018.

\bibitem[Fu and Levine(2021)]{fu2021offline}
J.~Fu and S.~Levine.
\newblock Offline model-based optimization via normalized maximum likelihood
  estimation.
\newblock \emph{arXiv preprint arXiv:2102.07970}, 2021.

\bibitem[Gogianu et~al.(2021)Gogianu, Berariu, Rosca, Clopath, Busoniu, and
  Pascanu]{gogianu2021spectral}
F.~Gogianu, T.~Berariu, M.~Rosca, C.~Clopath, L.~Busoniu, and R.~Pascanu.
\newblock Spectral normalisation for deep reinforcement learning: an
  optimisation perspective.
\newblock \emph{arXiv preprint arXiv:2105.05246}, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1406.2661}, 2014.

\bibitem[Gouk et~al.(2021)Gouk, Frank, Pfahringer, and
  Cree]{gouk2021regularisation}
H.~Gouk, E.~Frank, B.~Pfahringer, and M.~J. Cree.
\newblock Regularisation of neural networks by enforcing lipschitz continuity.
\newblock \emph{Machine Learning}, 110\penalty0 (2):\penalty0 393--416, 2021.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~Courville.
\newblock Improved training of wasserstein gans.
\newblock \emph{arXiv preprint arXiv:1704.00028}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[Hafner et~al.(2019{\natexlab{a}})Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
D.~Hafner, T.~Lillicrap, J.~Ba, and M.~Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{arXiv preprint arXiv:1912.01603}, 2019{\natexlab{a}}.

\bibitem[Hafner et~al.(2019{\natexlab{b}})Hafner, Lillicrap, Fischer, Villegas,
  Ha, Lee, and Davidson]{hafner2019learning}
D.~Hafner, T.~Lillicrap, I.~Fischer, R.~Villegas, D.~Ha, H.~Lee, and
  J.~Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International Conference on Machine Learning}, pages
  2555--2565. PMLR, 2019{\natexlab{b}}.

\bibitem[Hamrick et~al.(2020)Hamrick, Friesen, Behbahani, Guez, Viola,
  Witherspoon, Anthony, Buesing, Veli{\v{c}}kovi{\'c}, and
  Weber]{hamrick2020role}
J.~B. Hamrick, A.~L. Friesen, F.~Behbahani, A.~Guez, F.~Viola, S.~Witherspoon,
  T.~Anthony, L.~Buesing, P.~Veli{\v{c}}kovi{\'c}, and T.~Weber.
\newblock On the role of planning in model-based deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2011.04021}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2017deep}
P.~Henderson, R.~Islam, P.~Bachman, J.~Pineau, D.~Precup, and D.~Meger.
\newblock Deep reinforcement learning that matters.
\newblock \emph{arXiv preprint arXiv:1709.06560}, 2017.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
S.~Kapturowski, G.~Ostrovski, J.~Quan, R.~Munos, and W.~Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International conference on learning representations}, 2018.

\bibitem[Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani,
  and P{\'e}rez]{kiran2021deep}
B.~R. Kiran, I.~Sobh, V.~Talpaert, P.~Mannion, A.~A. Al~Sallab, S.~Yogamani,
  and P.~P{\'e}rez.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 2021.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
J.~Kober, J.~A. Bagnell, and J.~Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
I.~Kostrikov, D.~Yarats, and R.~Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock 2020.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Laskin et~al.()Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin_lee2020rad}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock arXiv:2004.14990.

\bibitem[Laskin et~al.(2020{\natexlab{a}})Laskin, Lee, Stooke, Pinto, Abbeel,
  and Srinivas]{laskin2020reinforcement}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock \emph{arXiv preprint arXiv:2004.14990}, 2020{\natexlab{a}}.

\bibitem[Laskin et~al.(2020{\natexlab{b}})Laskin, Srinivas, and
  Abbeel]{laskin2020curl}
M.~Laskin, A.~Srinivas, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{Proceedings of the 37th Annual International Conference on
  Machine Learning (ICML)}, 2020{\natexlab{b}}.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Lee et~al.(2019)Lee, Nagabandi, Abbeel, and Levine]{lee2019stochastic}
A.~X. Lee, A.~Nagabandi, P.~Abbeel, and S.~Levine.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock \emph{arXiv preprint arXiv:1907.00953}, 2019.

\bibitem[Lee et~al.(2021)Lee, Lee, and Kim]{lee2021representation}
B.-J. Lee, J.~Lee, and K.-E. Kim.
\newblock Representation balancing offline model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2021)Liu, Zhang, Zhao, Qin, Zhu, Li, Yu, and
  Liu]{liu2021return}
G.~Liu, C.~Zhang, L.~Zhao, T.~Qin, J.~Zhu, J.~Li, N.~Yu, and T.-Y. Liu.
\newblock Return-based contrastive representation learning for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2102.10960}, 2021.

\bibitem[Liu et~al.(2020)Liu, Yeh, and Schwing]{liu2020high}
I.-J. Liu, R.~Yeh, and A.~Schwing.
\newblock High-throughput synchronous deep rl.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Mises and Pollaczek-Geiringer(1929)]{mises1929praktische}
R.~Mises and H.~Pollaczek-Geiringer.
\newblock Praktische verfahren der gleichungsaufl{\"o}sung.
\newblock \emph{ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift
  f{\"u}r Angewandte Mathematik und Mechanik}, 9\penalty0 (1):\penalty0 58--77,
  1929.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem[Narang et~al.(2021)Narang, Chung, Tay, Fedus, Fevry, Matena, Malkan,
  Fiedel, Shazeer, Lan, et~al.]{narang2021transformer}
S.~Narang, H.~W. Chung, Y.~Tay, W.~Fedus, T.~Fevry, M.~Matena, K.~Malkan,
  N.~Fiedel, N.~Shazeer, Z.~Lan, et~al.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock \emph{arXiv preprint arXiv:2102.11972}, 2021.

\bibitem[Parisotto and Salakhutdinov(2021)]{parisotto2021efficient}
E.~Parisotto and R.~Salakhutdinov.
\newblock Efficient transformers in reinforcement learning using actor-learner
  distillation.
\newblock \emph{arXiv preprint arXiv:2104.01655}, 2021.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre,
  Jayakumar, Jaderberg, Kaufman, Clark, Noury,
  et~al.]{parisotto2020stabilizing}
E.~Parisotto, F.~Song, J.~Rae, R.~Pascanu, C.~Gulcehre, S.~Jayakumar,
  M.~Jaderberg, R.~L. Kaufman, A.~Clark, S.~Noury, et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7487--7498. PMLR, 2020.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
R.~Pascanu, T.~Mikolov, and Y.~Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318. PMLR, 2013.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  2778--2787. PMLR, 2017.

\bibitem[Petrenko et~al.(2020)Petrenko, Huang, Kumar, Sukhatme, and
  Koltun]{petrenko2020sample}
A.~Petrenko, Z.~Huang, T.~Kumar, G.~Sukhatme, and V.~Koltun.
\newblock Sample factory: Egocentric 3d control from pixels at 100000 fps with
  asynchronous reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7652--7662. PMLR, 2020.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2019regularized}
E.~Real, A.~Aggarwal, Y.~Huang, and Q.~V. Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{Proceedings of the aaai conference on artificial
  intelligence}, volume~33, pages 4780--4789, 2019.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
T.~Salimans and D.~P. Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1602.07868}, 2016.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Schwarzer et~al.(2020)Schwarzer, Anand, Goel, Hjelm, Courville, and
  Bachman]{schwarzer2020data}
M.~Schwarzer, A.~Anand, R.~Goel, R.~D. Hjelm, A.~Courville, and P.~Bachman.
\newblock Data-efficient reinforcement learning with momentum predictive
  representations.
\newblock \emph{arXiv preprint arXiv:2007.05929}, 2020.

\bibitem[Sinha et~al.(2020)Sinha, Bharadhwaj, Srinivas, and
  Garg]{sinha2020d2rl}
S.~Sinha, H.~Bharadhwaj, A.~Srinivas, and A.~Garg.
\newblock D2rl: Deep dense architectures in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.09163}, 2020.

\bibitem[Sontakke et~al.(2020)Sontakke, Mehrjou, Itti, and
  Sch{\"o}lkopf]{sontakke2020causal}
S.~A. Sontakke, A.~Mehrjou, L.~Itti, and B.~Sch{\"o}lkopf.
\newblock Causal curiosity: Rl agents discovering self-supervised experiments
  for causal representation learning.
\newblock \emph{arXiv preprint arXiv:2010.03110}, 2020.

\bibitem[Sutton(1988)]{sutton1988learning}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
M.~Tan and Q.~Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem[Tassa et~al.(2020)Tassa, Tunyasuvunakool, Muldal, Doron, Liu, Bohez,
  Merel, Erez, Lillicrap, and Heess]{tassa2020dmcontrol}
Y.~Tassa, S.~Tunyasuvunakool, A.~Muldal, Y.~Doron, S.~Liu, S.~Bohez, J.~Merel,
  T.~Erez, T.~Lillicrap, and N.~Heess.
\newblock dm control: Software and tasks for continuous control, 2020.

\bibitem[van Hasselt et~al.(2015)van Hasselt, Guez, and Silver]{van2015deep}
H.~van Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double q-learning. corr
  abs/1509.06461 (2015).
\newblock \emph{arXiv preprint arXiv:1509.06461}, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and
  Freitas]{wang2016dueling}
Z.~Wang, T.~Schaul, M.~Hessel, H.~Hasselt, M.~Lanctot, and N.~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1995--2003. PMLR, 2016.

\bibitem[Watkins(1989)]{watkins1989learning}
C.~J. C.~H. Watkins.
\newblock Learning from delayed rewards.
\newblock 1989.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.~Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages
  10524--10533. PMLR, 2020.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, Lazaric, and
  Pinto]{yarats2021mastering}
D.~Yarats, R.~Fergus, A.~Lazaric, and L.~Pinto.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.09645}, 2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
T.~Yu, G.~Thomas, L.~Yu, S.~Ermon, J.~Zou, S.~Levine, C.~Finn, and T.~Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[Zadaianchuk et~al.(2020)Zadaianchuk, Seitzer, and
  Martius]{zadaianchuk2020self}
A.~Zadaianchuk, M.~Seitzer, and G.~Martius.
\newblock Self-supervised visual reinforcement learning with object-centric
  representations.
\newblock \emph{arXiv preprint arXiv:2011.14381}, 2020.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Ballas, and
  Pineau]{zhang2018dissection}
A.~Zhang, N.~Ballas, and J.~Pineau.
\newblock A dissection of overfitting and generalization in continuous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.07937}, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2020)Zhang, McAllister, Calandra, Gal, and
  Levine]{zhang2020learning}
A.~Zhang, R.~McAllister, R.~Calandra, Y.~Gal, and S.~Levine.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock \emph{arXiv preprint arXiv:2006.10742}, 2020.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Vinyals, Munos, and
  Bengio]{zhang2018study}
C.~Zhang, O.~Vinyals, R.~Munos, and S.~Bengio.
\newblock A study on overfitting in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.06893}, 2018{\natexlab{b}}.

\end{thebibliography}
