\providecommand{\noopsort}[1]{}
\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akhlaghi et~al.(2017)Akhlaghi, Zhou, and Huang]{akhlaghi:2017}
Akhlaghi, S., Zhou, N., and Huang, Z.
\newblock {A}daptive adjustment of noise covariance in {K}alman filter for
  dynamic state estimation.
\newblock In \emph{{I}{E}{E}{E} {P}ower \& {E}nergy {S}ociety {G}eneral
  {M}eeting}, pp.\  1--5, 2017.

\bibitem[{B}ar {S}halom et~al.(2004){B}ar {S}halom, {L}i, and
  {K}irubarajan]{bar:2004}
{B}ar {S}halom, Y., {L}i, X.~R., and {K}irubarajan, T.
\newblock \emph{{E}stimation with applications to tracking and navigation:
  theory algorithms and software}.
\newblock {J}ohn Wiley \& {S}ons, 2004.

\bibitem[{B}ifet \& {G}avalda(2007){B}ifet and {G}avalda]{bifet:2007}
{B}ifet, A. and {G}avalda, R.
\newblock {L}earning from time-changing data with adaptive windowing.
\newblock In \emph{{P}roceedings of the 2007 {S}{I}{A}{M} {I}nternational
  {C}onference on {D}ata {M}ining}, pp.\  443--448. {S}{I}{A}{M}, 2007.

\bibitem[Bifet et~al.(2010)Bifet, Holmes, Pfahringer, Kranen, Kremer, Jansen,
  and Seidl]{bifet:2010}
Bifet, A., Holmes, G., Pfahringer, B., Kranen, P., Kremer, H., Jansen, T., and
  Seidl, T.
\newblock {M}{O}{A}: {M}assive online analysis, a framework for stream
  classification and clustering.
\newblock In \emph{{P}roceedings of the {F}irst {W}orkshop on {A}pplications of
  {P}attern {A}nalysis}, pp.\  44--50. PMLR, 2010.

\bibitem[{C}avallanti et~al.(2007){C}avallanti, {C}esa {B}ianchi, and
  {G}entile]{cavallanti:2007}
{C}avallanti, G., {C}esa {B}ianchi, N., and {G}entile, C.
\newblock {T}racking the best hyperplane with a simple budget perceptron.
\newblock \emph{{M}achine {L}earning}, 69\penalty0 (2-3):\penalty0 143--167,
  2007.

\bibitem[Cutkosky(2020)]{cutkosky2020parameter}
Cutkosky, A.
\newblock {P}arameter-free, dynamic, and strongly-adaptive online learning.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, pp.\
  2250--2259. PMLR, 2020.

\bibitem[{D}ekel et~al.(2006){D}ekel, {S}halev {S}hwartz, and
  {S}inger]{dekel:2006}
{D}ekel, O., {S}halev {S}hwartz, S., and {S}inger, Y.
\newblock {T}he {F}orgetron: {A} kernel-based perceptron on a fixed budget.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing
  {S}ystems}, pp.\  259--266, 2006.

\bibitem[Delany et~al.(2005)Delany, Cunningham, Tsymbal, and
  Coyle]{delany:2005}
Delany, S.~J., Cunningham, P., Tsymbal, A., and Coyle, L.
\newblock A case-based technique for tracking concept drift in spam filtering.
\newblock \emph{Knowledge-Based Systems}, 4\penalty0 (18):\penalty0 187--195,
  2005.

\bibitem[Ditzler et~al.(2015)Ditzler, Roveri, Alippi, and
  Polikar]{ditzler2015learning}
Ditzler, G., Roveri, M., Alippi, C., and Polikar, R.
\newblock Learning in nonstationary environments: A survey.
\newblock \emph{IEEE Computational Intelligence Magazine}, 10\penalty0
  (4):\penalty0 12--25, 2015.

\bibitem[Gama et~al.(2014)Gama, {\v{Z}}liobait{\.e}, Bifet, Pechenizkiy, and
  Bouchachia]{gama:2014}
Gama, J., {\v{Z}}liobait{\.e}, I., Bifet, A., Pechenizkiy, M., and Bouchachia,
  A.
\newblock A survey on concept drift adaptation.
\newblock \emph{{A}{C}{M} {C}omputing {S}urveys (CSUR)}, 46\penalty0
  (4):\penalty0 1--37, 2014.

\bibitem[{H}umpherys et~al.(2012){H}umpherys, {R}edd, and West]{humpherys:2012}
{H}umpherys, J., {R}edd, P., and West, J.
\newblock {A} fresh look at the {K}alman filter.
\newblock \emph{{S}{I}{A}{M} review}, 54\penalty0 (4):\penalty0 801--823, 2012.

\bibitem[{K}ivinen et~al.(2004){K}ivinen, {S}mola, and
  Williamson]{kivinen:2004}
{K}ivinen, J., {S}mola, A.~J., and Williamson, R.~C.
\newblock {O}nline learning with kernels.
\newblock \emph{{I}{E}{E}{E} {T}ransactions on {S}ignal {P}rocessing},
  52\penalty0 (8):\penalty0 2165--2176, 2004.

\bibitem[{K}linkenberg(2004)]{klinkenberg:2004}
{K}linkenberg, R.
\newblock {L}earning drifting concepts: {E}xample selection vs. example
  weighting.
\newblock \emph{{I}ntelligent data analysis}, 8\penalty0 (3):\penalty0
  281--300, 2004.

\bibitem[{K}olter \& {M}aloof(2007){K}olter and {M}aloof]{kolter:2007}
{K}olter, J.~Z. and {M}aloof, M.~A.
\newblock {D}ynamic weighted majority: {A}n ensemble method for drifting
  concepts.
\newblock \emph{{J}ournal of {M}achine {L}earning {R}esearch}, 8\penalty0
  (12):\penalty0 2755--2790, 2007.

\bibitem[{K}umagai \& {I}wata(2016){K}umagai and {I}wata]{kumagai:2016}
{K}umagai, A. and {I}wata, T.
\newblock {L}earning future classifiers without additional data.
\newblock In \emph{{P}roceedings of the {T}hirtieth {A}{A}{A}{I} {C}onference
  on {{A}}rtificial {I}ntelligence}, pp.\  1772--1778, 2016.

\bibitem[Kumagai \& Iwata(2017)Kumagai and Iwata]{kumagai:2017}
Kumagai, A. and Iwata, T.
\newblock Learning non-linear dynamics of decision boundaries for maintaining
  classification performance.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2017.

\bibitem[Long(1999)]{long1999complexity}
Long, P.~M.
\newblock The complexity of learning according to two models of a drifting
  environment.
\newblock \emph{{M}achine {L}earning}, 37\penalty0 (3):\penalty0 337--354,
  1999.

\bibitem[{L}u et~al.(2016){L}u, {H}oi, Wang, Zhao, and {L}iu]{lu:2016}
{L}u, J., {H}oi, S.~C., Wang, J., Zhao, P., and {L}iu, Z.-Y.
\newblock {L}arge scale online kernel learning.
\newblock \emph{{T}he {J}ournal of {M}achine {L}earning {R}esearch},
  17\penalty0 (1):\penalty0 1613--1655, 2016.

\bibitem[Mazuelas et~al.(2020)Mazuelas, Zanoni, and Perez]{mazuelas:2020}
Mazuelas, S., Zanoni, A., and Perez, A.
\newblock Minimax classification with 0-1 loss and performance guarantees.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing
  {S}ystems}, pp.\  302--312, 2020.

\bibitem[Mazuelas et~al.(2022{\natexlab{a}})Mazuelas, Shen, and
  P{\'e}rez]{mazuelas2022}
Mazuelas, S., Shen, Y., and P{\'e}rez, A.
\newblock {G}eneralized maximum entropy for supervised classification.
\newblock \emph{{I}{E}{E}{E} {T}ransactions on {I}nformation {T}heory},
  68\penalty0 (4):\penalty0 2530--2550, 2022{\natexlab{a}}.
  
  \bibitem[Mazuelas et~al.(2022{\natexlab{b}})Mazuelas, Romero, and
  Gr\"{u}nwald]{mazuelas2022a}
Mazuelas, S., Romero, M., and Gr\"{u}nwald, P.
\newblock {M}inimax risk classifiers with 0-1 loss.
\newblock \emph{arXiv preprint}, arXiv:2201.06487, 2022{\natexlab{b}}.


\bibitem[Mohri \& Medina(2012)Mohri and Medina]{mohri2012new}
Mohri, M. and Medina, A.~M.
\newblock {N}ew analysis and algorithm for learning with drifting
  distributions.
\newblock In \emph{{I}nternational {C}onference on {A}lgorithmic {L}earning
  {T}heory}, pp.\  124--138. Springer, 2012.

\bibitem[{M}ohri et~al.(2018){M}ohri, {R}ostamizadeh, and
  {T}alwalkar]{mohri:2018}
{M}ohri, M., {R}ostamizadeh, A., and {T}alwalkar, A.
\newblock \emph{{F}oundations of machine learning}.
\newblock {M}{I}{T} press, 2018.

\bibitem[Namkoong \& Duchi(2017)Namkoong and Duchi]{namkoong2017variance}
Namkoong, H. and Duchi, J.~C.
\newblock Variance-based regularization with convex objectives.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Nesterov \& Shikhman(2015)Nesterov and Shikhman]{nesterov2015quasi}
Nesterov, Y. and Shikhman, V.
\newblock Quasi-monotone subgradient methods for nonsmooth convex minimization.
\newblock \emph{{J}ournal of {O}ptimization {T}heory and {A}pplications},
  165\penalty0 (3):\penalty0 917--940, 2015.

\bibitem[Nguyen et~al.(2017)Nguyen, Le, Bui, and Phung]{nguyen:2017}
Nguyen, T.~D., Le, T., Bui, H., and Phung, D.
\newblock Large-scale online kernel learning with random feature
  reparameterization.
\newblock In \emph{{I}nternational {J}oint {C}onference on {A}rtificial
  {I}ntelligence}, pp.\  2543--2549, 2017.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, Liew, and
  Wang]{nguyen2018variational}
Nguyen, T. T.~T., Nguyen, T.~T., Liew, A. W.-C., and Wang, S.-L.
\newblock {V}ariational inference based bayes online classifiers with concept
  drift adaptation.
\newblock \emph{{P}attern {R}ecognition}, 81:\penalty0 280--293, 2018.

\bibitem[Odelson et~al.(2006)Odelson, Rajamani, and Rawlings]{odelson:2006}
Odelson, B.~J., Rajamani, M.~R., and Rawlings, J.~B.
\newblock A new autocovariance least-squares method for estimating noise
  covariances.
\newblock \emph{{A}utomatica}, 42\penalty0 (2):\penalty0 303--308, 2006.

\bibitem[{O}rabona et~al.(2008){O}rabona, {K}eshet, and {C}aputo]{orabona:2008}
{O}rabona, F., {K}eshet, J., and {C}aputo, B.
\newblock {T}he projectron: a bounded kernel-based perceptron.
\newblock In \emph{{P}roceedings of the 25th {I}nternational {C}onference on
  {M}achine {L}earning}, pp.\  720--727, 2008.

\bibitem[Pavlidis et~al.(2011)Pavlidis, Tasoulis, Adams, and
  Hand]{pavlidis:2011}
Pavlidis, N.~G., Tasoulis, D.~K., Adams, N.~M., and Hand, D.~J.
\newblock $\lambda$-perceptron: An adaptive classifier for data streams.
\newblock \emph{Pattern Recognition}, 44\penalty0 (1):\penalty0 78--96, 2011.

\bibitem[Shafieezadeh-Abadeh et~al.(2019)Shafieezadeh-Abadeh, Kuhn, and
  Esfahani]{shafieezadeh2019regularization}
Shafieezadeh-Abadeh, S., Kuhn, D., and Esfahani, P.~M.
\newblock Regularization via mass transportation.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (103):\penalty0 1--68, 2019.

\bibitem[{S}hen et~al.(2019){S}hen, {C}hen, and {G}iannakis]{shen:2019}
{S}hen, Y., {C}hen, T., and {G}iannakis, G.~B.
\newblock {R}andom feature-based online multi-kernel learning in environments
  with unknown dynamics.
\newblock \emph{{T}he {J}ournal of {M}achine {L}earning {R}esearch},
  20\penalty0 (1):\penalty0 773--808, 2019.

\bibitem[Tao et~al.(2019)Tao, Pan, Wu, and Tao]{tao:2019}
Tao, W., Pan, Z., Wu, G., and Tao, Q.
\newblock {T}he strength of {N}esterovâ€™s extrapolation in the individual
  convergence of nonsmooth optimization.
\newblock \emph{{I}{E}{E}{E} {T}ransactions on {N}eural {N}etworks and
  {L}earning {S}ystems}, 31\penalty0 (7):\penalty0 2557--2568, 2019.

\bibitem[Wang et~al.(2003)Wang, Fan, Yu, and Han]{wang:2003mining}
Wang, H., Fan, W., Yu, P.~S., and Han, J.
\newblock Mining concept-drifting data streams using ensemble classifiers.
\newblock In \emph{{P}roceedings of the ninth {A}{C}{M} {S}{I}{G}{K}{D}{D}
  {I}nternational {C}onference on {K}nowledge {D}iscovery and {D}ata {M}ining},
  pp.\  226--235, 2003.

\bibitem[Webb et~al.(2018)Webb, Lee, Goethals, and Petitjean]{webb:2018}
Webb, G.~I., Lee, L.~K., Goethals, B., and Petitjean, F.
\newblock Analyzing concept drift and shift from sample data.
\newblock \emph{Data Mining and Knowledge Discovery}, 32\penalty0 (5):\penalty0
  1179--1199, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Lu, and Zhou]{zhang2018adaptive}
Zhang, L., Lu, S., and Zhou, Z.-H.
\newblock {A}daptive online learning in dynamic environments.
\newblock In \emph{{P}roceedings of the 32nd {I}nternational {C}onference on
  {N}eural {I}nformation {P}rocessing {S}ystems}, pp.\  1330--1340, 2018.

\end{thebibliography}
