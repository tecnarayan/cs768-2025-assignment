


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}




@article{ramsauer2020hopfield,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}




@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={782--791},
  year={2021}
}



@article{an2020repulsive,
  title={Repulsive attention: Rethinking multi-head attention as bayesian inference},
  author={An, Bang and Lyu, Jie and Wang, Zhenyi and Li, Chunyuan and Hu, Changwei and Tan, Fei and Zhang, Ruiyi and Hu, Yifan and Chen, Changyou},
  journal={arXiv preprint arXiv:2009.09364},
  year={2020}
}


@article{panahi2021shapeshifter,
  title={Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices},
  author={Panahi, Aliakbar and Saeedi, Seyran and Arodz, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{sawant2020understanding,
  title={Understanding attention: in minds and machines},
  author={Sawant, Shriraj P and Singh, Shruti},
  journal={arXiv preprint arXiv:2012.02659},
  year={2020}
}


@article{geng2021attention,
  title={Is attention better than matrix decomposition?},
  author={Geng, Zhengyang and Guo, Meng-Hao and Chen, Hongxu and Li, Xia and Wei, Ke and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2109.04553},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{shen2021efficient,
  title={Efficient attention: Attention with linear complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3531--3539},
  year={2021}
}


@article{mardani2015subspace,
  title={Subspace learning and imputation for streaming big data matrices and tensors},
  author={Mardani, Morteza and Mateos, Gonzalo and Giannakis, Georgios B},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={10},
  pages={2663--2677},
  year={2015},
  publisher={IEEE}
}



@article{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:2104.14294},
  year={2021}
}









@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}



@inproceedings{pilanci2020neural,
  title={Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks},
  author={Pilanci, Mert and Ergen, Tolga},
  booktitle={International Conference on Machine Learning},
  pages={7695--7705},
  year={2020},
  organization={PMLR}
}

@article{stanley2004introduction,
  title={An introduction to hyperplane arrangements},
  author={Stanley, Richard P and others},
  journal={Geometric combinatorics},
  volume={13},
  number={389-496},
  pages={24},
  year={2004}
}

@article{sahiner2020vector,
  title={Vector-output relu neural network problems are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms},
  author={Sahiner, Arda and Ergen, Tolga and Pauly, John and Pilanci, Mert},
  journal={arXiv preprint arXiv:2012.13329},
  year={2020}
}

@article{fiat2019decoupling,
  title={Decoupling gating from linearity},
  author={Fiat, Jonathan and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1906.05032},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{zhang2021sparse,
  title={Sparse Attention with Linear Units},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  journal={arXiv preprint arXiv:2104.07012},
  year={2021}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and others},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{yorsh2021simpletron,
  title={SIMPLETRON: ELIMINATING SOFTMAX FROM ATTENTION COMPUTATION},
  author={Yorsh, Uladzislau and Kord{\'\i}k, Pavel and Kovalenko, Alexander},
  journal={arXiv preprint arXiv:2111.15588},
  year={2021}
}

@article{yorsh2021pureformer,
  title={Pureformer: Do We Even Need Attention?},
  author={Yorsh, Uladzislau and Kovalenko, Alexander},
  journal={arXiv preprint arXiv:2111.15588},
  year={2021}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@article{guibas2021adaptive,
  title={Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2111.13587},
  year={2021}
}

@article{ergen2020implicit,
  title={Implicit convex regularizers of cnn architectures: Convex optimization of two-and three-layer networks in polynomial time},
  author={Ergen, Tolga and Pilanci, Mert},
  journal={arXiv preprint arXiv:2006.14798},
  year={2020}
}

@article{sahiner2020convex,
  title={Convex regularization behind neural reconstruction},
  author={Sahiner, Arda and Mardani, Morteza and Ozturkler, Batu and Pilanci, Mert and Pauly, John},
  journal={arXiv preprint arXiv:2012.05169},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}

@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1806.00468},
  year={2018}
}

@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{tatsunami2021raftmlp,
  title={RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?},
  author={Tatsunami, Yuki and Taki, Masato},
  journal={arXiv preprint arXiv:2108.04384},
  year={2021}
}

@article{touvron2021resmlp,
  title={Resmlp: Feedforward networks for image classification with data-efficient training},
  author={Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Izacard, Gautier and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and others},
  journal={arXiv preprint arXiv:2105.03404},
  year={2021}
}

@misc{liu2021pay,
      title={Pay Attention to MLPs},
      author={Hanxiao Liu and Zihang Dai and David R. So and Quoc V. Le},
      year={2021},
      eprint={2105.08050},
}

@article{burer2005local,
  title={Local minima and convergence in low-rank semidefinite programming},
  author={Burer, Samuel and Monteiro, Renato DC},
  journal={Mathematical programming},
  volume={103},
  number={3},
  pages={427--444},
  year={2005},
  publisher={Springer}
}

@article{mardani2013decentralized,
  title={Decentralized sparsity-regularized rank minimization: Algorithms and applications},
  author={Mardani, Morteza and Mateos, Gonzalo and Giannakis, Georgios B},
  journal={IEEE Transactions on Signal Processing},
  volume={61},
  number={21},
  pages={5374--5388},
  year={2013},
  publisher={IEEE}
}

@article{boumal2020deterministic,
  title={Deterministic Guarantees for Burer-Monteiro Factorizations of Smooth Semidefinite Programs},
  author={Boumal, Nicolas and Voroninski, Vladislav and Bandeira, Afonso S},
  journal={Communications on Pure and Applied Mathematics},
  volume={73},
  number={3},
  pages={581--608},
  year={2020},
  publisher={Wiley Online Library}
}

@article{lee2021fnet,
  title={FNet: Mixing Tokens with Fourier Transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@article{rao2021global,
  title={Global filter networks for image classification},
  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{yu2021metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2111.11418},
  year={2021}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}


@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2667--2690},
  year={2019},
  organization={PMLR}
}

@article{shapiro2009semi,
  title={Semi-infinite programming, duality, discretization and optimality conditions},
  author={Shapiro, Alexander},
  journal={Optimization},
  volume={58},
  number={2},
  pages={133--161},
  year={2009},
  publisher={Taylor \& Francis}
}

@book{magnus2019matrix,
  title={Matrix differential calculus with applications in statistics and econometrics},
  author={Magnus, Jan R and Neudecker, Heinz},
  year={2019},
  publisher={John Wiley \& Sons}
}

@article{recht2010guaranteed,
  title={Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization},
  author={Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A},
  journal={SIAM review},
  volume={52},
  number={3},
  pages={471--501},
  year={2010},
  publisher={SIAM}
}

@article{journee2010low,
  title={Low-rank optimization on the cone of positive semidefinite matrices},
  author={Journ{\'e}e, Michel and Bach, Francis and Absil, P-A and Sepulchre, Rodolphe},
  journal={SIAM Journal on Optimization},
  volume={20},
  number={5},
  pages={2327--2351},
  year={2010},
  publisher={SIAM}
}

@article{parikh2016decomposable,
  title={A decomposable attention model for natural language inference},
  author={Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1606.01933},
  year={2016}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{likhosherstov2021expressive,
  title={On the Expressive Power of Self-Attention Matrices},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Weller, Adrian},
  journal={arXiv preprint arXiv:2106.03764},
  year={2021}
}

@article{yun2020n,
  title={$ O (n) $ Connections are Expressive Enough: Universal Approximability of Sparse Transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2006.04862},
  year={2020}
}

@article{cordonnier2019relationship,
  title={On the relationship between self-attention and convolutional layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:1911.03584},
  year={2019}
}

@inproceedings{kim2021lipschitz,
  title={The lipschitz constant of self-attention},
  author={Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={5562--5571},
  year={2021},
  organization={PMLR}
}

@inproceedings{ergen2021global,
  title={Global optimality beyond two layers: Training deep relu networks via convex programs},
  author={Ergen, Tolga and Pilanci, Mert},
  booktitle={International Conference on Machine Learning},
  pages={2993--3003},
  year={2021},
  organization={PMLR}
}

@article{ergen2021demystifying,
  title={Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization},
  author={Ergen, Tolga and Sahiner, Arda and Ozturkler, Batu and Pauly, John and Mardani, Morteza and Pilanci, Mert},
  journal={arXiv preprint arXiv:2103.01499},
  year={2021}
}

@article{sahiner2021hidden,
  title={Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions},
  author={Sahiner, Arda and Ergen, Tolga and Ozturkler, Batu and Bartan, Burak and Pauly, John and Mardani, Morteza and Pilanci, Mert},
  journal={arXiv preprint arXiv:2107.05680},
  year={2021}
}

@article{bai2022efficient,
  title={Efficient Global Optimization of Two-layer ReLU Networks: Quadratic-time Algorithms and Adversarial Training},
  author={Bai, Yatong and Gautam, Tanmay and Sojoudi, Somayeh},
  journal={arXiv preprint arXiv:2201.01965},
  year={2022}
}

@article{xiong2021nystr,
  title={Nystr$\backslash$" omformer: A Nystr$\backslash$" om-Based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  journal={arXiv preprint arXiv:2102.03902},
  year={2021}
}

@article{mosbach2020stability,
  title={On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines},
  author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  journal={arXiv preprint arXiv:2006.04884},
  year={2020}
}

@article{kirichenko2022last,
  title={Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations},
  author={Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2204.02937},
  year={2022}
}

@article{mishkin2022fast,
  title={Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions},
  author={Mishkin, Aaron and Sahiner, Arda and Pilanci, Mert},
  journal={arXiv preprint arXiv:2202.01331},
  year={2022}
}

@article{toh2010accelerated,
  title={An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems},
  author={Toh, Kim-Chuan and Yun, Sangwoon},
  journal={Pacific Journal of optimization},
  volume={6},
  number={615-640},
  pages={15},
  year={2010},
  publisher={Citeseer}
}

@inproceedings{jaggi2013revisiting,
  title={Revisiting Frank-Wolfe: Projection-free sparse convex optimization},
  author={Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={427--435},
  year={2013},
  organization={PMLR}
}

@article{candes2010power,
  title={The power of convex relaxation: Near-optimal matrix completion},
  author={Cand{\`e}s, Emmanuel J and Tao, Terence},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={5},
  pages={2053--2080},
  year={2010},
  publisher={IEEE}
}


@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}