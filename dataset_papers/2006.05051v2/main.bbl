\begin{thebibliography}{}

\bibitem[Achiam et~al., 2017]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 22--31. JMLR. org.

\bibitem[Agrawal and Devanur, 2014]{AgrawalDevanurEC14}
Agrawal, S. and Devanur, N.~R. (2014).
\newblock Bandits with concave rewards and convex knapsacks.
\newblock In {\em Proceedings of the 15th ACM Conference on Economics and
  Computatxion (EC)}.

\bibitem[Altman, 1999]{altman-constrainedMDP}
Altman, E. (1999).
\newblock {\em Constrained Markov Decision Processes}.
\newblock Chapman and Hall.

\bibitem[Azar et~al., 2017]{AzarOsMu17}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}.

\bibitem[Babaioff et~al., 2015]{DynPricing-ec12}
Babaioff, M., Dughmi, S., Kleinberg, R.~D., and Slivkins, A. (2015).
\newblock Dynamic pricing with limited supply.
\newblock {\em TEAC}, 3(1):4.
\newblock Special issue for \emph{13th ACM EC}, 2012.

\bibitem[Badanidiyuru et~al., 2018]{BwK-focs13}
Badanidiyuru, A., Kleinberg, R., and Slivkins, A. (2018).
\newblock Bandits with knapsacks.
\newblock {\em Journal of the ACM}, 65(3):13:1--13:55.
\newblock Preliminary version in {\em FOCS 2013}.

\bibitem[Bellman, 1957]{Bellman1957}
Bellman, R. (1957).
\newblock A markovian decision process.
\newblock {\em Indiana Univ. Math. J.}, 6:679--684.

\bibitem[Besbes and Zeevi, 2009]{BZ09}
Besbes, O. and Zeevi, A. (2009).
\newblock Dynamic pricing without knowing the demand function: Risk bounds and
  near-optimal algorithms.
\newblock {\em Operations Research}, 57(6):1407--1420.

\bibitem[Besbes and Zeevi, 2011]{BesbesZeevi-OR11}
Besbes, O. and Zeevi, A. (2011).
\newblock On the minimax complexity of pricing in a changing environment.
\newblock {\em Operations Reseach}, 59(1):66--79.

\bibitem[Cesa-Bianchi and Lugosi, 2006]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G. (2006).
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[Cheung, 2019]{Cheung19}
Cheung, W.~C. (2019).
\newblock Regret minimization for reinforcement learning with vectorial
  feedback and complex objectives.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Dann et~al., 2017]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E. (2017).
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5713--5723.

\bibitem[Ding et~al., 2020]{ding2020provably}
Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovi{\'c}, M.~R. (2020).
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock {\em arXiv preprint arXiv:2003.00534}.

\bibitem[Efroni et~al., 2020]{efroni2020exploration}
Efroni, Y., Mannor, S., and Pirotta, M. (2020).
\newblock Exploration-exploitation in constrained mdps.
\newblock {\em arXiv preprint arXiv:2003.02189}.

\bibitem[Jaksch et~al., 2010]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P. (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600.

\bibitem[Jin et~al., 2019]{jin2019provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2019).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1907.05388}.

\bibitem[Kearns and Singh, 2002]{Kearns2002}
Kearns, M. and Singh, S. (2002).
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine Learning}, 49(2):209--232.

\bibitem[Le et~al., 2019]{Hoang2019BPLUC}
Le, H.~M., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock {\em CoRR}, abs/1903.08738.

\bibitem[Leike et~al., 2017]{leike2017ai}
Leike, J., Martic, M., Krakovna, V., Ortega, P.~A., Everitt, T., Lefrancq, A.,
  Orseau, L., and Legg, S. (2017).
\newblock Ai safety gridworlds.
\newblock {\em arXiv preprint arXiv:1711.09883}.

\bibitem[Mao et~al., 2016]{Mao2016RLSystems}
Mao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016).
\newblock Resource management with deep reinforcement learning.
\newblock In {\em Proceedings of the 15th ACM Workshop on Hot Topics in
  Networks}, page 50–56, New York, NY, USA. Association for Computing
  Machinery.

\bibitem[Miryoosefi et~al., 2019]{MiryoosefiBrDaDuSc19}
Miryoosefi, S., Brantley, K., Daume~III, H., Dudik, M., and Schapire, R.~E.
  (2019).
\newblock Reinforcement learning with convex constraints.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Mnih et~al., 2016]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K. (2016).
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1928--1937.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[Qiu et~al., 2020]{qiu2020upper}
Qiu, S., Wei, X., Yang, Z., Ye, J., and Wang, Z. (2020).
\newblock Upper confidence primal-dual optimization: Stochastically constrained
  markov decision processes with adversarial losses and unknown transitions.
\newblock {\em arXiv preprint arXiv:2003.00660}.

\bibitem[Ray et~al., 2020]{Ray2019}
Ray, A., Achiam, J., and Amodei, D. (2020).
\newblock Benchmarking safe exploration in deep reinforcement learning.
\newblock https://cdn.openai.com/safexp-short.pdf.
\newblock Accessed March 11, 2020.

\bibitem[Rosenberg and Mansour, 2019]{rosenberg2019online}
Rosenberg, A. and Mansour, Y. (2019).
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  5478--5486.

\bibitem[Schulman et~al., 2015]{Schulman2015TRPO}
Schulman, J., Levine, S., Moritz, P., Jordan, M.~I., and Abbeel, P. (2015).
\newblock Trust region policy optimization.
\newblock {\em CoRR}, abs/1502.05477.

\bibitem[Singh et~al., 2020]{singh2020learning}
Singh, R., Gupta, A., and Shroff, N.~B. (2020).
\newblock Learning in markov decision processes under constraints.
\newblock {\em arXiv preprint arXiv:2002.12435}.

\bibitem[Slivkins, 2019]{slivkins-MABbook}
Slivkins, A. (2019).
\newblock Introduction to multi-armed bandits.
\newblock {\em Foundations and Trends® in Machine Learning}, 12(1-2):1--286.
\newblock Also available at {\tt https://arxiv.org/abs/1904.07272}.

\bibitem[Song and Sun, 2019]{song2019efficient}
Song, Z. and Sun, W. (2019).
\newblock Efficient model-free reinforcement learning in metric spaces.
\newblock {\em arXiv preprint arXiv:1905.00475}.

\bibitem[Sun et~al., 2019]{sun2019provably}
Sun, W., Vemula, A., Boots, B., and Bagnell, J.~A. (2019).
\newblock Provably efficient imitation learning from observation alone.
\newblock {\em arXiv preprint arXiv:1905.10948}.

\bibitem[Sutton, 1991]{dyna1991stutton}
Sutton, R.~S. (1991).
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock {\em SIGART Bull.}, 2(4):160–163.

\bibitem[Sutton and Barto, 1998]{SuttonBa98}
Sutton, R.~S. and Barto, A.~G. (1998).
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock MIT Press, first edition.

\bibitem[Sutton and Barto, 2018]{SuttonBa18}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock MIT Press, second edition.

\bibitem[Syed and Schapire, 2007]{SyedSchapire2008}
Syed, U. and Schapire, R.~E. (2007).
\newblock A game-theoretic approach to apprenticeship learning.
\newblock In {\em Proceedings of the 20th International Conference on Neural
  Information Processing Systems}, NIPS’07, page 1449–1456, Red Hook, NY,
  USA. Curran Associates Inc.

\bibitem[Tarbouriech and Lazaric, 2019]{tarbouriech2019active}
Tarbouriech, J. and Lazaric, A. (2019).
\newblock Active exploration in markov decision processes.
\newblock {\em arXiv preprint arXiv:1902.11199}.

\bibitem[Tessler et~al., 2019]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S. (2019).
\newblock Reward constrained policy optimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Wang et~al., 2014]{Wang-OR14}
Wang, Z., Deng, S., and Ye, Y. (2014).
\newblock Close the gaps: {A} learning-while-doing algorithm for single-product
  revenue management problems.
\newblock {\em Operations Research}, 62(2):318--331.

\bibitem[Zheng and Ratliff, 2020]{zheng2020constrained}
Zheng, L. and Ratliff, L.~J. (2020).
\newblock Constrained upper confidence reinforcement learning.
\newblock {\em arXiv preprint arXiv:2001.09377}.

\bibitem[Ziebart et~al., 2008]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K. (2008).
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA.

\bibitem[Zinkevich, 2003]{Zinkevich03}
Zinkevich, M. (2003).
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning (ICML)}.

\end{thebibliography}
