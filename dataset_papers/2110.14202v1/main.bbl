\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Vuorio et~al.(2019)Vuorio, Sun, Hu, and Lim]{vuorio2019multimodal}
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph~J. Lim.
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Fifty et~al.(2020)Fifty, Amid, Zhao, Yu, Anil, and
  Finn]{fifty2020measuring}
Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea
  Finn.
\newblock Measuring and harnessing transference in multi-task learning.
\newblock \emph{arXiv preprint arXiv:2010.15413}, 2020.

\bibitem[Zhang and Yang(2021)]{zhang2021survey}
Yu~Zhang and Qiang Yang.
\newblock A survey on multi-task learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2021.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Sharif~Razavian et~al.(2014)Sharif~Razavian, Azizpour, Sullivan, and
  Carlsson]{sharif2014cnn}
Ali Sharif~Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pages 806--813, 2014.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Malekzadeh et~al.(2017)Malekzadeh, Abdollahzadeh, Nejati, and
  Cheung]{malekzadeh2017aircraft}
Touba Malekzadeh, Milad Abdollahzadeh, Hossein Nejati, and Ngai-Man Cheung.
\newblock Aircraft fuselage defect detection using deep neural networks.
\newblock \emph{arXiv preprint arXiv:1712.09213}, 2017.

\bibitem[Yan et~al.(2019)Yan, Chen, Xu, Wang, Liang, and Lin]{Yan_2019_ICCV}
Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, and Liang Lin.
\newblock Meta r-cnn: Towards general solver for instance-level low-shot
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, October 2019.

\bibitem[Triantafillou et~al.(2020)Triantafillou, Zhu, Dumoulin, Lamblin, Evci,
  Xu, Goroshin, Gelada, Swersky, Manzagol, et~al.]{triantafillou2019meta}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci,
  Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine
  Manzagol, et~al.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Thrun and Pratt(2012)]{thrun2012learning}
Sebastian Thrun and Lorien Pratt.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 4080--4090, 2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
  Wierstra]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan
  Wierstra.
\newblock Matching networks for one shot learning.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 3637--3645, 2016.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
Flood Sung, Yongxin Yang, Li~Zhang, Tao Xiang, Philip~HS Torr, and Timothy~M
  Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1199--1208, 2018.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{chen2019closer}
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang~Frank Wang, and Jia-Bin
  Huang.
\newblock A closer look at few-shot classification.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kye et~al.(2020)Kye, Lee, Kim, and Hwang]{kye2020meta}
Seong~Min Kye, Hae~Beom Lee, Hoirin Kim, and Sung~Ju Hwang.
\newblock Meta-learned confidence for few-shot learning.
\newblock \emph{arXiv e-prints}, pages arXiv--2002, 2020.

\bibitem[Ravichandran et~al.(2019)Ravichandran, Bhotika, and
  Soatto]{ravichandran2019few}
Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto.
\newblock Few-shot learning with embedded class models and shot-free meta
  training.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 331--339, 2019.

\bibitem[Ravi and Larochelle(2017)]{Ravi2017OptimizationAA}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Munkhdalai and Yu(2017)]{munkhdalai2017meta}
Tsendsuren Munkhdalai and Hong Yu.
\newblock Meta networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2554--2563. PMLR, 2017.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Rusu et~al.(2018)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2018meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{li2017meta}
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li.
\newblock Meta-sgd: Learning to learn quickly for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Baik et~al.(2020)Baik, Hong, and Lee]{Baik_2020_CVPR}
Sungyong Baik, Seokil Hong, and Kyoung~Mu Lee.
\newblock Learning to forget for meta-learning.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2020.

\bibitem[Antoniou et~al.(2017)Antoniou, Storkey, and Edwards]{antoniou2017data}
Antreas Antoniou, Amos Storkey, and Harrison Edwards.
\newblock Data augmentation generative adversarial networks.
\newblock 2017.

\bibitem[Hariharan and Girshick(2017)]{hariharan2017low}
Bharath Hariharan and Ross Girshick.
\newblock Low-shot visual recognition by shrinking and hallucinating features.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 3018--3027, 2017.

\bibitem[Qiao et~al.(2018)Qiao, Liu, Shen, and Yuille]{qiao2018few}
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan~L Yuille.
\newblock Few-shot image recognition by predicting parameters from activations.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7229--7238, 2018.

\bibitem[Guo and Cheung(2020)]{guo2020attentive}
Yiluan Guo and Ngai-Man Cheung.
\newblock Attentive weights generation for few shot learning via information
  maximization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13499--13508, 2020.

\bibitem[Caruana(1993)]{Caruana93multitasklearning}
Richard Caruana.
\newblock Multitask learning: A knowledge-based source of inductive bias.
\newblock In \emph{Proceedings of the Tenth International Conference on Machine
  Learning}, pages 41--48. Morgan Kaufmann, 1993.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Duong et~al.(2015)Duong, Cohn, Bird, and Cook]{duong2015low}
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook.
\newblock Low resource dependency parsing: Cross-lingual parameter sharing in a
  neural network parser.
\newblock In \emph{Proceedings of the 53rd annual meeting of the Association
  for Computational Linguistics and the 7th international joint conference on
  natural language processing (volume 2: short papers)}, pages 845--850, 2015.

\bibitem[Yang and Hospedales(2016{\natexlab{a}})]{yang2016trace}
Yongxin Yang and Timothy~M Hospedales.
\newblock Trace norm regularised deep multi-task learning.
\newblock \emph{arXiv preprint arXiv:1606.04038}, 2016{\natexlab{a}}.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and
  Hebert]{misra2016cross}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3994--4003, 2016.

\bibitem[Guo et~al.(2020)Guo, Lee, and Ulbricht]{guo2020learning}
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht.
\newblock Learning to branch for multi-task learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3854--3863. PMLR, 2020.

\bibitem[Sun et~al.(2019)Sun, Panda, Feris, and Saenko]{sun2019adashare}
Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko.
\newblock Adashare: Learning what to share for efficient deep multi-task
  learning.
\newblock \emph{arXiv preprint arXiv:1911.12423}, 2019.

\bibitem[Vandenhende et~al.(2019)Vandenhende, Georgoulis, De~Brabandere, and
  Van~Gool]{vandenhende2019branched}
Simon Vandenhende, Stamatios Georgoulis, Bert De~Brabandere, and Luc Van~Gool.
\newblock Branched multi-task networks: deciding what layers to share.
\newblock \emph{arXiv preprint arXiv:1904.02920}, 2019.

\bibitem[Bingel and S{\o}gaard(2017)]{bingel2017identifying}
Joachim Bingel and Anders S{\o}gaard.
\newblock Identifying beneficial task relations for multi-task learning in deep
  neural networks.
\newblock \emph{arXiv preprint arXiv:1702.08303}, 2017.

\bibitem[Standley et~al.(2020)Standley, Zamir, Chen, Guibas, Malik, and
  Savarese]{standley2020tasks}
Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and
  Silvio Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In \emph{International Conference on Machine Learning}, pages
  9120--9132. PMLR, 2020.

\bibitem[Yang and Hospedales(2016{\natexlab{b}})]{yang2016deep}
Yongxin Yang and Timothy Hospedales.
\newblock Deep multi-task representation learning: A tensor factorisation
  approach.
\newblock \emph{arXiv preprint arXiv:1605.06391}, 2016{\natexlab{b}}.

\bibitem[Maninis et~al.(2019)Maninis, Radosavovic, and
  Kokkinos]{maninis2019attentive}
Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos.
\newblock Attentive single-tasking of multiple tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1851--1860, 2019.

\bibitem[Chen et~al.(2020)Chen, Wu, Li, Li, Zhan, and Chung]{chen2020closer}
Jiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai Li, Li-Ming Zhan, and Fu-lai Chung.
\newblock A closer look at the training strategy for modern meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Baltru{\v{s}}aitis et~al.(2018)Baltru{\v{s}}aitis, Ahuja, and
  Morency]{baltruvsaitis2018multimodal}
Tadas Baltru{\v{s}}aitis, Chaitanya Ahuja, and Louis-Philippe Morency.
\newblock Multimodal machine learning: A survey and taxonomy.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (2):\penalty0 423--443, 2018.

\bibitem[Liu et~al.(2019)Liu, Liang, and Gitter]{liu2019loss}
Shengchao Liu, Yingyu Liang, and Anthony Gitter.
\newblock Loss-balanced task weighting to reduce negative transfer in
  multi-task learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 9977--9978, 2019.

\bibitem[Tran et~al.(2019)Tran, Nguyen, and Hassner]{tran2019transferability}
Anh~T Tran, Cuong~V Nguyen, and Tal Hassner.
\newblock Transferability and hardness of supervised classification tasks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1395--1405, 2019.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and
  Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Davis Blalock, Jose Javier~Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{Sandler_2018_CVPR}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2018.

\bibitem[Simon et~al.(2020)Simon, Koniusz, Nock, and
  Harandi]{simon2020modulating}
Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi.
\newblock On modulating the gradient for meta-learning.
\newblock In \emph{European Conference on Computer Vision}, pages 556--572.
  Springer, 2020.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{lake2011one}
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodriguez, and
  Lacoste]{oreshkin2018tadam}
Boris~N Oreshkin, Pau Rodriguez, and Alexandre Lacoste.
\newblock Tadam: task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 719--729, 2018.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and
  Vedaldi]{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pages 818--833.
  Springer, 2014.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Antoniou et~al.(2019)Antoniou, Edwards, and Storkey]{antoniou2018how}
Antreas Antoniou, Harrison Edwards, and Amos Storkey.
\newblock How to train your {MAML}.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJGven05Y7}.

\bibitem[Van~der Maaten and Hinton(2008)]{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\end{thebibliography}
