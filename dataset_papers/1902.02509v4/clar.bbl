\providecommand{\AC}{{A.-C}}\providecommand{\AM}{{A.-M}}\providecommand{\CA}{{C.-A}}\providecommand{\CH}{{C.-H}}\providecommand{\CN}{{C.-N}}\providecommand{\CC}{{C.-C}}\providecommand{\CJ}{{C.-J}}\providecommand{\DY}{{D.-Y}}\providecommand{\HJ}{{H.-J}}\providecommand{\HT}{{H.-T}}\providecommand{\HY}{{H.-Y}}\providecommand{\JC}{{J.-C}}\providecommand{\JP}{{J.-P}}\providecommand{\JB}{{J.-B}}\providecommand{\JF}{{J.-F}}\providecommand{\JJ}{{J.-J}}\providecommand{\JL}{{J.-L}}\providecommand{\JM}{{J.-M}}\providecommand{\JS}{{J.-S}}\providecommand{\JY}{{J.-Y}}\providecommand{\KC}{{K.-C}}\providecommand{\KW}{{K.-W}}\providecommand{\KR}{{K.-R}}\providecommand{\LJ}{{L.-J}}\providecommand{\MR}{{M.-R}}\providecommand{\PL}{{P.-L}}\providecommand{\RE}{{R.-E}}\providecommand{\SJ}{{S.-J}}\providecommand{\XR}{{X.-R}}\providecommand{\WX}{{W.-X}}\providecommand{\YX}{{Y.-X}}
\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Argyriou et~al.(2008)Argyriou, Evgeniou, and
  Pontil]{Argyriou_Evgeniou_Pontil08}
A.~Argyriou, T.~Evgeniou, and M.~Pontil.
\newblock Convex multi-task feature learning.
\newblock \emph{Machine Learning}, 73\penalty0 (3):\penalty0 243--272, 2008.

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, and
  Obozinski]{Bach_Jenatton_Mairal_Obozinski12}
F.~Bach, R.~Jenatton, J.~Mairal, and G.~Obozinski.
\newblock Convex optimization with sparsity-inducing norms.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (1):\penalty0 1--106, 2012.

\bibitem[Beck(2017)]{Beck17}
A.~Beck.
\newblock \emph{First-Order Methods in Optimization}, volume~25.
\newblock SIAM, 2017.

\bibitem[Beck and Teboulle(2012)]{Beck_Teboulle12}
A.~Beck and M.~Teboulle.
\newblock Smoothing and first order methods: A unified framework.
\newblock \emph{SIAM J. Optim.}, 22\penalty0 (2):\penalty0 557--580, 2012.

\bibitem[Belloni et~al.(2011)Belloni, Chernozhukov, and
  Wang]{Belloni_Chernozhukov_Wang11}
A.~Belloni, V.~Chernozhukov, and L.~Wang.
\newblock Square-root {Lasso}: pivotal recovery of sparse signals via conic
  programming.
\newblock \emph{Biometrika}, 98\penalty0 (4):\penalty0 791--806, 2011.

\bibitem[Bickel et~al.(2009)Bickel, Ritov, and
  Tsybakov]{Bickel_Ritov_Tsybakov09}
P.~J. Bickel, Y.~Ritov, and A.~B. Tsybakov.
\newblock Simultaneous analysis of {Lasso} and {D}antzig selector.
\newblock \emph{Ann. Statist.}, 37\penalty0 (4):\penalty0 1705--1732, 2009.

\bibitem[Boyd and Vandenberghe(2004)]{Boyd_Vandenberghe04}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Chambolle and Pock(2011)]{Chambolle_Pock11}
A.~Chambolle and T.~Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{J. Math. Imaging Vis.}, 40\penalty0 (1):\penalty0 120--145,
  2011.

\bibitem[Chen and Banerjee(2017)]{Chen_Banerjee17}
S.~Chen and A.~Banerjee.
\newblock Alternating estimation for structured high-dimensional multi-response
  models.
\newblock In \emph{NIPS}, pages 2838--2848, 2017.

\bibitem[Dalalyan et~al.(2013)Dalalyan, Hebiri, Meziani, and
  Salmon]{Dalalyan_Hebiri_Meziani_Salmon13}
A.~S. Dalalyan, M.~Hebiri, K.~Meziani, and J.~Salmon.
\newblock Learning heteroscedastic models by convex programming under group
  sparsity.
\newblock In \emph{ICML}, 2013.

\bibitem[Daye et~al.(2012)Daye, Chen, and Li]{Daye_Chen_Li12}
J.~Daye, J.~Chen, and H.~Li.
\newblock High-dimensional heteroscedastic regression with an application to
  {eQTL} data analysis.
\newblock \emph{Biometrics}, 68\penalty0 (1):\penalty0 316--326, 2012.

\bibitem[{El Ghaoui} et~al.(2012){El Ghaoui}, Viallon, and
  Rabbani]{ElGhaoui_Viallon_Rabbani12}
L.~{El Ghaoui}, V.~Viallon, and T.~Rabbani.
\newblock Safe feature elimination in sparse supervised learning.
\newblock \emph{J. Pacific Optim.}, 8\penalty0 (4):\penalty0 667--698, 2012.

\bibitem[Engemann and Gramfort(2015)]{Engemann_Gramfort14}
D.~A. Engemann and A.~Gramfort.
\newblock Automated model selection in covariance estimation and spatial
  whitening of {MEG} and {EEG} signals.
\newblock \emph{NeuroImage}, 108:\penalty0 328--342, 2015.

\bibitem[Fan and Lv(2008)]{Fan_Lv2008}
J.~Fan and J.~Lv.
\newblock Sure independence screening for ultrahigh dimensional feature space.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 70\penalty0
  (5):\penalty0 849--911, 2008.

\bibitem[Fercoq et~al.(2015)Fercoq, Gramfort, and
  Salmon]{Fercoq_Gramfort_Salmon15}
O.~Fercoq, A.~Gramfort, and J.~Salmon.
\newblock Mind the duality gap: safer rules for the lasso.
\newblock In \emph{ICML}, pages 333--342, 2015.

\bibitem[Friedman et~al.(2008)Friedman, Hastie, and
  Tibshirani]{Friedman_Hastie_Tibshirani08}
J.~Friedman, T.~J. Hastie, and R.~Tibshirani.
\newblock Sparse inverse covariance estimation with the graphical lasso.
\newblock \emph{Biostatistics}, 9\penalty0 (3):\penalty0 432--441, 2008.

\bibitem[Gramfort et~al.(2013)Gramfort, Strohmeier, Haueisen,
  H{\"a}m{\"a}l{\"a}inen, and
  Kowalski]{Gramfort_Strohmeier_Haueisen_Hamalainen_Kowalski13}
A.~Gramfort, D.~Strohmeier, J.~Haueisen, M.~S. H{\"a}m{\"a}l{\"a}inen, and
  M.~Kowalski.
\newblock Time-frequency mixed-norm estimates: Sparse {M/EEG} imaging with
  non-stationary source activations.
\newblock \emph{NeuroImage}, 70:\penalty0 410--422, 2013.

\bibitem[Gramfort et~al.(2014)Gramfort, Luessi, Larson, Engemann, Strohmeier,
  Brodbeck, Parkkonen, and H{\"a}m{\"a}l{\"a}inen]{mne}
A.~Gramfort, M.~Luessi, E.~Larson, D.~A. Engemann, D.~Strohmeier, C.~Brodbeck,
  L.~Parkkonen, and M.~S. H{\"a}m{\"a}l{\"a}inen.
\newblock {MNE} software for processing {MEG} and {EEG} data.
\newblock \emph{NeuroImage}, 86:\penalty0 446 -- 460, 2014.
\newblock \doi{http://dx.doi.org/10.1016/j.neuroimage.2013.10.027}.

\bibitem[Huber(1981)]{Huber81}
P.~J. Huber.
\newblock \emph{Robust Statistics}.
\newblock John Wiley \& Sons Inc., 1981.

\bibitem[Huber and Dutter(1974)]{Huber_Dutter74}
P.~J. Huber and R.~Dutter.
\newblock Numerical solution of robust regression problems.
\newblock In \emph{Compstat 1974 ({P}roc. {S}ympos. {C}omputational {S}tatist.,
  {U}niv. {V}ienna, {V}ienna, 1974)}, pages 165--172. Physica Verlag, Vienna,
  1974.

\bibitem[Johnson and Guestrin(2015)]{Johnson_Guestrin15}
T.~B. Johnson and C.~Guestrin.
\newblock Blitz: A principled meta-algorithm for scaling sparse optimization.
\newblock In \emph{ICML}, pages 1171--1179, 2015.

\bibitem[Kolar and Sharpnack(2012)]{Kolar_Sharpnack12}
M.~Kolar and J.~Sharpnack.
\newblock Variance function estimation in high-dimensions.
\newblock In \emph{ICML}, pages 1447--1454, 2012.

\bibitem[Lam et~al.(2015)Lam, Pitrou, and Seibert]{Lam_Pitrou_Seibert15}
S.~K. Lam, A.~Pitrou, and S.~Seibert.
\newblock {Numba: A LLVM-based Python JIT Compiler}.
\newblock In \emph{Proceedings of the Second Workshop on the LLVM Compiler
  Infrastructure in HPC}, pages 1--6. ACM, 2015.

\bibitem[Lee and Liu(2012)]{Lee_Liu12}
W.~Lee and Y.~Liu.
\newblock Simultaneous multiple response regression and inverse covariance
  matrix estimation via penalized {Gaussian} maximum likelihood.
\newblock \emph{Journal of multivariate analysis}, 111:\penalty0 241--255,
  2012.

\bibitem[Massias et~al.(2018{\natexlab{a}})Massias, Fercoq, Gramfort, and
  Salmon]{Massias_Fercoq_Gramfort_Salmon17}
M.~Massias, O.~Fercoq, A.~Gramfort, and J.~Salmon.
\newblock Generalized concomitant multi-task lasso for sparse multimodal
  regression.
\newblock In \emph{AISTATS}, volume~84, pages 998--1007, 2018{\natexlab{a}}.

\bibitem[Massias et~al.(2018{\natexlab{b}})Massias, Gramfort, and
  Salmon]{Massias_Gramfort_Salmon18}
M.~Massias, A.~Gramfort, and J.~Salmon.
\newblock {Celer: a fast solver for the Lasso with dual extrapolation}.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Molstad(2019)]{Molstad19}
A.~J. Molstad.
\newblock Insights and algorithms for the multivariate square-root lasso.
\newblock \emph{arXiv preprint arXiv:1909.05041}, 2019.

\bibitem[Moreau(1965)]{Moreau65}
{J.-J}. Moreau.
\newblock Proximit\'e et dualit\'e dans un espace hilbertien.
\newblock \emph{Bull. Soc. Math. France}, 93:\penalty0 273--299, 1965.

\bibitem[Ndiaye et~al.(2015)Ndiaye, Fercoq, Gramfort, and
  Salmon]{Ndiaye_Fercoq_Gramfort_Salmon15}
E.~Ndiaye, O.~Fercoq, A.~Gramfort, and J.~Salmon.
\newblock Gap safe screening rules for sparse multi-task and multi-class
  models.
\newblock In \emph{NIPS}, pages 811--819, 2015.

\bibitem[Ndiaye et~al.(2017)Ndiaye, Fercoq, Gramfort, {Lecl\`ere}, and
  Salmon]{Ndiaye_Fercoq_Gramfort_Leclere_Salmon16}
E.~Ndiaye, O.~Fercoq, A.~Gramfort, V.~{Lecl\`ere}, and J.~Salmon.
\newblock Efficient smoothed concomitant lasso estimation for high dimensional
  regression.
\newblock \emph{Journal of Physics: Conference Series}, 904\penalty0
  (1):\penalty0 012006, 2017.

\bibitem[Nesterov(2005)]{Nesterov05}
Y.~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Math. Program.}, 103\penalty0 (1):\penalty0 127--152, 2005.

\bibitem[Obozinski et~al.(2010)Obozinski, Taskar, and
  Jordan]{Obozinski_Taskar_Jordan10}
G.~Obozinski, B.~Taskar, and M.~I. Jordan.
\newblock Joint covariate selection and joint subspace selection for multiple
  classification problems.
\newblock \emph{Statistics and Computing}, 20\penalty0 (2):\penalty0 231--252,
  2010.

\bibitem[Ou et~al.(2009)Ou, H{\"a}mal{\"a}inen, and
  Golland]{Ou_Hamalainen_Golland2009}
W.~Ou, M.~H{\"a}mal{\"a}inen, and P.~Golland.
\newblock A distributed spatio-temporal {EEG}/{MEG} inverse solver.
\newblock \emph{NeuroImage}, 44\penalty0 (3):\penalty0 932--946, Feb 2009.

\bibitem[Owen(2007)]{Owen07}
A.~B. Owen.
\newblock A robust hybrid of lasso and ridge regression.
\newblock \emph{Contemporary Mathematics}, 443:\penalty0 59--72, 2007.

\bibitem[Parikh et~al.(2013)Parikh, Boyd, Chu, Peleato, and
  Eckstein]{Parikh_Boyd_Chu_Peleato_Eckstein13}
N.~Parikh, S.~Boyd, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Proximal algorithms.
\newblock \emph{Foundations and Trends in Machine Learning}, 1\penalty0
  (3):\penalty0 1--108, 2013.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{Pedregosa_etal11}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2825--2830, 2011.

\bibitem[Rai et~al.(2012)Rai, Kumar, and Daume]{Rai_Kumar_Daume12}
P.~Rai, A.~Kumar, and H.~Daume.
\newblock Simultaneously leveraging output and task structures for
  multiple-output regression.
\newblock In \emph{NIPS}, pages 3185--3193, 2012.

\bibitem[Rothman et~al.(2010)Rothman, Levina, and Zhu]{Rothman_Levina_Zhu10}
A.~J. Rothman, E.~Levina, and J.~Zhu.
\newblock Sparse multivariate regression with covariance estimation.
\newblock \emph{Journal of Computational and Graphical Statistics}, 19\penalty0
  (4):\penalty0 947--962, 2010.

\bibitem[Sun and Zhang(2012)]{Sun_Zhang12}
T.~Sun and \CH. Zhang.
\newblock Scaled sparse linear regression.
\newblock \emph{Biometrika}, 99\penalty0 (4):\penalty0 879--898, 2012.

\bibitem[Tibshirani(1996)]{Tibshirani96}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 58\penalty0
  (1):\penalty0 267--288, 1996.

\bibitem[Tibshirani et~al.(2012)Tibshirani, Bien, Friedman, Hastie, Simon,
  Taylor, and Tibshirani]{Tibshirani_Bien_Friedman_Hastie_Simon_Tibshirani12}
R.~Tibshirani, J.~Bien, J.~Friedman, T.~J. Hastie, N.~Simon, J.~Taylor, and
  R.~J. Tibshirani.
\newblock Strong rules for discarding predictors in lasso-type problems.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 74\penalty0
  (2):\penalty0 245--266, 2012.

\bibitem[Tseng(2001)]{Tseng01}
P.~Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock \emph{J. Optim. Theory Appl.}, 109\penalty0 (3):\penalty0 475--494,
  2001.

\bibitem[Tseng and Yun(2009)]{Tseng_Yun09}
P.~Tseng and S.~Yun.
\newblock Block-coordinate gradient descent method for linearly constrained
  nonsmooth separable optimization.
\newblock \emph{J. Optim. Theory Appl.}, 140\penalty0 (3):\penalty0 513, 2009.

\bibitem[{van de Geer}(2016)]{vandeGeer16}
S.~{van de Geer}.
\newblock \emph{Estimation and testing under sparsity}, volume 2159 of
  \emph{Lecture Notes in Mathematics}.
\newblock Springer, 2016.
\newblock Lecture notes from the 45th Probability Summer School held in
  Saint-Four, 2015, \'Ecole d'\'Et\'e de Probabilit\'es de Saint-Flour.

\bibitem[{van de Geer} and Stucky(2016)]{vandeGeer_Stucky16}
S.~{van de Geer} and B.~Stucky.
\newblock $\chi$ 2-confidence sets in high-dimensional regression.
\newblock In \emph{Statistical analysis for high-dimensional data}, pages
  279--306. Springer, 2016.

\bibitem[Wagener and Dette(2012)]{Wagener_Dette12}
J.~Wagener and H.~Dette.
\newblock Bridge estimators and the adaptive {Lasso} under heteroscedasticity.
\newblock \emph{Math. Methods Statist.}, 21:\penalty0 109--126, 2012.

\end{thebibliography}
