\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asi \& Duchi(2019)Asi and Duchi]{Asi2019}
Asi, H. and Duchi, J.~C.
\newblock Stochastic (approximate) proximal point methods: convergence,
  optimality, and adaptivity.
\newblock \emph{SIAM J. Optim.}, 29\penalty0 (3):\penalty0 2257--2290, 2019.
\newblock ISSN 1052-6234.
\newblock \doi{10.1137/18M1230323}.

\bibitem[Berrada et~al.(2020)Berrada, Zisserman, and Kumar]{Berrada2019}
Berrada, L., Zisserman, A., and Kumar, M.~P.
\newblock Training neural networks for and by interpolation.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of \emph{Proceedings of Machine Learning Research},
  pp.\  799--809. PMLR, 13--18 Jul 2020.

\bibitem[Chadha et~al.(2021)Chadha, Cheng, and Duchi]{Chadha2021}
Chadha, K., Cheng, G., and Duchi, J.~C.
\newblock Accelerated, optimal, and parallel: Some results on model-based
  stochastic optimization.
\newblock January 2021.

\bibitem[Chen et~al.(2022)Chen, Cutkosky, and Orabona]{Chen2022}
Chen, K., Cutkosky, A., and Orabona, F.
\newblock Implicit parameter-free online learning with truncated linear models.
\newblock In Dasgupta, S. and Haghtalab, N. (eds.), \emph{Proceedings of The
  33rd International Conference on Algorithmic Learning Theory}, volume 167 of
  \emph{Proceedings of Machine Learning Research}, pp.\  148--175. PMLR, 29
  Mar--01 Apr 2022.
\newblock URL \url{https://proceedings.mlr.press/v167/chen22a.html}.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong,
  Hsieh, Lu, and Le]{Chen2023}
Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong,
  T., Hsieh, C., Lu, Y., and Le, Q.~V.
\newblock Symbolic discovery of optimization algorithms.
\newblock In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and
  Levine, S. (eds.), \emph{Advances in Neural Information Processing Systems
  36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS
  2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.

\bibitem[Davis \& Drusvyatskiy(2019)Davis and Drusvyatskiy]{Davis2019}
Davis, D. and Drusvyatskiy, D.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM J. Optim.}, 29\penalty0 (1):\penalty0 207--239, 2019.
\newblock ISSN 1052-6234.
\newblock \doi{10.1137/18M1178244}.

\bibitem[Defazio \& Gower(2021)Defazio and Gower]{Defazio2021}
Defazio, A. and Gower, R.~M.
\newblock The power of factorial powers: New parameter settings for
  (stochastic) optimization.
\newblock In Balasubramanian, V.~N. and Tsang, I. (eds.), \emph{Proceedings of
  The 13th Asian Conference on Machine Learning}, volume 157 of
  \emph{Proceedings of Machine Learning Research}, pp.\  49--64. PMLR, 17--19
  Nov 2021.
\newblock URL \url{https://proceedings.mlr.press/v157/defazio21a.html}.

\bibitem[Defazio \& Mishchenko(2023{\natexlab{a}})Defazio and
  Mishchenko]{Defazio2023}
Defazio, A. and Mishchenko, K.
\newblock Learning-rate-free learning by {D}-adaptation.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  7449--7479. PMLR, 23--29 Jul 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/defazio23a.html}.

\bibitem[Defazio \& Mishchenko(2023{\natexlab{b}})Defazio and
  Mishchenko]{defazio2023learningratefree}
Defazio, A. and Mishchenko, K.
\newblock Learning-rate-free learning by {D}-{A}daptation.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  7449--7479. PMLR, 23--29 Jul 2023{\natexlab{b}}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Dosovitskiy2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi2011}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011.
\newblock ISSN 1532-4435.

\bibitem[Garrigos \& Gower(2023)Garrigos and Gower]{handbookgrad}
Garrigos, G. and Gower, R.~M.
\newblock Handbook of convergence theorems for (stochastic) gradient methods,
  2023.

\bibitem[Gower et~al.(2021)Gower, Sebbouh, and Loizou]{Gower2021}
Gower, R., Sebbouh, O., and Loizou, N.
\newblock {SGD} for structured nonconvex functions: Learning rates,
  minibatching and interpolation.
\newblock In Banerjee, A. and Fukumizu, K. (eds.), \emph{Proceedings of The
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pp.\
  1315--1323. PMLR, 13--15 Apr 2021.
\newblock URL \url{https://proceedings.mlr.press/v130/gower21a.html}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{scalinglaws}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{CoRR}, abs/2001.08361, 2020.
\newblock URL \url{https://arxiv.org/pdf/2001.08361.pdf}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma2015}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Krogh \& Hertz(1991)Krogh and Hertz]{Krogh1991}
Krogh, A. and Hertz, J.
\newblock A simple weight decay can improve generalization.
\newblock In Moody, J., Hanson, S., and Lippmann, R. (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~4. Morgan-Kaufmann, 1991.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf}.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Hadj~Laradji, and
  Lacoste-Julien]{Loizou2021}
Loizou, N., Vaswani, S., Hadj~Laradji, I., and Lacoste-Julien, S.
\newblock Stochastic {P}olyak step-size for {SGD}: An adaptive learning rate
  for fast convergence.
\newblock In Banerjee, A. and Fukumizu, K. (eds.), \emph{Proceedings of The
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pp.\
  1306--1314. PMLR, 13--15 Apr 2021.
\newblock URL \url{https://proceedings.mlr.press/v130/loizou21a.html}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{Loshchilov2019}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{Luo2019}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations}, May 2019.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{Ma2018}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3325--3334. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/ma18a.html}.

\bibitem[Meng \& Gower(2023)Meng and Gower]{meng2022a}
Meng, S.~Y. and Gower, R.~M.
\newblock A model-based method for minimizing {CV}a{R} and beyond.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  24436--24456. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/meng23a.html}.

\bibitem[Orabona(2019)]{Orabona2019}
Orabona, F.
\newblock A modern introduction to online learning.
\newblock \emph{CoRR}, abs/1912.13213, 2019.
\newblock URL \url{http://arxiv.org/abs/1912.13213}.

\bibitem[Orabona \& Tommasi(2017)Orabona and Tommasi]{coin-betting}
Orabona, F. and Tommasi, T.
\newblock Training deep networks without learning rates through coin betting.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Orvieto et~al.(2022)Orvieto, Lacoste{-}Julien, and
  Loizou]{Orvieto2022}
Orvieto, A., Lacoste{-}Julien, S., and Loizou, N.
\newblock Dynamics of {SGD} with stochastic polyak stepsizes: Truly adaptive
  variants and convergence to exact solution.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{Ott2019}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Paren et~al.(2022)Paren, Berrada, Poudel, and Kumar]{Paren2022}
Paren, A., Berrada, L., Poudel, R. P.~K., and Kumar, M.~P.
\newblock A stochastic bundle method for interpolation.
\newblock \emph{J Mach Learn Res}, 23\penalty0 (15):\penalty0 1--57, 2022.
\newblock URL \url{http://jmlr.org/papers/v23/20-1248.html}.

\bibitem[Polyak(1964)]{Polyak1964}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.
\newblock ISSN 0041-5553.

\bibitem[Polyak(1987)]{Polyak1987}
Polyak, B.~T.
\newblock \emph{Introduction to optimization}.
\newblock Translations Series in Mathematics and Engineering. Optimization
  Software, Inc., Publications Division, New York, 1987.
\newblock ISBN 0-911575-14-6.
\newblock Translated from the Russian, With a foreword by Dimitri P. Bertsekas.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J Mach Learn Res}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{Robbins1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{Ann. Math. Statistics}, 22:\penalty0 400--407, 1951.
\newblock ISSN 0003-4851.
\newblock \doi{10.1214/aoms/1177729586}.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{Ronneberger2015}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Lecture Notes in Computer Science}, pp.\  234--241. Springer
  International Publishing, 2015.
\newblock \doi{10.1007/978-3-319-24574-4_28}.

\bibitem[Schaipp et~al.(2023)Schaipp, Gower, and Ulbrich]{Schaipp2023}
Schaipp, F., Gower, R.~M., and Ulbrich, M.
\newblock A stochastic proximal {P}olyak step size.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=jWr41htaB3}.
\newblock Reproducibility Certification.

\bibitem[Schmidt et~al.(2021)Schmidt, Schneider, and Hennig]{Schmidt2021}
Schmidt, R.~M., Schneider, F., and Hennig, P.
\newblock Descending through a crowded valley - benchmarking deep learning
  optimizers.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9367--9376. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/schmidt21a.html}.

\bibitem[Sebbouh et~al.(2021)Sebbouh, Gower, and Defazio]{Sebbouh2021}
Sebbouh, O., Gower, R.~M., and Defazio, A.
\newblock Almost sure convergence rates for stochastic gradient descent and
  stochastic heavy ball.
\newblock In Belkin, M. and Kpotufe, S. (eds.), \emph{Proceedings of Thirty
  Fourth Conference on Learning Theory}, volume 134 of \emph{Proceedings of
  Machine Learning Research}, pp.\  3935--3971. PMLR, 15--19 Aug 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/sebbouh21a.html}.

\bibitem[Sharir et~al.(2020)Sharir, Peleg, and Shoham]{sharir2020cost}
Sharir, O., Peleg, B., and Shoham, Y.
\newblock The cost of training {NLP} models: A concise overview, 2020.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{Simonyan2015}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Sun(2020)]{Sun2020}
Sun, R.-Y.
\newblock Optimization for deep learning: An overview.
\newblock \emph{J. Oper. Res. Soc. China}, 8\penalty0 (2):\penalty0 249--294,
  jun 2020.
\newblock \doi{10.1007/s40305-020-00309-6}.

\bibitem[Tien \& Chapelle(2014)Tien and Chapelle]{criteo-display-ad-challenge}
Tien, J.-B. and Chapelle, O.
\newblock Display advertising challenge, 2014.
\newblock URL
  \url{https://kaggle.com/competitions/criteo-display-ad-challenge}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste{-}Julien]{Vaswani2019}
Vaswani, S., Mishkin, A., Laradji, I.~H., Schmidt, M., Gidel, G., and
  Lacoste{-}Julien, S.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  3727--3740, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html}.

\bibitem[Wang et~al.(2023)Wang, Johansson, and Zhang]{Wang2023}
Wang, X., Johansson, M., and Zhang, T.
\newblock Generalized {P}olyak step size for first order optimization with
  momentum.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  35836--35863. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/wang23l.html}.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and Grosse]{Zhang2019}
Zhang, G., Wang, C., Xu, B., and Grosse, R.~B.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{Zhuang2020}
Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N.~C., Papademetris,
  X., and Duncan, J.~S.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html}.

\bibitem[Zhuang et~al.(2022)Zhuang, Liu, Cutkosky, and Orabona]{Zhuang2022}
Zhuang, Z., Liu, M., Cutkosky, A., and Orabona, F.
\newblock Understanding {A}dam{W} through proximal methods and scale-freeness.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/forum?id=IKhEPWGdwK}.

\end{thebibliography}
