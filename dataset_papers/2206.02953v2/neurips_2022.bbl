\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{AhnRR2020}
K.~Ahn, C.~Yun, and S.~Sra.
\newblock {SGD} with shuffling: Optimal rates without component convexity and
  large epoch requirements.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{WassersteinGAN}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[Bailey et~al.(2020)Bailey, Gidel, and Piliouras]{GidelStabilityAlt}
J.~P. Bailey, G.~Gidel, and G.~Piliouras.
\newblock Finite regret and cycles with fixed step-size via alternating
  gradient descent-ascent.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[Bengio(2012)]{BengioDLOpt}
Y.~Bengio.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock \emph{Neural Networks: Tricks of the Trade: Second Edition}, 2012.

\bibitem[Bertsekas(2011)]{BertsekasIG}
D.~P. Bertsekas.
\newblock Incremental gradient, subgradient, and proximal methods for convex
  optimization: A survey.
\newblock \emph{Report LIDS - 2848, Laboratory for Information and Decision
  Systems, MIT}, 2011.

\bibitem[Bose et~al.(2020)Bose, Gidel, Berard, Cianflone, Vincent,
  Lacoste-Julien, and Hamilton]{AdversarialExampleGames}
J.~Bose, G.~Gidel, H.~Berard, A.~Cianflone, P.~Vincent, S.~Lacoste-Julien, and
  W.~Hamilton.
\newblock Adversarial example games.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Bottou(2009)]{BottouRR2009}
L.~Bottou.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock In \emph{Proceedings of the Symposium on Learning and Data Science},
  2009.

\bibitem[Cai et~al.(2019)Cai, Hong, Chen, and Wang]{CaiLQR}
Q.~Cai, M.~Hong, Y.~Chen, and Z.~Wang.
\newblock On the global convergence of imitation learning: A case for linear
  quadratic regulator.
\newblock \emph{arXiv:1901.03674}, 2019.

\bibitem[Cochran(1977)]{CochranSamplingWOR}
W.~G. Cochran.
\newblock \emph{Sampling Techniques}.
\newblock Wiley, 1977.

\bibitem[Daskalakis et~al.(2021)Daskalakis, Skoulakis, and
  Zampetakis]{CostisComplexityMinimax}
C.~Daskalakis, S.~Skoulakis, and M.~Zampetakis.
\newblock The complexity of constrained min-max optimization.
\newblock In \emph{Proceedings of the ACM SIGACT Symposium on the Theory of
  Computing}, 2021.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{SimonDuMinimaxRL}
S.~S. Du, J.~Chen, L.~Li, L.~Xiao, and D.~Zhou.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[El~Ghaoui and Lebret(1997)]{RobustLS}
L.~El~Ghaoui and H.~Lebret.
\newblock Robust solutions to least-squares problems with uncertain data.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 1997.

\bibitem[Facchinei and Pang(2007)]{FacchineiPang}
F.~Facchinei and J.-S. Pang.
\newblock \emph{Finite-dimensional variational inequalities and complementarity
  problems}.
\newblock Springer Science \& Business Media, 2007.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{FazelLQR}
M.~Fazel, R.~Ge, S.~Kakade, and M.~Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Fiez and Ratliff(2021)]{TannerTimescaleLocal}
T.~Fiez and L.~J. Ratliff.
\newblock Local convergence analysis of gradient descent ascent with finite
  timescale separation.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fiez et~al.(2021)Fiez, Ratliff, Mazumdar, Faulkner, and
  Narang]{TannerTimescaleGlobal}
T.~Fiez, L.~Ratliff, E.~Mazumdar, E.~Faulkner, and A.~Narang.
\newblock Global convergence to local minmax equilibrium in classes of
  nonconvex zero-sum games.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Gidel et~al.(2019)Gidel, Hemmat, Pezeshki, Priol, Huang,
  Lacoste-Julien, and Mitliagkas]{GidelNegativeMomentum}
G.~Gidel, R.~A. Hemmat, M.~Pezeshki, R.~L. Priol, G.~Huang, S.~Lacoste-Julien,
  and I.~Mitliagkas.
\newblock Negative momentum for improved game dynamics.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{GAN_Original}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{GoodfellowPGD}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{GowerSGD}
R.~M. Gower, N.~Loizou, X.~Qian, A.~Sailanbayev, E.~Shulgin, and
  P.~Richt{\'a}rik.
\newblock {SGD}: General analysis and improved rates.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2019)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{GurbuzRR2019}
M.~G{\"u}rb{\"u}zbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, 2019.

\bibitem[Gürbüzbalaban et~al.(2019)Gürbüzbalaban, Ozdaglar, and
  Parrilo]{GurbuzIG2019}
M.~Gürbüzbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Convergence rate of incremental gradient and incremental {N}ewton
  methods.
\newblock \emph{SIAM Journal on Optimization}, 2019.

\bibitem[Haochen and Sra(2019)]{HaochenSraRR19}
J.~Haochen and S.~Sra.
\newblock Random shuffling beats {SGD} after finite epochs.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Ho and Ermon(2016)]{ErmonGAIL}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Jain et~al.(2019)Jain, Nagaraj, and Netrapalli]{JainNagarajSGD}
P.~Jain, D.~Nagaraj, and P.~Netrapalli.
\newblock Making the last iterate of {SGD} information theoretically optimal.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2019.

\bibitem[Koolen et~al.(2014)Koolen, Malek, and Bartlett]{KoolenOnlineQuadratic}
W.~M. Koolen, A.~Malek, and P.~L. Bartlett.
\newblock Efficient minimax strategies for square loss games.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Lai and Lim(2020)]{LaiLimAMGM}
Z.~Lai and L.-H. Lim.
\newblock Recht-{R}e noncommutative arithmetic-geometric mean conjecture is
  false.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Li et~al.(2021)Li, Milzarek, and Qiu]{KLRR2021}
X.~Li, A.~Milzarek, and J.~Qiu.
\newblock Convergence of random reshuffling under the {K}urdyka-Łojasiewicz
  inequality.
\newblock \emph{arXiv:2110.04926}, 2021.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{LinJinJordanSGDA2020}
T.~Lin, C.~Jin, and M.~I. Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Littman(1994)]{Littman94markovgames}
M.~Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 1994.

\bibitem[Loizou et~al.(2021)Loizou, Berard, Gidel, Mitliagkas, and
  Lacoste-Julien]{GidelSGDAEC}
N.~Loizou, H.~Berard, G.~Gidel, I.~Mitliagkas, and S.~Lacoste-Julien.
\newblock Stochastic gradient descent-ascent and consensus optimization for
  smooth games: Convergence analysis under expected co-coercivity.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{Madry_Adv_Robust}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Maheshwari et~al.(2022)Maheshwari, Chiu, Mazumdar, Sastry, and
  Ratliff]{Eric_DRO_OGDA}
C.~Maheshwari, C.-Y. Chiu, E.~Mazumdar, S.~Sastry, and L.~Ratliff.
\newblock Zeroth-order methods for convex-concave minmax problems: applications
  to decision-dependent risk minimization.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2022.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and
  Richt{\'a}rik]{MischenkoRR2020}
K.~Mishchenko, A.~Khaled, and P.~Richt{\'a}rik.
\newblock Random reshuffling: simple analysis with vast improvements.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Khaled, and
  Richt{\'a}rik]{MischenkoFED}
K.~Mishchenko, A.~Khaled, and P.~Richt{\'a}rik.
\newblock Proximal and federated random reshuffling.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2022.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{NagarajRR2020}
D.~Nagaraj, P.~Jain, and P.~Netrapalli.
\newblock {SGD} without replacement: sharper rates for general smooth convex
  functions.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Nedic and Bertsekas(1999)]{NedicBertsekas}
A.~Nedic and D.~Bertsekas.
\newblock Incremental subgradient methods for nondifferentiable optimization.
\newblock In \emph{Proceedings of the IEEE Conference on Decision and Control},
  1999.

\bibitem[Neumann and Morgenstern(1944)]{VonNeumannMorgenstern}
J.~v. Neumann and O.~Morgenstern.
\newblock \emph{Theory of games and economic behavior}.
\newblock Princeton University Press, 1944.

\bibitem[Nguyen et~al.(2021)Nguyen, Tran-Dinh, Phan, Nguyen, and
  v.~Dijk]{NgyuenRR2020}
L.~M. Nguyen, Q.~Tran-Dinh, D.~T. Phan, P.~H. Nguyen, and M.~v.~Dijk.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{NouhiedGDMax2019}
M.~Nouiehed, M.~Sanjabi, T.~Huang, J.~D. Lee, and M.~Razaviyayn.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Patrascu and Necoara(2018)]{NecoaraSPPM}
A.~Patrascu and I.~Necoara.
\newblock Nonasymptotic convergence of stochastic proximal point methods for
  constrained convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 2018.

\bibitem[Rajeswaran et~al.(2020)Rajeswaran, Mordatch, and Kumar]{GameMBRL}
A.~Rajeswaran, I.~Mordatch, and V.~Kumar.
\newblock A game theoretic framework for model based reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Rajput et~al.(2020)Rajput, Gupta, and Papailiopoulos]{RajputRR2020}
S.~Rajput, A.~Gupta, and D.~Papailiopoulos.
\newblock Closing the convergence gap of {SGD} without replacement.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{RakhlinSridharan}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2012.

\bibitem[Recht and Re(2012)]{RechtReAMGM}
B.~Recht and C.~Re.
\newblock Toward a noncommutative arithmetic-geometric mean inequality:
  Conjectures, case-studies, and consequences.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2012.

\bibitem[Rice(1988)]{RiceSamplingWOR}
J.~Rice.
\newblock \emph{Mathematical Statistics and Data Analysis}.
\newblock Wadsworth, 1988.

\bibitem[Rockafellar(1976)]{RockafellarMinimax}
R.~T. Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock \emph{SIAM Journal on Control and Optimization}, 1976.

\bibitem[Safran and Shamir(2020)]{SafranShamirRR2020}
I.~Safran and O.~Shamir.
\newblock How good is {SGD} with random shuffling?
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[Safran and Shamir(2021)]{SafranShamir2021}
I.~Safran and O.~Shamir.
\newblock Random shuffling beats {SGD} only after many epochs on
  ill-conditioned problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Scutari et~al.(2010)Scutari, Palomar, Facchinei, and
  Pang]{ScutariGamesVIP}
G.~Scutari, D.~P. Palomar, F.~Facchinei, and J.-S. Pang.
\newblock Convex optimization, game theory, and variational inequality theory.
\newblock \emph{IEEE Signal Processing Magazine}, 2010.

\bibitem[Shumailov et~al.(2021)Shumailov, Shumaylov, Kazhdan, Zhao, Papernot,
  Erdogdu, and Anderson]{Papernot_DataOrdering_2021}
I.~Shumailov, Z.~Shumaylov, D.~Kazhdan, Y.~Zhao, N.~Papernot, M.~A. Erdogdu,
  and R.~J. Anderson.
\newblock Manipulating {SGD} with data ordering attacks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{Sinha_Adv_Robust}
A.~Sinha, H.~Namkoong, and J.~Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Tran et~al.(2021)Tran, Nguyen, and Tran-Dinh]{NgyuenMomentum}
T.~H. Tran, L.~M. Nguyen, and Q.~Tran-Dinh.
\newblock {SMG}: A shuffling gradient-based method with momentum.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Yang et~al.(2020)Yang, Kiyavash, and He]{YangAGDA2020}
J.~Yang, N.~Kiyavash, and N.~He.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yu et~al.(2022)Yu, Lin, Mazumdar, and Jordan]{Eric_DRO}
Y.~Yu, T.~Lin, E.~Mazumdar, and M.~I. Jordan.
\newblock Fast distributionally robust learning with variance reduced min-max
  optimization.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2022.

\bibitem[Yun et~al.(2022)Yun, Rajput, and Sra]{SuvritFED}
C.~Yun, S.~Rajput, and S.~Sra.
\newblock Minibatch vs local {SGD} with shuffling: Tight convergence bounds and
  beyond.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Poupart, and Yu]{GuojunStabilityGames}
G.~Zhang, P.~Poupart, and Y.~Yu.
\newblock Optimality and stability in non-convex smooth games.
\newblock \emph{Journal of Machine Learning Research}, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Yang, and Başar]{MARLSurvey}
K.~Zhang, Z.~Yang, and T.~Başar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, 2019.

\end{thebibliography}
