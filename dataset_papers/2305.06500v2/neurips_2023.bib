@inproceedings{flan,
  author = {Jason Wei and
               Maarten Bosma and
               Vincent Y. Zhao and
               Kelvin Guu and
               Adams Wei Yu and
               Brian Lester and
               Nan Du and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  booktitle = {{ICLR}},
  year      = {2022},
}

@article{VL_T5,
	title={Unifying vision-and-language tasks via text generation},
	author={Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
	journal={arXiv preprint arXiv:2102.02779},
	year={2021}
}

@inproceedings{T0,
  author       = {Victor Sanh and
                  Albert Webson and
                  Colin Raffel and
                  Stephen H. Bach and
                  Lintang Sutawika and
                  Zaid Alyafeai and
                  Antoine Chaffin and
                  Arnaud Stiegler and
                  Arun Raja and
                  Manan Dey and
                  M Saiful Bari and
                  Canwen Xu and
                  Urmish Thakker and
                  Shanya Sharma Sharma and
                  Eliza Szczechla and
                  Taewoon Kim and
                  Gunjan Chhablani and
                  Nihal V. Nayak and
                  Debajyoti Datta and
                  Jonathan Chang and
                  Mike Tian{-}Jian Jiang and
                  Han Wang and
                  Matteo Manica and
                  Sheng Shen and
                  Zheng Xin Yong and
                  Harshit Pandey and
                  Rachel Bawden and
                  Thomas Wang and
                  Trishala Neeraj and
                  Jos Rozen and
                  Abheesht Sharma and
                  Andrea Santilli and
                  Thibault F{\'{e}}vry and
                  Jason Alan Fries and
                  Ryan Teehan and
                  Teven Le Scao and
                  Stella Biderman and
                  Leo Gao and
                  Thomas Wolf and
                  Alexander M. Rush},
  title        = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  booktitle    = {{ICLR}},
  year         = {2022},
}

@inproceedings{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {{ICLR}},
  year         = {2022},
}

@article{GPT3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal={arXiv preprint arXiv:2005.14165},
  year      = {2020},
}

@article{flanT5,
  author    = {Hyung Won Chung and
               Le Hou and
               Shayne Longpre and
               Barret Zoph and
               Yi Tay and
               William Fedus and
               Eric Li and
               Xuezhi Wang and
               Mostafa Dehghani and
               Siddhartha Brahma and
               Albert Webson and
               Shixiang Shane Gu and
               Zhuyun Dai and
               Mirac Suzgun and
               Xinyun Chen and
               Aakanksha Chowdhery and
               Sharan Narang and
               Gaurav Mishra and
               Adams Yu and
               Vincent Y. Zhao and
               Yanping Huang and
               Andrew M. Dai and
               Hongkun Yu and
               Slav Petrov and
               Ed H. Chi and
               Jeff Dean and
               Jacob Devlin and
               Adam Roberts and
               Denny Zhou and
               Quoc V. Le and
               Jason Wei},
  title     = {Scaling Instruction-Finetuned Language Models},
  journal={arXiv preprint arXiv:2210.11416},
  year      = {2022},
}

@article{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      journal={arXiv preprint arXiv:2302.13971},
}
@misc{vicuna,
  title         = {Vicuna},
  year          = {2023},
  howpublished  = {\url{https://github.com/lm-sys/FastChat}},
}

@article{T5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  year={2020},
}

@inproceedings{flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\'{n}kowski, Miko\l aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar\'{e}n},
 booktitle = {NeurIPS},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 year = {2022}
}

@InProceedings{blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  booktitle = 	 {ICML},
  year = 	 {2023},
}

@InProceedings{vilt,
  title = 	 {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author =       {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@article{multi-instruct,
  title={MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning},
  author={Zhiyang Xu and Ying Shen and Lifu Huang},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10773}
}

@inproceedings{pnpvqa,
    title = "Plug-and-Play {VQA}: Zero-shot {VQA} by Conjoining Large Pretrained Models with Zero Training",
    author = "Tiong, Anthony Meng Huat  and
      Li, Junnan  and
      Li, Boyang  and
      Savarese, Silvio  and
      Hoi, Steven C.H.",
    booktitle = "EMNLP Findings",
}

@inproceedings{fewvlm,
    title = "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
    author = "Jin, Woojeong  and
      Cheng, Yu  and
      Shen, Yelong  and
      Chen, Weizhu  and
      Ren, Xiang",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.197",
    doi = "10.18653/v1/2022.acl-long.197",
    pages = "2763--2775",
    abstract = "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks.Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2{\%} point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at \url{https://github.com/woojeongjin/FewVLM}",
}

@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@inproceedings{coco,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
title="Microsoft COCO: Common Objects in Context",
booktitle="ECCV",
year="2014",
}

@inproceedings{textcaps,
    title={TextCaps: a Dataset for Image Captioningwith Reading Comprehension},
    author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
    journal={ECCV},
    year={2020}
}

@inproceedings{textvqa,
    title={Towards VQA Models That Can Read},
    author={Singh, Amanpreet and Natarjan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Parikh, Devi and Rohrbach, Marcus},
    booktitle={CVPR},
    pages={8317-8326},
    year={2019}
}

@inproceedings{nocaps,
  title={nocaps: novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={ICCV},
  pages={8948--8957},
  year={2019}
}

@article{flickr30k,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    publisher = "MIT Press",
}

@InProceedings{gqa,
author = {Hudson, Drew A. and Manning, Christopher D.},
title = {GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
booktitle = {CVPR},
year = {2019}
}

@article{vsr,
  title={Visual Spatial Reasoning},
  author={Fangyu Liu and Guy Edward Toh Emerson and Nigel Collier},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
}

@inproceedings{iconqa,
    title = {IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning},
    author = {Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
    booktitle = {NeurIPS Track on Datasets and Benchmarks},
    year = {2021}
}

@InProceedings{clevr,
author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},
title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
booktitle = {CVPR},
month = {July},
year = {2017}
}

@inproceedings{msvdqa,
author = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
title = {Video Question Answering via Gradually Refined Attention over Appearance and Motion},
year = {2017},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1645–1653},
}

@inproceedings{ivqa,
title={Just ask: Learning to answer questions from millions of narrated videos},
author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
booktitle={ICCV},
pages={1686--1697},
year={2021}
}

@InProceedings{vqav2,
author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
title = {Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
booktitle = {CVPR},
month = {July},
year = {2017}
}

@InProceedings{okvqa,
author = {Kenneth Marino and Mohammad Rastegari and Ali Farhadi and Roozbeh Mottaghi},
title = {OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge},
booktitle = {CVPR},
year = {2019},
}

@InProceedings{aokvqa,
author="Schwenk, Dustin
and Khandelwal, Apoorv
and Clark, Christopher
and Marino, Kenneth
and Mottaghi, Roozbeh",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge",
booktitle="ECCV",
year="2022",
abstract="The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision--language models.",
isbn="978-3-031-20074-8"
}

@InProceedings{ocrvqa,
  author    = "Anand Mishra and Shashank Shekhar and Ajeet Kumar Singh and Anirban Chakraborty",
  title     = "OCR-VQA: Visual Question Answering by Reading Text in Images",
  booktitle = "ICDAR",
  year      = "2019",
}

@inproceedings{scienceqa,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={NeurIPS},
    year={2022}
}

@InProceedings{vizwiz,
author = {Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
title = {VizWiz Grand Challenge: Answering Visual Questions From Blind People},
booktitle = {CVPR},
year = {2018}
}

@InProceedings{visdial,
author = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jose M. F. and Parikh, Devi and Batra, Dhruv},
title = {Visual Dialog},
booktitle = {CVPR},
year = {2017}
}

@inproceedings{clevrdialog,
    title = "{CLEVR}-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog",
    author = "Kottur, Satwik  and
      Moura, Jos{\'e} M. F.  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Rohrbach, Marcus",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1058",
    doi = "10.18653/v1/N19-1058",
    pages = "582--595",
    abstract = "Visual Dialog is a multimodal task of answering a sequence of questions grounded in an image (using the conversation history as context). It entails challenges in vision, language, reasoning, and grounding. However, studying these subtasks in isolation on large, real datasets is infeasible as it requires prohibitively-expensive complete annotation of the {`}state{'} of all images and dialogs. We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, we construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs. We use CLEVR-Dialog to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. We hope the findings from CLEVR-Dialog will help inform the development of future models for visual dialog. Our code and dataset are publicly available.",
}

@inproceedings{hatefulmemes,
author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
title = {The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
year = {2020},
abstract = {This work proposes a new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes. It is constructed such that unimodal models struggle and only multimodal models can succeed: difficult examples ("benign confounders") are added to the dataset to make it hard to rely on unimodal signals. The task requires subtle reasoning, yet is straightforward to evaluate as a binary classification problem. We provide baseline performance numbers for unimodal models, as well as for multimodal models with various degrees of sophistication. We find that state-of-the-art methods perform poorly compared to humans, illustrating the difficulty of the task and highlighting the challenge that this important problem poses to the community.},
booktitle = {NeurIPS},
}

@article{llava,
author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
title       = {Visual Instruction Tuning},
publisher   = {arXiv:2304.08485},
year        = {2023}
}

@article{flan_collection,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={S. Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.13688}
}

@article{eva,
  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  author={Yuxin Fang and Wen Wang and Binhui Xie and Quan-Sen Sun and Ledell Yu Wu and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.07636}
}

@inproceedings{
adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={ICLR},
year={2019},
}

@inproceedings{blip1,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@inproceedings{albef,
 author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
 booktitle = {NeurIPS},
 title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
 year = {2021}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{super-natural-instruct,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    booktitle = "EMNLP",
    year={2022}, 
}

@article{unnatural-instruct,
  title={Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
  author={Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09689}
}

@article{self-instruct,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10560}
}

@article{toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Timo Schick and Jane Dwivedi-Yu and Roberto Dess{\`i} and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04761}
}

@article{instruction-tuning-with-gpt4,
  title={Instruction Tuning with GPT-4},
  author={Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.03277}
}

@misc{dolly,
  title         = {Dolly},
  year          = {2023},
  howpublished  = {\url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}},
}

@misc{stable-lm,
  title         = {StableLM},
  year          = {2023},
  howpublished  = {\url{https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models}},
}

@misc{chatgpt,
  title         = {ChatGPT},
  year          = {2023},
  howpublished  = {\url{https://openai.com/blog/chatgpt}},
}

@inproceedings{instructgpt,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}

@misc{zhu2022minigpt4,
      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
      journal={arXiv preprint arXiv:2304.10592},
      year={2023},
}

@inproceedings{mplug-owl,
  title={mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
  author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Chao Zhang and Feiyan Huang},
  year={2023}
}

@misc{llama-adapter-v1,
      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, 
      author={Renrui Zhang and Jiaming Han and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Peng Gao and Yu Qiao},
      year={2023},
      eprint={2303.16199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{llama-adapter-v2,
      title={LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model}, 
      author={Peng Gao and Jiaming Han and Renrui Zhang and Ziyi Lin and Shijie Geng and Aojun Zhou and Wei Zhang and Pan Lu and Conghui He and Xiangyu Yue and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2304.15010},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{visdial_pretrain,
author="Murahari, Vishvak
and Batra, Dhruv
and Parikh, Devi
and Das, Abhishek",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Large-Scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline",
booktitle="ECCV",
year="2020",
}

@misc{git,
      title={GIT: A Generative Image-to-text Transformer for Vision and Language}, 
      author={Jianfeng Wang and Zhengyuan Yang and Xiaowei Hu and Linjie Li and Kevin Lin and Zhe Gan and Zicheng Liu and Ce Liu and Lijuan Wang},
      year={2022},
      eprint={2205.14100},
      archivePrefix={arXiv},
}

@article{prophet,
  title={Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering},
  author={Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
  journal={Computer Vision and Pattern Recognition (CVPR)},
  year={2023}
}

@misc{promptcap,
      title={PromptCap: Prompt-Guided Task-Aware Image Captioning}, 
      author={Yushi Hu and Hang Hua and Zhengyuan Yang and Weijia Shi and Noah A. Smith and Jiebo Luo},
      year={2023},
      eprint={2211.09699},
      archivePrefix={arXiv},
}

@inproceedings{vd-bert,
    title = "{VD-BERT}: {A} {U}nified {V}ision and {D}ialog {T}ransformer with {BERT}",
    author = "Wang, Yue  and
      Joty, Shafiq  and
      Lyu, Michael  and
      King, Irwin  and
      Xiong, Caiming  and
      Hoi, Steven C.H.",
    booktitle = "EMNLP",
    year = "2020",
}

@misc{palme,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{cider,
  author={Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CIDEr: Consensus-based image description evaluation}, 
  year={2015},
  volume={},
  number={},
  pages={4566-4575},
  doi={10.1109/CVPR.2015.7299087}}

@InProceedings{karpathy_split,
author = {Karpathy, Andrej and Fei-Fei, Li},
title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{12in1,
  author       = {Jiasen Lu and
                  Vedanuj Goswami and
                  Marcus Rohrbach and
                  Devi Parikh and
                  Stefan Lee},
  title        = {12-in-1: Multi-Task Vision and Language Representation Learning},
  booktitle    = {{CVPR}},
  year         = {2020},

}

@misc{lavis,
      title={LAVIS: A Library for Language-Vision Intelligence}, 
      author={Dongxu Li and Junnan Li and Hung Le and Guangsen Wang and Silvio Savarese and Steven C. H. Hoi},
      year={2022},
      eprint={2209.09019},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
