\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, and de~Freitas]{andrychowicz2016learning}
Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew~W, Pfau,
  David, Schaul, Tom, and de~Freitas, Nando.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3981--3989, 2016.

\bibitem[Ba et~al.(2016{\natexlab{a}})Ba, Hinton, Mnih, Leibo, and
  Ionescu]{ba2016using}
Ba, Jimmy, Hinton, Geoffrey~E, Mnih, Volodymyr, Leibo, Joel~Z, and Ionescu,
  Catalin.
\newblock Using fast weights to attend to the recent past.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4331--4339, 2016{\natexlab{a}}.

\bibitem[Ba et~al.(2016{\natexlab{b}})Ba, Kiros, and Hinton]{ba2016layer}
Ba, Jimmy~Lei, Kiros, Jamie~Ryan, and Hinton, Geoffrey~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016{\natexlab{b}}.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018convolutional}
Bai, Shaojie, Kolter, J.~Zico, and Koltun, Vladlen.
\newblock Convolutional sequence modeling revisited, 2018.
\newblock URL \url{https://openreview.net/forum?id=rk8wKk-R-}.

\bibitem[Bridle(1990)]{bridle1990training}
Bridle, John~S.
\newblock Training stochastic model recognition algorithms as networks can lead
  to maximum mutual information estimation of parameters.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  211--217, 1990.

\bibitem[Chen et~al.(2015)Chen, Grangier, and Auli]{chen2015strategies}
Chen, Welin, Grangier, David, and Auli, Michael.
\newblock Strategies for training large vocabulary neural language models.
\newblock \emph{arXiv preprint arXiv:1512.04906}, 2015.

\bibitem[Collobert \& Weston(2008)Collobert and Weston]{collobert2008unified}
Collobert, Ronan and Weston, Jason.
\newblock A unified architecture for natural language processing: Deep neural
  networks with multitask learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  160--167. ACM, 2008.

\bibitem[Dauphin et~al.(2016)Dauphin, Fan, Auli, and
  Grangier]{dauphin2016language}
Dauphin, Yann~N, Fan, Angela, Auli, Michael, and Grangier, David.
\newblock Language modeling with gated convolutional networks.
\newblock \emph{arXiv preprint arXiv:1612.08083}, 2016.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{arXiv preprint arXiv:1703.03400}, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, Xavier and Bengio, Yoshua.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[Goodman(2001)]{goodman2001classes}
Goodman, Joshua.
\newblock Classes for fast maximum entropy training.
\newblock In \emph{Acoustics, Speech, and Signal Processing, 2001.
  Proceedings.(ICASSP'01). 2001 IEEE International Conference on}, volume~1,
  pp.\  561--564. IEEE, 2001.

\bibitem[Grave et~al.(2016{\natexlab{a}})Grave, Joulin, Ciss{\'e}, Grangier,
  and J{\'e}gou]{grave2016efficient}
Grave, Edouard, Joulin, Armand, Ciss{\'e}, Moustapha, Grangier, David, and
  J{\'e}gou, Herv{\'e}.
\newblock Efficient softmax approximation for gpus.
\newblock \emph{arXiv preprint arXiv:1609.04309}, 2016{\natexlab{a}}.

\bibitem[Grave et~al.(2016{\natexlab{b}})Grave, Joulin, and
  Usunier]{grave2016improving}
Grave, Edouard, Joulin, Armand, and Usunier, Nicolas.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{arXiv preprint arXiv:1612.04426}, 2016{\natexlab{b}}.

\bibitem[Grave et~al.(2017)Grave, Cisse, and Joulin]{grave2017unbounded}
Grave, Edouard, Cisse, Moustapha~M, and Joulin, Armand.
\newblock Unbounded cache model for online language modeling with open
  vocabulary.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6044--6054, 2017.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, Alex, Wayne, Greg, and Danihelka, Ivo.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwi{\'n}ska, Colmenarejo, Grefenstette, Ramalho, Agapiou,
  et~al.]{graves2016hybrid}
Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo,
  Grabska-Barwi{\'n}ska, Agnieszka, Colmenarejo, Sergio~G{\'o}mez,
  Grefenstette, Edward, Ramalho, Tiago, Agapiou, John, et~al.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471, 2016.

\bibitem[Gu et~al.(2017)Gu, Wang, Cho, and Li]{gu2017search}
Gu, Jiatao, Wang, Yong, Cho, Kyunghyun, and Li, Victor~OK.
\newblock Search engine guided non-parametric neural machine translation.
\newblock \emph{arXiv preprint arXiv:1705.07267}, 2017.

\bibitem[Gulcehre et~al.(2016)Gulcehre, Ahn, Nallapati, Zhou, and
  Bengio]{gulcehre2016pointing}
Gulcehre, Caglar, Ahn, Sungjin, Nallapati, Ramesh, Zhou, Bowen, and Bengio,
  Yoshua.
\newblock Pointing the unknown words.
\newblock \emph{arXiv preprint arXiv:1603.08148}, 2016.

\bibitem[Hebb(1949)]{hebb1949organization}
Hebb, Donald~O.
\newblock The organization of behavior: A neurophysiological approach, 1949.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, Sepp and Schmidhuber, J{\"u}rgen.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Hochreiter, Sepp, Younger, A~Steven, and Conwell, Peter~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  87--94. Springer, 2001.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{jozefowicz2016exploring}
Jozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu,
  Yonghui.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Kaiser et~al.(2017)Kaiser, Nachum, Roy, and
  Bengio]{kaiser2017learning}
Kaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy.
\newblock Learning to remember rare events.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kalchbrenner et~al.(2014)Kalchbrenner, Grefenstette, and
  Blunsom]{kalchbrenner2014convolutional}
Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil.
\newblock A convolutional neural network for modelling sentences.
\newblock \emph{arXiv preprint arXiv:1404.2188}, 2014.

\bibitem[Kawakami et~al.(2017)Kawakami, Dyer, and
  Blunsom]{kawakami2017learning}
Kawakami, Kazuya, Dyer, Chris, and Blunsom, Phil.
\newblock Learning to create and reuse words in open-vocabulary neural language
  modeling.
\newblock \emph{arXiv preprint arXiv:1704.06986}, 2017.

\bibitem[Kneser \& Ney(1995)Kneser and Ney]{kneser1995improved}
Kneser, Reinhard and Ney, Hermann.
\newblock Improved backing-off for m-gram language modeling.
\newblock In \emph{Acoustics, Speech, and Signal Processing, 1995. ICASSP-95.,
  1995 International Conference on}, volume~1, pp.\  181--184. IEEE, 1995.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, Brenden~M, Salakhutdinov, Ruslan, and Tenenbaum, Joshua~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[McClelland et~al.(1995)McClelland, McNaughton, and
  O'reilly]{mcclelland1995there}
McClelland, James~L, McNaughton, Bruce~L, and O'reilly, Randall~C.
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: insights from the successes and failures of connectionist models
  of learning and memory.
\newblock \emph{Psychological review}, 102\penalty0 (3):\penalty0 419, 1995.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
Melis, G{\'a}bor, Dyer, Chris, and Blunsom, Phil.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[O'Reilly(1996b)]{o1996biologically}
O'Reilly, Randall~C.
\newblock Biologically plausible error-driven learning using local activation
  differences: The generalized recirculation algorithm.
\newblock \emph{Neural computation}, 8\penalty0 (5):\penalty0 895--938, 1996b.

\bibitem[O’Reilly(1996a)]{o1996leabra}
O’Reilly, Randall~C.
\newblock \emph{The Leabra model of neural interactions and learning in the
  neocortex}.
\newblock PhD thesis, PhD thesis, Carnegie Mellon University, Pittsburgh, PA,
  USA, 1996a.

\bibitem[Parker et~al.(2011)Parker, Graff, Kong, Chen, and
  Maeda]{parker2011english}
Parker, Robert, Graff, David, Kong, Junbo, Chen, Ke, and Maeda, Kazuaki.
\newblock English gigaword fifth edition ldc2011t07. dvd.
\newblock \emph{Philadelphia: Linguistic Data Consortium}, 2011.

\bibitem[Rae et~al.(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,
  and Lillicrap]{rae2016scaling}
Rae, Jack, Hunt, Jonathan~J, Danihelka, Ivo, Harley, Timothy, Senior, Andrew~W,
  Wayne, Gregory, Graves, Alex, and Lillicrap, Tim.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3621--3629, 2016.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016one}
Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and
  Lillicrap, Timothy.
\newblock One-shot learning with memory-augmented neural networks.
\newblock \emph{arXiv preprint arXiv:1605.06065}, 2016.

\bibitem[Sprechmann et~al.(2018)Sprechmann, Jayakumar, Rae, Pritzel,
  Puigdomenech, Uria, Vinyals, Hassabis, Pascanu, and
  Blundell]{sprechmann2018memorybased}
Sprechmann, Pablo, Jayakumar, Siddhant, Rae, W.~Jack, Pritzel, Alexander,
  Puigdomenech, Adria~Badia, Uria, Benigno, Vinyals, Oriol, Hassabis, Demis,
  Pascanu, Razvan, and Blundell, Charles.
\newblock Memory-based parameter adaptation.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Weston, Fergus,
  et~al.]{sukhbaatar2015end}
Sukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et~al.
\newblock End-to-end memory networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2440--2448, 2015.

\bibitem[Sundermeyer et~al.(2012)Sundermeyer, Schl{\"u}ter, and
  Ney]{sundermeyer2012lstm}
Sundermeyer, Martin, Schl{\"u}ter, Ralf, and Ney, Hermann.
\newblock Lstm neural networks for language modeling.
\newblock In \emph{Thirteenth Annual Conference of the International Speech
  Communication Association}, 2012.

\bibitem[Thrun(1998)]{thrun1998lifelong}
Thrun, Sebastian.
\newblock Lifelong learning algorithms.
\newblock In \emph{Learning to learn}, pp.\  181--209. Springer, 1998.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, Tijmen and Hinton, Geoffrey.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Vinyals et~al.(2015)Vinyals, Fortunato, and
  Jaitly]{vinyals2015pointer}
Vinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep.
\newblock Pointer networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2692--2700, 2015.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3630--3638, 2016.

\bibitem[Yang et~al.(2017)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2017breaking}
Yang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William~W.
\newblock Breaking the softmax bottleneck: a high-rank rnn language model.
\newblock \emph{arXiv preprint arXiv:1711.03953}, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Wu, and Li]{zhou2018deep}
Zhou, Fengwei, Wu, Bin, and Li, Zhenguo.
\newblock Deep meta-learning: Learning to learn in the concept space.
\newblock \emph{arXiv preprint arXiv:1802.03596}, 2018.

\bibitem[Zipf(1935)]{zipf1935psychology}
Zipf, George~K.
\newblock The psychology of language.
\newblock \emph{NY Houghton-Mifflin}, 1935.

\end{thebibliography}
