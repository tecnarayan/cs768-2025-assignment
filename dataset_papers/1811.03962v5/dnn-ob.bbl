\newcommand{\noopsort}[1]{}
\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alaeddini et~al.(2018)Alaeddini, Alemzadeh, Mesbahi, and
  Mesbahi]{aamm18}
Atiye Alaeddini, Siavash Alemzadeh, Afshin Mesbahi, and Mehran Mesbahi.
\newblock Linear model regression on time-series data: Non-asymptotic error
  bounds and applications.
\newblock \emph{arXiv preprint arXiv:1807.06611}, 2018.

\bibitem[{Allen-Zhu} and Li(2017)]{AL2017-MMWU}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Follow the Compressed Leader: Faster Online Learning of Eigenvectors
  and Faster MMWU}.
\newblock In \emph{ICML}, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1701.01722}.

\bibitem[{Allen-Zhu} and Li(2018)]{AllenLi2017-neon2}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Neon2: Finding Local Minima via First-Order Oracles}.
\newblock In \emph{NeurIPS}, 2018.
\newblock Full version available at \url{http://arxiv.org/abs/1711.06673}.

\bibitem[{Allen-Zhu} and Li(2019)]{AL2019-resnet}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {What Can ResNet Learn Efficiently, Going Beyond Kernels?}
\newblock \emph{ArXiv e-prints}, abs/1905.10337, May 2019.

\bibitem[{Allen-Zhu} et~al.(2018{\natexlab{a}}){Allen-Zhu}, Li, and
  Liang]{all18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Yingyu Liang.
\newblock {Learning and Generalization in Overparameterized Neural Networks,
  Going Beyond Two Layers}.
\newblock \emph{arXiv preprint arXiv:1811.04918}, November 2018{\natexlab{a}}.

\bibitem[{Allen-Zhu} et~al.(2018{\natexlab{b}}){Allen-Zhu}, Li, and
  Song]{als18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1810.12065}, October 2018{\natexlab{b}}.

\bibitem[Amodei et~al.(2016)Amodei, Ananthanarayanan, Anubhai, Bai, Battenberg,
  Case, Casper, Catanzaro, Cheng, Chen, et~al.]{aaa16}
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric
  Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang
  Chen, et~al.
\newblock Deep speech 2: End-to-end speech recognition in {E}nglish and
  {M}andarin.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  173--182, 2016.

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{acgh18}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02281}, 2018{\natexlab{a}}.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Hazan, Lee, Singh, Zhang, and
  Zhang]{ahlszz18}
Sanjeev Arora, Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi~Zhang.
\newblock Towards provable control for unknown linear dynamical systems.
\newblock In \emph{ICLR workshop}, 2018{\natexlab{b}}.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and Long]{bhl18}
Peter Bartlett, Dave Helmbold, and Phil Long.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  520--529, 2018.

\bibitem[Blum and Rivest(1993)]{br93}
Avrim~L Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is np-complete.
\newblock In \emph{Machine learning: From theory to applications (A preliminary
  version of this paper was appeared in NIPS 1989)}, pages 9--28. Springer,
  1993.

\bibitem[Brutzkus and Globerson(2017)]{bg17}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In \emph{International Conference on Machine Learning (ICML)}.
  http://arxiv.org/abs/1702.07966, 2017.

\bibitem[Burke et~al.(2005)Burke, Lewis, and Overton]{burke2005robust}
James~V Burke, Adrian~S Lewis, and Michael~L Overton.
\newblock A robust gradient sampling algorithm for nonsmooth, nonconvex
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (3):\penalty0
  751--779, 2005.

\bibitem[Daniely(2016)]{d16}
Amit Daniely.
\newblock Complexity theoretic limitations on learning halfspaces.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing (STOC)}, pages 105--117. ACM, 2016.

\bibitem[Daniely(2017)]{d17}
Amit Daniely.
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 2422--2430, 2017.

\bibitem[Daniely and Shalev-Shwartz(2016)]{ds16}
Amit Daniely and Shai Shalev-Shwartz.
\newblock Complexity theoretic limitations on learning dnfâ€™s.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 815--830, 2016.

\bibitem[Dean et~al.(2017)Dean, Mania, Matni, Recht, and Tu]{dmmrt17}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock \emph{arXiv preprint arXiv:1710.01688}, 2017.

\bibitem[Dean et~al.(2018)Dean, Tu, Matni, and Recht]{dtmr18}
Sarah Dean, Stephen Tu, Nikolai Matni, and Benjamin Recht.
\newblock Safely learning to control the constrained linear quadratic
  regulator.
\newblock \emph{arXiv preprint arXiv:1809.10121}, 2018.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{dllwz18}
Simon~S. Du, Jason~D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, November 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Lee, Tian, P{\'{o}}czos, and
  Singh]{dltps18}
Simon~S. Du, Jason~D. Lee, Yuandong Tian, Barnab{\'{a}}s P{\'{o}}czos, and
  Aarti Singh.
\newblock Gradient descent learns one-hidden-layer {CNN:} don't be afraid of
  spurious local minima.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Du et~al.(2018{\natexlab{c}})Du, Zhai, Poczos, and Singh]{dzps18}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock {Gradient Descent Provably Optimizes Over-parameterized Neural
  Networks}.
\newblock \emph{ArXiv e-prints}, 2018{\natexlab{c}}.

\bibitem[Ge et~al.(2017)Ge, Lee, and Ma]{glm17}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In \emph{ICLR}, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.00501}.

\bibitem[Goel et~al.(2017)Goel, Kanade, Klivans, and Thaler]{gkkt17}
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler.
\newblock Reliably learning the {R}e{LU} in polynomial time.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2017.

\bibitem[Goel et~al.(2018)Goel, Klivans, and Meka]{gkm18}
Surbhi Goel, Adam Klivans, and Raghu Meka.
\newblock Learning one convolutional layer with overlapping patches.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{ICLR}, 2015.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{gmh13}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 6645--6649. IEEE, 2013.

\bibitem[Hardt and Ma(2017)]{hm17}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{ICLR}, 2017.
\newblock URL \url{http://arxiv.org/abs/1611.04231}.

\bibitem[Hardt et~al.(2018)Hardt, Ma, and Recht]{hmr16}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 19\penalty0
  (29):\penalty0 1--44, 2018.

\bibitem[Hazan et~al.(2017)Hazan, Singh, and Zhang]{hsz17}
Elad Hazan, Karan Singh, and Cyril Zhang.
\newblock Learning linear dynamical systems via spectral filtering.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6702--6712, 2017.

\bibitem[Hazan et~al.(2018)Hazan, Lee, Singh, Zhang, and Zhang]{hlszz18}
Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi~Zhang.
\newblock Spectral filtering for general linear dynamical systems.
\newblock In \emph{Advances in Neural Information Processing Systems (NINPS)},
  2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{hzrs16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Klivans and Sherstov(2009)]{ks09}
Adam~R Klivans and Alexander~A Sherstov.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock \emph{Journal of Computer and System Sciences}, 75\penalty0
  (1):\penalty0 2--12, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{ksh12}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Li and Liang(2018)]{ll18}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Li and Yuan(2017)]{ly17}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with {R}e{LU}
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}. http://arxiv.org/abs/1705.09886, 2017.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lhphetsw15}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and Shamir]{lss14}
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 855--863, 2014.

\bibitem[Manurangsi and Reichman(2018)]{mr18}
Pasin Manurangsi and Daniel Reichman.
\newblock The computational complexity of training {R}e{LU}(s).
\newblock \emph{arXiv preprint arXiv:1810.04207}, 2018.

\bibitem[Marecek and Tchrakian(2018)]{mt18}
Jakub Marecek and Tigran Tchrakian.
\newblock Robust spectral filtering and anomaly detection.
\newblock \emph{arXiv preprint arXiv:1808.01181}, 2018.

\bibitem[Nesterov(2004)]{Nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Programming Volume: A Basic
  course}, volume~I.
\newblock Kluwer Academic Publishers, 2004.
\newblock ISBN 1402075537.

\bibitem[Oymak(2018)]{o18}
Samet Oymak.
\newblock Learning compact neural networks with regularization.
\newblock \emph{arXiv preprint arXiv:1802.01223}, 2018.

\bibitem[Oymak and Ozay(2018)]{oo18}
Samet Oymak and Necmiye Ozay.
\newblock Non-asymptotic identification of {LTI} systems from a single
  trajectory.
\newblock \emph{arXiv preprint arXiv:1806.05722}, 2018.

\bibitem[Panigrahy et~al.(2018)Panigrahy, Rahimi, Sachdeva, and Zhang]{prsz18}
Rina Panigrahy, Ali Rahimi, Sushant Sachdeva, and Qiuyi Zhang.
\newblock Convergence results for neural networks via electrodynamics.
\newblock In \emph{ITCS}, 2018.

\bibitem[Safran and Shamir(2018)]{ss18}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer {R}e{LU} neural
  networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}.
  http://arxiv.org/abs/1712.08968, 2018.

\bibitem[Shamir(2011)]{Shamir2011-azuma-subgaussian}
Ohad Shamir.
\newblock A variant of azuma's inequality for martingales with subgaussian
  tails.
\newblock \emph{ArXiv e-prints}, abs/1110.2392, 10 2011.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{alphago16}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{alphago17}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of {G}o without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Simchowitz et~al.(2018)Simchowitz, Mania, Tu, Jordan, and
  Recht]{smtjr18}
Max Simchowitz, Horia Mania, Stephen Tu, Michael~I Jordan, and Benjamin Recht.
\newblock Learning without mixing: Towards a sharp analysis of linear system
  identification.
\newblock In \emph{Conference on Learning Theory (COLT)}. arXiv preprint
  arXiv:1802.08334, 2018.

\bibitem[Simonyan and Zisserman(2014)]{sz14}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Soltanolkotabi(2017)]{s17}
Mahdi Soltanolkotabi.
\newblock Learning {R}e{LU}s via gradient descent.
\newblock \emph{CoRR}, abs/1705.04591, 2017.
\newblock URL \url{http://arxiv.org/abs/1705.04591}.

\bibitem[Song et~al.(2017)Song, Vempala, Wilmes, and Xie]{svwx17}
Le~Song, Santosh Vempala, John Wilmes, and Bo~Xie.
\newblock On the complexity of learning neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5514--5522, 2017.

\bibitem[Soudry and Carmon(2016)]{sc16}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and Schmidhuber]{sgs15}
Rupesh~K Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Training very deep networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 2377--2385, 2015.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{googlenet15}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem[Tian(2017)]{t17}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered {R}e{LU}
  network and its applications in convergence and critical point analysis.
\newblock In \emph{International Conference on Machine Learning (ICML)}.
  http://arxiv.org/abs/1703.00560, 2017.

\bibitem[Yang(2018)]{pytorch-classification}
Wei Yang.
\newblock {Classification on CIFAR-10/100 and ImageNet with PyTorch}, 2018.
\newblock URL \url{https://github.com/bearpaw/pytorch-classification}.
\newblock Accessed: 2018-04.

\bibitem[Zagoruyko and Komodakis(2016)]{zk16}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zbhrv17}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Lin, Song, and Dhillon]{zlsd18}
Jiong Zhang, Yibo Lin, Zhao Song, and Inderjit~S Dhillon.
\newblock Learning long term dependencies via {F}ourier recurrent units.
\newblock In \emph{International Conference on Machine Learning (ICML)}. arXiv
  preprint arXiv:1803.06585, 2018.

\bibitem[Zhong et~al.(2017{\natexlab{a}})Zhong, Song, and Dhillon]{zsd17}
Kai Zhong, Zhao Song, and Inderjit~S Dhillon.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \emph{arXiv preprint arXiv:1711.03440}, 2017{\natexlab{a}}.

\bibitem[Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{zsjbd17}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}. arXiv
  preprint arXiv:1706.03175, 2017{\natexlab{b}}.

\end{thebibliography}
