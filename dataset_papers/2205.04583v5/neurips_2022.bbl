\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asi and Duchi(2019{\natexlab{a}})]{asi2019importance}
H.~Asi and J.~C. Duchi.
\newblock The importance of better models in stochastic optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (46):\penalty0 22924--22930, 2019{\natexlab{a}}.

\bibitem[Asi and Duchi(2019{\natexlab{b}})]{asi2019stochastic}
H.~Asi and J.~C. Duchi.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (3):\penalty0
  2257--2290, 2019{\natexlab{b}}.

\bibitem[Berrada et~al.(2020)Berrada, Zisserman, and
  Kumar]{berrada2019training}
L.~Berrada, A.~Zisserman, and M.~P. Kumar.
\newblock Training neural networks for and by interpolation.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Boyd et~al.(2003)Boyd, Xiao, and Mutapcic]{boyd2003subgradient}
S.~Boyd, L.~Xiao, and A.~Mutapcic.
\newblock Subgradient methods.
\newblock \emph{Lecture Notes of EE392o, Stanford University, Autumn Quarter},
  2004:\penalty0 2004--2005, 2003.

\bibitem[Chang(2011)]{chang2011libsvm}
C.-C. Chang.
\newblock {LIBSVM}: a library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2011.

\bibitem[Davis et~al.(2018)Davis, Drusvyatskiy, MacPhee, and
  Paquette]{davis2018subgradient}
D.~Davis, D.~Drusvyatskiy, K.~J. MacPhee, and C.~Paquette.
\newblock Subgradient methods for sharp weakly convex functions.
\newblock \emph{Journal of Optimization Theory and Applications}, 179\penalty0
  (3):\penalty0 962--982, 2018.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020simple}
A.~D{\'e}fossez, L.~Bottou, F.~Bach, and N.~Usunier.
\newblock A simple convergence proof of {Adam} and {Adagrad}.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[D'Orazio et~al.(2021)D'Orazio, Loizou, Laradji, and
  Mitliagkas]{d2021stochastic}
R.~D'Orazio, N.~Loizou, I.~Laradji, and I.~Mitliagkas.
\newblock Stochastic mirror descent: Convergence analysis and adaptive variants
  via the mirror stochastic {Polyak} stepsize.
\newblock \emph{arXiv preprint arXiv:2110.15412}, 2021.

\bibitem[Dua and Graff(2017)]{Dua:2019}
D.~Dua and C.~Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (7), 2011.

\bibitem[Ene and Nguyen(2020)]{ene2020adaptive}
A.~Ene and H.~L. Nguyen.
\newblock Adaptive and universal algorithms for variational inequalities with
  optimal convergence.
\newblock \emph{arXiv preprint arXiv:2010.07799}, 2020.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4), 2013.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep {L}earning}.
\newblock MIT press, 2016.

\bibitem[Gower et~al.(2021{\natexlab{a}})Gower, Sebbouh, and
  Loizou]{gower2021sgd}
R.~Gower, O.~Sebbouh, and N.~Loizou.
\newblock {SGD} for structured nonconvex functions: Learning rates,
  minibatching and interpolation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021{\natexlab{a}}.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
R.~M. Gower, N.~Loizou, X.~Qian, A.~Sailanbayev, E.~Shulgin, and
  P.~Richt{\'a}rik.
\newblock {SGD}: General analysis and improved rates.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Gower et~al.(2021{\natexlab{b}})Gower, Defazio, and
  Rabbat]{gower2021stochastic}
R.~M. Gower, A.~Defazio, and M.~Rabbat.
\newblock Stochastic {Polyak} stepsize with a moving target.
\newblock \emph{arXiv preprint arXiv:2106.11851}, 2021{\natexlab{b}}.

\bibitem[Hazan and Kakade(2019)]{hazan2019revisiting}
E.~Hazan and S.~Kakade.
\newblock Revisiting the {Polyak} step size.
\newblock \emph{arXiv preprint arXiv:1905.00313}, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv:1412.6980}, 2014.

\bibitem[Kushner and Yin(2003)]{kushner2003stochastic}
H.~Kushner and G.~G. Yin.
\newblock \emph{Stochastic {A}pproximation and {R}ecursive {A}lgorithms and
  {A}pplications}, volume~35.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Levy et~al.(2018)Levy, Yurtsever, and Cevher]{levy2018online}
K.~Y. Levy, A.~Yurtsever, and V.~Cevher.
\newblock Online adaptive methods, universality and acceleration.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{loizou2021stochastic}
N.~Loizou, S.~Vaswani, I.~H. Laradji, and S.~Lacoste-Julien.
\newblock Stochastic {Polyak} step-size for {SGD}: An adaptive learning rate
  for fast convergence.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Oberman and Prazeres(2019)]{oberman2019stochastic}
A.~M. Oberman and M.~Prazeres.
\newblock Stochastic gradient descent with {Polyak}'s learning rate.
\newblock \emph{arXiv preprint arXiv:1903.08688}, 2019.

\bibitem[Polyak(1987)]{polyak1987introduction}
B.~Polyak.
\newblock \emph{Introduction to {O}ptimization}.
\newblock Inc., Publications Division, New York, 1987.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 1951.

\bibitem[Rolinek and Martius(2018)]{rolinek2018l4}
M.~Rolinek and G.~Martius.
\newblock L4: Practical loss-based stepsize adaptation for deep learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.5 - {RMSprop}, {Coursera}: Neural networks for machine
  learning.
\newblock \emph{University of Toronto, Technical Report}, 2012.

\bibitem[Traor{\'e} and Pauwels(2021)]{traore2021sequential}
C.~Traor{\'e} and E.~Pauwels.
\newblock Sequential convergence of {AdaGrad} algorithm for smooth convex
  optimization.
\newblock \emph{Operations Research Letters}, 49\penalty0 (4):\penalty0
  452--458, 2021.

\bibitem[Vaswani et~al.(2020)Vaswani, Laradji, Kunstner, Meng, Schmidt, and
  Lacoste-Julien]{vaswani2020adaptive}
S.~Vaswani, I.~Laradji, F.~Kunstner, S.~Y. Meng, M.~Schmidt, and
  S.~Lacoste-Julien.
\newblock Adaptive gradient methods converge faster with over-parameterization
  (but you should do a line-search).
\newblock \emph{arXiv preprint arXiv:2006.06835}, 2020.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
R.~Ward, X.~Wu, and L.~Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{xie2020linear}
Y.~Xie, X.~Wu, and R.~Ward.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2019adaptive}
J.~Zhang, S.~P. Karimireddy, A.~Veit, S.~Kim, S.~Reddi, S.~Kumar, and S.~Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
