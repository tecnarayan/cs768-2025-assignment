\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed et~al.(2022{\natexlab{a}})Ahmed, Teso, Chang, Van~den Broeck, and Vergari]{ahmed2022SPLs}
Ahmed, K., Teso, S., Chang, K.-W., Van~den Broeck, G., and Vergari, A.
\newblock Semantic probabilistic layers for neuro-symbolic learning.
\newblock In \emph{Advances in Neural Information Processing Systems 35 (NeurIPS)}, 2022{\natexlab{a}}.

\bibitem[Ahmed et~al.(2022{\natexlab{b}})Ahmed, Wang, Chang, and Van~den Broeck]{ahmed2022entropy}
Ahmed, K., Wang, E., Chang, K.-W., and Van~den Broeck, G.
\newblock Neuro-symbolic entropy regularization.
\newblock In \emph{Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (UAI)}, 2022{\natexlab{b}}.

\bibitem[Ahmed et~al.(2023{\natexlab{a}})Ahmed, Chang, and Van~den Broeck]{ahmed2023pseudosl}
Ahmed, K., Chang, K.-W., and Van~den Broeck, G.
\newblock A pseudo-semantic loss for deep autoregressive models with logical constraints.
\newblock In \emph{Advances in Neural Information Processing Systems 36 (NeurIPS)}, 2023{\natexlab{a}}.

\bibitem[Ahmed et~al.(2023{\natexlab{b}})Ahmed, Zeng, Niepert, and Van~den Broeck]{ahmed2023simple}
Ahmed, K., Zeng, Z., Niepert, M., and Van~den Broeck, G.
\newblock Simple: A gradient estimator for k-subset sampling.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023{\natexlab{b}}.

\bibitem[Choi et~al.(2020)Choi, Vergari, and Van~den Broeck]{choi20probabilistic}
Choi, Y., Vergari, A., and Van~den Broeck, G.
\newblock Probabilistic circuits: A unifying framework for tractable probabilistic models.
\newblock \emph{techreport}, 2020.
\newblock URL \url{http://starai.cs.ucla.edu/papers/ProbCirc20.pdf}.

\bibitem[Choi et~al.(2021)Choi, Dang, and Van~den Broeck]{choi21group}
Choi, Y., Dang, M., and Van~den Broeck, G.
\newblock Group fairness by probabilistic modeling with latent fair decisions.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial Intelligence}, 2021.

\bibitem[Correia et~al.(2020)Correia, Peharz, and de~Campos]{correia2020joints}
Correia, A., Peharz, R., and de~Campos, C.~P.
\newblock Joints in random forests.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 11404--11415, 2020.

\bibitem[Correia et~al.(2023)Correia, Gala, Quaeghebeur, de~Campos, and Peharz]{correia2023continuous}
Correia, A.~H., Gala, G., Quaeghebeur, E., de~Campos, C., and Peharz, R.
\newblock Continuous mixtures of tractable probabilistic models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  7244--7252, 2023.

\bibitem[Dadu et~al.(2019)Dadu, Weng, Liu, and Nowatzki]{dadu2019towards}
Dadu, V., Weng, J., Liu, S., and Nowatzki, T.
\newblock Towards general purpose acceleration by exploiting common data-dependence forms.
\newblock In \emph{Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture}, pp.\  924--939, 2019.

\bibitem[Dang et~al.(2020)Dang, Vergari, and Van~den Broeck]{dang2020strudel}
Dang, M., Vergari, A., and Van~den Broeck, G.
\newblock Strudel: Learning structured-decomposable probabilistic circuits.
\newblock In \emph{International Conference on Probabilistic Graphical Models}, pp.\  137--148. PMLR, 2020.

\bibitem[Dang et~al.(2021)Dang, Khosravi, Liang, Vergari, and Van~den Broeck]{dang2021juice}
Dang, M., Khosravi, P., Liang, Y., Vergari, A., and Van~den Broeck, G.
\newblock Juice: A julia package for logic and probabilistic circuits.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  16020--16023, 2021.

\bibitem[Dang et~al.(2022)Dang, Liu, and Van~den Broeck]{dang2022sparse}
Dang, M., Liu, A., and Van~den Broeck, G.
\newblock Sparse probabilistic circuits via pruning and growing.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28374--28385, 2022.

\bibitem[Darwiche(2002)]{darwiche2002logical}
Darwiche, A.
\newblock A logical approach to factoring belief networks.
\newblock \emph{KR}, 2:\penalty0 409--420, 2002.

\bibitem[Darwiche(2003)]{darwiche2003differential}
Darwiche, A.
\newblock A differential approach to inference in bayesian networks.
\newblock \emph{Journal of the ACM (JACM)}, 50\penalty0 (3):\penalty0 280--305, 2003.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Gala et~al.(2024)Gala, de~Campos, Peharz, Vergari, and Quaeghebeur]{gala2024probabilistic}
Gala, G., de~Campos, C., Peharz, R., Vergari, A., and Quaeghebeur, E.
\newblock Probabilistic integral circuits.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  2143--2151. PMLR, 2024.

\bibitem[Gens \& Pedro(2013)Gens and Pedro]{gens2013learning}
Gens, R. and Pedro, D.
\newblock Learning the structure of sum-product networks.
\newblock In \emph{International conference on machine learning}, pp.\  873--880. PMLR, 2013.

\bibitem[Lin et~al.(2020)Lin, Zhou, Shen, Zhou, Bhagavatula, Choi, and Ren]{lin2020commongen}
Lin, B.~Y., Zhou, W., Shen, M., Zhou, P., Bhagavatula, C., Choi, Y., and Ren, X.
\newblock Commongen: A constrained text generation challenge for generative commonsense reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pp.\  1823--1840, 2020.

\bibitem[Liu \& Van~den Broeck(2021)Liu and Van~den Broeck]{liu2021tractable}
Liu, A. and Van~den Broeck, G.
\newblock Tractable regularization of probabilistic circuits.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 3558--3570, 2021.

\bibitem[Liu et~al.(2022)Liu, Mandt, and Van~den Broeck]{liu2021lossless}
Liu, A., Mandt, S., and Van~den Broeck, G.
\newblock Lossless compression with probabilistic circuits.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Zhang, and Van~den Broeck]{liu2022scaling}
Liu, A., Zhang, H., and Van~den Broeck, G.
\newblock Scaling up probabilistic circuits by latent variable distillation.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024)Liu, Niepert, and Van~den Broeck]{liu2023image}
Liu, A., Niepert, M., and Van~den Broeck, G.
\newblock Image inpainting via tractable steering of diffusion models.
\newblock 2024.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Liu, Van~den Broeck, and Liang]{liu2023expressive}
Liu, X., Liu, A., Van~den Broeck, G., and Liang, Y.
\newblock Expressive modeling is insufficient for offline rl: A tractable inference perspective.
\newblock \emph{arXiv preprint arXiv:2311.00094}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Liu, Van~den Broeck, and Liang]{liu2023understanding}
Liu, X., Liu, A., Van~den Broeck, G., and Liang, Y.
\newblock Understanding the distillation process from deep generative models to tractable probabilistic circuits.
\newblock In \emph{International Conference on Machine Learning}, pp.\  21825--21838. PMLR, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision (ICCV)}, 2015.

\bibitem[Loconte et~al.(2023)Loconte, Di~Mauro, Peharz, and Vergari]{loconte2023turn}
Loconte, L., Di~Mauro, N., Peharz, R., and Vergari, A.
\newblock How to turn your knowledge graph embeddings into generative models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Loconte et~al.(2024)Loconte, Sladek, Mengel, Trapp, Solin, Gillis, and Vergari]{loconte2023subtractive}
Loconte, L., Sladek, A.~M., Mengel, S., Trapp, M., Solin, A., Gillis, N., and Vergari, A.
\newblock Subtractive mixture models via squaring: Representation and learning.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Lowd \& Rooshenas(2015)Lowd and Rooshenas]{lowd&rooshenas2015}
Lowd, D. and Rooshenas, A.
\newblock The libra toolkit for probabilistic models.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 2459--2463, 2015.

\bibitem[Manhaeve et~al.(2018)Manhaeve, Dumancic, Kimmig, Demeester, and Raedt]{manhaeve18deepproblog}
Manhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., and Raedt, L.~D.
\newblock Deepproblog: neural probabilistic logic programming.
\newblock In \emph{Advances in Neural Information Processing Systems 36 (NeurIPS)}, 2018.

\bibitem[Mari et~al.(2023)Mari, Vessio, and Vergari]{mari2023unifying}
Mari, A., Vessio, G., and Vergari, A.
\newblock Unifying and understanding overparameterized circuit representations via low-rank tensor decompositions.
\newblock In \emph{The 6th Workshop on Tractable Probabilistic Modeling}, 2023.

\bibitem[Mathur et~al.(2023)Mathur, Gogate, and Natarajan]{mathur2023knowledge}
Mathur, S., Gogate, V., and Natarajan, S.
\newblock Knowledge intensive learning of cutset networks.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1380--1389. PMLR, 2023.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Molina et~al.(2019)Molina, Vergari, Stelzner, Peharz, Subramani, Di~Mauro, Poupart, and Kersting]{molina2019spflow}
Molina, A., Vergari, A., Stelzner, K., Peharz, R., Subramani, P., Di~Mauro, N., Poupart, P., and Kersting, K.
\newblock Spflow: An easy and extensible library for deep probabilistic learning using sum-product networks.
\newblock \emph{arXiv preprint arXiv:1901.03704}, 2019.

\bibitem[Murphy et~al.(2023)Murphy, Linderman, Chang, Li, Kara, Harper-Donnelly, and Duran-Martin]{murphy2023dynamax}
Murphy, K., Linderman, S., Chang, P.~G., Li, X., Kara, A., Harper-Donnelly, G., and Duran-Martin, G.
\newblock Dynamax, 2023.
\newblock URL \url{https://github.com/probml/dynamax}.

\bibitem[Peharz et~al.(2020{\natexlab{a}})Peharz, Lang, Vergari, Stelzner, Molina, Trapp, Van~den Broeck, Kersting, and Ghahramani]{peharz2020einsum}
Peharz, R., Lang, S., Vergari, A., Stelzner, K., Molina, A., Trapp, M., Van~den Broeck, G., Kersting, K., and Ghahramani, Z.
\newblock Einsum networks: Fast and scalable learning of tractable probabilistic circuits.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7563--7574. PMLR, 2020{\natexlab{a}}.

\bibitem[Peharz et~al.(2020{\natexlab{b}})Peharz, Vergari, Stelzner, Molina, Shao, Trapp, Kersting, and Ghahramani]{peharz2020random}
Peharz, R., Vergari, A., Stelzner, K., Molina, A., Shao, X., Trapp, M., Kersting, K., and Ghahramani, Z.
\newblock Random sum-product networks: A simple and effective approach to probabilistic deep learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  334--344. PMLR, 2020{\natexlab{b}}.

\bibitem[Poon \& Domingos(2011)Poon and Domingos]{poon2011sum}
Poon, H. and Domingos, P.
\newblock Sum-product networks: A new deep architecture.
\newblock In \emph{2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)}, pp.\  689--690. IEEE, 2011.

\bibitem[Pronobis et~al.(2017)Pronobis, Ranganath, and Rao]{pronobis2017libspn}
Pronobis, A., Ranganath, A., and Rao, R.~P.
\newblock Libspn: A library for learning and inference with sum-product networks and tensorflow.
\newblock In \emph{Principled Approaches to Deep Learning Workshop}, 2017.

\bibitem[Qian et~al.(2023)Qian, Manolache, Ahmed, Zeng, Van~den Broeck, Niepert, and Morris]{qian2023rewiring}
Qian, C., Manolache, A., Ahmed, K., Zeng, Z., Van~den Broeck, G., Niepert, M., and Morris, C.
\newblock Probabilistic task-adaptive graph rewiring.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Rabiner \& Juang(1986)Rabiner and Juang]{rabiner1986introduction}
Rabiner, L. and Juang, B.
\newblock An introduction to hidden markov models.
\newblock \emph{ieee assp magazine}, 3\penalty0 (1):\penalty0 4--16, 1986.

\bibitem[Rahman et~al.(2014)Rahman, Kothalkar, and Gogate]{rahman2014cutset}
Rahman, T., Kothalkar, P., and Gogate, V.
\newblock Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of chow-liu trees.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14}, pp.\  630--645. Springer, 2014.

\bibitem[Shah et~al.(2021)Shah, Olascoaga, Zhao, Meert, and Verhelst]{shah2021dpu}
Shah, N., Olascoaga, L. I.~G., Zhao, S., Meert, W., and Verhelst, M.
\newblock Dpu: Dag processing unit for irregular graphs with precision-scalable posit arithmetic in 28 nm.
\newblock \emph{IEEE Journal of Solid-State Circuits}, 57\penalty0 (8):\penalty0 2586--2596, 2021.

\bibitem[Vergari et~al.(2020)Vergari, Choi, Peharz, and Van~den Broeck]{vergari2020probabilistic}
Vergari, A., Choi, Y., Peharz, R., and Van~den Broeck, G.
\newblock Probabilistic circuits: Representations, inference, learning and applications.
\newblock \emph{AAAI Tutorial}, 2020.

\bibitem[Vergari et~al.(2021)Vergari, Choi, Liu, Teso, and Van~den Broeck]{vergari2021compositional}
Vergari, A., Choi, Y., Liu, A., Teso, S., and Van~den Broeck, G.
\newblock A compositional atlas of tractable circuit operations for probabilistic inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 13189--13201, 2021.

\bibitem[Wang \& Kwiatkowska(2023)Wang and Kwiatkowska]{wang2023compositional}
Wang, B. and Kwiatkowska, M.
\newblock Compositional probabilistic and causal inference using tractable circuit models.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  9488--9498. PMLR, 2023.

\bibitem[Xu et~al.(2018)Xu, Zhang, Friedman, Liang, and Van~den Broeck]{xu2018sl}
Xu, J., Zhang, Z., Friedman, T., Liang, Y., and Van~den Broeck, G.
\newblock A semantic loss function for deep learning with symbolic knowledge.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, 2018.

\bibitem[Yang et~al.(2023)Yang, Gala, and Peharz]{yang2023bayesian}
Yang, Y., Gala, G., and Peharz, R.
\newblock Bayesian structure scores for probabilistic circuits.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  563--575. PMLR, 2023.

\bibitem[Yao et~al.(2023)Yao, Trapp, Periasamy, Leslin, Singh, and Andraud]{yao2023logarithm}
Yao, L., Trapp, M., Periasamy, K., Leslin, J., Singh, G., and Andraud, M.
\newblock Logarithm-approximate floating-point multiplier for hardware-efficient inference in probabilistic circuits.
\newblock In \emph{The 6th Workshop on Tractable Probabilistic Modeling}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Dang, Peng, and Van~den Broeck]{zhang2023tractable}
Zhang, H., Dang, M., Peng, N., and Van~den Broeck, G.
\newblock Tractable control for autoregressive language generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  40932--40945. PMLR, 2023.

\end{thebibliography}
