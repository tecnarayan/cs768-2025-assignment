
@article{borkowski2019lung,
  title={Lung and colon cancer histopathological image dataset (lc25000)},
  author={Borkowski, Andrew A and Bui, Marilyn M and Thomas, L Brannon and Wilson, Catherine P and DeLand, Lauren A and Mastorides, Stephen M},
  journal={arXiv preprint arXiv:1912.12142},
  year={2019}
}


@InProceedings{pmlr-v151-yang22b,
  title = 	 { Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity },
  author =       {Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {5485--5517},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/yang22b/yang22b.pdf},
  url = 	 {https://proceedings.mlr.press/v151/yang22b.html},
  abstract = 	 { Gradient descent ascent (GDA), the simplest single-loop algorithm for nonconvex minimax optimization, is widely used in practical applications such as generative adversarial networks (GANs) and adversarial training. Albeit its desirable simplicity, recent work shows inferior convergence rates of GDA in theory, even when assuming strong concavity of the objective in terms of one variable. This paper establishes new convergence results for two alternative single-loop algorithms – alternating GDA and smoothed GDA – under the mild assumption that the objective satisfies the Polyak-Lojasiewicz (PL) condition about one variable. We prove that, to find an $\epsilon$-stationary point, (i) alternating GDA and its stochastic variant (without mini batch) respectively require $O(\kappa^{2} \epsilon^{-2})$ and $O(\kappa^{4} \epsilon^{-4})$ iterations, while (ii) smoothed GDA and its stochastic variant (without mini batch) respectively require $O(\kappa \epsilon^{-2})$ and $O(\kappa^{2} \epsilon^{-4})$ iterations. The latter greatly improves over the vanilla GDA and gives the hitherto best known complexity results among single-loop algorithms under similar settings. We further showcase the empirical efficiency of these algorithms in training GANs and robust nonlinear regression. }
}


@inproceedings{NEURIPS2020_52aaa62e,
 author = {Zhang, Jiawei and Xiao, Peijun and Sun, Ruoyu and Luo, Zhiquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7377--7389},
 publisher = {Curran Associates, Inc.},
 title = {A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/52aaa62e71f829d41d74892a18a11d59-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{NEURIPS2022_880d8999,
 author = {Zhang, Xuan and Aybat, Necdet Serhat and Gurbuzbalaban, Mert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {21668--21681},
 publisher = {Curran Associates, Inc.},
 title = {SAPD+: An Accelerated Stochastic Method for Nonconvex-Concave Minimax Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/880d8999c07a8efc9bbbeb0c38f50765-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{celona2018fine,
  title={Fine-grained face annotation using deep multi-task cnn},
  author={Celona, Luigi and Bianco, Simone and Schettini, Raimondo},
  journal={Sensors},
  volume={18},
  number={8},
  pages={2666},
  year={2018},
  publisher={MDPI}
}

@inproceedings{park2022fair,
  title={Fair contrastive learning for facial attribute classification},
  author={Park, Sungho and Lee, Jewook and Lee, Pilhyeon and Hwang, Sunhee and Kim, Dohyung and Byun, Hyeran},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10389--10398},
  year={2022}
}


@misc{boţ2023alternating,
      title={Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems}, 
      author={Radu Ioan Boţ and Axel Böhm},
      year={2023},
      eprint={2007.13605},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@inproceedings{Xie2017ControllableIT,
  title={Controllable Invariance through Adversarial Feature Learning},
  author={Qizhe Xie and Zihang Dai and Yulun Du and Eduard H. Hovy and Graham Neubig},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:34807644}
}

@article{DBLP:journals/mp/DrusvyatskiyP19,
  author       = {Dmitriy Drusvyatskiy and
                  Courtney Paquette},
  title        = {Efficiency of minimizing compositions of convex functions and smooth
                  maps},
  journal      = {Math. Program.},
  volume       = {178},
  number       = {1-2},
  pages        = {503--558},
  year         = {2019},
  url          = {https://doi.org/10.1007/s10107-018-1311-3},
  doi          = {10.1007/s10107-018-1311-3},
  timestamp    = {Thu, 07 Nov 2019 09:20:17 +0100},
  biburl       = {https://dblp.org/rec/journals/mp/DrusvyatskiyP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhu2023mil,
  title={Provable Multi-instance Deep AUC Maximization with Stochastic Pooling},
  author={Zhu, Dixian and Wang, Bokun and Chen, Zhi and Wang, Yaxing and Sonka, Milan and Wu, Xiaodong and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={},
  year={2023},
  organization={PMLR}
}

@InProceedings{pmlr-v80-ilse18a,
  title = 	 {Attention-based Deep Multiple Instance Learning},
  author =       {Ilse, Maximilian and Tomczak, Jakub and Welling, Max},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2127--2136},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ilse18a/ilse18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/ilse18a.html},
  abstract = 	 {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.}
}

@misc{https://doi.org/10.48550/arxiv.1902.08297,
  doi = {10.48550/ARXIV.1902.08297},
  
  url = {https://arxiv.org/abs/1902.08297},
  
  author = {Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D. and Razaviyayn, Meisam},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1608.04636,
  doi = {10.48550/ARXIV.1608.04636},
  
  url = {https://arxiv.org/abs/1608.04636},
  
  author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, G.1.6; I.2.6, 65K10},
  
  title = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Chen_2021,
	doi = {10.1109/tsp.2021.3092377},
  
	url = {https://doi.org/10.1109%2Ftsp.2021.3092377},
  
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {69},
  
	pages = {4937--4948},
  
	author = {Tianyi Chen and Yuejiao Sun and Wotao Yin},
  
	title = {Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization},
  
	journal = {{IEEE} Transactions on Signal Processing}
}

@article{hu2020biased,
	title={Biased Stochastic First-Order Methods for Conditional Stochastic Optimization and Applications in Meta Learning},
	author={Hu, Yifan and Zhang, Siqi and Chen, Xin and He, Niao},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}


@article{DBLP:journals/corr/abs-2206-00439,
  author       = {Tianbao Yang},
  title        = {Algorithmic Foundation of Deep X-Risk Optimization},
  journal      = {CoRR},
  volume       = {abs/2206.00439},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2206.00439},
  doi          = {10.48550/arXiv.2206.00439},
  eprinttype    = {arXiv},
  eprint       = {2206.00439},
  timestamp    = {Mon, 13 Jun 2022 15:31:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2206-00439.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.1906.00331,
  doi = {10.48550/ARXIV.1906.00331},
  
  url = {https://arxiv.org/abs/1906.00331},
  
  author = {Lin, Tianyi and Jin, Chi and Jordan, Michael I.},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{davis2018stochastic,
      title={Stochastic model-based minimization of weakly convex functions}, 
      author={Damek Davis and Dmitriy Drusvyatskiy},
      year={2018},
      eprint={1803.06523},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{drusvyatskiy2017efficiency,
      title={Efficiency of minimizing compositions of convex functions and smooth maps}, 
      author={Dmitriy Drusvyatskiy and Courtney Paquette},
      year={2017},
      eprint={1605.00125},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{jiang2022multiblocksingleprobe,
      title={Multi-block-Single-probe Variance Reduced Estimator for Coupled Compositional Optimization}, 
      author={Wei Jiang and Gang Li and Yibo Wang and Lijun Zhang and Tianbao Yang},
      year={2022},
      eprint={2207.08540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pmlr-v162-wang22ak,
  title = 	 {Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications},
  author =       {Wang, Bokun and Yang, Tianbao},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23292--23317},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wang22ak/wang22ak.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wang22ak.html},
  abstract = 	 {This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibits new insights for improving the practical implementation by sampling the batches of equal size for the outer and inner levels. Numerical experiments on AP maximization, NCA, and p-norm push corroborate some aspects of the theory.}
}

@InProceedings{quan23icml,
title={Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization},
author={Quanqi Hu and Zi-Hao Qiu and Zhishuai Guo and Lijun Zhang and Tianbao Yang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
year={2023}
}

@article{DBLP:journals/corr/abs-2202-12183,
  author       = {Zi{-}Hao Qiu and
                  Quanqi Hu and
                  Yongjian Zhong and
                  Lijun Zhang and
                  Tianbao Yang},
  title        = {Large-scale Stochastic Optimization of {NDCG} Surrogates for Deep
                  Learning with Provable Convergence},
  journal      = {CoRR},
  volume       = {abs/2202.12183},
  year         = {2022},
  url          = {https://arxiv.org/abs/2202.12183},
  eprinttype    = {arXiv},
  eprint       = {2202.12183},
  timestamp    = {Wed, 02 Mar 2022 16:35:04 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2202-12183.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhang2021RobustAP,
  title={Robust Accelerated Primal-Dual Methods for Computing Saddle Points},
  author={Xuan Zhang and Necdet Serhat Aybat and Mert Gurbuzbalaban},
  year={2021}
}

@article{DBLP:journals/csur/YangY23,
  author       = {Tianbao Yang and
                  Yiming Ying},
  title        = {{AUC} Maximization in the Era of Big Data and {AI:} {A} Survey},
  journal      = {{ACM} Comput. Surv.},
  volume       = {55},
  number       = {8},
  pages        = {172:1--172:37},
  year         = {2023},
  url          = {https://doi.org/10.1145/3554729},
  doi          = {10.1145/3554729},
  timestamp    = {Tue, 31 Jan 2023 20:44:01 +0100},
  biburl       = {https://dblp.org/rec/journals/csur/YangY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Moreau1965,
author = {Moreau, J.J.},
journal = {Bulletin de la Société Mathématique de France},
keywords = {functional analysis},
language = {fre},
pages = {273-299},
publisher = {Société mathématique de France},
title = {Proximité et dualité dans un espace hilbertien},
url = {http://eudml.org/doc/87067},
volume = {93},
year = {1965},
}



@InProceedings{pmlr-v119-lin20a,
  title = 	 {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  author =       {Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6083--6093},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/lin20a/lin20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/lin20a.html},
  abstract = 	 {We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding light on its superior practical performance in training generative adversarial networks (GANs) and other real applications.}
}


@article{DBLP:journals/mp/WangFL17,
	title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
	author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
	journal={Mathematical Programming},
	volume={161},
	number={1-2},
	pages={419--449},
	year={2017},
	publisher={Springer}
}

@article{Ghadimi2020AST,
  title={A Single Timescale Stochastic Approximation Method for Nested Stochastic Optimization},
  author={S. Ghadimi and Andrzej Ruszczy'nski and Mengdi Wang},
  journal={SIAM J. Optim.},
  year={2020},
  volume={30},
  pages={960-979}
}

@inproceedings{NEURIPS2019_b8002139,
 author = {Cutkosky, Ashok and Orabona, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Momentum-Based Variance Reduction in Non-Convex SGD},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/b8002139cdde66b87638f7f91d169d96-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2022_be76ca29,
 author = {Hu, Quanqi and Zhong, Yongjian and Yang, Tianbao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {29552--29565},
 publisher = {Curran Associates, Inc.},
 title = {Multi-block Min-max Bilevel Optimization with Applications in Multi-task Deep AUC Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/be76ca290f1b30bd16cef178bfa8adbe-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}







@article{doi:10.1137/17M1151031,
author = {Davis, Damek and Grimmer, Benjamin},
title = {Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {3},
pages = {1908-1930},
year = {2019},
doi = {10.1137/17M1151031},

URL = { https://doi.org/10.1137/17M1151031},
eprint = {https://doi.org/10.1137/17M1151031},
    abstract = { In this paper, we introduce a stochastic projected subgradient method for weakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a wide class of functions which includes the additive and convex composite classes. At a high level, the method is an inexact proximal-point iteration in which the strongly convex proximal subproblems are quickly solved with a specialized stochastic projected subgradient method. The primary contribution of this paper is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result appears to be the first convergence rate analysis of a stochastic (or even deterministic) subgradient method for the class of weakly convex functions. In addition, a two-phase variant is proposed that significantly reduces the variance of the solutions returned by the algorithm. Finally, preliminary numerical experiments are also provided. }
}


@misc{zhang2022optimal,
      title={Optimal Algorithms for Convex Nested Stochastic Composite Optimization}, 
      author={Zhe Zhang and Guanghui Lan},
      year={2022},
      eprint={2011.10076},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{wang2016accelerating,
      title={Accelerating Stochastic Composition Optimization}, 
      author={Mengdi Wang and Ji Liu and Ethan X. Fang},
      year={2016},
      eprint={1607.07329},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{zhu2023distributionally,
      title={Distributionally Robust Learning with Weakly Convex Losses: Convergence Rates and Finite-Sample Guarantees}, 
      author={Landi Zhu and Mert Gürbüzbalaban and Andrzej Ruszczyński},
      year={2023},
      eprint={2301.06619},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}



@InProceedings{pmlr-v162-zhu22g,
  title = 	 {When {AUC} meets {DRO}: Optimizing Partial {AUC} for Deep Learning with Non-Convex Convergence Guarantee},
  author =       {Zhu, Dixian and Li, Gang and Wang, Bokun and Wu, Xiaodong and Yang, Tianbao},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27548--27573},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhu22g/zhu22g.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhu22g.html},
  abstract = 	 {In this paper, we propose systematic and efficient gradient-based methods for both one-way and two-way partial AUC (pAUC) maximization that are applicable to deep learning. We propose new formulations of pAUC surrogate objectives by using the distributionally robust optimization (DRO) to define the loss for each individual positive data. We consider two formulations of DRO, one of which is based on conditional-value-at-risk (CVaR) that yields a non-smooth but exact estimator for pAUC, and another one is based on a KL divergence regularized DRO that yields an inexact but smooth (soft) estimator for pAUC. For both one-way and two-way pAUC maximization, we propose two algorithms and prove their convergence for optimizing their two formulations, respectively. Experiments demonstrate the effectiveness of the proposed algorithms for pAUC maximization for deep learning on various datasets.}
}

@misc{yang2018multilevel,
      title={Multi-Level Stochastic Gradient Methods for Nested Composition Optimization}, 
      author={Shuoguang Yang and Mengdi Wang and Ethan X. Fang},
      year={2018},
      eprint={1801.03600},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{doi:10.1137/20M1312952,
author = {Ruszczy\'{n}ski, Andrzej},
title = {A Stochastic Subgradient Method for Nonsmooth Nonconvex Multilevel Composition Optimization},
journal = {SIAM Journal on Control and Optimization},
volume = {59},
number = {3},
pages = {2301-2320},
year = {2021},
doi = {10.1137/20M1312952},

URL = {https://doi.org/10.1137/20M1312952},
eprint = {https://doi.org/10.1137/20M1312952}
,
    abstract = { We propose a single time-scale stochastic subgradient method for constrained optimization of a composition of several nonsmooth and nonconvex functions. The functions are assumed to be locally Lipschitz and differentiable in a generalized sense. Only stochastic estimates of the values and generalized derivatives of the functions are used. The method is parameter-free. We prove convergence with probability one of the method, by associating with it a system of differential inclusions and devising a nondifferentiable Lyapunov function for this system. For problems with functions having Lipschitz continuous derivatives, the method finds a point satisfying an optimality measure with error of order \$1/\sqrt{N}\$, after executing \$N\$ iterations with constant stepsize. }
}

@article{doi:10.1137/21M1406222,
author = {Balasubramanian, Krishnakumar and Ghadimi, Saeed and Nguyen, Anthony},
title = {Stochastic Multilevel Composition Optimization Algorithms with Level-Independent Convergence Rates},
journal = {SIAM Journal on Optimization},
volume = {32},
number = {2},
pages = {519-544},
year = {2022},
doi = {10.1137/21M1406222},

URL = {https://doi.org/10.1137/21M1406222},
eprint = {https://doi.org/10.1137/21M1406222},
}


@misc{jiang2022optimal,
      title={Optimal Algorithms for Stochastic Multi-Level Compositional Optimization}, 
      author={Wei Jiang and Bokun Wang and Yibo Wang and Lijun Zhang and Tianbao Yang},
      year={2022},
      eprint={2202.07530},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{doi:10.1080/10556788.2021.1895152,
author = {Hassan Rafique and Mingrui Liu and Qihang Lin and Tianbao Yang},
title = {Weakly-convex–concave min–max optimization: provable algorithms and applications in machine learning},
journal = {Optimization Methods and Software},
volume = {37},
number = {3},
pages = {1087-1121},
year  = {2022},
publisher = {Taylor & Francis},
doi = {10.1080/10556788.2021.1895152},
URL = {https://doi.org/10.1080/10556788.2021.1895152},
eprint = {https://doi.org/10.1080/10556788.2021.1895152}
}


@InProceedings{pmlr-v162-yuan22b,
  title = 	 {Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance},
  author =       {Yuan, Zhuoning and Wu, Yuexin and Qiu, Zi-Hao and Du, Xianzhi and Zhang, Lijun and Zhou, Denny and Yang, Tianbao},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25760--25782},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yuan22b/yuan22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yuan22b.html},
  abstract = 	 {In this paper, we study contrastive learning from an optimization perspective, aiming to analyze and address a fundamental issue of existing contrastive learning methods that either rely on a large batch size or a large dictionary of feature vectors. We consider a global objective for contrastive learning, which contrasts each positive pair with all negative pairs for an anchor point. From the optimization perspective, we explain why existing methods such as SimCLR require a large batch size in order to achieve a satisfactory result. In order to remove such requirement, we propose a memory-efficient Stochastic Optimization algorithm for solving the Global objective of Contrastive Learning of Representations, named SogCLR. We show that its optimization error is negligible under a reasonable condition after a sufficient number of iterations or is diminishing for a slightly different global contrastive objective. Empirically, we demonstrate that SogCLR with small batch size (e.g., 256) can achieve similar performance as SimCLR with large batch size (e.g., 8192) on self-supervised learning task on ImageNet-1K. We also attempt to show that the proposed optimization technique is generic and can be applied to solving other contrastive losses, e.g., two-way contrastive losses for bimodal contrastive learning. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org).}
}


@InProceedings{pmlr-v162-qiu22a,
  title = 	 {Large-scale Stochastic Optimization of {NDCG} Surrogates for Deep Learning with Provable Convergence},
  author =       {Qiu, Zi-Hao and Hu, Quanqi and Zhong, Yongjian and Zhang, Lijun and Yang, Tianbao},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18122--18152},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/qiu22a/qiu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/qiu22a.html},
  abstract = 	 {NDCG, namely Normalized Discounted Cumulative Gain, is a widely used ranking metric in information retrieval and machine learning. However, efficient and provable stochastic methods for maximizing NDCG are still lacking, especially for deep models. In this paper, we propose a principled approach to optimize NDCG and its top-$K$ variant. First, we formulate a novel compositional optimization problem for optimizing the NDCG surrogate, and a novel bilevel compositional optimization problem for optimizing the top-$K$ NDCG surrogate. Then, we develop efficient stochastic algorithms with provable convergence guarantees for the non-convex objectives. Different from existing NDCG optimization methods, the per-iteration complexity of our algorithms scales with the mini-batch size instead of the number of total items. To improve the effectiveness for deep learning, we further propose practical strategies by using initial warm-up and stop gradient operator. Experimental results on multiple datasets demonstrate that our methods outperform prior ranking approaches in terms of NDCG. To the best of our knowledge, this is the first time that stochastic algorithms are proposed to optimize NDCG with a provable convergence guarantee. Our proposed methods are implemented in the LibAUC library at https://libauc.org.}
}

@misc{levy2020largescale,
      title={Large-Scale Methods for Distributionally Robust Optimization}, 
      author={Daniel Levy and Yair Carmon and John C. Duchi and Aaron Sidford},
      year={2020},
      eprint={2010.05893},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}





@misc{zhang2023sapd,
      title={SAPD+: An Accelerated Stochastic Method for Nonconvex-Concave Minimax Problems}, 
      author={Xuan Zhang and Necdet Serhat Aybat and Mert Gurbuzbalaban},
      year={2023},
      eprint={2205.15084},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{chen2021tighter,
      title={Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems}, 
      author={Tianyi Chen and Yuejiao Sun and Wotao Yin},
      year={2021},
      eprint={2106.13781},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{yuan2021largescale,
      title={Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification}, 
      author={Zhuoning Yuan and Yan Yan and Milan Sonka and Tianbao Yang},
      year={2021},
      eprint={2012.03173},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{10.1093/bioinformatics/btac112,
    author = {Wang, Zhengyang and Liu, Meng and Luo, Youzhi and Xu, Zhao and Xie, Yaochen and Wang, Limei and Cai, Lei and Qi, Qi and Yuan, Zhuoning and Yang, Tianbao and Ji, Shuiwang},
    title = "{Advanced graph and sequence neural networks for molecular property prediction and drug discovery}",
    journal = {Bioinformatics},
    volume = {38},
    number = {9},
    pages = {2579-2586},
    year = {2022},
    month = {02},
    abstract = "{Properties of molecules are indicative of their functions and thus are useful in many applications. With the advances of deep-learning methods, computational approaches for predicting molecular properties are gaining increasing momentum. However, there lacks customized and advanced methods and comprehensive tools for this task currently.Here, we develop a suite of comprehensive machine-learning methods and tools spanning different computational models, molecular representations and loss functions for molecular property prediction and drug discovery. Specifically, we represent molecules as both graphs and sequences. Built on these representations, we develop novel deep models for learning from molecular graphs and sequences. In order to learn effectively from highly imbalanced datasets, we develop advanced loss functions that optimize areas under precision–recall curves (PRCs) and receiver operating characteristic (ROC) curves. Altogether, our work not only serves as a comprehensive tool, but also contributes toward developing novel and advanced graph and sequence-learning methodologies. Results on both online and offline antibiotics discovery and molecular property prediction tasks show that our methods achieve consistent improvements over prior methods. In particular, our methods achieve #1 ranking in terms of both ROC-AUC (area under curve) and PRC-AUC on the AI Cures open challenge for drug discovery related to COVID-19.Our source code is released as part of the MoleculeX library (https://github.com/divelab/MoleculeX) under AdvProp.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btac112},
    url = {https://doi.org/10.1093/bioinformatics/btac112},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/38/9/2579/49874064/btac112\_supplementary\_data.pdf},
}



@article{doi:10.1177/0272989X8900900307,
author = {Donna Katzman McClish},
title ={Analyzing a Portion of the ROC Curve},
journal = {Medical Decision Making},
volume = {9},
number = {3},
pages = {190-195},
year = {1989},
doi = {10.1177/0272989X8900900307},
    note ={PMID: 2668680},

URL = { 
        https://doi.org/10.1177/0272989X8900900307
    
},
eprint = { 
        https://doi.org/10.1177/0272989X8900900307
    
}
,
    abstract = { The area under the ROC curve is a common index summarizing the information contained in the curve. When comparing two ROC curves, though, problems arise when interest does not lie in the entire range of false-positive rates (and hence the entire area). Numerical integration is suggested for evaluating the area under a portion of the ROC curve. Variance estimates are derived. The method is applicable for either continuous or rating scale binormal data, from independent or dependent samples. An example is presented which looks at rating scale data of computed tomographic scans of the head with and without concomitant use of clinical history. The areas under the two ROC curves over an a priori range of false- positive rates are examined, as well as the areas under the two curves at a specific point. }
}



@article{doi:10.1148/radiology.201.3.8939225,
author = {Jiang, Y and Metz, C E and Nishikawa, R M},
title = {A receiver operating characteristic partial area index for highly sensitive diagnostic tests.},
journal = {Radiology},
volume = {201},
number = {3},
pages = {745-750},
year = {1996},
doi = {10.1148/radiology.201.3.8939225},
    note ={PMID: 8939225},

URL = { 
    
        https://doi.org/10.1148/radiology.201.3.8939225
    
    

},
eprint = { 
    
        https://doi.org/10.1148/radiology.201.3.8939225
    
    

}
,
    abstract = { PURPOSE: Area under a receiver operating characteristic (ROC) curve (Az) is widely used as an index of diagnostic performance. However, Az is not a meaningful summary of clinical diagnostic performance when high sensitivity must be maintained clinically. The authors developed a new ROC partial area index, which measures clinical diagnostic performance more meaningfully in such situations, to summarize an ROC curve in only a high-sensitivity region. MATERIALS AND METHODS: The mathematical formation of the partial area index was derived from the conventional binormal model. Statistical tests of apparent differences in this index were formulated analogous to that of Az. One common statistical test involving the partial area index was validated by computer simulations under realistic conditions. RESULTS: An example in mammography illustrates a situation in which the partial area index is more meaningful than Az in measuring clinical diagnostic performance. CONCLUSION: The partial area index can be used as a more meaningful alternative to the conventional Az index for highly sensitive diagnostic tests. }
}

@article{https://doi.org/10.1002/sim.4780081011,
author = {Thompson, M. L. and Zucchini, W.},
title = {On the statistical analysis of ROC curves},
journal = {Statistics in Medicine},
volume = {8},
number = {10},
pages = {1277-1290},
keywords = {ROC curves, Partial area, Analysis of variance},
doi = {https://doi.org/10.1002/sim.4780081011},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780081011},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780081011},
abstract = {Abstract We introduce a new accuracy index for receiver operating characteristic (ROC) curves, namely the partial area under the binormal ROC graph over any specified region of interest. We propose a simple but general procedure, based on a conventional analysis of variance, for comparing accuracy indices derived from two or more different modalities. The proposed method is related to and compared with existing methodology, and is illustrated by results from an experiment on optimization of density and contrast yielded by multiform photographic images used for scintigraphy.},
year = {1989}
}

@inproceedings{NEURIPS2022_ca799866,
 author = {Yao, Yao and Lin, Qihang and Yang, Tianbao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31239--31253},
 publisher = {Curran Associates, Inc.},
 title = {Large-scale Optimization of Partial AUC in a Range of False Positive Rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ca7998666c2e53cc1e882b7268414d8a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@inproceedings{levy2020large,
  title={Large-Scale Methods for Distributionally Robust Optimization},
  author={Levy, Daniel and Carmon, Yair and Duchi, John C and Sidford, Aaron},
  booktitle={Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  year={2020}
}



@inproceedings{chen2018universal,
  title={Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions},
  author={Chen, Zaiyi and Yuan, Zhuoning and Yi, Jinfeng and Zhou, Bowen and Chen, Enhong and Yang, Tianbao},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{zhou2019momentum,
  title={Momentum schemes with stochastic variance reduction for nonconvex composite optimization},
  author={Zhou, Yi and Wang, Zhe and Ji, Kaiyi and Liang, Yingbin and Tarokh, Vahid},
  journal={arXiv preprint arXiv:1902.02715},
  year={2019}
}


@inproceedings{liu2019large,
  title={Large-scale long-tailed recognition in an open world},
  author={Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2537--2546},
  year={2019}
}

@article{zhou2017places,
  title={Places: A 10 million image database for scene recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1452--1464},
  year={2017},
  publisher={IEEE}
}



@TECHREPORT{RePEc:cor:louvrp:3000,
title = {Linear convergence of first order methods for non-strongly convex optimization},
author = {Necoara, Ion and Nesterov, Yurii and Glineur, Francois},
year = {2019},
institution = {Universite catholique de Louvain, Center for Operations Research and Econometrics (CORE)},
type = {LIDAM Reprints CORE},
number = {3000},
url = {https://EconPapers.repec.org/RePEc:cor:louvrp:3000}
}



@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{deng2020distributionally,
  title={Distributionally Robust Federated Averaging},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{mohri2019agnostic,
  title={Agnostic federated learning},
  author={Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:1902.00146},
  year={2019}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}


@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{arnold2014pareto,
  title={Pareto distribution},
  author={Arnold, Barry C},
  journal={Wiley StatsRef: Statistics Reference Online},
  pages={1--10},
  year={2014},
  publisher={Wiley Online Library}
}

@article{duchi2016statistics,
	title={Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach},
	author={Duchi, C. John and Glynn, W. Peter and Namkoong, Hongseok},
	journal={Mathematics of Operations Research},
	year={2016}
}

@article{huang2020accelerated,
  title={Accelerated Zeroth-Order Momentum Methods from Mini to Minimax Optimization},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  journal={arXiv preprint arXiv:2008.08170},
  year={2020}
}


@inproceedings{smith2018don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{DBLP:journals/siamjo/ZhangX21,
  author       = {Junyu Zhang and
                  Lin Xiao},
  title        = {MultiLevel Composite Stochastic Optimization via Nested Variance Reduction},
  journal      = {{SIAM} J. Optim.},
  volume       = {31},
  number       = {2},
  pages        = {1131--1157},
  year         = {2021},
  url          = {https://doi.org/10.1137/19M1285457},
  doi          = {10.1137/19M1285457},
  timestamp    = {Sun, 25 Jul 2021 11:35:30 +0200},
  biburl       = {https://dblp.org/rec/journals/siamjo/ZhangX21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{zhang2019stochastic,
  title={A stochastic composite gradient method with incremental variance reduction},
  author={Zhang, Junyu and Xiao, Lin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9075--9085},
  year={2019}
}

@article{guo2020fast,
  title={Fast Objective and Duality Gap Convergence for Non-convex Strongly-concave Min-max Problems},
  author={Guo, Zhishuai and Yuan, Zhuoning and Yan, Yan and Yang, Tianbao},
  journal={arXiv preprint arXiv:2006.06889},
  year={2020}
}


@article{DBLP:journals/corr/abs-2008-10526,
  author    = {Krishnakumar Balasubramanian and
               Saeed Ghadimi and
               Anthony Nguyen},
  title     = {Stochastic Multi-level Composition Optimization Algorithms with Level-Independent
               Convergence Rates},
  journal   = {CoRR},
  volume    = {abs/2008.10526},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.10526},
  archivePrefix = {arXiv},
  eprint    = {2008.10526},
  timestamp = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-10526.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{chen2020solving,
  title={Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization},
  author={Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
  journal={arXiv preprint arXiv:2008.10847},
  year={2020}
}

@inproceedings{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15210--15219},
  year={2019}
}


@article{arXiv:2001.03724,
author={Luo Luo and Haishan Ye and Tong Zhang},
	Journal = {CoRR},
	  volume    = {abs/2001.03724},
title={Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems
},
year={2020}
}

@article{yang2020global,
  title={Global Convergence and Variance-Reduced Optimization for a Class of Nonconvex-Nonconcave Minimax Problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={arXiv preprint arXiv:2002.09621},
  year={2020}
}

@inproceedings{DBLP:conf/nips/PalaniappanB16,
  author    = {Balamurugan Palaniappan and
               Francis R. Bach},
  title     = {Stochastic Variance Reduction Methods for Saddle-Point Problems},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  pages     = {1408--1416},
  year      = {2016}
}



@inproceedings{namkoong2016stochastic,
  title={Stochastic gradient methods for distributionally robust optimization with f-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in neural information processing systems},
  pages={2208--2216},
  year={2016}
}


@article{liu2019stochastic,
  title={Stochastic AUC Maximization with Deep Neural Networks},
  author={Liu, Mingrui and Yuan, Zhuoning and Ying, Yiming and Yang, Tianbao},
  journal={arXiv preprint arXiv:1908.10831},
  year={2019}
}


@article{ghadimi2020single,
  title={A single timescale stochastic approximation method for nested stochastic optimization},
  author={Ghadimi, Saeed and Ruszczynski, Andrzej and Wang, Mengdi},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={960--979},
  year={2020},
  publisher={SIAM}
}

@article{rafique2018non,
  title={Non-convex min-max optimization: Provable algorithms and applications in machine learning},
  author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal={arXiv preprint arXiv:1810.02060},
  year={2018}
}

@article{yan2020sharp,
  title={Sharp Analysis of Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization},
  author={Yan, Yan and Xu, Yi and Lin, Qihang and Liu, Wei and Yang, Tianbao},
  journal={arXiv preprint arXiv:2002.05309},
  year={2020}
}

@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{a}{{c}}, Martin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  pages={2613--2621},
  year={2017}
}


@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}


@inproceedings{namkoong2017variance,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in neural information processing systems},
  pages={2971--2980},
  year={2017}
}

@article{wang2017stochastic,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@inproceedings{yuan2019stagewise,
  title={Stagewise training accelerates convergence of testing error over SGD},
  author={Yuan, Zhuoning and Yan, Yan and Jin, Rong and Yang, Tianbao},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2604--2614},
  year={2019}
}

@article{xie2016diversity,
  title={Diversity leads to generalization in neural networks},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  journal={arXiv preprint arXiv},
  volume={1611},
  year={2016}
}

@inproceedings{chen2017robust,
  title={Robust optimization for non-convex objectives},
  author={Chen, Robert S and Lucier, Brendan and Singer, Yaron and Syrgkanis, Vasilis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4705--4714},
  year={2017}
}

@article{rahimian2019distributionally,
  title={Distributionally robust optimization: A review},
  author={Rahimian, Hamed and Mehrotra, Sanjay},
  journal={arXiv preprint arXiv:1908.05659},
  year={2019}
}

@article{liu2018accelerating,
  title={Accelerating SGD with momentum for over-parameterized learning},
  author={Liu, Chaoyue and Belkin, Mikhail},
  journal={arXiv preprint arXiv:1810.13395},
  year={2018}
}



@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{wittmann2019using,
  title={Using iNaturalist in a Coverboard Protocol to Measure Data Quality: Suggestions for Project Design},
  author={Wittmann, Julie and Girman, Derek and Crocker, Daniel},
  journal={Citizen Science: Theory and Practice},
  volume={4},
  number={1},
  year={2019},
  publisher={Ubiquity Press}
}


@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}

@article{li2021tilted,
  title={On Tilted Losses in Machine Learning: Theory and Applications},
  author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
  journal={arXiv preprint arXiv:2109.06141},
  year={2021}
}

@article{chen2021solving,
  title={Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization},
  author={Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={4937--4948},
  year={2021},
  publisher={IEEE}
}


@misc{code,
  title={RECOVER Code for the paper.},
  author={Qi Qi},
  howpublished={https://github.com/qiqi-helloworld/RECOVER},
  year={2021}
}


@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR}
}

@article{yuan2016influence,
  title={On the influence of momentum acceleration on online learning},
  author={Yuan, Kun and Ying, Bicheng and Sayed, Ali H},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={6602--6667},
  year={2016},
  publisher={JMLR. org}
}

@article{DBLP:journals/jmlr/YangL18,
  author    = {Tianbao Yang and
               Qihang Lin},
  title     = {{RSG:} Beating Subgradient Method without Smoothness and Strong Convexity},
  journal   = {J. Mach. Learn. Res.},
  volume    = {19},
  pages     = {6:1--6:33},
  year      = {2018},
  url       = {http://jmlr.org/papers/v19/17-016.html},
  timestamp = {Wed, 10 Jul 2019 15:28:09 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/YangL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/XuLY17,
  author    = {Yi Xu and
               Qihang Lin and
               Tianbao Yang},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Stochastic Convex Optimization: Faster Local Growth Implies Faster
               Global Convergence},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {3821--3830},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/xu17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/XuLY17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/icml/XuQLJY19,
  author    = {Yi Xu and
               Qi Qi and
               Qihang Lin and
               Rong Jin and
               Tianbao Yang},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Stochastic Optimization for {DC} Functions and Non-smooth Non-convex
               Regularizers with Non-asymptotic Convergence},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {6942--6951},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/xu19c.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/XuQLJY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{DBLP:conf/iclr/ChenYYZCY19,
  author    = {Zaiyi Chen and
               Zhuoning Yuan and
               Jinfeng Yi and
               Bowen Zhou and
               Enhong Chen and
               Tianbao Yang},
  title     = {Universal Stagewise Learning for Non-Convex Problems with Convergence
               on Averaged Solutions},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Syx5V2CcFm},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChenYYZCY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{DBLP:journals/corr/abs-1708-03888,
  author    = {Yang You and
               Igor Gitman and
               Boris Ginsburg},
  title     = {Scaling {SGD} Batch Size to 32K for ImageNet Training},
  journal   = {CoRR},
  volume    = {abs/1708.03888},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.03888},
  archivePrefix = {arXiv},
}

@inproceedings{DBLP:conf/aaai/HuoGLH18,
  author    = {Zhouyuan Huo and
               Bin Gu and
               Ji Liu and
               Heng Huang},
  title     = {Accelerated Method for Stochastic Composition Optimization With Nonsmooth
               Regularization},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI)},
  pages     = {3287--3294},
  year      = {2018},
}




@inproceedings{qi2020simple,
  title={A Simple and Effective Framework for Pairwise Deep Metric Learning},
  author={Qi, Qi and Yan, Yan and Wu, Zixuan and Wang, Xiaoyu and Yang, Tianbao},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXVII 16},
  pages={375--391},
  year={2020},
  organization={Springer}
}

@inproceedings{li2020tilted,
  title={Tilted Empirical Risk Minimization},
  author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}


@article{DBLP:journals/corr/abs-1809-02505,
  author    = {Liu Liu and
               Ji Liu and
               Cho{-}Jui Hsieh and
               Dacheng Tao},
  title     = {Stochastically Controlled Stochastic Gradient for the Convex and Non-convex
               Composition problem},
  journal   = {CoRR},
  volume    = {abs/1809.02505},
  year      = {2018},
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}
@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}



@inproceedings{zhu2019robust,
  title={A robust zero-sum game framework for pool-based active learning},
  author={Zhu, Dixian and Li, Zhe and Wang, Xiaoyu and Gong, Boqing and Yang, Tianbao},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={517--526},
  year={2019}
}

@article{yan2019stochastic,
  title={Stochastic Primal-Dual Algorithms with Faster Convergence than $\text{O}(1/\sqrt{T})$ for Problems without Bilinear Structure},
  author={Yan, Yan and Xu, Yi and Lin, Qihang and Zhang, Lijun and Yang, Tianbao},
  journal={arXiv preprint arXiv:1904.10112},
  year={2019}
}

@inproceedings{van2018inaturalist,
  title={The inaturalist species classification and detection dataset},
  author={Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8769--8778},
  year={2018}
}

@article{ge2018rethinking,
  title={Rethinking learning rate schedules for stochastic optimization},
  author={Ge, Rong and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth},
  year={2018}
}


@inproceedings{DBLP:conf/nips/DefazioB19,
  author    = {Aaron Defazio and
               L{\'{e}}on Bottou},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
               December 2019, Vancouver, BC, Canada},
  pages     = {1753--1763},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/8452-on-the-ineffectiveness-of-variance-reduced-optimization-for-deep-learning},
  timestamp = {Fri, 06 Mar 2020 16:59:09 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/DefazioB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cvpr/SzegedyVISW16,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {2818--2826},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.308},
  doi       = {10.1109/CVPR.2016.308},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/SzegedyVISW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{wang2017accelerating,
  title={Accelerating stochastic composition optimization},
  author={Wang, Mengdi and Liu, Ji and Fang, Ethan X},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3721--3743},
  year={2017},
  publisher={JMLR. org}
}

@article{DBLP:journals/corr/Krizhevsky14,
  author    = {Alex Krizhevsky},
  title     = {One weird trick for parallelizing convolutional neural networks},
  journal   = {CoRR},
  volume    = {abs/1404.5997},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.5997},
  archivePrefix = {arXiv},
  eprint    = {1404.5997},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{anonyNeuripsSubmission,
  title={Fast Objective \& Duality Gap Convergence for Non-convex Strongly-concave minmax problems},
  author={Anonymous},
  journal={NeurIPS 2020 Submission}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  journal={Citeseer}
}

@inproceedings{DBLP:journals/jmlr/CoatesNL11,
  author    = {Adam Coates and
               Andrew Y. Ng and
               Honglak Lee},
  editor    = {Geoffrey J. Gordon and
               David B. Dunson and
               Miroslav Dud{\'{\i}}k},
  title     = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial
Intelligence and Statistics (AISTATS)},
  pages     = {215--223},
  year      = {2011}
}

@book{pisier1999volume,
  title={The volume of convex bodies and Banach space geometry},
  author={Pisier, Gilles},
  volume={94},
  year={1999},
  publisher={Cambridge University Press}
}

@article{plan2013one,
  title={One-bit compressed sensing by linear programming},
  author={Plan, Yaniv and Vershynin, Roman},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={8},
  pages={1275--1297},
  year={2013}
}

@article{rockafellarCVaR,
  title={Optimization of conditional value-at-risk},
  author={R.T. Rockafellar and S. Uryasev},
  journal={Journal of risk},
  year={2000},
  volume={2},
  pages={21-42}
}

@book{boyd_vandenberghe_2004, 
place={Cambridge}, 
title={Convex Optimization}, 
DOI={10.1017/CBO9780511804441}, 
publisher={Cambridge University Press}, 
author={Boyd, Stephen and Vandenberghe, Lieven}, 
year={2004}}

@misc{hu2023blockwise,
      title={Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization}, 
      author={Quanqi Hu and Zi-Hao Qiu and Zhishuai Guo and Lijun Zhang and Tianbao Yang},
      year={2023},
      eprint={2305.18730},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{nouiehed2019solving,
  title={Solving a class of non-convex min-max games using iterative first order methods},
  author={Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@misc{gao2023moreau,
      title={Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs}, 
      author={Lucy L. Gao and Jane J. Ye and Haian Yin and Shangzhi Zeng and Jin Zhang},
      year={2023},
      eprint={2306.16761},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{yao2022largescale,
      title={Large-scale Optimization of Partial AUC in a Range of False Positive Rates}, 
      author={Yao Yao and Qihang Lin and Tianbao Yang},
      year={2022},
      eprint={2203.01505},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sun2022algorithms,
      title={Algorithms for Difference-of-Convex (DC) Programs Based on Difference-of-Moreau-Envelopes Smoothing}, 
      author={Kaizhao Sun and Xu Andy Sun},
      year={2022},
      eprint={2104.01470},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{doi:10.1137/18M1178244,
author = {Davis, Damek and Drusvyatskiy, Dmitriy},
title = {Stochastic Model-Based Minimization of Weakly Convex Functions},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {1},
pages = {207-239},
year = {2019},
doi = {10.1137/18M1178244},

URL = { 
    
        https://doi.org/10.1137/18M1178244
    
    

},
eprint = { 
    
        https://doi.org/10.1137/18M1178244
    
    

}
,
    abstract = { We consider a family of algorithms that successively sample and minimize simple stochastic models of the objective function. We show that under reasonable conditions on approximation quality and regularity of the models, any such algorithm drives a natural stationarity measure to zero at the rate \$O(k^{-1/4})\$. As a consequence, we obtain the first complexity guarantees for the stochastic proximal point, proximal subgradient, and regularized Gauss--Newton methods for minimizing compositions of convex functions with smooth maps. The guiding principle, underlying the complexity guarantees, is that all algorithms under consideration can be interpreted as approximate descent methods on an implicit smoothing of the problem, given by the Moreau envelope. Specializing to classical circumstances, we obtain the long-sought convergence rate of the stochastic projected gradient method, without batching, for minimizing a smooth function on a closed convex set. }
}

@book{rockafellar2009variational,
  title={Variational Analysis},
  author={Rockafellar, R.T. and Wets, M. and Wets, R.J.B.},
  isbn={9783540627722},
  lccn={97035520},
  series={Grundlehren der mathematischen Wissenschaften},
  url={https://books.google.com/books?id=8ujVd_aeKygC},
  year={2009},
  publisher={Springer Berlin Heidelberg}
}



@ARTICLE{9933731,
  author={Le Thi, Hoai An and Luu, Hoang Phuc Hau and Dinh, Tao Pham},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Online Stochastic DCA With Applications to Principal Component Analysis}, 
  year={2024},
  volume={35},
  number={5},
  pages={7035-7047},
  keywords={Optimization;Stochastic processes;Programming;Principal component analysis;Standards;Convex functions;Machine learning algorithms;Difference of Convex functions (DC) programming;DC~algorithm (DCA);nonconvex optimization;online stochastic DCA (osDCA);principal component analysis (PCA)},
  doi={10.1109/TNNLS.2022.3213558}}



@InProceedings{pmlr-v119-jin20e,
  title = 	 {What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?},
  author =       {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4880--4889},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jin20e/jin20e.pdf},
  url = 	 {https://proceedings.mlr.press/v119/jin20e.html},
  abstract = 	 {Minimax optimization has found extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training and multi-agent reinforcement learning. As most of these applications involve continuous nonconvex-nonconcave formulations, a very basic question arises—“what is a proper definition of local optima?” Most previous work answers this question using classical notions of equilibria from simultaneous games, where the min-player and the max-player act simultaneously. In contrast, most applications in machine learning, including GANs and adversarial training, correspond to sequential games, where the order of which player acts first is crucial (since minimax is in general not equal to maximin due to the nonconvex-nonconcave nature of the problems). The main contribution of this paper is to propose a proper mathematical definition of local optimality for this sequential setting—local minimax, as well as to present its properties and existence results. Finally, we establish a strong connection to a basic local search algorithm—gradient descent ascent (GDA): under mild conditions, all stable limit points of GDA are exactly local minimax points up to some degenerate points.}
}


@article{Tao1986AlgorithmsFS,
  title={Algorithms for Solving a Class of Nonconvex Optimization Problems. Methods of Subgradients},
  author={Pham Dinh Tao and El Bernoussi Souad},
  journal={North-holland Mathematics Studies},
  year={1986},
  volume={129},
  pages={249-271},
  url={https://api.semanticscholar.org/CorpusID:117927490}
}

@article{dcdevelopments,
author = {Le Thi, Hoai An and Pham Dinh, Tao},
year = {2018},
month = {01},
pages = {},
title = {DC programming and DCA: thirty years of developments},
volume = {169},
journal = {Mathematical Programming},
doi = {10.1007/s10107-018-1235-y}
}








@article{doi:10.1137/20M1385706,
author = {Le Thi, Hoai An and Huynh, Van Ngai and Dinh, Tao Pham and Hau Luu, Hoang Phuc},
title = {Stochastic Difference-of-Convex-Functions Algorithms for Nonconvex Programming},
journal = {SIAM Journal on Optimization},
volume = {32},
number = {3},
pages = {2263-2293},
year = {2022},
doi = {10.1137/20M1385706},

URL = {https://doi.org/10.1137/20M1385706},
eprint = {https://doi.org/10.1137/20M1385706}
,
    abstract = { The paper deals with stochastic difference-of-convex-functions (DC) programs, that is, optimization problems whose cost function is a sum of a lower semicontinuous DC function and the expectation of a stochastic DC function with respect to a probability distribution. This class of nonsmooth and nonconvex stochastic optimization problems plays a central role in many practical applications. Although there are many contributions in the context of convex and/or smooth stochastic optimization, algorithms dealing with nonconvex and nonsmooth programs remain rare. In deterministic optimization literature, the DC algorithm (DCA) is recognized to be one of the few algorithms able to effectively solve nonconvex and nonsmooth optimization problems. The main purpose of this paper is to present some new stochastic DCAs for solving stochastic DC programs. The convergence analysis of the proposed algorithms is carefully studied, and numerical experiments are conducted to justify the algorithms' behaviors. }
}


@InProceedings{pmlr-v70-thi17a,
  title = 	 {Stochastic {DCA} for the Large-sum of Non-convex Functions Problem and its Application to Group Variable Selection in Classification},
  author =       {Hoai An Le Thi and Hoai Minh Le and Duy Nhat Phan and Bach Tran},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3394--3403},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/thi17a/thi17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/thi17a.html},
  abstract = 	 {In this paper, we present a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems whose objective function is a large sum of non-convex functions and a regularization term. We consider the $\ell_{2,0}$ regularization to deal with the group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. As an application, we applied our algorithm for the group variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.}
}

@article{LETHI2020220,
title = {Stochastic DCA for minimizing a large sum of DC functions with application to multi-class logistic regression},
journal = {Neural Networks},
volume = {132},
pages = {220-231},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303233},
author = {Hoai An {Le Thi} and Hoai Minh Le and Duy Nhat Phan and Bach Tran},
keywords = {Large sum of DC functions, DC programming, DCA, Stochastic DCA, Inexact stochastic DCA, Multi-class logistic regression},
abstract = {We consider the large sum of DC (Difference of Convex) functions minimization problem which appear in several different areas, especially in stochastic optimization and machine learning. Two DCA (DC Algorithm) based algorithms are proposed: stochastic DCA and inexact stochastic DCA. We prove that the convergence of both algorithms to a critical point is guaranteed with probability one. Furthermore, we develop our stochastic DCA for solving an important problem in multi-task learning, namely group variables selection in multi class logistic regression. The corresponding stochastic DCA is very inexpensive, all computations are explicit. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithms and their superiority over existing methods, with respect to classification accuracy, sparsity of solution as well as running time.}
}


@article{Bo2020AlternatingPS,
  title={Alternating Proximal-Gradient Steps for (Stochastic) Nonconvex-Concave Minimax Problems},
  author={Radu Ioan Boț and Axel B{\"o}hm},
  journal={SIAM J. Optim.},
  year={2020},
  volume={33},
  pages={1884-1913},
  url={https://api.semanticscholar.org/CorpusID:220794007}
}

@inproceedings{Zhang2022SAPDAA,
  title={SAPD+: An Accelerated Stochastic Method for Nonconvex-Concave Minimax Problems},
  author={Xuan Zhang and Necdet Serhat Aybat and Mert G{\"u}rb{\"u}zbalaban},
  booktitle={Neural Information Processing Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249192021}
}




@article{doi:10.1287/moor.2022.0294,
author = {Cohen, Eyal and Teboulle, Marc},
title = {Alternating and Parallel Proximal Gradient Methods for Nonsmooth, Nonconvex Minimax: A Unified Convergence Analysis},
journal = {Mathematics of Operations Research},
volume = {0},
number = {0},
pages = {null},
year = {0},
doi = {10.1287/moor.2022.0294},
URL = {https://doi.org/10.1287/moor.2022.0294},
eprint = {https://doi.org/10.1287/moor.2022.0294}
,
    abstract = { There is growing interest in nonconvex minimax problems that is driven by an abundance of applications. Our focus is on nonsmooth, nonconvex-strongly concave minimax, thus departing from the more common weakly convex and smooth models assumed in the recent literature. We present proximal gradient schemes with either parallel or alternating steps. We show that both methods can be analyzed through a single scheme within a unified analysis that relies on expanding a general convergence mechanism used for analyzing nonconvex, nonsmooth optimization problems. In contrast to the current literature, which focuses on the complexity of obtaining nearly approximate stationary solutions, we prove subsequence convergence to a critical point of the primal objective and global convergence when the latter is semialgebraic. Furthermore, the complexity results we provide are with respect to approximate stationary solutions. Lastly, we expand the scope of problems that can be addressed by generalizing one of the steps with a Bregman proximal gradient update, and together with a few adjustments to the analysis, this allows us to extend the convergence and complexity results to this broader setting.Funding: The research of E. Cohen was partially supported by a doctoral fellowship from the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240]. The research of M. Teboulle was partially supported by the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240]. }
}
@article{doi:10.1287/moor.2023.1387,
author = {Zhao, Renbo},
title = {A Primal-Dual Smoothing Framework for Max-Structured Non-Convex Optimization},
journal = {Mathematics of Operations Research},
volume = {0},
number = {0},
pages = {null},
year = {0},
doi = {10.1287/moor.2023.1387},
URL = {https://doi.org/10.1287/moor.2023.1387},
eprint = {https://doi.org/10.1287/moor.2023.1387}
,
    abstract = { We propose a primal-dual smoothing framework for finding a near-stationary point of a class of nonsmooth nonconvex optimization problems with max-structure. We analyze the primal and dual gradient complexities of the framework via two approaches, that is, the dual-then-primal and primal-the-dual smoothing approaches. Our framework improves the best-known oracle complexities of the existing method, even in the restricted problem setting. As an important part of our framework, we propose a first-order method for solving a class of (strongly) convex-concave saddle-point problems, which is based on a newly developed non-Hilbertian inexact accelerated proximal gradient algorithm for strongly convex composite minimization that enjoys duality-gap convergence guarantees. Some variants and extensions of our framework are also discussed.Funding: R. Zhao’s research is partially supported by AFOSR [Grant FA9550-22-1-0356]. }
}


@inproceedings{NEURIPS2021_56503192,
 author = {Huang, Feihu and Wu, Xidong and Huang, Heng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10431--10443},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/56503192b14190d3826780d47c0d3bf3-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{NEURIPS2020_3f8b2a81,
 author = {Yan, Yan and Xu, Yi and Lin, Qihang and Liu, Wei and Yang, Tianbao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5789--5800},
 publisher = {Curran Associates, Inc.},
 title = {Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/3f8b2a81da929223ae025fcec26dde0d-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{MancinoBall2023VariancereducedAM,
  title={Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems},
  author={Gabriel Mancino-Ball and Yangyang Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.07113},
  url={https://api.semanticscholar.org/CorpusID:259924584}
}

@article{Yang2022NestYA,
  title={Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax Optimization},
  author={Junchi Yang and Xiang Li and Niao He},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.00743},
  url={https://api.semanticscholar.org/CorpusID:249282687}
}


@article{Huang2020AcceleratedZM,
  title={Accelerated Zeroth-Order Momentum Methods from Mini to Minimax Optimization},
  author={Feihu Huang and Shangqian Gao and Jian Pei and Heng Huang},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.08170},
  url={https://api.semanticscholar.org/CorpusID:221173136}
}

@inproceedings{NEURIPS2020_ecb47fbb,
 author = {Luo, Luo and Ye, Haishan and Huang, Zhichao and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20566--20577},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ecb47fbb07a752413640f82a945530f8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Xu2020EnhancedFA,
  title={Enhanced First and Zeroth Order Variance Reduced Algorithms for Min-Max Optimization},
  author={Tengyu Xu and Zhe Wang and Yingbin Liang and H. Vincent Poor},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.09361},
  url={https://api.semanticscholar.org/CorpusID:219708545}
}



@InProceedings{pmlr-v54-nitanda17a,
  title = 	 {{Stochastic Difference of Convex Algorithm and its Application to Training Deep Boltzmann Machines}},
  author = 	 {Nitanda, Atsushi and Suzuki, Taiji},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {470--478},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/nitanda17a/nitanda17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/nitanda17a.html},
  abstract = 	 {Difference of convex functions (DC) programming is an important approach to nonconvex optimization problems because these structures can be encountered in several fields. Effective optimization methods, called DC algorithms, have been developed in deterministic optimization literature. In machine learning, a lot of important learning problems such as the Boltzmann machines (BMs) can be formulated as DC programming. However, there is no DC-like algorithm guaranteed by convergence rate analysis for stochastic problems that are more suitable settings for machine learning tasks.  In this paper, we propose a stochastic variant of DC algorithm and give computational complexities to converge to a stationary point under several situations. Moreover, we show our method includes expectation-maximization (EM) and Monte Carlo EM (MCEM) algorithm as special cases on training BMs. In other words, we extend EM/MCEM algorithm to more effective methods from DC viewpoint with theoretical convergence guarantees. Experimental results indicate that our method performs well for training binary restricted Boltzmann machines and deep Boltzmann machines without pre-training.}
}


@article{LeThi2019StochasticDA,
  title={Stochastic Difference-of-Convex Algorithms for Solving nonconvex optimization problems},
  author={Hoai An Le Thi and Ngai Van Huynh and Tao Pham Dinh},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.04334},
  url={https://api.semanticscholar.org/CorpusID:207853168}
}





@article{Sun2022AlgorithmsFD,
  title={Algorithms for Difference-of-Convex Programs Based on Difference-of-Moreau-Envelopes Smoothing},
  author={Kaizhao Sun and Xu Andy Sun},
  journal={INFORMS J. Optim.},
  year={2022},
  volume={5},
  pages={321-339},
  url={https://api.semanticscholar.org/CorpusID:233025038}
}

@article{Ye2021DifferenceOC,
  title={Difference of convex algorithms for bilevel programs with applications in hyperparameter selection},
  author={Jane J. Ye and Xiaoming Yuan and Shangzhi Zeng and Jin Zhang},
  journal={Mathematical Programming},
  year={2021},
  volume={198},
  pages={1583-1616},
  url={https://api.semanticscholar.org/CorpusID:231951344}
}


@misc{abbaszadehpeivasti2023rate,
      title={On the rate of convergence of the Difference-of-Convex Algorithm (DCA)}, 
      author={Hadi Abbaszadehpeivasti and Etienne de Klerk and Moslem Zamani},
      year={2023},
      eprint={2109.13566},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{moudafi:hal-03581239,
  TITLE = {{A Regularization of DC Optimization.}},
  AUTHOR = {Moudafi, Abdellatif},
  URL = {https://amu.hal.science/hal-03581239},
  JOURNAL = {{Pure and Applied Functional Analysis}},
  PUBLISHER = {{Yokohama Publishers}},
  YEAR = {2022},
  HAL_ID = {hal-03581239},
  HAL_VERSION = {v1},
}


@InProceedings{pmlr-v115-xu20b,
  title = 	 {Learning with Non-Convex Truncated Losses by SGD},
  author =       {Xu, Yi and Zhu, Shenghuo and Yang, Sen and Zhang, Chi and Jin, Rong and Yang, Tianbao},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {701--711},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/xu20b/xu20b.pdf},
  url = 	 {https://proceedings.mlr.press/v115/xu20b.html},
  abstract = 	 {Learning with a convex loss function has been a dominating paradigm for many years. It remains an interesting question how non-convex loss functions help improve the generalization of learning with broad applicability. In this paper, we study a family of objective functions  formed by truncating traditional loss functions, which  is applicable to both shallow learning and deep learning. Truncating loss functions has  potential to be less vulnerable and more robust to large noise in observations that could be adversarial. More importantly, it is a generic technique without assuming the knowledge of  noise distribution. To justify non-convex learning with truncated losses, we establish excess risk bounds of empirical risk minimization based on truncated losses for heavy-tailed output, and statistical error of an approximate stationary point found by stochastic gradient descent (SGD) method. Our experiments for shallow and deep learning for regression with outliers, corrupted data and heavy-tailed noise further justify the proposed method.}
}


@inproceedings{10.1145/1143844.1143870,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L\'{e}on},
title = {Trading convexity for scalability},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143870},
doi = {10.1145/1143844.1143870},
abstract = {Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {201–208},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}


@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}


@article{Grbzbalaban2022ASS,
  title={A Stochastic Subgradient Method for Distributionally Robust Non-convex and Non-smooth Learning},
  author={Mert G{\"u}rb{\"u}zbalaban and A. Ruszczynski and Landi Zhu},
  journal={Journal of Optimization Theory and Applications},
  year={2022},
  volume={194},
  pages={1014 - 1041},
  url={https://api.semanticscholar.org/CorpusID:250400128}
}

@article{Hu2023NonSmoothWF,
  title={Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization},
  author={Quanqi Hu and Dixian Zhu and Tianbao Yang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03234},
  url={https://api.semanticscholar.org/CorpusID:263672151}
}


@misc{guo2022novel,
      title={A Novel Convergence Analysis for Algorithms of the Adam Family and Beyond}, 
      author={Zhishuai Guo and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
      year={2022},
      eprint={2104.14840},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{Kiryo2017PositiveUnlabeledLW,
  title={Positive-Unlabeled Learning with Non-Negative Risk Estimator},
  author={Ryuichi Kiryo and Gang Niu and Marthinus Christoffel du Plessis and Masashi Sugiyama},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.00593},
  url={https://api.semanticscholar.org/CorpusID:7117359}
}


@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{Xiao2017FashionMNISTAN,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Han Xiao and Kashif Rasul and Roland Vollgraf},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.07747},
  url={https://api.semanticscholar.org/CorpusID:702279}
}

@misc{goodfellow2013challenges,
      title={Challenges in Representation Learning: A report on three machine learning contests}, 
      author={Ian J. Goodfellow and Dumitru Erhan and Pierre Luc Carrier and Aaron Courville and Mehdi Mirza and Ben Hamner and Will Cukierski and Yichuan Tang and David Thaler and Dong-Hyun Lee and Yingbo Zhou and Chetan Ramaiah and Fangxiang Feng and Ruifan Li and Xiaojie Wang and Dimitris Athanasakis and John Shawe-Taylor and Maxim Milakov and John Park and Radu Ionescu and Marius Popescu and Cristian Grozea and James Bergstra and Jingjing Xie and Lukasz Romaszko and Bing Xu and Zhang Chuang and Yoshua Bengio},
      year={2013},
      eprint={1307.0414},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sinha2020certifyingdistributionalrobustnessprincipled,
      title={Certifying Some Distributional Robustness with Principled Adversarial Training}, 
      author={Aman Sinha and Hongseok Namkoong and Riccardo Volpi and John Duchi},
      year={2020},
      eprint={1710.10571},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1710.10571}, 
}

@misc{madry2019deeplearningmodelsresistant,
      title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
      author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
      year={2019},
      eprint={1706.06083},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.06083}, 
}