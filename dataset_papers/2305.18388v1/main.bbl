\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achab(2020)]{achab2020ranking}
Mastane Achab.
\newblock \emph{Ranking and risk-aware reinforcement learning}.
\newblock PhD thesis, Institut Polytechnique de Paris, 2020.

\bibitem[Achab and Neu(2021)]{achab2021robustness}
Mastane Achab and Gergely Neu.
\newblock Robustness and risk management via distributional dynamic
  programming.
\newblock \emph{arXiv}, 2021.

\bibitem[Achab et~al.(2022)Achab, Alami, Djilali, Fedyanin, Moulines, and
  Panov]{achab2022distributional}
Mastane Achab, Reda Alami, Yasser Abdelaziz~Dahou Djilali, Kirill Fedyanin,
  Eric Moulines, and Maxim Panov.
\newblock Distributional deep {Q}-learning with {CVaR} regression.
\newblock In \emph{Deep Reinforcement Learning Workshop, NeurIPS}, 2022.

\bibitem[Achab et~al.(2023)Achab, Alami, Djilali, Fedyanin, and
  Moulines]{achab2023one}
Mastane Achab, Reda Alami, Yasser Abdelaziz~Dahou Djilali, Kirill Fedyanin, and
  Eric Moulines.
\newblock One-step distributional reinforcement learning.
\newblock \emph{arXiv}, 2023.

\bibitem[Andrews et~al.(1972)Andrews, Bickel, Hampel, Huber, Rogers, and
  Tukey]{andrews1972robustness}
David~F. Andrews, Peter~J. Bickel, Frank~R. Hampel, Peter~J. Huber, William~H.
  Rogers, and John~W. Tukey.
\newblock \emph{Robust Estimates of Location: Survey and Advances}.
\newblock Princeton University Press, 1972.

\bibitem[Archibald et~al.(1995)Archibald, McKinnon, and
  Thomas]{archibald1995generation}
T.~W. Archibald, K.~I.~M. McKinnon, and L.~C. Thomas.
\newblock On the generation of {M}arkov decision processes.
\newblock \emph{Journal of the Operational Research Society}, 46\penalty0
  (3):\penalty0 354--361, 1995.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{barth2018distributed}
Gabriel Barth-Maron, Matthew~W. Hoffman, David Budden, Will Dabney, Dan Horgan,
  Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare13arcade}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The {A}rcade {L}earning {E}nvironment: An evaluation platform for
  general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, June 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G. Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[Bellemare et~al.(2020)Bellemare, Candido, Castro, Gong, Machado,
  Moitra, Ponda, and Wang]{bellemare2020autonomous}
Marc~G. Bellemare, Salvatore Candido, Pablo~Samuel Castro, Jun Gong, Marlos~C.
  Machado, Subhodeep Moitra, Sameera~S. Ponda, and Ziyu Wang.
\newblock Autonomous navigation of stratospheric balloons using reinforcement
  learning.
\newblock \emph{Nature}, 588\penalty0 (7836):\penalty0 77--82, 2020.

\bibitem[Bellemare et~al.(2023)Bellemare, Dabney, and Rowland]{bdr2022}
Marc~G. Bellemare, Will Dabney, and Mark Rowland.
\newblock \emph{Distributional reinforcement learning}.
\newblock MIT Press, 2023.

\bibitem[Bertsekas and Tsitsiklis(1996)]{bertsekas1996neuro}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bodnar et~al.(2020)Bodnar, Li, Hausman, Pastor, and
  Kalakrishnan]{bodnar2020quantile}
Cristian Bodnar, Adrian Li, Karol Hausman, Peter Pastor, and Mrinal
  Kalakrishnan.
\newblock Quantile {QT-Opt} for risk-aware vision-based robotic grasping.
\newblock In \emph{Robotics: Science and Systems}, 2020.

\bibitem[Bossaerts et~al.(2020)Bossaerts, Huang, and
  Yadav]{bossaerts2020exploiting}
Peter Bossaerts, Shijie Huang, and Nitin Yadav.
\newblock Exploiting distributional temporal difference learning to deal with
  tail risk.
\newblock \emph{Risks}, 8\penalty0 (4):\penalty0 113, 2020.

\bibitem[Cheikhi and Russo(2023)]{cheikhi2023statistical}
David Cheikhi and Daniel Russo.
\newblock On the statistical benefits of temporal difference learning.
\newblock \emph{arXiv}, 2023.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Will Dabney, Georg Ostrovski, David Silver, and R{\'e}mi Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Will Dabney, Mark Rowland, Marc~G. Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018{\natexlab{b}}.

\bibitem[Dabney et~al.(2021)Dabney, Barreto, Rowland, Dadashi, Quan, Bellemare,
  and Silver]{dabney2021value}
Will Dabney, Andr{\'e} Barreto, Mark Rowland, Robert Dadashi, John Quan,
  Marc~G. Bellemare, and David Silver.
\newblock The value-improvement path: Towards better representations for
  reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Daniell(1920)]{daniell1920observations}
P.~J. Daniell.
\newblock Observations weighted according to order.
\newblock \emph{American Journal of Mathematics}, 42\penalty0 (4):\penalty0
  222--236, 1920.

\bibitem[Dayan(1992)]{dayan1992convergence}
Peter Dayan.
\newblock The convergence of {TD}($\lambda$) for general $\lambda$.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 341--362, 1992.

\bibitem[Dayan and Sejnowski(1994)]{dayan1994td}
Peter Dayan and Terrence~J. Sejnowski.
\newblock {TD}($\lambda$) converges with probability 1.
\newblock \emph{Machine Learning}, 14\penalty0 (3):\penalty0 295--301, 1994.

\bibitem[Even-Dar and Mansour(2003)]{even2003learning}
Eyal Even-Dar and Yishay Mansour.
\newblock Learning rates for {Q}-learning.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0 (1), 2003.

\bibitem[Fawzi et~al.(2022)Fawzi, Balog, Huang, Hubert, Romera-Paredes,
  Barekatain, Novikov, Ruiz, Schrittwieser, Swirszcz, Silver, Hassabis, and
  Kohli]{fawzi2022discovering}
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino
  Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J.~R.
  Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis,
  and Pushmeet Kohli.
\newblock Discovering faster matrix multiplication algorithms with
  reinforcement learning.
\newblock \emph{Nature}, 610\penalty0 (7930):\penalty0 47--53, 2022.

\bibitem[Gastwirth(1966)]{gastwirth1966robust}
Joseph~L Gastwirth.
\newblock On robust procedures.
\newblock \emph{Journal of the American Statistical Association}, 61\penalty0
  (316):\penalty0 929--948, 1966.

\bibitem[Ghiassian et~al.(2020)Ghiassian, Patterson, Garg, Gupta, White, and
  White]{ghiassian2020gradient}
Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and
  Martha White.
\newblock Gradient temporal-difference learning with regularized corrections.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv}, 2023.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf
  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,
  Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
  Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez
  del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant,
  Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
  Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, September
  2020.

\bibitem[Hessel et~al.(2021)Hessel, Danihelka, Viola, Guez, Schmitt, Sifre,
  Weber, Silver, and van Hasselt]{hessel2021muesli}
Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent
  Sifre, Theophane Weber, David Silver, and Hado van Hasselt.
\newblock Muesli: Combining improvements in policy optimization.
\newblock In \emph{Proceeding of the International Conference on Machine
  Learning}, 2021.

\bibitem[Huber(1964)]{huber1964robust}
Peter~J. Huber.
\newblock {Robust Estimation of a Location Parameter}.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73 -- 101, 1964.

\bibitem[Hunter(2007)]{hunter2007matplotlib}
J.~D. Hunter.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.

\bibitem[Imani and White(2018)]{imani2018improving}
Ehsan Imani and Martha White.
\newblock Improving regression performance with distributional losses.
\newblock In \emph{International Conference on Machine Learning}, pages
  2157--2166. PMLR, 2018.

\bibitem[Jaakkola et~al.(1994)Jaakkola, Jordan, and
  Singh]{jaakkola1994convergence}
Tommi Jaakkola, Michael~I. Jordan, and Satinder~P. Singh.
\newblock On the convergence of stochastic iterative dynamic programming
  algorithms.
\newblock \emph{Neural computation}, 6\penalty0 (6):\penalty0 1185--1201, 1994.

\bibitem[Kearns and Singh(2000)]{kearns2000bias}
Michael~J. Kearns and Satinder Singh.
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2000.

\bibitem[Klima et~al.(2019)Klima, Bloembergen, Kaisers, and
  Tuyls]{klima2019robust}
Richard Klima, Daan Bloembergen, Michael Kaisers, and Karl Tuyls.
\newblock Robust temporal difference learning for critical domains.
\newblock \emph{arXiv}, 2019.

\bibitem[Koenker(2005)]{koenker2005quantile}
Roger Koenker.
\newblock \emph{Quantile Regression}.
\newblock Econometric Society Monographs. Cambridge University Press, 2005.

\bibitem[Koenker and Bassett(1978)]{koenker1978regression}
Roger Koenker and Gilbert Bassett.
\newblock Regression quantiles.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pages
  33--50, 1978.

\bibitem[Kushner and Yin(2003)]{kushner2003stochastic}
Harold Kushner and G.~George Yin.
\newblock \emph{Stochastic approximation and recursive algorithms and
  applications}.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Lhéritier and Bondoux(2022)]{lheritier2022cramer}
Alix Lhéritier and Nicolas Bondoux.
\newblock A {Cramér} distance perspective on quantile regression based
  distributional reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2022.

\bibitem[Liu et~al.(2012)Liu, Mahadevan, and Liu]{liu2012regularized}
Bo~Liu, Sridhar Mahadevan, and Ji~Liu.
\newblock Regularized off-policy {TD}-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Lu and Giannakis(2021)]{lu2021robust}
Qin Lu and Georgios~B. Giannakis.
\newblock Robust and adaptive temporal-difference learning using an ensemble of
  {G}aussian processes.
\newblock \emph{arXiv}, 2021.

\bibitem[Luo et~al.(2021)Luo, Liu, Duan, Schulte, and
  Poupart]{luo2021distributional}
Yudong Luo, Guiliang Liu, Haonan Duan, Oliver Schulte, and Pascal Poupart.
\newblock Distributional reinforcement learning with monotonic splines.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Lyle et~al.(2019)Lyle, Bellemare, and Castro]{lyle2019comparative}
Clare Lyle, Marc~G. Bellemare, and Pablo~Samuel Castro.
\newblock A comparative analysis of expected and distributional reinforcement
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machado2018revisiting}
Marlos~C. Machado, Marc~G. Bellemare, Erik Talvitie, Joel Veness, Matthew
  Hausknecht, and Michael Bowling.
\newblock Revisiting the {A}rcade {L}earning {E}nvironment: Evaluation
  protocols and open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018.

\bibitem[Mahmood et~al.(2017)Mahmood, Yu, and Sutton]{mahmood2017multi}
Ashique~Rupam Mahmood, Huizhen Yu, and Richard~S Sutton.
\newblock Multi-step off-policy learning without importance sampling ratios.
\newblock \emph{arXiv}, 2017.

\bibitem[Manek and Kolter(2022)]{manek2022pitfalls}
Gaurav Manek and J~Zico Kolter.
\newblock The pitfalls of regularization in off-policy td learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Meyer(2021)]{meyer2021accelerated}
Dominik~Jakob Meyer.
\newblock \emph{Accelerated Gradient Algorithms for Robust Temporal Difference
  Learning}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 2021.

\bibitem[Mikolov(2012)]{mikolov2012statistical}
Tom{\'a}{\v{s}} Mikolov.
\newblock \emph{Statistical language models based on neural networks}.
\newblock PhD thesis, Brno University of Technology, Faculty of Information
  Technology, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih15human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2016.

\bibitem[Morimura et~al.(2010{\natexlab{a}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura2010nonparametric}
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
  Toshiyuki Tanaka.
\newblock Nonparametric return density estimation for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2010{\natexlab{a}}.

\bibitem[Morimura et~al.(2010{\natexlab{b}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura2010parametric}
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
  Toshiyuki Tanaka.
\newblock Parametric return density estimation for reinforcement learning.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2010{\natexlab{b}}.

\bibitem[Mosteller(1946)]{mosteller1946some}
Frederick Mosteller.
\newblock On some useful ``inefficient'' statistics.
\newblock \emph{Annals of Mathematical Statistics}, 17:\penalty0 377--408,
  1946.

\bibitem[Nguyen et~al.(2021)Nguyen, Gupta, and
  Venkatesh]{nguyen2021distributional}
Thanh~Tang Nguyen, Sunil Gupta, and Svetha Venkatesh.
\newblock Distributional reinforcement learning via moment matching.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem[Riedmiller and Braun(1993)]{riedmiller1993direct}
Martin Riedmiller and Heinrich Braun.
\newblock A direct adaptive method for faster backpropagation learning: The
  {RPROP} algorithm.
\newblock In \emph{IEEE International Conference on Neural Networks}, pages
  586--591. IEEE, 1993.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Rowland et~al.(2018)Rowland, Bellemare, Dabney, Munos, and
  Teh]{rowland2018analysis}
Mark Rowland, Marc~G. Bellemare, Will Dabney, R{\'e}mi Munos, and Yee~Whye Teh.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2018.

\bibitem[Rowland et~al.(2019)Rowland, Dadashi, Kumar, Munos, Bellemare, and
  Dabney]{rowland2019statistics}
Mark Rowland, Robert Dadashi, Saurabh Kumar, R{\'e}mi Munos, Marc~G. Bellemare,
  and Will Dabney.
\newblock Statistics and samples in distributional reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Rowland et~al.(2020)Rowland, Dabney, and Munos]{rowland2020adaptive}
Mark Rowland, Will Dabney, and R{\'e}mi Munos.
\newblock Adaptive trade-offs in off-policy learning.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2020.

\bibitem[Rowland et~al.(2023)Rowland, Munos, Azar, Tang, Ostrovski,
  Harutyunyan, Tuyls, Bellemare, and Dabney]{rowland2023analysis}
Mark Rowland, R\'emi Munos, Mohammad~Gheshlaghi Azar, Yunhao Tang, Georg
  Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc~G. Bellemare, and Will Dabney.
\newblock An analysis of quantile temporal-difference learning.
\newblock \emph{arXiv}, 2023.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, Timothy Lillicrap, and David Silver.
\newblock Mastering {A}tari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Sun et~al.(2022{\natexlab{a}})Sun, Jiang, and Kong]{sun2022does}
Ke~Sun, Bei Jiang, and Linglong Kong.
\newblock How does value distribution in distributional reinforcement learning
  help optimization?
\newblock \emph{arXiv}, 2022{\natexlab{a}}.

\bibitem[Sun et~al.(2022{\natexlab{b}})Sun, Zhao, Liu, Jiang, and
  Kong]{sun2022distributional}
Ke~Sun, Yingnan Zhao, Yi~Liu, Bei Jiang, and Linglong Kong.
\newblock Distributional reinforcement learning via {S}inkhorn iterations.
\newblock \emph{arXiv}, 2022{\natexlab{b}}.

\bibitem[Sutton(1984)]{sutton1984temporal}
Richard~S. Sutton.
\newblock \emph{Temporal credit assignment in reinforcement learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 1984.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[Th{\'e}ate et~al.(2021)Th{\'e}ate, Wehenkel, Bolland, Louppe, and
  Ernst]{theate2021distributional}
Thibaut Th{\'e}ate, Antoine Wehenkel, Adrien Bolland, Gilles Louppe, and Damien
  Ernst.
\newblock Distributional reinforcement learning with unconstrained monotonic
  neural networks.
\newblock \emph{arXiv}, 2021.

\bibitem[Tsitsiklis(1994)]{tsitsiklis1994asynchronous}
John~N. Tsitsiklis.
\newblock Asynchronous stochastic approximation and {Q}-learning.
\newblock \emph{Machine learning}, 16\penalty0 (3):\penalty0 185--202, 1994.

\bibitem[Tsitsiklis and Van~Roy(1997)]{tsitsiklis1997analysis}
John~N. Tsitsiklis and Benjamin Van~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[Vieillard et~al.(2020)Vieillard, Pietquin, and
  Geist]{vieillard2020munchausen}
Nino Vieillard, Olivier Pietquin, and Matthieu Geist.
\newblock Munchausen reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Villani(2009)]{villani2009optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal transport: Old and new}.
\newblock Springer, 2009.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{2020SciPy-NMeth}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod
  Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric
  Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake
  {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,
  E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro,
  Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.

\bibitem[Watkins(1989)]{watkins1989learning}
Christopher J. C.~H. Watkins.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge, 1989.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Christopher J. C.~H. Watkins and Peter Dayan.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[White and White(2016)]{white2016greedy}
Martha White and Adam White.
\newblock A greedy approach to adapting the trace parameter for temporal
  difference learning.
\newblock In \emph{Proceedings of the International Conference on Autonomous
  Agents and Multiagent Systems}, 2016.

\bibitem[Wurman et~al.(2022)Wurman, Barrett, Kawamoto, MacGlashan, Subramanian,
  Walsh, Capobianco, Devlic, Eckert, Fuchs, Gilpin, Khandelwal, Kompella, Lin,
  MacAlpine, Oller, Seno, Sherstan, Thomure, Aghabozorgi, Barrett, Douglas,
  Whitehead, D{\"u}rr, Stone, Spranger, and Kitano]{wurman2022outracing}
Peter~R. Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik
  Subramanian, Thomas~J. Walsh, Roberto Capobianco, Alisa Devlic, Franziska
  Eckert, Florian Fuchs, Leilani Gilpin, Piyush Khandelwal, Varun Kompella,
  HaoChih Lin, Patrick MacAlpine, Declan Oller, Takuma Seno, Craig Sherstan,
  Michael~D. Thomure, Houmehr Aghabozorgi, Leon Barrett, Rory Douglas, Dion
  Whitehead, Peter D{\"u}rr, Peter Stone, Michael Spranger, and Hiroaki Kitano.
\newblock Outracing champion {G}ran {T}urismo drivers with deep reinforcement
  learning.
\newblock \emph{Nature}, 602\penalty0 (7896):\penalty0 223--228, 2022.

\bibitem[Yang et~al.(2019)Yang, Zhao, Lin, Qin, Bian, and Liu]{yang2019fully}
Derek Yang, Li~Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Zhou et~al.(2020)Zhou, Wang, and Feng]{zhou2020non}
Fan Zhou, Jianing Wang, and Xingdong Feng.
\newblock Non-crossing quantile regression for distributional reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\end{thebibliography}
