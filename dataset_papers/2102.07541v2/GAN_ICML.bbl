\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramowitz \& Stegun(1972)Abramowitz and Stegun]{ASbook}
Abramowitz, M. and Stegun, I.~A.
\newblock \emph{Handbook of Mathematical Functions with Formulas, Graphs, and
  Mathematical Tables}.
\newblock 9th edition, 1972.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{bottou2017_wass_gan}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock {W}asserstein generative adversarial networks.
\newblock \emph{ICML}, 2017.

\bibitem[Barron(1993)]{barron1993universal}
Barron, A.~R.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information Theory}, 39\penalty0
  (3):\penalty0 930--945, 1993.

\bibitem[Chizat(2021)]{chizat2019sparse}
Chizat, L.
\newblock Sparse optimization on measures with over-parameterized gradient
  descent.
\newblock \emph{Mathematical Programming}, 2021.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Cho \& Suh(2019)Cho and Suh]{cho2019}
Cho, J. and Suh, C.
\newblock Wasserstein {GAN} can perform {PCA}.
\newblock \emph{Allerton Conference}, 2019.

\bibitem[Cybenko(1989)]{Cybenko1989}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Domingo-Enrich et~al.(2020)Domingo-Enrich, Jelassi, Mensch, Rotskoff,
  and Bruna]{mean-field-gan2020}
Domingo-Enrich, Jelassi, Mensch, Rotskoff, and Bruna.
\newblock A mean-field analysis of two-player zero-sum games.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{ICLR}, 2019.

\bibitem[Feizi et~al.(2020)Feizi, Farnia, Ginart, and Tse]{Feizi2020}
Feizi, S., Farnia, F., Ginart, T., and Tse, D.
\newblock Understanding {GANs} in the {LQG} setting: Formulation,
  generalization and stability.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 304--311, 2020.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{NEURIPS2018_be3087e7}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of {DNN}s.
\newblock \emph{NeurIPS}, 31, 2018.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge15}
Ge, R., Huang, F., Jin, C., and Yuan, Y.
\newblock Escaping from saddle points-online stochastic gradient for tensor
  decomposition.
\newblock \emph{COLT}, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016}
Ge, R., Lee, J.~D., and Ma, T.
\newblock Matrix completion has no spurious local minimum.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017}
Ge, R., Jin, C., and Zheng, Y.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock \emph{ICML}, 2017.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and Wyart]{Geiger_2020}
Geiger, M., Spigler, S., Jacot, A., and Wyart, M.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (11):\penalty0 113301, 2020.

\bibitem[Goodfellow(2016)]{goodfellow_tutorial}
Goodfellow, I.
\newblock {NIPS} 2016 tutorial: Generative adversarial networks.
\newblock \emph{arXiv:1701.00160}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow_gan_2014}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{NeurIPS}, 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{wass_gan_training2017}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
\newblock Improved training of {W}asserstein {GANs}.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Hornik(1991)]{HORNIK1991251}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hsieh et~al.(2019)Hsieh, Liu, and Cevher]{pmlr-v97-hsieh19b}
Hsieh, Y.-P., Liu, C., and Cevher, V.
\newblock Finding mixed {N}ash equilibria of generative adversarial networks.
\newblock \emph{ICML}, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NTK2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and Jordan]{jin17}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock \emph{ICML}, 2017.

\bibitem[Jones(1992)]{jones1992}
Jones, L.~K.
\newblock A simple lemma on greedy approximation in {H}ilbert space and
  convergence rates for projection pursuit regression and neural network
  training.
\newblock \emph{Annals of Statistics}, 20\penalty0 (1):\penalty0 608--613,
  1992.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and
  Arora]{NEURIPS2019_46a4378f}
Kuditipudi, R., Wang, X., Lee, H., Zhang, Y., Li, Z., Hu, W., Ge, R., and
  Arora, S.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as {G}aussian processes.
\newblock \emph{ICLR}, 2018.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016}
Lee, J.~D., Simchowitz, M., Jordan, M.~I., and Recht, B.
\newblock Gradient descent only converges to minimizers.
\newblock \emph{COLT}, 2016.

\bibitem[Lee et~al.(2019)Lee, Panageas, Piliouras, Simchowitz, Jordan, and
  Recht]{lee2019}
Lee, J.~D., Panageas, I., Piliouras, G., Simchowitz, M., Jordan, M.~I., and
  Recht, B.
\newblock First-order methods almost always avoid strict saddle points.
\newblock \emph{Mathematical Programming}, 176\penalty0 (1--2):\penalty0
  311--337, 2019.

\bibitem[Lei et~al.(2020)Lei, Lee, Dimakis, and Daskalakis]{pmlr-v119-lei20b}
Lei, Q., Lee, J., Dimakis, A., and Daskalakis, C.
\newblock {SGD} learns one-layer networks in {WGAN}s.
\newblock \emph{ICML}, 2020.

\bibitem[Leshno et~al.(1993)Leshno, Lin, Pinkus, and Schocken]{LESHNO1993}
Leshno, M., Lin, V.~Y., Pinkus, A., and Schocken, S.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 861--867, 1993.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Ding, and Sun]{Li2021}
Li, D., Ding, T., and Sun, R.
\newblock On the benefit of width for neural networks: Disappearance of basins.
\newblock \emph{arXiv:1812.11039}, 2018{\natexlab{a}}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[Li \& Liang(2018)Li and Liang]{NEURIPS2018_54fe976b}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Liang et~al.(2018{\natexlab{a}})Liang, Sun, Lee, and
  Srikant]{NEURIPS2018_a0128693}
Liang, S., Sun, R., Lee, J.~D., and Srikant, R.
\newblock Adding one neuron can eliminate all bad local minima.
\newblock \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Liang et~al.(2018{\natexlab{b}})Liang, Sun, Li, and
  Srikant]{pmlr-v80-liang18a}
Liang, S., Sun, R., Li, Y., and Srikant, R.
\newblock Understanding the loss surface of neural networks for binary
  classification.
\newblock \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{MeiE7665}
Mei, S., Montanari, A., and Nguyen, P.-M.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Mean-field theory of two-layers neural networks: Dimension-free
  bounds and kernel limit.
\newblock \emph{COLT}, \penalty0 (99), 2019.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{gan_training_mescheder2018}
Mescheder, L., Geiger, A., and Nowozin, S.
\newblock Which training methods for {GAN}s do actually converge?
\newblock \emph{ICML}, 2018.

\bibitem[Neal(1996)]{neal1996}
Neal, R.~M.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pp.\  29--53. 1996.

\bibitem[Nguyen et~al.(2019)Nguyen, Mukkamala, and Hein]{nguyen2018on}
Nguyen, Q., Mukkamala, M.~C., and Hein, M.
\newblock On the loss landscape of a class of deep neural networks with no bad
  local valleys.
\newblock \emph{ICLR}, 2019.

\bibitem[Pisier(1980-1981)]{SAF_1980-1981____A5_0}
Pisier, G.
\newblock Remarques sur un r\'esultat non publi\'e de b. maurey.
\newblock \emph{S\'eminaire d'Analyse fonctionnelle (dit ``Maurey-Schwartz'')},
  1980-1981.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi2007random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock \emph{NeurIPS}, 2007.

\bibitem[Rahimi \& Recht(2008{\natexlab{a}})Rahimi and Recht]{4797607}
Rahimi, A. and Recht, B.
\newblock Uniform approximation of functions with random bases.
\newblock \emph{Allerton Conference}, 2008{\natexlab{a}}.

\bibitem[Rahimi \& Recht(2008{\natexlab{b}})Rahimi and Recht]{kitchensink2008}
Rahimi, A. and Recht, B.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock \emph{NeurIPS}, 2008{\natexlab{b}}.

\bibitem[Rotskoff \& Vanden-Eijnden(2018)Rotskoff and
  Vanden-Eijnden]{rotskoff2018}
Rotskoff, G. and Vanden-Eijnden, E.
\newblock Parameters as interacting particles: Long time convergence and
  asymptotic error scaling of neural networks.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Rotskoff et~al.(2019)Rotskoff, Jelassi, Bruna, and
  Vanden-Eijnden]{Rotskoff2019}
Rotskoff, G., Jelassi, S., Bruna, J., and Vanden-Eijnden, E.
\newblock Neuron birth-death dynamics accelerates gradient descent and
  converges asymptotically.
\newblock \emph{ICML}, 2019.

\bibitem[Sanjabi et~al.(2018)Sanjabi, Ba, Razaviyayn, and Lee]{sanjabi2018}
Sanjabi, M., Ba, J., Razaviyayn, M., and Lee, J.~D.
\newblock On the convergence and robustness of training {GANs} with regularized
  optimal transport.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Sanjabi et~al.(2019)Sanjabi, Baharlouei, Razaviyayn, and
  Lee]{Sanjabi2019}
Sanjabi, M., Baharlouei, S., Razaviyayn, M., and Lee, J.~D.
\newblock When does non-orthogonal tensor decomposition have no spurious local
  minima?
\newblock \emph{arXiv:1911.09815}, 2019.

\bibitem[Shevchenko \& Mondelli(2020)Shevchenko and
  Mondelli]{pmlr-v119-shevchenko20a}
Shevchenko, A. and Mondelli, M.
\newblock Landscape connectivity and dropout stability of {SGD} solutions for
  over-parameterized neural networks.
\newblock \emph{ICML}, 2020.

\bibitem[Sirignano \& Spiliopoulos(2020{\natexlab{a}})Sirignano and
  Spiliopoulos]{Sirignano2020}
Sirignano, J. and Spiliopoulos, K.
\newblock Mean field analysis of neural networks: A law of large numbers.
\newblock \emph{SIAM Journal on Applied Mathematics}, 80\penalty0 (2):\penalty0
  725--752, 2020{\natexlab{a}}.

\bibitem[Sirignano \& Spiliopoulos(2020{\natexlab{b}})Sirignano and
  Spiliopoulos]{Sirignano20202}
Sirignano, J. and Spiliopoulos, K.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock \emph{Stochastic Processes and Their Applications}, 130\penalty0
  (3):\penalty0 1820 -- 1852, 2020{\natexlab{b}}.

\bibitem[Sun(2020)]{sun2020_review}
Sun, R.
\newblock Optimization for deep learning: An overview.
\newblock \emph{Journal of the Operations Research Society of China},
  8\penalty0 (2):\penalty0 249--294, 2020.

\bibitem[Sun et~al.(2020{\natexlab{a}})Sun, Fang, and Schwing]{sun2020}
Sun, R., Fang, T., and Schwing, A.
\newblock Towards a better global loss landscape of {GANs}.
\newblock \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Sun et~al.(2020{\natexlab{b}})Sun, Li, Liang, Ding, and
  Srikant]{9194023}
Sun, R., Li, D., Liang, S., Ding, T., and Srikant, R.
\newblock The global landscape of neural networks: An overview.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  95--108, 2020{\natexlab{b}}.

\bibitem[Sussmann(1992)]{SUSSMANN1992589}
Sussmann, H.~J.
\newblock Uniqueness of the weights for minimal feedforward nets with a given
  input-output map.
\newblock \emph{Neural Networks}, 5\penalty0 (4):\penalty0 589--593, 1992.

\bibitem[Telgarsky(2020)]{Telgarsky2020}
Telgarsky, M.
\newblock Deep learning theory lecture notes, Fall 2020.

\bibitem[Wu et~al.(2018)Wu, Luo, and Lee]{wu2018}
Wu, C., Luo, J., and Lee, J.~D.
\newblock No spurious local minima in a two hidden unit {ReLU} network.
\newblock \emph{ICLR Workshop}, 2018.

\end{thebibliography}
