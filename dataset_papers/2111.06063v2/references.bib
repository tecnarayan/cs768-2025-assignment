
% Robustness and NTK

@article{gao2019convergence,
  title={Convergence of adversarial training in overparametrized neural networks},
  author={Gao, Ruiqi and Cai, Tianle and Li, Haochuan and Wang, Liwei and Hsieh, Cho-Jui and Lee, Jason D},
  journal={arXiv preprint arXiv:1906.07916},
  year={2019}
}

@article{wu2020does,
  title={Does Network Width Really Help Adversarial Robustness?},
  author={Wu, Boxi and Chen, Jinghui and Cai, Deng and He, Xiaofei and Gu, Quanquan},
  journal={arXiv preprint arXiv:2010.01279},
  year={2020}
}

@article{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={arXiv preprint arXiv:1905.12173},
  year={2019}
}

@article{allen2020feature,
  title={Feature purification: How adversarial training performs robust deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2005.10190},
  year={2020}
}

@inproceedings{deng2020interpreting,
  title={Interpreting Robust Optimization via Adversarial Influence Functions},
  author={Deng, Zhun and Dwork, Cynthia and Wang, Jialiang and Zhang, Linjun},
  booktitle={International Conference on Machine Learning},
  pages={2464--2473},
  year={2020},
  organization={PMLR}
}



% Robustness

@inproceedings{weng2018towards,
  title={Towards fast computation of certified robustness for relu networks},
  author={Weng, Lily and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Daniel, Luca and Boning, Duane and Dhillon, Inderjit},
  booktitle={International Conference on Machine Learning},
  pages={5276--5285},
  year={2018},
  organization={PMLR}
}

@article{zhang2018efficient,
  title={Efficient neural network robustness certification with general activation functions},
  author={Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  journal={arXiv preprint arXiv:1811.00866},
  year={2018}
}

% IBP bound, + robust training
@article{gowal2018effectiveness,
  title={On the effectiveness of interval bound propagation for training verifiably robust models},
  author={Gowal, Sven and Dvijotham, Krishnamurthy and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1810.12715},
  year={2018}
}

@article{yang2020closer,
  title={A closer look at accuracy vs. robustness},
  author={Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Ruslan and Chaudhuri, Kamalika},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={7472--7482},
  year={2019},
  organization={PMLR}
}

@inproceedings{yin2019rademacher,
  title={Rademacher complexity for adversarially robust generalization},
  author={Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={7085--7094},
  year={2019},
  organization={PMLR}
}






% NTK

@article{domingos2020every,
  title={Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
  author={Domingos, Pedro},
  journal={arXiv preprint arXiv:2012.00152},
  year={2020}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8141--8150},
  year={2019}
}

@article{du2019graph,
  title={Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
  author={Du, Simon S and Hou, Kangcheng and P{\'o}czos, Barnab{\'a}s and Salakhutdinov, Ruslan and Wang, Ruosong and Xu, Keyulu},
  journal={arXiv preprint arXiv:1905.13192},
  year={2019}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8572--8583},
  year={2019}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}


@article{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{liu2020loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@book{ambrosio2008gradient,
  title={Gradient flows: in metric spaces and in the space of probability measures},
  author={Ambrosio, Luigi and Gigli, Nicola and Savar{\'e}, Giuseppe},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{yang2020feature,
  title={Feature Learning in Infinite-Width Neural Networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{hu2019simple,
  title={Simple and effective regularization methods for training on noisily labeled data with generalization guarantee},
  author={Hu, Wei and Li, Zhiyuan and Yu, Dingli},
  journal={arXiv preprint arXiv:1905.11368},
  year={2019}
}

@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1905.13210},
  year={2019}
}

@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}


@inproceedings{hu2021regularization,
  title={Regularization Matters: A Nonparametric Perspective on Overparametrized Neural Network},
  author={Hu, Tianyang and Wang, Wenjia and Lin, Cong and Cheng, Guang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={829--837},
  year={2021},
  organization={PMLR}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{huang2020neural,
  title={On the neural tangent kernel of deep networks with orthogonal initialization},
  author={Huang, Wei and Du, Weitao and Da Xu, Richard Yi},
  journal={arXiv preprint arXiv:2004.05867},
  year={2020}
}





% SVM
@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{shalev2011pegasos,
  title={Pegasos: Primal estimated sub-gradient solver for svm},
  author={Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan and Cotter, Andrew},
  journal={Mathematical programming},
  volume={127},
  number={1},
  pages={3--30},
  year={2011},
  publisher={Springer}
}

@inproceedings{hsieh2008dual,
  title={A dual coordinate descent method for large-scale linear SVM},
  author={Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and Keerthi, S Sathiya and Sundararajan, Sellamanickam},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={408--415},
  year={2008}
}


@article{tang2013deep,
  title={Deep learning using linear support vector machines},
  author={Tang, Yichuan},
  journal={arXiv preprint arXiv:1306.0239},
  year={2013}
}
@inproceedings{liu2016large,
  title={Large-margin softmax loss for convolutional neural networks.},
  author={Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
  booktitle={ICML},
  volume={2},
  number={3},
  pages={7},
  year={2016}
}
@article{sokolic2017robust,
  title={Robust large margin deep neural networks},
  author={Sokoli{\'c}, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel RD},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={16},
  pages={4265--4280},
  year={2017},
  publisher={IEEE}
}
@inproceedings{liang2017soft,
  title={Soft-margin softmax for deep classification},
  author={Liang, Xuezhi and Wang, Xiaobo and Lei, Zhen and Liao, Shengcai and Li, Stan Z},
  booktitle={International Conference on Neural Information Processing},
  pages={413--421},
  year={2017},
  organization={Springer}
}
@inproceedings{sun2016depth,
  title={On the depth of deep neural networks: A theoretical view},
  author={Sun, Shizhao and Chen, Wei and Wang, Liwei and Liu, Xiaoguang and Liu, Tie-Yan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}
@article{elsayed2018large,
  title={Large margin deep networks for classification},
  author={Elsayed, Gamaleldin F and Krishnan, Dilip and Mobahi, Hossein and Regan, Kevin and Bengio, Samy},
  journal={arXiv preprint arXiv:1803.05598},
  year={2018}
}

@article{xu2009robustness,
  title={Robustness and Regularization of Support Vector Machines.},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={Journal of machine learning research},
  volume={10},
  number={7},
  year={2009}
}

@article{smola1998connection,
  title={The connection between regularization operators and support vector kernels},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard and M{\"u}ller, Klaus-Robert},
  journal={Neural networks},
  volume={11},
  number={4},
  pages={637--649},
  year={1998},
  publisher={Elsevier}
}

@article{andras2002equivalence,
  title={The equivalence of support vector machine and regularization neural networks},
  author={Andras, Peter},
  journal={Neural Processing Letters},
  volume={15},
  number={2},
  pages={97--104},
  year={2002},
  publisher={Springer}
}

@article{poggio1990networks,
  title={Networks for approximation and learning},
  author={Poggio, Tomaso and Girosi, Federico},
  journal={Proceedings of the IEEE},
  volume={78},
  number={9},
  pages={1481--1497},
  year={1990},
  publisher={IEEE}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
 }

@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks},
  author={Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2285--2301},
  year={2019},
  publisher={JMLR. org}
}

@article{long2019generalization,
  title={Generalization bounds for deep convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{pellegrini2020analytic,
  title={An analytic theory of shallow networks dynamics for hinge loss classification},
  author={Pellegrini, Franco and Biroli, Giulio},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}





% implicit bias
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}
@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
@article{rosset2004boosting,
  title={Boosting as a regularized path to a maximum margin classifier},
  author={Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
  journal={The Journal of Machine Learning Research},
  volume={5},
  pages={941--973},
  year={2004},
  publisher={JMLR. org}
}
@inproceedings{nacson2019lexicographic,
  title={Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4683--4692},
  year={2019},
  organization={PMLR}
}
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}
@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason and Liu, Qiang and Ma, Tengyu},
  year={2019}
}

@phdthesis{cho2012kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin},
  year={2012},
  school={UC San Diego}
}
@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}
@article{garriga2018deep,
  title={Deep convolutional networks as shallow gaussian processes},
  author={Garriga-Alonso, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  journal={arXiv preprint arXiv:1808.05587},
  year={2018}
}

@incollection{neal1996priors,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
  publisher={Springer}
}




% mean field
@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1805.09545},
  year={2018}
}

@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}
















% multiple kernel learning

@article{arora2019harnessing,
  title={Harnessing the power of infinitely wide deep nets on small-data tasks},
  author={Arora, Sanjeev and Du, Simon S and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
  journal={arXiv preprint arXiv:1910.01663},
  year={2019}
}

@article{lanckriet2004learning,
  title={Learning the kernel matrix with semidefinite programming},
  author={Lanckriet, Gert RG and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I},
  journal={Journal of Machine learning research},
  volume={5},
  number={Jan},
  pages={27--72},
  year={2004}
}

@article{gonen2011multiple,
  title={Multiple kernel learning algorithms},
  author={G{\"o}nen, Mehmet and Alpayd{\i}n, Ethem},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={2211--2268},
  year={2011},
  publisher={JMLR. org}
}





% books
@book{cristianini2000introduction,
  title={An introduction to support vector machines and other kernel-based learning methods},
  author={Cristianini, Nello and Shawe-Taylor, John and others},
  year={2000},
  publisher={Cambridge university press}
}


@book{shawe2004kernel,
  title={Kernel methods for pattern analysis},
  author={Shawe-Taylor, John and Cristianini, Nello and others},
  year={2004},
  publisher={Cambridge university press}
}


@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}

@book{steinwart2008support,
  title={Support vector machines},
  author={Steinwart, Ingo and Christmann, Andreas},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}




% Optimization
@article{boyd2003subgradient,
  title={Subgradient methods},
  author={Boyd, Stephen and Xiao, Lin and Mutapcic, Almir},
  journal={lecture notes of EE392o, Stanford University, Autumn Quarter},
  volume={2004},
  pages={2004--2005},
  year={2003}
}
@book{bertsekas2015convex,
  title={Convex optimization algorithms},
  author={Bertsekas, Dimitri P and Scientific, Athena},
  year={2015},
  publisher={Athena Scientific Belmont}
}










@article{fan2020spectra,
  title={Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks},
  author={Fan, Zhou and Wang, Zhichao},
  journal={arXiv preprint arXiv:2005.11879},
  year={2020}
}

% lottery ticket
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@inproceedings{ramanujan2020s,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11893--11902},
  year={2020}
}

@article{malach2020proving,
  title={Proving the Lottery Ticket Hypothesis: Pruning is All You Need},
  author={Malach, Eran and Yehudai, Gilad and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={arXiv preprint arXiv:2002.00585},
  year={2020}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}





% others

@ARTICLE{xsede,
author = {J. Towns and T. Cockerill and M. Dahan and I. Foster and K. Gaither and A. Grimshaw and V. Hazlewood and S. Lathrop and D. Lifka and G. D. Peterson and R. Roskies and J. R. Scott and N. Wilkins-Diehr},
journal = {Computing in Science \& Engineering},
title = {XSEDE: Accelerating Scientific Discovery},
year = {2014},
volume = {16},
number = {5},
pages = {62-74},
keywords={Knowledge discovery;Scientific computing;Digital systems;Materials engineering;Supercomputers},
doi = {10.1109/MCSE.2014.80},
url = {doi.ieeecomputersociety.org/10.1109/MCSE.2014.80},
ISSN = {1521-9615},
month={Sept.-Oct.}
}

@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}

@article{wenthundersvm18,
 author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
 title = {{ThunderSVM}: A Fast {SVM} Library on {GPUs} and {CPUs}},
 journal = {Journal of Machine Learning Research},
 volume={19},
 pages={797--801},
 year = {2018}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

