\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anwar et~al.(2017)Anwar, Hwang, and Sung]{anwar2017structured}
Anwar, S., Hwang, K., and Sung, W.
\newblock Structured pruning of deep convolutional neural networks.
\newblock \emph{ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 13\penalty0 (3):\penalty0 1--18, 2017.

\bibitem[Azad \& Bulu{\c{c}}(2017)Azad and Bulu{\c{c}}]{azad2017work}
Azad, A. and Bulu{\c{c}}, A.
\newblock A work-efficient parallel sparse matrix-sparse vector multiplication
  algorithm.
\newblock In \emph{2017 IEEE International Parallel and Distributed Processing
  Symposium (IPDPS)}, pp.\  688--697. IEEE, 2017.

\bibitem[Baskaran \& Bordawekar(2009)Baskaran and
  Bordawekar]{baskaran2009optimizing}
Baskaran, M.~M. and Bordawekar, R.
\newblock Optimizing sparse matrix-vector multiplication on gpus.
\newblock \emph{IBM research report RC24704}, 1\penalty0 (W0812--047), 2009.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Bengio, Y., L{\'e}onard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Blalock et~al.(2020)Blalock, Gonzalez~Ortiz, Frankle, and
  Guttag]{blalock2020state}
Blalock, D., Gonzalez~Ortiz, J.~J., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock \emph{Proceedings of machine learning and systems}, 2:\penalty0
  129--146, 2020.

\bibitem[Bulu{\c{c}} \& Gilbert(2012)Bulu{\c{c}} and
  Gilbert]{bulucc2012parallel}
Bulu{\c{c}}, A. and Gilbert, J.~R.
\newblock Parallel sparse matrix-matrix multiplication and indexing:
  Implementation and experiments.
\newblock \emph{SIAM Journal on Scientific Computing}, 34\penalty0
  (4):\penalty0 C170--C191, 2012.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  28, 2015.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Frantar \& Alistarh(2022)Frantar and Alistarh]{frantar2022spdy}
Frantar, E. and Alistarh, D.
\newblock Spdy: Accurate pruning with speedup guarantees.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  6726--6743. PMLR, 2022.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  29, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  28, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017{\natexlab{a}})He, Zhang, and Sun]{He_2017_ICCV}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, Oct 2017{\natexlab{a}}.

\bibitem[He et~al.(2017{\natexlab{b}})He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  1389--1397, 2017{\natexlab{b}}.

\bibitem[He et~al.(2018)He, Lin, Liu, Wang, Li, and Han]{he2018amc}
He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  784--800, 2018.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{fpgm}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  4340--4349, 2019.

\bibitem[He et~al.(2022)He, Xie, Zhu, and Qin]{he2022sparse}
He, Z., Xie, Z., Zhu, Q., and Qin, Z.
\newblock Sparse double descent: Where network pruning aggravates overfitting.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  8635--8659. PMLR, 2022.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, et~al.]{mobilenetv3}
Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W.,
  Zhu, Y., Pang, R., Vasudevan, V., et~al.
\newblock Searching for mobilenetv3.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  1314--1324, 2019.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{densenet}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  4700--4708, 2017.

\bibitem[Huang et~al.(2018)Huang, Zhou, You, and Neumann]{huang2018learning}
Huang, Q., Zhou, K., You, S., and Neumann, U.
\newblock Learning to prune filters in convolutional neural networks.
\newblock In \emph{IEEE Winter Conference on Applications of Computer Vision
  (WACV)}, pp.\  709--718. IEEE, 2018.

\bibitem[Huang \& Lee(2021)Huang and Lee]{huang2021training}
Huang, Z.-S. and Lee, C.-p.
\newblock Training structured neural networks through manifold identification
  and variance reduction.
\newblock \emph{arXiv preprint arXiv:2112.02612}, 2021.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{squeezenet}
Iandola, F.~N., Han, S., Moskewicz, M.~W., Ashraf, K., Dally, W.~J., and
  Keutzer, K.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Janowsky(1989)]{janowsky1989pruning}
Janowsky, S.~A.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 39\penalty0 (12):\penalty0 6600, 1989.

\bibitem[Lee et~al.(2021)Lee, Park, Mo, Ahn, and Shin]{lamp}
Lee, J., Park, S., Mo, S., Ahn, S., and Shin, J.
\newblock Layer-adaptive sparsity for the magnitude-based pruning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=H6ATjJ0TKdf}.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.~H.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{l1}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Adamczewski, Li, Gu, Timofte, and
  Van~Gool]{Li_2022_CVPR}
Li, Y., Adamczewski, K., Li, W., Gu, S., Timofte, R., and Van~Gool, L.
\newblock Revisiting random channel pruning for neural network compression.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  191--201, June 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Zhao, Yuan, Lin, Wang, and
  Chen]{li2022pruning}
Li, Y., Zhao, P., Yuan, G., Lin, X., Wang, Y., and Chen, X.
\newblock Pruning-as-search: Efficient neural architecture search via channel
  pruning and structural reparameterization.
\newblock \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2022{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Ji, Wang, Zhang, Zhang, Tian, and Shao]{hrank}
Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y., and Shao, L.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  1529--1538, 2020.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  2736--2744, 2017.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJlnB3C5Ym}.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Miao et~al.(2021)Miao, Luo, Chen, Chen, Liu, and
  Wang]{miao2021learning}
Miao, L., Luo, X., Chen, T., Chen, W., Liu, D., and Wang, Z.
\newblock Learning pruning-friendly networks via frank-wolfe: One-shot,
  any-sparsity, and no retraining.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Molchanov et~al.(2017{\natexlab{a}})Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2498--2507. PMLR, 2017{\natexlab{a}}.

\bibitem[Molchanov et~al.(2017{\natexlab{b}})Molchanov, Tyree, Karras, Aila,
  and Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017{\natexlab{b}}.

\bibitem[Narang et~al.(2017)Narang, Undersander, and Diamos]{narang2017block}
Narang, S., Undersander, E., and Diamos, G.
\newblock Block-sparse recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1711.02782}, 2017.

\bibitem[Reed(1993)]{reed1993pruning}
Reed, R.
\newblock Pruning algorithms-a survey.
\newblock \emph{IEEE transactions on Neural Networks}, 4\penalty0 (5):\penalty0
  740--747, 1993.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Savarese et~al.(2020)Savarese, Silva, and Maire]{savarese2020winning}
Savarese, P., Silva, H., and Maire, M.
\newblock Winning the lottery with continuous sparsification.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  33:\penalty0 11380--11390, 2020.

\bibitem[Shang et~al.(2022)Shang, Wu, Hong, and Qian]{shang2022neural}
Shang, H., Wu, J.-L., Hong, W., and Qian, C.
\newblock Neural network pruning by cooperative coevolution.
\newblock \emph{arXiv preprint arXiv:2204.05639}, 2022.

\bibitem[Srinivas et~al.(2017)Srinivas, Subramanya, and
  Venkatesh~Babu]{srinivas2017training}
Srinivas, S., Subramanya, A., and Venkatesh~Babu, R.
\newblock Training sparse neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pp.\  138--145, 2017.

\bibitem[Sze et~al.(2017)Sze, Chen, Yang, and Emer]{sze2017efficient}
Sze, V., Chen, Y.-H., Yang, T.-J., and Emer, J.~S.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock \emph{Proceedings of the IEEE}, 105\penalty0 (12):\penalty0
  2295--2329, 2017.

\bibitem[Tan \& Le(2021)Tan and Le]{efficientnetv2}
Tan, M. and Le, Q.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  10096--10106, 2021.

\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and
  Le]{mnasnet}
Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le,
  Q.~V.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  2820--2828, 2019.

\bibitem[Vuduc et~al.(2005)Vuduc, Demmel, and Yelick]{vuduc2005oski}
Vuduc, R., Demmel, J.~W., and Yelick, K.~A.
\newblock Oski: A library of automatically tuned sparse matrix kernels.
\newblock In \emph{Journal of Physics: Conference Series}, volume~16, pp.\
  521. IOP Publishing, 2005.

\bibitem[Wang et~al.(2022)Wang, Qin, Bai, Zhang, and Fu]{wang2022recent}
Wang, H., Qin, C., Bai, Y., Zhang, Y., and Fu, Y.
\newblock Recent advances on neural network pruning at initialization.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, pp.\  23--29, 2022.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{l2}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Xiao et~al.(2019)Xiao, Wang, and Rajasekaran]{xiao2019autoprune}
Xiao, X., Wang, Z., and Rajasekaran, S.
\newblock Autoprune: Automatic network pruning by regularizing auxiliary
  parameters.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  32, 2019.

\bibitem[Yamamoto \& Maeno(2019)Yamamoto and Maeno]{yamamoto2018pcas}
Yamamoto, K. and Maeno, K.
\newblock Pcas: Pruning channels with attention statistics for deep network
  compression.
\newblock \emph{Proceedings of the British Machine Vision Conference (BMVC)},
  2019.

\bibitem[Yu et~al.(2022{\natexlab{a}})Yu, Mazaheri, and
  Jannesari]{yu2022topology}
Yu, S., Mazaheri, A., and Jannesari, A.
\newblock Topology-aware network pruning using multi-stage graph embedding and
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  25656--25667. PMLR, 2022{\natexlab{a}}.

\bibitem[Yu et~al.(2022{\natexlab{b}})Yu, Serra, Ramalingam, and
  Zhe]{yu2022combinatorial}
Yu, X., Serra, T., Ramalingam, S., and Zhe, S.
\newblock The combinatorial brain surgeon: Pruning weights that cancel one
  another in neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  25668--25683. PMLR, 2022{\natexlab{b}}.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\end{thebibliography}
