\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Yuan(2016)Allen-Zhu and Yuan]{allen2016improved}
Allen-Zhu, Z. and Yuan, Y.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In \emph{International conference on machine learning}, pp.\
  1080--1089, 2016.

\bibitem[Arora et~al.(2005)Arora, Hazan, and Kale]{arora2005fast}
Arora, S., Hazan, E., and Kale, S.
\newblock Fast algorithms for approximate semidefinite programming using the
  multiplicative weights update method.
\newblock In \emph{46th Annual IEEE Symposium on Foundations of Computer
  Science (FOCS'05)}, pp.\  339--348. IEEE, 2005.

\bibitem[Arora et~al.(2009)Arora, Rao, and Vazirani]{arv}
Arora, S., Rao, S., and Vazirani, U.
\newblock Expander flows, geometric embeddings and graph partitioning.
\newblock \emph{Journal of the ACM (JACM)}, 56\penalty0 (2):\penalty0 5, 2009.

\bibitem[Balasubramanian \& Ghadimi(2018)Balasubramanian and
  Ghadimi]{balasubramanian2018zeroth}
Balasubramanian, K. and Ghadimi, S.
\newblock Zeroth-order (non)-convex stochastic optimization via conditional
  gradient and gradient updates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3455--3464, 2018.

\bibitem[Bauschke et~al.()Bauschke, Combettes, et~al.]{bauschke2011convex}
Bauschke, H.~H., Combettes, P.~L., et~al.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}, volume 408.
\newblock Springer.

\bibitem[Burer \& Monteiro(2005)Burer and Monteiro]{burer2005local}
Burer, S. and Monteiro, R.~D.
\newblock Local minima and convergence in low-rank semidefinite programming.
\newblock \emph{Mathematical Programming}, 103\penalty0 (3):\penalty0 427--444,
  2005.

\bibitem[Chatziafratis et~al.(2018)Chatziafratis, Niazadeh, and
  Charikar]{chatziafratis2018hierarchical}
Chatziafratis, V., Niazadeh, R., and Charikar, M.
\newblock Hierarchical clustering with structural constraints.
\newblock \emph{arXiv preprint arXiv:1805.09476}, 2018.

\bibitem[Clarkson(2010)]{clarkson2010coresets}
Clarkson, K.~L.
\newblock Coresets, sparse greedy approximation, and the frank-wolfe algorithm.
\newblock \emph{ACM Transactions on Algorithms (TALG)}, 6\penalty0
  (4):\penalty0 63, 2010.

\bibitem[Dasgupta(2016)]{dasgupta2016cost}
Dasgupta, S.
\newblock A cost function for similarity-based hierarchical clustering.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pp.\  118--127, 2016.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1646--1654, 2014.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  689--699, 2018.

\bibitem[Fercoq et~al.(2019)Fercoq, Alacaoglu, Necoara, and
  Cevher]{fercoq2019almost}
Fercoq, O., Alacaoglu, A., Necoara, I., and Cevher, V.
\newblock Almost surely constrained convex optimization.
\newblock \emph{arXiv preprint arXiv:1902.00126}, 2019.

\bibitem[Frank \& Wolfe(1956)Frank and Wolfe]{originalfw}
Frank, M. and Wolfe, P.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval Research Logistics Quarterly}, 3:\penalty0 95--110, 1956.
\newblock \doi{10.1002/nav.3800030109}.
\newblock URL
  \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109}.

\bibitem[Garber(2016)]{garber2016projection}
Garber, D.
\newblock \emph{Projection-free Algorithms for Convex Optimization and Online
  Learning}.
\newblock PhD thesis, Technion-Israel Institute of Technology, Faculty of
  Industrial and~â€¦, 2016.

\bibitem[Garber \& Hazan(2016)Garber and Hazan]{garber2016sublinear}
Garber, D. and Hazan, E.
\newblock Sublinear time algorithms for approximate semidefinite programming.
\newblock \emph{Mathematical Programming}, 158\penalty0 (1-2):\penalty0
  329--361, 2016.

\bibitem[Gidel et~al.(2018)Gidel, Pedregosa, and
  Lacoste-Julien]{gidel2018frank}
Gidel, G., Pedregosa, F., and Lacoste-Julien, S.
\newblock Frank-wolfe splitting via augmented lagrangian method.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1456--1465, 2018.

\bibitem[Hazan(2008)]{hazan2008sparse}
Hazan, E.
\newblock Sparse approximate solutions to semidefinite programs.
\newblock In \emph{Latin American symposium on theoretical informatics}, pp.\
  306--316. Springer, 2008.

\bibitem[Hazan \& Luo(2016)Hazan and Luo]{svrf}
Hazan, E. and Luo, H.
\newblock Variance-reduced and projection-free stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1271, 2016.

\bibitem[Hazan \& Kale(2012)Hazan and Kale]{hazan2012projection}
Hazan, E.~E. and Kale, S.
\newblock Projection-free online learning.
\newblock In \emph{29th International Conference on Machine Learning, ICML
  2012}, pp.\  521--528, 2012.

\bibitem[Huang et~al.(2014)Huang, Chen, and Guibas]{huang2014scalable}
Huang, Q., Chen, Y., and Guibas, L.
\newblock Scalable semidefinite relaxation for maximum a posterior estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  64--72,
  2014.

\bibitem[Iyengar et~al.(2010)Iyengar, Phillips, and Stein]{iyengar2010feasible}
Iyengar, G., Phillips, D.~J., and Stein, C.
\newblock Feasible and accurate algorithms for covering semidefinite programs.
\newblock In \emph{Scandinavian Workshop on Algorithm Theory}, pp.\  150--162.
  Springer, 2010.

\bibitem[Jaggi(2013)]{revisitingfw}
Jaggi, M.
\newblock Revisiting {Frank-Wolfe}: Projection-free sparse convex optimization.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  427--435, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v28/jaggi13.html}.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  315--323, 2013.

\bibitem[Lan(2013)]{lan2013complexity}
Lan, G.
\newblock The complexity of large-scale convex programming under a linear
  optimization oracle.
\newblock \emph{arXiv preprint arXiv:1309.5550}, 2013.

\bibitem[Lan \& Zhou(2016)Lan and Zhou]{lan2016conditional}
Lan, G. and Zhou, Y.
\newblock Conditional gradient sliding for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1379--1409, 2016.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{lecun-mnist}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Ledoux(2001)]{ledoux2001concentration}
Ledoux, M.
\newblock \emph{The concentration of measure phenomenon}.
\newblock Number~89. American Mathematical Soc., 2001.

\bibitem[Liu et~al.(2019)Liu, Liu, and Ma]{liu2019nonergodic}
Liu, Y.-F., Liu, X., and Ma, S.
\newblock On the nonergodic convergence rate of an inexact augmented lagrangian
  framework for composite convex programming.
\newblock \emph{Mathematics of Operations Research}, 44\penalty0 (2):\penalty0
  632--650, 2019.

\bibitem[Locatello et~al.(2019)Locatello, Yurtsever, Fercoq, and
  Cevher]{locatello2019stochastic}
Locatello, F., Yurtsever, A., Fercoq, O., and Cevher, V.
\newblock Stochastic conditional gradient method for composite convex
  minimization.
\newblock \emph{arXiv preprint arXiv:1901.10348}, 2019.

\bibitem[Mahdavi et~al.(2013)Mahdavi, Zhang, and Jin]{mahdavi2013mixed}
Mahdavi, M., Zhang, L., and Jin, R.
\newblock Mixed optimization for smooth functions.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  674--682, 2013.

\bibitem[Mixon et~al.(2016)Mixon, Villar, and Ward]{mixon2016clustering}
Mixon, D.~G., Villar, S., and Ward, R.
\newblock Clustering subgaussian mixtures by semidefinite programming.
\newblock \emph{arXiv preprint arXiv:1602.06612}, 2016.

\bibitem[Mokhtari et~al.(2018)Mokhtari, Hassani, and Karbasi]{mokhtari}
Mokhtari, A., Hassani, H., and Karbasi, A.
\newblock Stochastic conditional gradient methods: From convex minimization to
  submodular maximization.
\newblock \emph{arXiv preprint arXiv:1804.09554}, 2018.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{nemirovsky1983problem}
Nemirovsky, A.~S. and Yudin, D.~B.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2613--2621. JMLR. org, 2017.

\bibitem[Patrascu \& Necoara(2017)Patrascu and
  Necoara]{patrascu2017nonasymptotic}
Patrascu, A. and Necoara, I.
\newblock Nonasymptotic convergence of stochastic proximal point methods for
  constrained convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 198--1,
  2017.

\bibitem[Peng \& Wei(2007)Peng and Wei]{peng2007approximating}
Peng, J. and Wei, Y.
\newblock Approximating k-means-type clustering via semidefinite programming.
\newblock \emph{SIAM journal on optimization}, 18\penalty0 (1):\penalty0
  186--205, 2007.

\bibitem[Rossi \& Ahmed(2015)Rossi and Ahmed]{nr}
Rossi, R.~A. and Ahmed, N.~K.
\newblock The network data repository with interactive graph analytics and
  visualization.
\newblock In \emph{AAAI}, 2015.
\newblock URL \url{http://networkrepository.com}.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{roux2012stochastic}
Roux, N.~L., Schmidt, M., and Bach, F.~R.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2663--2671, 2012.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Silveti-Falls et~al.(2019)Silveti-Falls, Molinari, and
  Fadili]{silveti2019generalized}
Silveti-Falls, A., Molinari, C., and Fadili, J.
\newblock Generalized conditional gradient with augmented lagrangian for
  composite minimization.
\newblock \emph{arXiv preprint arXiv:1901.01287}, 2019.

\bibitem[Tran-Dinh et~al.(2018)Tran-Dinh, Fercoq, and Cevher]{tran2018smooth}
Tran-Dinh, Q., Fercoq, O., and Cevher, V.
\newblock A smooth primal-dual optimization framework for nonsmooth composite
  convex minimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  96--134, 2018.

\bibitem[Weinberger \& Saul(2009)Weinberger and Saul]{weinberger2009distance}
Weinberger, K.~Q. and Saul, L.~K.
\newblock Distance metric learning for large margin nearest neighbor
  classification.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0 (2), 2009.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Xu(2017)]{xu2017accelerated}
Xu, Y.
\newblock Accelerated first-order primal-dual proximal methods for linearly
  constrained composite convex programming.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (3):\penalty0
  1459--1484, 2017.

\bibitem[Xu(2018)]{xu2018primal}
Xu, Y.
\newblock Primal-dual stochastic gradient method for convex programs with many
  functional constraints.
\newblock \emph{arXiv preprint arXiv:1802.02724}, 2018.

\bibitem[Yang et~al.(2015)Yang, Sun, and Toh]{yang2015sdpnal}
Yang, L., Sun, D., and Toh, K.-C.
\newblock Sdpnal+: a majorized semismooth newton-cg augmented lagrangian method
  for semidefinite programming with nonnegative constraints.
\newblock \emph{Mathematical Programming Computation}, 7\penalty0 (3):\penalty0
  331--366, 2015.

\bibitem[Yurtsever et~al.(2018)Yurtsever, Fercoq, Locatello, and
  Cevher]{yurtsever2018conditional}
Yurtsever, A., Fercoq, O., Locatello, F., and Cevher, V.
\newblock A conditional gradient framework for composite convex minimization
  with applications to semidefinite programming.
\newblock In \emph{35th International Conference on Machine Learning (ICML)},
  pp.\  5727--5736. PMLR, 2018.

\bibitem[Yurtsever et~al.(2019{\natexlab{a}})Yurtsever, Fercoq, and
  Cevher]{yurtsever2019conditional}
Yurtsever, A., Fercoq, O., and Cevher, V.
\newblock A conditional-gradient-based augmented lagrangian framework.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7272--7281, 2019{\natexlab{a}}.

\bibitem[Yurtsever et~al.(2019{\natexlab{b}})Yurtsever, Sra, and
  Cevher]{spiderfw}
Yurtsever, A., Sra, S., and Cevher, V.
\newblock Conditional gradient methods via stochastic path-integrated
  differential estimator.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7282--7291, Long
  Beach, California, USA, 09--15 Jun 2019{\natexlab{b}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/yurtsever19b.html}.

\bibitem[Zhang et~al.(2019)Zhang, Shen, Mokhtari, Hassani, and
  Karbasi]{zhang2019one}
Zhang, M., Shen, Z., Mokhtari, A., Hassani, H., and Karbasi, A.
\newblock One sample stochastic frank-wolfe.
\newblock \emph{arXiv preprint arXiv:1910.04322}, 2019.

\end{thebibliography}
