\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azoury \& Warmuth(2001)Azoury and Warmuth]{azoury2001relative}
Azoury, K.~S. and Warmuth, M.~K.
\newblock Relative loss bounds for on-line density estimation with the
  exponential family of distributions.
\newblock \emph{Machine learning}, 43:\penalty0 211--246, 2001.

\bibitem[Baby \& Wang(2021)Baby and Wang]{baby2021optimal}
Baby, D. and Wang, Y.-X.
\newblock Optimal dynamic regret in exp-concave online learning.
\newblock In \emph{Proceedings of Thirty Fourth Conference on Learning Theory}.
  PMLR, 2021.

\bibitem[Baby \& Wang(2022{\natexlab{a}})Baby and Wang]{baby2022lqr}
Baby, D. and Wang, Y.-X.
\newblock Optimal dynamic regret in lqr control.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.

\bibitem[Baby \& Wang(2022{\natexlab{b}})Baby and Wang]{baby2022optimal}
Baby, D. and Wang, Y.-X.
\newblock Optimal dynamic regret in proper online learning with strongly convex
  losses and beyond.
\newblock In \emph{Proceedings of The 25th International Conference on
  Artificial Intelligence and Statistics}. PMLR, 2022{\natexlab{b}}.

\bibitem[Baby et~al.(2021)Baby, Hasson, and Wang]{baby2021dynamic}
Baby, D., Hasson, H., and Wang, Y.
\newblock Dynamic regret for strongly adaptive methods and optimality of online
  krr, 2021.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{besbes2015nonstationary}
Besbes, O., Gur, Y., and Zeevi, A.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 2015.

\bibitem[Campolongo \& Orabona(2021)Campolongo and
  Orabona]{campolongo2021closer}
Campolongo, N. and Orabona, F.
\newblock A closer look at temporal variability in dynamic online learning,
  2021.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Cesa-Bianchi et~al.(2012)Cesa-Bianchi, Gaillard, Lugosi, and
  Stoltz]{cesa2012mirror}
Cesa-Bianchi, N., Gaillard, P., Lugosi, G., and Stoltz, G.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Cutkosky(2020)]{cutkosky2020parameter}
Cutkosky, A.
\newblock Parameter-free, dynamic, and strongly-adaptive online learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Daniely et~al.(2015)Daniely, Gonen, and
  Shalev-Shwartz]{daniely2015strongly}
Daniely, A., Gonen, A., and Shalev-Shwartz, S.
\newblock Strongly adaptive online learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}. PMLR, 2015.

\bibitem[Foster et~al.(2016)Foster, Kale, and Karloff]{foster2017sparse}
Foster, D., Kale, S., and Karloff, H.
\newblock Online sparse linear regression.
\newblock In \emph{29th Annual Conference on Learning Theory}. PMLR, 2016.

\bibitem[Gaillard et~al.(2019)Gaillard, Gerchinovitz, Huard, and
  Stoltz]{gaillard2019uniform}
Gaillard, P., Gerchinovitz, S., Huard, M., and Stoltz, G.
\newblock Uniform regret bounds over $\mathbb{R}^d$ for the sequential linear
  regression problem with the square loss.
\newblock In \emph{Proceedings of the 30th International Conference on
  Algorithmic Learning Theory}. PMLR, 2019.

\bibitem[Ghai et~al.(2020)Ghai, Lee, Singh, Zhang, and Zhang]{ghai2020noregret}
Ghai, U., Lee, H., Singh, K., Zhang, C., and Zhang, Y.
\newblock No-regret prediction in marginally stable systems.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory}.
  PMLR, 2020.

\bibitem[Hazan(2019)]{hazan2019introduction}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{CoRR}, abs/1909.05207, 2019.

\bibitem[Hazan \& Seshadhri(2007)Hazan and Seshadhri]{hazan2007adaptive}
Hazan, E. and Seshadhri, C.
\newblock Adaptive algorithms for online decision problems.
\newblock In \emph{Electronic colloquium on computational complexity (ECCC)},
  number 088, 2007.

\bibitem[Hazan \& Seshadhri(2009)Hazan and Seshadhri]{hazan2009efficient}
Hazan, E. and Seshadhri, C.
\newblock Efficient learning algorithms for changing environments.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, 2009.

\bibitem[Hazan \& Singh(2022)Hazan and Singh]{hazan2022introduction}
Hazan, E. and Singh, K.
\newblock Introduction to online nonstochastic control, 2022.

\bibitem[Hazan et~al.(2017)Hazan, Singh, and Zhang]{hazan2017learning}
Hazan, E., Singh, K., and Zhang, C.
\newblock Learning linear dynamical systems via spectral filtering.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Hazan et~al.(2018)Hazan, Lee, Singh, Zhang, and
  Zhang]{hazan2018spectral}
Hazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Y.
\newblock Spectral filtering for general linear dynamical systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Jacobsen \& Cutkosky(2022)Jacobsen and
  Cutkosky]{jacobsen2022parameter}
Jacobsen, A. and Cutkosky, A.
\newblock Parameter-free mirror descent.
\newblock In \emph{Proceedings of Thirty Fifth Conference on Learning Theory}.
  PMLR, 2022.

\bibitem[Jacobsen \& Cutkosky(2023)Jacobsen and
  Cutkosky]{jacobsen2023unconstrained}
Jacobsen, A. and Cutkosky, A.
\newblock Unconstrained online learning with unbounded losses.
\newblock In \emph{International Conference on Machine Learning (ICML)}. PMLR,
  2023.

\bibitem[Jun et~al.(2017)Jun, Orabona, Wright, and Willett]{jun2017improved}
Jun, K.-S., Orabona, F., Wright, S., and Willett, R.
\newblock {Improved Strongly Adaptive Online Learning using Coin Betting}.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}. PMLR, 2017.

\bibitem[Kalman(1960)]{kalman1960filter}
Kalman, R.~E.
\newblock A new approach to linear filtering and prediction problems.
\newblock \emph{Transactions of the ASME--Journal of Basic Engineering}, 1960.

\bibitem[Kempka et~al.(2019)Kempka, Kotlowski, and Warmuth]{kempka2019adaptive}
Kempka, M., Kotlowski, W., and Warmuth, M.~K.
\newblock Adaptive scale-invariant online algorithms for learning linear
  models.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}. PMLR, 2019.

\bibitem[Kivinen et~al.(2006)Kivinen, Warmuth, and Hassibi]{kivinen2006pnorm}
Kivinen, J., Warmuth, M., and Hassibi, B.
\newblock The p-norm generalization of the lms algorithm for adaptive
  filtering.
\newblock \emph{IEEE Transactions on Signal Processing}, 2006.

\bibitem[Kotłowski(2017)]{kotlowski2017scale}
Kotłowski, W.
\newblock Scale-invariant unconstrained online learning.
\newblock In \emph{Proceedings of the 28th International Conference on
  Algorithmic Learning Theory}. PMLR, 2017.

\bibitem[Kozdoba et~al.(2019)Kozdoba, Marecek, Tchrakian, and
  Mannor]{kozdoba2019online}
Kozdoba, M., Marecek, J., Tchrakian, T., and Mannor, S.
\newblock On-line learning of linear dynamical systems: Exponential forgetting
  in kalman filters.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Lu \& Hazan(2022)Lu and Hazan]{lu2022efficient}
Lu, Z. and Hazan, E.
\newblock Efficient adaptive regret minimization, 2022.

\bibitem[Luo et~al.(2016)Luo, Agarwal, Cesa-Bianchi, and
  Langford]{luo2016efficient}
Luo, H., Agarwal, A., Cesa-Bianchi, N., and Langford, J.
\newblock Efficient second order online learning by sketching.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Luo et~al.(2022)Luo, Zhang, Zhao, and Zhou]{luo2022corralling}
Luo, H., Zhang, M., Zhao, P., and Zhou, Z.-H.
\newblock Corralling a larger band of bandits: A case study on switching regret
  for linear bandits.
\newblock In \emph{Proceedings of Thirty Fifth Conference on Learning Theory}.
  PMLR, 2022.

\bibitem[Mayo et~al.(2022)Mayo, Hadiji, and van Erven]{mayo2022scalefree}
Mayo, J.~J., Hadiji, H., and van Erven, T.
\newblock Scale-free unconstrained online learning for curved losses.
\newblock In \emph{Proceedings of Thirty Fifth Conference on Learning Theory},
  2022.

\bibitem[Mhammedi \& Koolen(2020)Mhammedi and Koolen]{mhammedi2020lipschitz}
Mhammedi, Z. and Koolen, W.~M.
\newblock Lipschitz and comparator-norm adaptivity in online learning.
\newblock In Abernethy, J. and Agarwal, S. (eds.), \emph{Proceedings of Thirty
  Third Conference on Learning Theory}. PMLR, 2020.

\bibitem[Orabona et~al.(2015)Orabona, Crammer, and
  Cesa-Bianchi]{orabona2015generalized}
Orabona, F., Crammer, K., and Cesa-Bianchi, N.
\newblock A generalized online mirror descent with applications to
  classification and regression.
\newblock \emph{Mach. Learn.}, 2015.

\bibitem[Rashidinejad et~al.(2020)Rashidinejad, Jiao, and
  Russell]{rashidinejad2020slip}
Rashidinejad, P., Jiao, J., and Russell, S.
\newblock Slip: Learning to predict in unknown dynamical systems with long-term
  memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Simon(2006)]{simon2006optimal}
Simon, D.
\newblock \emph{Optimal state estimation: Kalman, H infinity, and nonlinear
  approaches}.
\newblock John Wiley \& Sons, 2006.

\bibitem[Tsiamis \& Pappas(2022)Tsiamis and Pappas]{tsiamis2022online}
Tsiamis, A. and Pappas, G.~J.
\newblock Online learning of the kalman filter with logarithmic regret.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem[Veness et~al.(2013)Veness, White, Bowling, and
  György]{veness2013partition}
Veness, J., White, M., Bowling, M., and György, A.
\newblock Partition tree weighting.
\newblock In \emph{2013 Data Compression Conference}, 2013.

\bibitem[Vovk(2001)]{vovk2001competitive}
Vovk, V.
\newblock Competitive on-line statistics.
\newblock \emph{International Statistical Review}, 2001.

\bibitem[Yuan \& Lamperski(2019)Yuan and Lamperski]{yuan2019trading}
Yuan, J. and Lamperski, A.~G.
\newblock Trading-off static and dynamic regret in online least-squares and
  beyond.
\newblock \emph{CoRR}, abs/1909.03118, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Lu, and Zhou]{zhang2018adaptive}
Zhang, L., Lu, S., and Zhou, Z.-H.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Zhang et~al.(2023)Zhang, Cutkosky, and
  Paschalidis]{zhang2023unconstrained}
Zhang, Z., Cutkosky, A., and Paschalidis, Y.
\newblock Unconstrained dynamic regret via sparse coding.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Zhao et~al.(2020)Zhao, Zhang, Zhang, and Zhou]{zhao2020dynamic}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Dynamic regret of convex and smooth functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Zhang, and Zhou]{zhao2024adaptivity}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Adaptivity and non-stationarity: Problem-dependent dynamic regret for
  online convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 2024.

\end{thebibliography}
