@article{sparsespurious,
  author    = {R{\'{e}}mi Gribonval and
               Rodolphe Jenatton and
               Francis R. Bach},
  title     = {Sparse and spurious: dictionary learning with noise and outliers},
  journal   = {CoRR},
  volume    = {abs/1407.5155},
  year      = {2014},
  url       = {http://arxiv.org/abs/1407.5155},
  eprinttype = {arXiv},
  eprint    = {1407.5155},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GribonvalJB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{arora15,
  author    = {Sanjeev Arora and
               Rong Ge and
               Tengyu Ma and
               Ankur Moitra},
  title     = {Simple, Efficient, and Neural Algorithms for Sparse Coding},
  journal   = {CoRR},
  volume    = {abs/1503.00778},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.00778},
  eprinttype = {arXiv},
  eprint    = {1503.00778},
  timestamp = {Sun, 08 Aug 2021 16:40:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AroraGMM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{overrealized,
  author    = {Jeremias Sulam and
               Chong You and
               Zhihui Zhu},
  title     = {Recovery and Generalization in Over-Realized Dictionary Learning},
  journal   = {CoRR},
  volume    = {abs/2006.06179},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.06179},
  eprinttype = {arXiv},
  eprint    = {2006.06179},
  timestamp = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-06179.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hdp,
  added-at = {2019-06-09T20:28:28.000+0200},
  author = {Vershynin, Roman},
  biburl = {https://www.bibsonomy.org/bibtex/29da0fdbecdd86a12bef65108c846edd8/kirk86},
  description = {HDP-book.pdf},
  interhash = {003cd7d6bc4132b9da72a900b7549ddb},
  intrahash = {9da0fdbecdd86a12bef65108c846edd8},
  keywords = {book foundations probability stats theory},
  timestamp = {2019-06-09T20:28:43.000+0200},
  title = {High-Dimensional Probability},
  url = {https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf},
  year = 2019
}

@ARTICLE{candestao1,
  author={Candes, E.J. and Romberg, J. and Tao, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
  year={2006},
  volume={52},
  number={2},
  pages={489-509},
  doi={10.1109/TIT.2005.862083}}

@ARTICLE{candestao2,
  author={Candes, Emmanuel J. and Tao, Terence},
  journal={IEEE Transactions on Information Theory}, 
  title={Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}, 
  year={2006},
  volume={52},
  number={12},
  pages={5406-5425},
  doi={10.1109/TIT.2006.885507}}

@ARTICLE{donohocs,
  author={Donoho, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Compressed sensing}, 
  year={2006},
  volume={52},
  number={4},
  pages={1289-1306},
  doi={10.1109/TIT.2006.871582}}

@ARTICLE{troppconvex,
  author={Tropp, J.A.},
  journal={IEEE Transactions on Information Theory}, 
  title={Just relax: convex programming methods for identifying sparse signals in noise}, 
  year={2006},
  volume={52},
  number={3},
  pages={1030-1051},
  doi={10.1109/TIT.2005.864420}}

@ARTICLE{donohostable,
  author={Donoho, D.L. and Elad, M. and Temlyakov, V.N.},
  journal={IEEE Transactions on Information Theory}, 
  title={Stable recovery of sparse overcomplete representations in the presence of noise}, 
  year={2006},
  volume={52},
  number={1},
  pages={6-18},
  doi={10.1109/TIT.2005.860430}}

@article{lars,
author = {Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},
title = {{Least angle regression}},
volume = {32},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {407 -- 499},
keywords = {boosting, coefficient paths, Lasso, Linear regression, Variable selection},
year = {2004},
doi = {10.1214/009053604000000067},
URL = {https://doi.org/10.1214/009053604000000067}
}

@article{
amp,
author = {David L. Donoho  and Arian Maleki  and Andrea Montanari },
title = {Message-passing algorithms for compressed sensing},
journal = {Proceedings of the National Academy of Sciences},
volume = {106},
number = {45},
pages = {18914-18919},
year = {2009},
doi = {10.1073/pnas.0909892106},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0909892106},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0909892106},
abstract = {Compressed sensing aims to undersample certain high-dimensional signals yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity–undersampling tradeoff is achieved when reconstructing by convex optimization, which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity–undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity–undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity–undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity–undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this apparently very different theoretical formalism.}}

@ARTICLE{omptropp,
  author={Tropp, Joel A. and Gilbert, Anna C.},
  journal={IEEE Transactions on Information Theory}, 
  title={Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit}, 
  year={2007},
  volume={53},
  number={12},
  pages={4655-4666},
  doi={10.1109/TIT.2007.909108}}

@misc{daubechies,
  doi = {10.48550/ARXIV.MATH/0307152},
  
  url = {https://arxiv.org/abs/math/0307152},
  
  author = {Daubechies, Ingrid and Defrise, Michel and De Mol, Christine},
  
  keywords = {Functional Analysis (math.FA), Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics},
  
  title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
  }

@ARTICLE{donohothresh,
  author={Maleki, Arian and Donoho, David L.},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Optimally Tuned Iterative Reconstruction Algorithms for Compressed Sensing}, 
  year={2010},
  volume={4},
  number={2},
  pages={330-341},
  doi={10.1109/JSTSP.2009.2039176}}

@misc{amp2018,
  doi = {10.48550/ARXIV.1807.03182},
  
  url = {https://arxiv.org/abs/1807.03182},
  
  author = {Musa, Osman and Jung, Peter and Goertz, Norbert},
  
  keywords = {Signal Processing (eess.SP), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Generalized Approximate Message Passing for Unlimited Sampling of Sparse Signals},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bregmanconvex,
author = {Yin, Wotao and Osher, Stanley and Goldfarb, Donald and Darbon, Jerome},
title = {Bregman Iterative Algorithms for {$\ell_1$}-Minimization with Applications to Compressed Sensing},
journal = {SIAM Journal on Imaging Sciences},
volume = {1},
number = {1},
pages = {143-168},
year = {2008},
doi = {10.1137/070703983},
}

@article{duarterev,
  author    = {Marco F. Duarte and
               Yonina C. Eldar},
  title     = {Structured Compressed Sensing: From Theory to Applications},
  journal   = {CoRR},
  volume    = {abs/1106.6224},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.6224},
  eprinttype = {arXiv},
  eprint    = {1106.6224},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1106-6224.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{candesrev,
  author={Candes, Emmanuel J. and Wakin, Michael B.},
  journal={IEEE Signal Processing Magazine}, 
  title={An Introduction To Compressive Sampling}, 
  year={2008},
  volume={25},
  number={2},
  pages={21-30},
  doi={10.1109/MSP.2007.914731}}

@inproceedings{NIPS2009_cfecdb27,
 author = {Zhou, Mingyuan and Chen, Haojun and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence and Paisley, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations},
 url = {https://proceedings.neurips.cc/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf},
 volume = {22},
 year = {2009}
}

@misc{convexsparse,
  doi = {10.48550/ARXIV.0812.1869},
  url = {https://arxiv.org/abs/0812.1869},
  author = {Bach, Francis and Mairal, Julien and Ponce, Jean},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Convex Sparse Matrix Factorizations},
  publisher = {arXiv},
  year = {2008},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{krausesparse,
author = {Krause, Andreas and Cevher, Volkan},
title = {Submodular Dictionary Selection for Sparse Representation},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop an efficient learning framework to construct signal dictionaries for sparse representation by selecting the dictionary columns from multiple candidate bases. By sparse, we mean that only a few dictionary elements, compared to the ambient signal dimension, can exactly represent or well-approximate the signals of interest. We formulate both the selection of the dictionary columns and the sparse representation of signals as a joint combinatorial optimization problem. The proposed combinatorial objective maximizes variance reduction over the set of training signals by constraining the size of the dictionary as well as the number of dictionary columns that can be used to represent each signal. We show that if the available dictionary column vectors are incoherent, our objective function satisfies approximate submodularity. We exploit this property to develop SDSOMP and SDSMA, two greedy algorithms with approximation guarantees. We also describe how our learning framework enables dictionary selection for structured sparse representations, e.g., where the sparse coefficients occur in restricted patterns. We evaluate our approach on synthetic signals and natural images for representation and inpainting problems.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {567–574},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{onlinesparse,
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
title = {Online Learning for Matrix Factorization and Sparse Coding},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {19–60},
numpages = {42}
}

@ARTICLE{ksvd,
  author={Aharon, M. and Elad, M. and Bruckstein, A.},
  journal={IEEE Transactions on Signal Processing}, 
  title={K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation}, 
  year={2006},
  volume={54},
  number={11},
  pages={4311-4322},
  doi={10.1109/TSP.2006.881199}}

@article{arora14,
  author    = {Sanjeev Arora and
               Aditya Bhaskara and
               Rong Ge and
               Tengyu Ma},
  title     = {More Algorithms for Provable Dictionary Learning},
  journal   = {CoRR},
  volume    = {abs/1401.0579},
  year      = {2014},
  url       = {http://arxiv.org/abs/1401.0579},
  eprinttype = {arXiv},
  eprint    = {1401.0579},
  timestamp = {Sun, 08 Aug 2021 16:40:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AroraBGM14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{arora13,
  doi = {10.48550/ARXIV.1308.6273},
  url = {https://arxiv.org/abs/1308.6273},
  author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
  keywords = {Data Structures and Algorithms (cs.DS), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {New Algorithms for Learning Incoherent and Overcomplete Dictionaries},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{OLSHAUSEN19973311,
title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
journal = {Vision Research},
volume = {37},
number = {23},
pages = {3311-3325},
year = {1997},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(97)00169-7},
url = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
author = {Bruno A. Olshausen and David J. Field},
keywords = {Coding, V1, Gabor-wavelet, Natural images},
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.}
}

@INPROCEEDINGS{engan,
  author={Engan, K. and Aase, S.O. and Hakon Husoy, J.},
  booktitle={1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)}, 
  title={Method of optimal directions for frame design}, 
  year={1999},
  volume={5},
  number={},
  pages={2443-2446 vol.5},
  doi={10.1109/ICASSP.1999.760624}}

@article{AHARON200648,
title = {On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them},
journal = {Linear Algebra and its Applications},
volume = {416},
number = {1},
pages = {48-67},
year = {2006},
note = {Special Issue devoted to the Haifa 2005 conference on matrix theory},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2005.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0024379505003459},
author = {Michal Aharon and Michael Elad and Alfred M. Bruckstein},
keywords = {K-Means, Vector quantization, Codebook, Uniqueness, K-SVD, Training, Dictionary, Atom decomposition, Sparse representation, Basis pursuit, Matching pursuit},
abstract = {A full-rank under-determined linear system of equations Ax=b has in general infinitely many possible solutions. In recent years there is a growing interest in the sparsest solution of this equation—the one with the fewest non-zero entries, measured by ∥x∥0. Such solutions find applications in signal and image processing, where the topic is typically referred to as “sparse representation”. Considering the columns of A as atoms of a dictionary, it is assumed that a given signal b is a linear composition of few such atoms. Recent work established that if the desired solution x is sparse enough, uniqueness of such a result is guaranteed. Also, pursuit algorithms, approximation solvers for the above problem, are guaranteed to succeed in finding this solution. Armed with these recent results, the problem can be reversed, and formed as an implied matrix factorization problem: Given a set of vectors {bi}, known to emerge from such sparse constructions, Axi=bi, with sufficiently sparse representations xi, we seek the matrix A. In this paper we present both theoretical and algorithmic studies of this problem. We establish the uniqueness of the dictionary A, depending on the quantity and nature of the set {bi}, and the sparsity of {xi}. We also describe a recently developed algorithm, the K-SVD, that practically find the matrix A, in a manner similar to the K-Means algorithm. Finally, we demonstrate this algorithm on several stylized applications in image processing.}
}

@article{geng,
  author    = {Quan Geng and
               Huan Wang and
               John Wright},
  title     = {On the Local Correctness of L{\^{}}1 Minimization for Dictionary Learning},
  journal   = {CoRR},
  volume    = {abs/1101.5672},
  year      = {2011},
  url       = {http://arxiv.org/abs/1101.5672},
  eprinttype = {arXiv},
  eprint    = {1101.5672},
  timestamp = {Mon, 11 Mar 2019 09:54:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1101-5672.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{schnass,
  author={Gribonval, Rémi and Schnass, Karin},
  journal={IEEE Transactions on Information Theory}, 
  title={Dictionary Identification—Sparse Matrix-Factorization via $\ell_1$ -Minimization}, 
  year={2010},
  volume={56},
  number={7},
  pages={3523-3539},
  doi={10.1109/TIT.2010.2048466}}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{l0hard,
author = {Natarajan, B. K.},
title = {Sparse Approximate Solutions to Linear Systems},
journal = {SIAM Journal on Computing},
volume = {24},
number = {2},
pages = {227-234},
year = {1995},
doi = {10.1137/S0097539792240406},
}

@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2023-01-08},
 volume = {58},
 year = {1996}
}

@ARTICLE{rip,
  author={Candes, E.J. and Tao, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={Decoding by linear programming}, 
  year={2005},
  volume={51},
  number={12},
  pages={4203-4215},
  doi={10.1109/TIT.2005.858979}}

@article{rudelson,
  doi = {10.48550/ARXIV.0802.3956},
  url = {https://arxiv.org/abs/0802.3956},
  author = {Rudelson, Mark and Vershynin, Roman},
  keywords = {Probability (math.PR), Functional Analysis (math.FA), FOS: Mathematics, FOS: Mathematics, 15A52, 11P70},
  title = {The smallest singular value of a random rectangular matrix},
  publisher = {arXiv},
  year = {2008},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{pytorch,
  author    = {Adam Paszke and
               Sam Gross and
               Francisco Massa and
               Adam Lerer and
               James Bradbury and
               Gregory Chanan and
               Trevor Killeen and
               Zeming Lin and
               Natalia Gimelshein and
               Luca Antiga and
               Alban Desmaison and
               Andreas K{\"{o}}pf and
               Edward Z. Yang and
               Zach DeVito and
               Martin Raison and
               Alykhan Tejani and
               Sasank Chilamkurthy and
               Benoit Steiner and
               Lu Fang and
               Junjie Bai and
               Soumith Chintala},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  journal   = {CoRR},
  volume    = {abs/1912.01703},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01703},
  eprinttype = {arXiv},
  eprint    = {1912.01703},
  timestamp = {Tue, 02 Nov 2021 15:18:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01703.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{omperror,
  author={Cai, T. Tony and Wang, Lie},
  journal={IEEE Transactions on Information Theory}, 
  title={Orthogonal Matching Pursuit for Sparse Signal Recovery With Noise}, 
  year={2011},
  volume={57},
  number={7},
  pages={4680-4688},
  doi={10.1109/TIT.2011.2146090}}

@article{zhang2017medical,
  title={Medical image classification based on multi-scale non-negative sparse coding},
  author={Zhang, Ruijie and Shen, Jian and Wei, Fushan and Li, Xiong and Sangaiah, Arun Kumar},
  journal={Artificial intelligence in medicine},
  volume={83},
  pages={44--51},
  year={2017},
  publisher={Elsevier}
}

@article{olshausen2004sparse,
  title={Sparse coding of sensory inputs},
  author={Olshausen, Bruno A and Field, David J},
  journal={Current opinion in neurobiology},
  volume={14},
  number={4},
  pages={481--487},
  year={2004},
  publisher={Elsevier}
}

@article{tibshirani2008spatial,
  title={Spatial smoothing and hot spot detection for CGH data using the fused lasso},
  author={Tibshirani, Robert and Wang, Pei},
  journal={Biostatistics},
  volume={9},
  number={1},
  pages={18--29},
  year={2008},
  publisher={Oxford University Press}
}

@ARTICLE{omporiginal,
  author={Mallat, S.G. and Zhifeng Zhang},
  journal={IEEE Transactions on Signal Processing}, 
  title={Matching pursuits with time-frequency dictionaries}, 
  year={1993},
  volume={41},
  number={12},
  pages={3397-3415},
  doi={10.1109/78.258082}}

@inproceedings{Rubinstein2008EfficientIO,
  title={Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit},
  author={Ron Rubinstein and Michael Zibulevsky and Michael Elad},
  year={2008}
}

@article{randomrip,
title = "A simple proof of the restricted isometry property for random matrices",
abstract = "We give a simple technique for verifying the Restricted Isometry Property (as introduced by Cand{\`e}s and Tao) for random matrices that underlies Compressed Sensing. Our approach has two main ingredients: (i) concentration inequalities for random inner products that have recently provided algorithmically simple proofs of the Johnson-Lindenstrauss lemma; and (ii) covering numbers for finite-dimensional balls in Euclidean space. This leads to an elementary proof of the Restricted Isometry Property and brings out connections between Compressed Sensing and the Johnson-Lindenstrauss lemma. As a result, we obtain simple and direct proofs of Kashin's theorems on widths of finite balls in Euclidean space (and their improvements due to Gluskin) and proofs of the existence of optimal Compressed Sensing measurement matrices. In the process, we also prove that these measurements have a certain universality with respect to the sparsity-inducing basis.",
keywords = "Compressed sensing, Concentration inequalities, Random matrices, Sampling",
author = "Richard Baraniuk and Mark Davenport and Ronald DeVore and Michael Wakin",
year = "2008",
month = dec,
doi = "10.1007/s00365-007-9003-x",
language = "English (US)",
volume = "28",
pages = "253--263",
journal = "Constructive Approximation",
issn = "0176-4276",
publisher = "Springer New York",
number = "3",
}

@misc{detrip,
  doi = {10.48550/ARXIV.1202.1234},
  
  url = {https://arxiv.org/abs/1202.1234},
  
  author = {Bandeira, Afonso S. and Fickus, Matthew and Mixon, Dustin G. and Wong, Percy},
  
  keywords = {Functional Analysis (math.FA), FOS: Mathematics, FOS: Mathematics},
  
  title = {The road to deterministic matrices with the restricted isometry property},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {Creative Commons Public Domain Dedication and Certification}
}

@article{lee2021predicting,
  title={Predicting what you already know helps: Provable self-supervised learning},
  author={Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={309--323},
  year={2021}
}

@article{tsai2020self,
  title={Self-supervised learning from a multi-view perspective},
  author={Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2006.05576},
  year={2020}
}

@inproceedings{tosh2021contrastive,
  title={Contrastive learning, multi-view redundancy, and linear models},
  author={Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
  booktitle={Algorithmic Learning Theory},
  pages={1179--1206},
  year={2021},
  organization={PMLR}
}

@article{wang2022self,
  title={Self-Supervised Metric Learning in Multi-View Data: A Downstream Task Perspective},
  author={Wang, Shulei},
  journal={Journal of the American Statistical Association},
  number={just-accepted},
  pages={1--41},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{wei2021pretrained,
  title={Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning},
  author={Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16158--16170},
  year={2021}
}

@article{cao2022understand,
  title={How to understand masked autoencoders},
  author={Cao, Shuhao and Xu, Peng and Clifton, David A},
  journal={arXiv preprint arXiv:2202.03670},
  year={2022}
}

@inproceedings{chen1994basis,
  title={Basis pursuit},
  author={Chen, Shaobing and Donoho, David},
  booktitle={Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers},
  volume={1},
  pages={41--44},
  year={1994},
  organization={IEEE}
}

@article{pan2022towards,
  title={Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks},
  author={Pan, Jiachun and Zhou, Pan and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2206.03826},
  year={2022}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2019_dc6a7e65,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{He_2022_CVPR,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}

@INPROCEEDINGS{boufounos2007,
  author={Boufounos, Petros and Duarte, Marco F. and Baraniuk, Richard G.},
  booktitle={2007 IEEE/SP 14th Workshop on Statistical Signal Processing}, 
  title={Sparse Signal Reconstruction from Noisy Compressive Measurements using Cross Validation}, 
  year={2007},
  volume={},
  number={},
  pages={299-303},
  doi={10.1109/SSP.2007.4301267}}

@ARTICLE{ward2009,
  author={Ward, Rachel},
  journal={IEEE Transactions on Information Theory}, 
  title={Compressed Sensing With Cross Validation}, 
  year={2009},
  volume={55},
  number={12},
  pages={5773-5782},
  doi={10.1109/TIT.2009.2032712}}