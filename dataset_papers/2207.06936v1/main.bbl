\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2020federated}
Acar, D. A.~E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., and Saligrama,
  V.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Al-Shedivat et~al.(2021)Al-Shedivat, Gillenwater, Xing, and
  Rostamizadeh]{al2020federated}
Al-Shedivat, M., Gillenwater, J., Xing, E., and Rostamizadeh, A.
\newblock Federated learning via posterior averaging: A new perspective and
  practical algorithms.
\newblock In \emph{ICLR}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{hinton2014distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPSW}, 2014.

\bibitem[Hsieh et~al.(2020)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2020non}
Hsieh, K., Phanishayee, A., Mutlu, O., and Gibbons, P.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Iandola, F.~N., Han, S., Moskewicz, M.~W., Ashraf, K., Dally, W.~J., and
  Keutzer, K.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and $<$
  0.5 {MB} model size.
\newblock In \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S.~J., Stich, S.~U., and
  Suresh, A.~T.
\newblock {SCAFFOLD:} stochastic controlled averaging for on-device federated
  learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019first}
Khaled, A., Mishchenko, K., and Richt{\'a}rik, P.
\newblock First analysis of local {GD} on heterogeneous data.
\newblock In \emph{NeurIPSW}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Lee et~al.(2021)Lee, Shin, Jeong, and Yun]{lee2021preservation}
Lee, G., Shin, Y., Jeong, M., and Yun, S.-Y.
\newblock Preservation of the global knowledge by not-true self knowledge
  distillation in federated learning.
\newblock \emph{arXiv preprint arXiv:2106.03097}, 2021.

\bibitem[Li et~al.(2021)Li, He, and Song]{li2021model}
Li, Q., He, B., and Song, D.
\newblock Model-contrastive federated learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Li et~al.(2019)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smithy]{li2019feddane}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smithy, V.
\newblock Fed{DANE}: A federated newton-type method.
\newblock In \emph{ACSCC}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock In \emph{MLSys}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and
  Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{AISTATS}, 2017.

\bibitem[Park et~al.(2021)Park, Cha, Jeong, Kim, and Han]{park2021learning}
Park, D.~Y., Cha, M.-H., Jeong, C., Kim, D.~S., and Han, B.
\newblock Learning student-friendly teacher networks for knowledge
  distillation.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2021adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\`y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Romero et~al.(2015)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Romero, A., Ballas, N., Kahou, S.~E., Chassang, A., Gatta, C., and Bengio, Y.
\newblock Fitnets: Hints for thin deep nets.
\newblock In \emph{ICLR}, 2015.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Seo et~al.(2020)Seo, Park, Oh, Bennis, and Kim]{seo2020federated}
Seo, H., Park, J., Oh, S., Bennis, M., and Kim, S.-L.
\newblock Federated knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2011.02367}, 2020.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.~B., Al-Shedivat, M.,
  Andrew, G., Avestimehr, S., Daly, K., Data, D., et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wang et~al.(2019)Wang, Tuor, Salonidis, Leung, Makaya, He, and
  Chan]{wang2019adaptive}
Wang, S., Tuor, T., Salonidis, T., Leung, K.~K., Makaya, C., He, T., and Chan,
  K.
\newblock Adaptive federated learning in resource constrained edge computing
  systems.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 37\penalty0
  (6):\penalty0 1205--1221, 2019.

\bibitem[Xu et~al.(2021)Xu, Wang, Wang, and Yao]{xu2021fedcm}
Xu, J., Wang, S., Wang, L., and Yao, A. C.-C.
\newblock Fedcm: Federated learning with client-level momentum.
\newblock \emph{arXiv preprint arXiv:2106.10874}, 2021.

\bibitem[Yao et~al.(2021)Yao, Pan, Dai, Wan, Ding, Jin, Xu, and
  Sun]{yao2021local}
Yao, D., Pan, W., Dai, Y., Wan, Y., Ding, X., Jin, H., Xu, Z., and Sun, L.
\newblock Local-global knowledge distillation in heterogeneous federated
  learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:2107.00051}, 2021.

\bibitem[Yoon et~al.(2021)Yoon, Shin, Hwang, and Yang]{yoon2021fedmix}
Yoon, T., Shin, S., Hwang, S.~J., and Yang, E.
\newblock Fedmix: Approximation of mixup under mean augmented federated
  learning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Zhou, Lin, and Sun]{zhang2018shufflenet}
Zhang, X., Zhou, X., Lin, M., and Sun, J.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{CVPR}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Hong, Dhople, Yin, and Liu]{zhang2020fedpd}
Zhang, X., Hong, M., Dhople, S., Yin, W., and Liu, Y.
\newblock {FedPD:} a federated learning framework with optimal rates and
  adaptivity to non-iid data.
\newblock In \emph{arXiv preprint arXiv:2005.11418}, 2020.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{zhu2021data}
Zhu, Z., Hong, J., and Zhou, J.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock In \emph{ICML}, 2021.

\end{thebibliography}
