\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abe and Kaneko(2020)]{abe2020off}
Kenshi Abe and Yusuke Kaneko.
\newblock Off-policy exploitability-evaluation in two-player zero-sum markov
  games.
\newblock \emph{arXiv preprint arXiv:2007.02141}, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Yu~Bai, Chi Jin, and Tiancheng Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 2159--2170, 2020.

\bibitem[Brown and Sandholm(2019)]{brown2019superhuman}
Noam Brown and Tuomas Sandholm.
\newblock Superhuman ai for multiplayer poker.
\newblock \emph{Science}, 365\penalty0 (6456):\penalty0 885--890, 2019.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Cui and Du(2022)]{cui2022offline}
Qiwen Cui and Simon~S Du.
\newblock When is offline two-player zero-sum markov game solvable?
\newblock \emph{arXiv preprint arXiv:2201.03522}, 2022.

\bibitem[Cui and Yang(2021)]{cui2021minimax}
Qiwen Cui and Lin~F Yang.
\newblock Minimax sample complexity for turn-based stochastic game.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1496--1504.
  PMLR, 2021.

\bibitem[Daskalakis(2013)]{daskalakis2013complexity}
Constantinos Daskalakis.
\newblock On the complexity of approximating a nash equilibrium.
\newblock \emph{ACM Transactions on Algorithms (TALG)}, 9\penalty0
  (3):\penalty0 1--35, 2013.

\bibitem[Dou et~al.(2021)Dou, Yang, Wang, and Du]{dou2021gap}
Zehao Dou, Zhuoran Yang, Zhaoran Wang, and Simon~S Du.
\newblock Gap-dependent bounds for two-player markov games.
\newblock \emph{arXiv preprint arXiv:2107.00685}, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Hansen et~al.(2013)Hansen, Miltersen, and Zwick]{hansen2013strategy}
Thomas~Dueholm Hansen, Peter~Bro Miltersen, and Uri Zwick.
\newblock Strategy iteration is strongly polynomial for 2-player turn-based
  stochastic games with a constant discount factor.
\newblock \emph{Journal of the ACM (JACM)}, 60\penalty0 (1):\penalty0 1--16,
  2013.

\bibitem[Huang et~al.(2021)Huang, Lee, Wang, and Yang]{huang2021towards}
Baihe Huang, Jason~D Lee, Zhaoran Wang, and Zhuoran Yang.
\newblock Towards general function approximation in zero-sum markov games.
\newblock \emph{arXiv preprint arXiv:2107.14702}, 2021.

\bibitem[Jiang and Lu(2021)]{jiang2021offline}
Jiechuan Jiang and Zongqing Lu.
\newblock Offline decentralized multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2108.01832}, 2021.

\bibitem[Jin et~al.(2021{\natexlab{a}})Jin, Liu, Wang, and Yu]{jin2021v}
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu.
\newblock V-learning--a simple, efficient, decentralized algorithm for
  multiagent rl.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2021{\natexlab{b}})Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Shi, Chen, Chi, and Wei]{li2022settling}
Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei.
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2204.05275}, 2022.

\bibitem[Liu et~al.(2021)Liu, Yu, Bai, and Jin]{liu2021sharp}
Qinghua Liu, Tiancheng Yu, Yu~Bai, and Chi Jin.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In \emph{International Conference on Machine Learning}, pages
  7001--7010. PMLR, 2021.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{lowe2017multi}
Ryan Lowe, Yi~I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter~Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mao et~al.(2021)Mao, Basar, Yang, and Zhang]{mao2021decentralized}
Weichao Mao, Tamer Basar, Lin~F Yang, and Kaiqing Zhang.
\newblock Decentralized cooperative multi-agent reinforcement learning with
  exploration.
\newblock \emph{arXiv preprint arXiv:2110.05707}, 2021.

\bibitem[Meng et~al.(2021)Meng, Wen, Yang, Le, Li, Zhang, Wen, Zhang, Wang, and
  Xu]{meng2021offline}
Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang,
  Ying Wen, Haifeng Zhang, Jun Wang, and Bo~Xu.
\newblock Offline pre-trained multi-agent decision transformer: One big
  sequence model conquers all starcraftii tasks.
\newblock \emph{arXiv preprint arXiv:2112.02845}, 2021.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem[Pan et~al.(2021)Pan, Huang, Ma, and Xu]{pan2021plan}
Ling Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu.
\newblock Plan better amid conservatism: Offline multi-agent reinforcement
  learning with actor rectification.
\newblock \emph{arXiv preprint arXiv:2111.11188}, 2021.

\bibitem[Perolat et~al.(2015)Perolat, Scherrer, Piot, and
  Pietquin]{perolat2015approximate}
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin.
\newblock Approximate dynamic programming for two-player zero-sum markov games.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1329. PMLR, 2015.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ren et~al.(2021)Ren, Li, Dai, Du, and Sanghavi]{ren2021nearly}
Tongzheng Ren, Jialian Li, Bo~Dai, Simon~S Du, and Sujay Sanghavi.
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Rubinstein(2016)]{rubinstein2016settling}
Aviad Rubinstein.
\newblock Settling the complexity of computing approximate two-player nash
  equilibria.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 258--265. IEEE, 2016.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Sidford et~al.(2020)Sidford, Wang, Yang, and Ye]{sidford2020solving}
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye.
\newblock Solving discounted stochastic two-player games with near-optimal time
  and sample complexity.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2992--3002. PMLR, 2020.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Song et~al.(2021)Song, Mei, and Bai]{song2021can}
Ziang Song, Song Mei, and Yu~Bai.
\newblock When can we learn general-sum markov games with a large number of
  players sample-efficiently?
\newblock \emph{arXiv preprint arXiv:2110.04184}, 2021.

\bibitem[Subramanian et~al.(2021)Subramanian, Sinha, and
  Mahajan]{subramanian2021robustness}
Jayakumar Subramanian, Amit Sinha, and Aditya Mahajan.
\newblock Robustness and sample complexity of model-based marl for general-sum
  markov games.
\newblock \emph{arXiv preprint arXiv:2110.02355}, 2021.

\bibitem[Szepesv{\'a}ri and Munos(2005)]{szepesvari2005finite}
Csaba Szepesv{\'a}ri and R{\'e}mi Munos.
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 880--887, 2005.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In \emph{Conference on learning theory}, pages 3674--3682. PMLR,
  2020.

\bibitem[Xie and Jiang(2021)]{xie2021batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, pages
  11404--11413. PMLR, 2021.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Yin and Wang(2021)]{yin2021towards}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Yin et~al.(2020)Yin, Bai, and Wang]{yin2020near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Kakade, Basar, and Yang]{zhang2020model}
Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang.
\newblock Model-based multi-agent rl in zero-sum markov games with near-optimal
  sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1166--1178, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Yang, and
  Basar]{zhang2021multi}
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, pages
  321--384, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Liu, Zhang, and
  Basar]{zhang2021finite}
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar.
\newblock Finite-sample analysis for decentralized batch multiagent
  reinforcement learning with networked agents.
\newblock \emph{IEEE Transactions on Automatic Control}, 66\penalty0
  (12):\penalty0 5925--5940, 2021{\natexlab{b}}.

\bibitem[Zhong et~al.(2021)Zhong, Yang, Wang, and Jordan]{zhong2021can}
Han Zhong, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Can reinforcement learning find stackelberg-nash equilibria in
  general-sum markov games with myopic followers?
\newblock \emph{arXiv preprint arXiv:2112.13521}, 2021.

\bibitem[Zhong et~al.(2022)Zhong, Xiong, Tan, Wang, Zhang, Wang, and
  Yang]{zhong2022pessimistic}
Han Zhong, Wei Xiong, Jiyuan Tan, Liwei Wang, Tong Zhang, Zhaoran Wang, and
  Zhuoran Yang.
\newblock Pessimistic minimax value iteration: Provably efficient equilibrium
  learning from offline datasets.
\newblock \emph{arXiv preprint arXiv:2202.07511}, 2022.

\end{thebibliography}
