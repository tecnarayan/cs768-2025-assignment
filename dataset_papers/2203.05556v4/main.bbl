\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{optuna}
T.~Akiba, S.~Sano, T.~Yanase, T.~Ohta, and M.~Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In \emph{KDD}, 2019.

\bibitem[Arik and Pfister(2020)]{tabnet}
S.~O. Arik and T.~Pfister.
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock \emph{arXiv}, 1908.07442v5, 2020.

\bibitem[Badirli et~al.(2020)Badirli, Liu, Xing, Bhowmik, Doan, and
  Keerthi]{grownet}
S.~Badirli, X.~Liu, Z.~Xing, A.~Bhowmik, K.~Doan, and S.~S. Keerthi.
\newblock Gradient boosting neural networks: Grownet.
\newblock \emph{arXiv}, 2002.07971v2, 2020.

\bibitem[Baldi et~al.(2014)Baldi, Sadowski, and Whiteson]{higgs}
P.~Baldi, P.~Sadowski, and D.~Whiteson.
\newblock Searching for exotic particles in high-energy physics with deep
  learning.
\newblock \emph{Nature Communications}, 5, 2014.

\bibitem[Blackard and Dean.(2000)]{covertype}
J.~A. Blackard and D.~J. Dean.
\newblock Comparative accuracies of artificial neural networks and discriminant
  analysis in predicting forest cover types from cartographic variables.
\newblock \emph{Computers and Electronics in Agriculture}, 24\penalty0
  (3):\penalty0 131--151, 2000.

\bibitem[Buckman et~al.(2018)Buckman, Roy, Raffel, and Goodfellow]{thermometer}
J.~Buckman, A.~Roy, C.~Raffel, and I.~J. Goodfellow.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chen and Guestrin(2016)]{xgboost}
T.~Chen and C.~Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{SIGKDD}, 2016.

\bibitem[Covington et~al.(2016)Covington, Adams, and Sargin]{youtube}
P.~Covington, J.~Adams, and E.~Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In \emph{{RecSys}}, 2016.

\bibitem[Cybenko(1989)]{universal-approximation-1989}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Math. Control. Signals Syst.}, 2\penalty0 (4), 1989.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Dougherty et~al.(1995)Dougherty, Kohavi, and
  Sahami]{discretization-review}
J.~Dougherty, R.~Kohavi, and M.~Sahami.
\newblock Supervised and unsupervised discretization of continuous features.
\newblock In \emph{{ICML}}, 1995.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Gorishniy et~al.(2021)Gorishniy, Rubachev, Khrulkov, and
  Babenko]{revisiting}
Y.~Gorishniy, I.~Rubachev, V.~Khrulkov, and A.~Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Guo et~al.(2021)Guo, Chen, Tang, Zhang, Li, and He]{autodis}
H.~Guo, B.~Chen, R.~Tang, W.~Zhang, Z.~Li, and X.~He.
\newblock An embedding learning framework for numerical features in {CTR}
  prediction.
\newblock In \emph{{KDD}}, 2021.

\bibitem[Hazimeh et~al.(2020)Hazimeh, Ponomareva, Mol, Tan, and Mazumder]{tel}
H.~Hazimeh, N.~Ponomareva, P.~Mol, Z.~Tan, and R.~Mazumder.
\newblock The tree ensemble layer: Differentiability meets conditional
  computation.
\newblock In \emph{ICML}, 2020.

\bibitem[Hornik(1991)]{universal-approximation-1991}
K.~Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2), 1991.

\bibitem[Huang et~al.(2020)Huang, Khetan, Cvitkovic, and
  Karnin]{tabtransformer}
X.~Huang, A.~Khetan, M.~Cvitkovic, and Z.~Karnin.
\newblock Tabtransformer: Tabular data modeling using contextual embeddings.
\newblock \emph{arXiv}, 2012.06678v1, 2020.

\bibitem[Kadra et~al.(2021)Kadra, Lindauer, Hutter, and Grabocka]{cocktails}
A.~Kadra, M.~Lindauer, F.~Hutter, and J.~Grabocka.
\newblock Well-tuned simple nets excel on tabular datasets.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and
  Liu]{lightgbm}
G.~Ke, Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, and T.-Y. Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock \emph{Advances in neural information processing systems},
  30:\penalty0 3146--3154, 2017.

\bibitem[{Kelley Pace} and Barry(1997)]{california}
R.~{Kelley Pace} and R.~Barry.
\newblock Sparse spatial autoregressions.
\newblock \emph{Statistics \& Probability Letters}, 33\penalty0 (3):\penalty0
  291--297, 1997.

\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and
  Hochreiter]{snn}
G.~Klambauer, T.~Unterthiner, A.~Mayr, and S.~Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Kohavi(1996)]{adult}
R.~Kohavi.
\newblock Scaling up the accuracy of naive-bayes classifiers: a decision-tree
  hybrid.
\newblock In \emph{KDD}, 1996.

\bibitem[Kohavi and Sahami(1996)]{c4.5-disc}
R.~Kohavi and M.~Sahami.
\newblock Error-based and entropy-based discretization of continuous features.
\newblock In \emph{{KDD}}, pages 114--119. {AAAI} Press, 1996.

\bibitem[Kossen et~al.(2021)Kossen, Band, Lyle, Gomez, Rainforth, and Gal]{npt}
J.~Kossen, N.~Band, C.~Lyle, A.~N. Gomez, T.~Rainforth, and Y.~Gal.
\newblock Self-attention between datapoints: Going beyond individual
  input-output pairs in deep learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Li et~al.(2021)Li, Si, Li, Hsieh, and Bengio]{learnable-fourier}
Y.~Li, S.~Si, G.~Li, C.~Hsieh, and S.~Bengio.
\newblock Learnable fourier features for multi-dimensional spatial positional
  encoding.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Loshchilov and Hutter(2019)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Madeo et~al.(2013)Madeo, Lima, and Peres]{gesture}
R.~C.~B. Madeo, C.~A.~M. Lima, and S.~M. Peres.
\newblock Gesture unit segmentation using support vector machines: segmenting
  gestures from rest positions.
\newblock In \emph{Proceedings of the 28th Annual {ACM} Symposium on Applied
  Computing, {SAC}}, 2013.

\bibitem[Mildenhall et~al.(2020)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{nerf}
B.~Mildenhall, P.~P. Srinivasan, M.~Tancik, J.~T. Barron, R.~Ramamoorthi, and
  R.~Ng.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[M{\"{u}}ller et~al.(2019)M{\"{u}}ller, McWilliams, Rousselle, Gross,
  and Nov{\'{a}}k]{one-blob}
T.~M{\"{u}}ller, B.~McWilliams, F.~Rousselle, M.~Gross, and J.~Nov{\'{a}}k.
\newblock Neural importance sampling.
\newblock \emph{{ACM} Trans. Graph.}, 38\penalty0 (5), 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Popov et~al.(2020)Popov, Morozov, and Babenko]{node}
S.~Popov, S.~Morozov, and A.~Babenko.
\newblock Neural oblivious decision ensembles for deep learning on tabular
  data.
\newblock In \emph{ICLR}, 2020.

\bibitem[Prokhorenkova et~al.(2018)Prokhorenkova, Gusev, Vorobev, Dorogush, and
  Gulin]{catboost}
L.~Prokhorenkova, G.~Gusev, A.~Vorobev, A.~V. Dorogush, and A.~Gulin.
\newblock Catboost: unbiased boosting with categorical features.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Qin and Liu(2013)]{microsoft}
T.~Qin and T.~Liu.
\newblock Introducing {LETOR} 4.0 datasets.
\newblock \emph{arXiv}, 1306.2597v1, 2013.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{spectral-bias}
N.~Rahaman, A.~Baratin, D.~Arpit, F.~Draxler, M.~Lin, F.~A. Hamprecht,
  Y.~Bengio, and A.~C. Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Ramsauer et~al.(2021)Ramsauer, Sch{\"{a}}fl, Lehner, Seidl, Widrich,
  Gruber, Holzleitner, Adler, Kreil, Kopp, Klambauer, Brandstetter, and
  Hochreiter]{hopfield}
H.~Ramsauer, B.~Sch{\"{a}}fl, J.~Lehner, P.~Seidl, M.~Widrich, L.~Gruber,
  M.~Holzleitner, T.~Adler, D.~P. Kreil, M.~K. Kopp, G.~Klambauer,
  J.~Brandstetter, and S.~Hochreiter.
\newblock Hopfield networks is all you need.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Shwartz{-}Ziv and Armon(2021)]{dl_is_not}
R.~Shwartz{-}Ziv and A.~Armon.
\newblock Tabular data: Deep learning is not all you need.
\newblock \emph{arXiv}, 2106.03253v1, 2021.

\bibitem[Singh et~al.(2015)Singh, Sandhu, and Kumar]{fb-comments}
K.~Singh, R.~K. Sandhu, and D.~Kumar.
\newblock Comment volume prediction using neural networks and decision trees.
\newblock In \emph{IEEE UKSim-AMSS 17th International Conference on Computer
  Modelling and Simulation, UKSim}, 2015.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{siren}
V.~Sitzmann, J.~N.~P. Martel, A.~W. Bergman, D.~B. Lindell, and G.~Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Somepalli et~al.(2021)Somepalli, Goldblum, Schwarzschild, Bruss, and
  Goldstein]{saint}
G.~Somepalli, M.~Goldblum, A.~Schwarzschild, C.~B. Bruss, and T.~Goldstein.
\newblock {SAINT:} improved neural networks for tabular data via row attention
  and contrastive pre-training.
\newblock \emph{arXiv}, 2106.01342v1, 2021.

\bibitem[Song et~al.(2019)Song, Shi, Xiao, Duan, Xu, Zhang, and Tang]{autoint}
W.~Song, C.~Shi, Z.~Xiao, Z.~Duan, Y.~Xu, M.~Zhang, and J.~Tang.
\newblock Autoint: Automatic feature interaction learning via self-attentive
  neural networks.
\newblock In \emph{CIKM}, 2019.

\bibitem[Sundararaman et~al.(2020)Sundararaman, Si, Subramanian, Wang,
  Hazarika, and Carin]{dice}
D.~Sundararaman, S.~Si, V.~Subramanian, G.~Wang, D.~Hazarika, and L.~Carin.
\newblock Methods for numeracy-preserving word embeddings.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing}, 2020.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich{-}Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{fourier-features}
M.~Tancik, P.~P. Srinivasan, B.~Mildenhall, S.~Fridovich{-}Keil, N.~Raghavan,
  U.~Singhal, R.~Ramamoorthi, J.~T. Barron, and R.~Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Turner et~al.(2021)Turner, Eriksson, McCourt, Kiili, Laaksonen, Xu,
  and Guyon]{hp-tuning}
R.~Turner, D.~Eriksson, M.~McCourt, J.~Kiili, E.~Laaksonen, Z.~Xu, and
  I.~Guyon.
\newblock Bayesian optimization is superior to random search for machine
  learning hyperparameter tuning: Analysis of the black-box optimization
  challenge 2020.
\newblock \emph{arXiv}, https://arxiv.org/abs/2104.10201v1, 2021.

\bibitem[Vanschoren et~al.(2014)Vanschoren, van Rijn, Bischl, and
  Torgo]{openml}
J.~Vanschoren, J.~N. van Rijn, B.~Bischl, and L.~Torgo.
\newblock Openml: networked science in machine learning.
\newblock \emph{arXiv}, 1407.7722v1, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Wang et~al.(2017)Wang, Fu, Fu, and Wang]{dcn}
R.~Wang, B.~Fu, G.~Fu, and M.~Wang.
\newblock Deep \& cross network for ad click predictions.
\newblock In \emph{ADKDD}, 2017.

\end{thebibliography}
