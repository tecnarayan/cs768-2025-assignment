\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani et~al.(2020)Advani, Saxe, and
  Sompolinsky]{advani2020highdimensional}
Advani, M., Saxe, A., and Sompolinsky, H.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{Neural Networks}, 132:\penalty0 428 -- 446, 2020.

\bibitem[Arivazhagan et~al.(2019)Arivazhagan, Bapna, Firat, Lepikhin, Johnson,
  Krikun, Chen, Cao, Foster, Cherry, et~al.]{arivazhagan2019massively}
Arivazhagan, N., Bapna, A., Firat, O., Lepikhin, D., Johnson, M., Krikun, M.,
  Chen, M.~X., Cao, Y., Foster, G., Cherry, C., et~al.
\newblock Massively multilingual neural machine translation in the wild:
  Findings and challenges.
\newblock \emph{arXiv preprint arXiv:1907.05019}, 2019.

\bibitem[Asanuma et~al.(2021)Asanuma, Takagi, Nagano, Yoshida, Igarashi, and
  Okada]{asanuma2021statistical}
Asanuma, H., Takagi, S., Nagano, Y., Yoshida, Y., Igarashi, Y., and Okada, M.
\newblock Statistical mechanical analysis of catastrophic forgetting in
  continual learning with teacher and student networks.
\newblock \emph{arXiv preprint arXiv:2105.07385}, 2021.

\bibitem[Aubin et~al.(2018)Aubin, Maillard, Barbier, Krzakala, Macris, and
  Zdeborov{\'{a}}]{aubin2018committee}
Aubin, B., Maillard, A., Barbier, J., Krzakala, F., Macris, N., and
  Zdeborov{\'{a}}, L.
\newblock {The committee machine: Computational to statistical gaps in learning
  a two-layers neural network}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  3227--3238, 2018.

\bibitem[Bahri et~al.(2020)Bahri, Kadmon, Pennington, Schoenholz,
  Sohl-Dickstein, and Ganguli]{bahri2020statistical}
Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S., Sohl-Dickstein, J., and
  Ganguli, S.
\newblock {Statistical Mechanics of Deep Learning}.
\newblock \emph{Annual Review of Condensed Matter Physics}, 11\penalty0
  (1):\penalty0 501--528, 2020.

\bibitem[Baity-Jesi et~al.(2018)Baity-Jesi, Sagun, Geiger, Spigler, Arous,
  Cammarota, LeCun, Wyart, and Biroli]{Baity-Jesi2018}
Baity-Jesi, M., Sagun, L., Geiger, M., Spigler, S., Arous, G., Cammarota, C.,
  LeCun, Y., Wyart, M., and Biroli, G.
\newblock {Comparing Dynamics: Deep Neural Networks versus Glassy Systems}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Bennani \& Sugiyama(2020)Bennani and
  Sugiyama]{bennani2020generalisation}
Bennani, M.~A. and Sugiyama, M.
\newblock Generalisation guarantees for continual learning with orthogonal
  gradient descent.
\newblock \emph{arXiv preprint arXiv:2006.11942}, 2020.

\bibitem[Biehl \& Schwarze(1993)Biehl and Schwarze]{biehl1993learning}
Biehl, M. and Schwarze, H.
\newblock Learning drifting concepts with neural networks.
\newblock \emph{Journal of Physics A: Mathematical and General}, 26\penalty0
  (11):\penalty0 2651, 1993.

\bibitem[Biehl \& Schwarze(1995)Biehl and Schwarze]{Biehl1995}
Biehl, M. and Schwarze, H.
\newblock {Learning by on-line gradient descent}.
\newblock \emph{J. Phys. A. Math. Gen.}, 28\penalty0 (3):\penalty0 643--656,
  1995.

\bibitem[Biehl et~al.(1996)Biehl, Riegler, and W{\"{o}}hler]{Biehl1996}
Biehl, M., Riegler, P., and W{\"{o}}hler, C.
\newblock {Transient dynamics of on-line learning in two-layered neural
  networks}.
\newblock \emph{Journal of Physics A: Mathematical and General}, 29\penalty0
  (16), 1996.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  3040--3050, 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pp.\
  forthcoming, 2019.

\bibitem[Cichon \& Gan(2015)Cichon and Gan]{cichon2015branch}
Cichon, J. and Gan, W.-B.
\newblock Branch-specific dendritic ca 2+ spikes cause persistent synaptic
  plasticity.
\newblock \emph{Nature}, 520\penalty0 (7546):\penalty0 180--185, 2015.

\bibitem[Dhifallah \& Lu(2021)Dhifallah and Lu]{dhifallah2021phase}
Dhifallah, O. and Lu, Y.~M.
\newblock Phase transitions in transfer learning for high-dimensional
  perceptrons.
\newblock \emph{arXiv preprint arXiv:2101.01918}, 2021.

\bibitem[Doan et~al.(2020)Doan, Bennani, Mazoure, Rabusseau, and
  Alquier]{doan2020theoretical}
Doan, T., Bennani, M., Mazoure, B., Rabusseau, G., and Alquier, P.
\newblock A theoretical analysis of catastrophic forgetting through the ntk
  overlap matrix.
\newblock \emph{arXiv preprint arXiv:2010.04003}, 2020.

\bibitem[Du et~al.(2018)Du, Lee, Tian, Singh, and Poczos]{du2018gradient}
Du, S., Lee, J., Tian, Y., Singh, A., and Poczos, B.
\newblock Gradient descent learns one-hidden-layer {CNN}: Donâ€™t be afraid of
  spurious local minima.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pp.\  1339--1348, 2018.

\bibitem[Engel \& Van~den Broeck(2001)Engel and Van~den
  Broeck]{engel2001statistical}
Engel, A. and Van~den Broeck, C.
\newblock \emph{Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem[Farquhar \& Gal(2018)Farquhar and Gal]{farquhar2018towards}
Farquhar, S. and Gal, Y.
\newblock Towards robust evaluations of continual learning.
\newblock \emph{arXiv preprint arXiv:1805.09733}, 2018.

\bibitem[Flesch et~al.(2018)Flesch, Balaguer, Dekker, Nili, and
  Summerfield]{flesch2018comparing}
Flesch, T., Balaguer, J., Dekker, R., Nili, H., and Summerfield, C.
\newblock Comparing continual task learning in minds and machines.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (44):\penalty0 E10313--E10322, 2018.

\bibitem[Gabri{\'{e}}(2020)]{gabrie2020meanfield}
Gabri{\'{e}}, M.
\newblock Mean-field inference methods for neural networks.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  53\penalty0 (22):\penalty0 223002, 2020.

\bibitem[Gardner \& Derrida(1989)Gardner and Derrida]{gardner1989}
Gardner, E. and Derrida, B.
\newblock {Three unfinished works on the optimal storage capacity of networks}.
\newblock \emph{Journal of Physics A: Mathematical and General}, 22\penalty0
  (12):\penalty0 1983--1994, 1989.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  9111--9121, 2019.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov{\'a}]{goldt2019dynamics}
Goldt, S., Advani, M., Saxe, A.~M., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6979--6989, 2019.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8571--8580, 2018.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Lampinen \& Ganguli(2018)Lampinen and Ganguli]{lampinen2018analytic}
Lampinen, A.~K. and Ganguli, S.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{arXiv preprint arXiv:1809.10374}, 2018.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{li2017learning}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[McClelland et~al.(1995)McClelland, McNaughton, and
  O'Reilly]{McClelland1995}
McClelland, J., McNaughton, B., and O'Reilly, R.
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: insights from the successes and failures of connectionist models
  of learning and memory.
\newblock \emph{Psychological review}, 102\penalty0 (3):\penalty0 419--57, July
  1995.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{mccloskey1989catastrophic}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Pascanu, and
  Ghasemzadeh]{mirzadeh2020understanding}
Mirzadeh, S.~I., Farajtabar, M., Pascanu, R., and Ghasemzadeh, H.
\newblock Understanding the role of training regimes in continual learning.
\newblock \emph{arXiv preprint arXiv:2006.06958}, 2020.

\bibitem[Ndirango \& Lee(2019)Ndirango and Lee]{ndirango2019generalization}
Ndirango, A. and Lee, T.
\newblock Generalization in multitask deep neural classifiers: a statistical
  physics approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15862--15871, 2019.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
Neyshabur, B., Sedghi, H., and Zhang, C.
\newblock What is being transferred in transfer learning?
\newblock In \emph{Advances in neural information processing systems},
  volume~33, 2020.

\bibitem[Nguyen et~al.(2019)Nguyen, Achille, Lam, Hassner, Mahadevan, and
  Soatto]{nguyen2019toward}
Nguyen, C.~V., Achille, A., Lam, M., Hassner, T., Mahadevan, V., and Soatto, S.
\newblock Toward understanding catastrophic forgetting in continual learning.
\newblock \emph{arXiv preprint arXiv:1908.01091}, 2019.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 2019.

\bibitem[Ramasesh et~al.(2020)Ramasesh, Dyer, and Raghu]{ramasesh2020anatomy}
Ramasesh, V.~V., Dyer, E., and Raghu, M.
\newblock Anatomy of catastrophic forgetting: Hidden representations and task
  semantics.
\newblock \emph{arXiv preprint arXiv:2007.07400}, 2020.

\bibitem[Riegler \& Biehl(1995)Riegler and Biehl]{riegler1995line}
Riegler, P. and Biehl, M.
\newblock On-line backpropagation in two-layered neural networks.
\newblock \emph{Journal of Physics A: Mathematical and General}, 28\penalty0
  (20):\penalty0 L507, 1995.

\bibitem[Rotskoff \& Vanden-Eijnden(2018)Rotskoff and
  Vanden-Eijnden]{rotskoff2018parameters}
Rotskoff, G. and Vanden-Eijnden, E.
\newblock Parameters as interacting particles: long time convergence and
  asymptotic error scaling of neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  7146--7155, 2018.

\bibitem[Ruder \& Plank(2017)Ruder and Plank]{ruder2017learning}
Ruder, S. and Plank, B.
\newblock Learning to select data for transfer learning with bayesian
  optimization.
\newblock \emph{arXiv preprint arXiv:1707.05246}, 2017.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
  Kavukcuoglu, K., Pascanu, R., and Hadsell, R.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Saad(2009)]{saad2009line}
Saad, D.
\newblock \emph{On-line learning in neural networks}, volume~17.
\newblock Cambridge University Press, 2009.

\bibitem[Saad \& Solla(1995{\natexlab{a}})Saad and Solla]{saad1995exact}
Saad, D. and Solla, S.~A.
\newblock Exact solution for on-line learning in multilayer neural networks.
\newblock \emph{Physical Review Letters}, 74\penalty0 (21):\penalty0 4337,
  1995{\natexlab{a}}.

\bibitem[Saad \& Solla(1995{\natexlab{b}})Saad and Solla]{saad1995line}
Saad, D. and Solla, S.~A.
\newblock On-line learning in soft committee machines.
\newblock \emph{Physical Review E}, 52\penalty0 (4):\penalty0 4225,
  1995{\natexlab{b}}.

\bibitem[Saxe et~al.(2018)Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey,
  and Cox]{saxe2018information}
Saxe, A., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B., and
  Cox, D.
\newblock {On the information bottleneck theory of deep learning}.
\newblock In \emph{ICLR}, 2018.

\bibitem[Seung et~al.(1992)Seung, Sompolinsky, and
  Tishby]{seung1992statistical}
Seung, H.~S., Sompolinsky, H., and Tishby, N.
\newblock Statistical mechanics of learning from examples.
\newblock \emph{Physical review A}, 45\penalty0 (8):\penalty0 6056, 1992.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock Continual learning with deep generative replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2990--2999, 2017.

\bibitem[Sirignano \& Spiliopoulos(2019)Sirignano and
  Spiliopoulos]{sirignano2018}
Sirignano, J. and Spiliopoulos, K.
\newblock {Mean field analysis of neural networks: A central limit theorem}.
\newblock \emph{Stochastic Processes and their Applications}, 2019.

\bibitem[Sirignano \& Spiliopoulos(2020)Sirignano and
  Spiliopoulos]{sirignano2020mean}
Sirignano, J. and Spiliopoulos, K.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock \emph{Stochastic Processes and their Applications}, 130\penalty0
  (3):\penalty0 1820--1852, 2020.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Soltanolkotabi, M., Javanmard, A., and Lee, J.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2018.

\bibitem[Tian(2017)]{tian2017analytical}
Tian, Y.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, pp.\  3404â€“3413, 2017.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, Combes, Trischler, Bengio, and
  Gordon]{toneva2018empirical}
Toneva, M., Sordoni, A., Combes, R. T.~d., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock \emph{arXiv preprint arXiv:1812.05159}, 2018.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{watkin1993}
Watkin, T., Rau, A., and Biehl, M.
\newblock {The statistical mechanics of learning a rule}.
\newblock \emph{Reviews of Modern Physics}, 65\penalty0 (2):\penalty0 499--556,
  1993.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017online}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock 2017.

\bibitem[Yang et~al.(2014)Yang, Lai, Cichon, Ma, Li, and Gan]{yang2014sleep}
Yang, G., Lai, C. S.~W., Cichon, J., Ma, L., Li, W., and Gan, W.-B.
\newblock Sleep promotes branch-specific formation of dendritic spines after
  learning.
\newblock \emph{Science}, 344\penalty0 (6188):\penalty0 1173--1178, 2014.

\bibitem[Yoshida \& Okada(2019)Yoshida and Okada]{yoshida2019datadependence}
Yoshida, Y. and Okada, M.
\newblock Data-dependence of plateau phenomenon in learning with neural network
  --- statistical mechanical analysis.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  1720--1728, 2019.

\bibitem[Zdeborov{\'{a}}(2020)]{zdeborova2020understanding}
Zdeborov{\'{a}}, L.
\newblock {Understanding deep learning is also a job for physicists}.
\newblock \emph{Nature Physics}, 2020.

\bibitem[Zdeborov{\'a} \& Krzakala(2016)Zdeborov{\'a} and
  Krzakala]{zdeborova2016statistical}
Zdeborov{\'a}, L. and Krzakala, F.
\newblock Statistical physics of inference: Thresholds and algorithms.
\newblock \emph{Advances in Physics}, 65\penalty0 (5):\penalty0 453--552, 2016.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{pmlr-v70-zenke17a}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3987--3995, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Zhong, K., Song, Z., Jain, P., Bartlett, P., and Dhillon, I.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  4140--4149, 2017.

\bibitem[Zimmer et~al.(2014)Zimmer, Viappiani, and Weng]{zimmer2014teacher}
Zimmer, M., Viappiani, P., and Weng, P.
\newblock Teacher-student framework: a reinforcement learning approach.
\newblock In \emph{AAMAS Workshop Autonomous Robots and Multirobot Systems},
  2014.

\end{thebibliography}
