\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asuncion and Newman(2007)]{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock Uci machine learning repository, 2007.

\bibitem[Awasthi et~al.(2022{\natexlab{a}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022h}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {$H$}-consistency bounds for surrogate loss minimizers.
\newblock In \emph{International Conference on Machine Learning}, pages
  1117--1174, 2022{\natexlab{a}}.

\bibitem[Awasthi et~al.(2022{\natexlab{b}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022multi}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Multi-class {$ H $}-consistency bounds.
\newblock In \emph{Advances in neural information processing systems}, pages
  782--795, 2022{\natexlab{b}}.

\bibitem[Bartlett and Wegkamp(2008)]{bartlett2008classification}
Peter~L Bartlett and Marten~H Wegkamp.
\newblock Classification with a reject option using a hinge loss.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (8), 2008.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L. Bartlett, Michael~I. Jordan, and Jon~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Benz and Rodriguez(2022)]{benz2022counterfactual}
Nina L~Corvelo Benz and Manuel~Gomez Rodriguez.
\newblock Counterfactual inference of second opinions.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 453--463.
  PMLR, 2022.

\bibitem[Berkson(1944)]{Berkson1944}
Joseph Berkson.
\newblock Application of the logistic function to bio-assay.
\newblock \emph{Journal of the American Statistical Association}, 39:\penalty0
  357–--365, 1944.

\bibitem[Berkson(1951)]{Berkson1951}
Joseph Berkson.
\newblock Why {I} prefer logits to probits.
\newblock \emph{Biometrics}, 7\penalty0 (4):\penalty0 327–--339, 1951.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cao et~al.(2022)Cao, Cai, Feng, Gu, Gu, An, Niu, and
  Sugiyama]{caogeneralizing}
Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo~An, Gang Niu, and
  Masashi Sugiyama.
\newblock Generalizing consistent multi-class classification with rejection to
  be compatible with arbitrary losses.
\newblock In \emph{Advances in neural information processing systems}, 2022.

\bibitem[Charoenphakdee et~al.(2021)Charoenphakdee, Cui, Zhang, and
  Sugiyama]{charoenphakdee2021classification}
Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama.
\newblock Classification with rejection based on cost-sensitive classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1517, 2021.

\bibitem[Cheng et~al.(2023)Cheng, Cao, Wang, Wei, An, and
  Feng]{cheng2023regression}
Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo~An, and Lei Feng.
\newblock Regression with cost-based rejection.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Chow(1970)]{chow1970optimum}
C~Chow.
\newblock On optimum recognition error and reject tradeoff.
\newblock \emph{IEEE Transactions on information theory}, 16\penalty0
  (1):\penalty0 41--46, 1970.

\bibitem[Chow(1957)]{Chow1957}
C.K. Chow.
\newblock An optimum character recognition system using decision function.
\newblock \emph{IEEE T. C.}, 1957.

\bibitem[Cortes and Vapnik(1995)]{cortes1995support}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20:\penalty0 273--297, 1995.

\bibitem[Cortes et~al.(2016{\natexlab{a}})Cortes, DeSalvo, and
  Mohri]{CortesDeSalvoMohri2016bis}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Boosting with abstention.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1660--1668, 2016{\natexlab{a}}.

\bibitem[Cortes et~al.(2016{\natexlab{b}})Cortes, DeSalvo, and
  Mohri]{cortes2016learning}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Learning with rejection.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 67--82, 2016{\natexlab{b}}.

\bibitem[Cortes et~al.(2023)Cortes, DeSalvo, and Mohri]{CortesDeSalvoMohri2023}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Theory and algorithms for learning with rejection in binary
  classification.
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, 2023.

\bibitem[Cruz et~al.(2018)Cruz, Sabourin, and Cavalcanti]{cruz2018dynamic}
Rafael~MO Cruz, Robert Sabourin, and George~DC Cavalcanti.
\newblock Dynamic classifier selection: Recent advances and perspectives.
\newblock \emph{Information Fusion}, 41:\penalty0 195--216, 2018.

\bibitem[De et~al.(2020)De, Koley, Ganguly, and
  Gomez-Rodriguez]{de2020regression}
Abir De, Paramita Koley, Niloy Ganguly, and Manuel Gomez-Rodriguez.
\newblock Regression under human assistance.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 2611--2620, 2020.

\bibitem[Ekanayake et~al.(2023)Ekanayake, Zois, and
  Chelmis]{ekanayake2023sequential}
Sachini~Piyoni Ekanayake, Daphney-Stavroula Zois, and Charalampos Chelmis.
\newblock Sequential datum--wise feature acquisition and classifier selection.
\newblock \emph{IEEE Transactions on Artificial Intelligence}, 2023.

\bibitem[El-Yaniv and Wiener(2012)]{el2012active}
Ran El-Yaniv and Yair Wiener.
\newblock Active learning via perfect selective classification.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (2), 2012.

\bibitem[El-Yaniv et~al.(2010)]{el2010foundations}
Ran El-Yaniv et~al.
\newblock On the foundations of noise-free selective classification.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (5), 2010.

\bibitem[Geifman and El-Yaniv(2017)]{geifman2017selective}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selective classification for deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Geifman and El-Yaniv(2019)]{geifman2019selectivenet}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selectivenet: A deep neural network with an integrated reject option.
\newblock In \emph{International conference on machine learning}, pages
  2151--2159, 2019.

\bibitem[Hemmer et~al.(2022)Hemmer, Schellhammer, V{\"o}ssing, Jakubik, and
  Satzger]{hemmer2022forming}
Patrick Hemmer, Sebastian Schellhammer, Michael V{\"o}ssing, Johannes Jakubik,
  and Gerhard Satzger.
\newblock Forming effective human-ai teams: Building machine learning models
  that complement the capabilities of multiple experts.
\newblock \emph{arXiv preprint arXiv:2206.07948}, 2022.

\bibitem[Jiang et~al.(2020)Jiang, Zhao, and Wang]{jiang2020risk}
Wenming Jiang, Ying Zhao, and Zehan Wang.
\newblock Risk-controlled selective prediction for regression deep neural
  network models.
\newblock In \emph{2020 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8, 2020.

\bibitem[Kerrigan et~al.(2021)Kerrigan, Smyth, and
  Steyvers]{kerrigan2021combining}
Gavin Kerrigan, Padhraic Smyth, and Mark Steyvers.
\newblock Combining human predictions with model probabilities via confusion
  matrices and calibration.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4421--4434, 2021.

\bibitem[Keswani et~al.(2021)Keswani, Lease, and
  Kenthapadi]{keswani2021towards}
Vijay Keswani, Matthew Lease, and Krishnaram Kenthapadi.
\newblock Towards unbiased and accurate deferral to multiple experts.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 154--165, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Ko et~al.(2008)Ko, Sabourin, and Britto~Jr]{ko2008dynamic}
Albert~HR Ko, Robert Sabourin, and Alceu~Souza Britto~Jr.
\newblock From dynamic classifier selection to dynamic ensemble selection.
\newblock \emph{Pattern recognition}, 41\penalty0 (5):\penalty0 1718--1731,
  2008.

\bibitem[Li et~al.(2023)Li, Liu, Sun, and Wang]{li2023no}
Xiaocheng Li, Shang Liu, Chunlin Sun, and Hanzhao Wang.
\newblock When no-rejection learning is optimal for regression with rejection.
\newblock \emph{arXiv preprint arXiv:2307.02932}, 2023.

\bibitem[Madras et~al.(2018)Madras, Pitassi, and Zemel]{madras2018predict}
David Madras, Toni Pitassi, and Richard Zemel.
\newblock Predict responsibly: improving fairness and accuracy by learning to
  defer.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Mao et~al.(2023{\natexlab{a}})Mao, Mohri, Mohri, and
  Zhong]{MaoMohriZhong2023}
Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong.
\newblock Two-stage learning to defer with multiple experts.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2023)}, New Orleans, Louisiana, 2023{\natexlab{a}}. MIT Press.

\bibitem[Mao et~al.(2023{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023characterization}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds: Characterization and extensions.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{b}}.

\bibitem[Mao et~al.(2023{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023ranking}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds for pairwise misranking loss surrogates.
\newblock In \emph{International conference on Machine learning},
  2023{\natexlab{c}}.

\bibitem[Mao et~al.(2023{\natexlab{d}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023rankingabs}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Ranking with abstention.
\newblock In \emph{ICML 2023 Workshop The Many Facets of Preference-Based
  Learning}, 2023{\natexlab{d}}.

\bibitem[Mao et~al.(2023{\natexlab{e}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023structured}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Structured prediction with stronger consistency guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{e}}.

\bibitem[Mao et~al.(2023{\natexlab{f}})Mao, Mohri, and Zhong]{mao2023cross}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Cross-entropy loss functions: Theoretical analysis and applications.
\newblock In \emph{International Conference on Machine Learning},
  2023{\natexlab{f}}.

\bibitem[Mao et~al.(2024{\natexlab{a}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Principled approaches for learning to defer with multiple experts.
\newblock In \emph{International Symposium on Artificial Intelligence and
  Mathematics}, 2024{\natexlab{a}}.

\bibitem[Mao et~al.(2024{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024predictor}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Predictor-rejector multi-class abstention: Theoretical analysis and
  algorithms.
\newblock In \emph{Algorithmic Learning Theory}, 2024{\natexlab{b}}.

\bibitem[Mao et~al.(2024{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024score}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Theoretically grounded loss functions and algorithms for score-based
  multi-class abstention.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2024{\natexlab{c}}.

\bibitem[Mohri et~al.(2024)Mohri, Andor, Choi, Collins, Mao, and
  Zhong]{MohriAndorChoiCollinsMaoZhong2024learning}
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and
  Yutao Zhong.
\newblock Learning to reject with a fixed predictor: Application to
  decontextualization.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Mozannar and Sontag(2020)]{mozannar2020consistent}
Hussein Mozannar and David Sontag.
\newblock Consistent estimators for learning to defer to an expert.
\newblock In \emph{International Conference on Machine Learning}, pages
  7076--7087, 2020.

\bibitem[Nair and Hinton(2010)]{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pages 807--814, 2010.

\bibitem[Narasimhan et~al.(2022)Narasimhan, Jitkrittum, Menon, Rawat, and
  Kumar]{narasimhanpost}
Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya~Krishna Menon, Ankit~Singh
  Rawat, and Sanjiv Kumar.
\newblock Post-hoc estimators for learning to defer to an expert.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  29292--29304, 2022.

\bibitem[Ni et~al.(2019)Ni, Charoenphakdee, Honda, and Sugiyama]{NiCHS19}
Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama.
\newblock On the calibration of multiclass classification with rejection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2582--2592, 2019.

\bibitem[Okati et~al.(2021)Okati, De, and Rodriguez]{okati2021differentiable}
Nastaran Okati, Abir De, and Manuel Rodriguez.
\newblock Differentiable learning under triage.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 9140--9151, 2021.

\bibitem[Ramaswamy et~al.(2018)Ramaswamy, Tewari, and
  Agarwal]{ramaswamy2018consistent}
Harish~G Ramaswamy, Ambuj Tewari, and Shivani Agarwal.
\newblock Consistent algorithms for multiclass classification with an abstain
  option.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (1):\penalty0
  530--554, 2018.

\bibitem[Shah et~al.(2022)Shah, Bu, Lee, Das, Panda, Sattigeri, and
  Wornell]{shah2022selective}
Abhin Shah, Yuheng Bu, Joshua~K Lee, Subhro Das, Rameswar Panda, Prasanna
  Sattigeri, and Gregory~W Wornell.
\newblock Selective regression under fairness criteria.
\newblock In \emph{International Conference on Machine Learning}, pages
  19598--19615, 2022.

\bibitem[Steinwart(2007)]{steinwart2007compare}
Ingo Steinwart.
\newblock How to compare different loss functions and their risks.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  225--287, 2007.

\bibitem[Straitouri et~al.(2022)Straitouri, Wang, Okati, and
  Rodriguez]{straitouri2022provably}
Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel~Gomez Rodriguez.
\newblock Provably improving expert predictions with conformal prediction.
\newblock \emph{arXiv preprint arXiv:2201.12006}, 2022.

\bibitem[Tailor et~al.(2024)Tailor, Patra, Verma, Manggala, and
  Nalisnick]{tailor2024learning}
Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, and Eric
  Nalisnick.
\newblock Learning to defer to a population: A meta-learning approach.
\newblock \emph{arXiv preprint arXiv:2403.02683}, 2024.

\bibitem[Tewari and Bartlett(2007)]{tewari2007consistency}
Ambuj Tewari and Peter~L. Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (36):\penalty0 1007--1025, 2007.

\bibitem[Verhulst(1838)]{Verhulst1838}
Pierre~François Verhulst.
\newblock Notice sur la loi que la population suit dans son accroissement.
\newblock \emph{Correspondance math\'ematique et physique}, 10:\penalty0
  113–--121, 1838.

\bibitem[Verhulst(1845)]{Verhulst1845}
Pierre~François Verhulst.
\newblock Recherches math\'ematiques sur la loi d'accroissement de la
  population.
\newblock \emph{Nouveaux M\'emoires de l'Acad\'emie Royale des Sciences et
  Belles-Lettres de Bruxelles}, 18:\penalty0 1–--42, 1845.

\bibitem[Verma and Nalisnick(2022)]{verma2022calibrated}
Rajeev Verma and Eric Nalisnick.
\newblock Calibrated learning to defer with one-vs-all classifiers.
\newblock In \emph{International Conference on Machine Learning}, pages
  22184--22202, 2022.

\bibitem[Verma et~al.(2023)Verma, Barrej{\'o}n, and
  Nalisnick]{verma2023learning}
Rajeev Verma, Daniel Barrej{\'o}n, and Eric Nalisnick.
\newblock Learning to defer to multiple experts: Consistent surrogate losses,
  confidence calibration, and conformal ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 11415--11434, 2023.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and
  Fedus]{WeiEtAl2022}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{CoRR}, abs/2206.07682, 2022.

\bibitem[Wiener and El-Yaniv(2011)]{wiener2011agnostic}
Yair Wiener and Ran El-Yaniv.
\newblock Agnostic selective classification.
\newblock In \emph{Advances in neural information processing systems}, 2011.

\bibitem[Wiener and El-Yaniv(2012)]{wiener2012pointwise}
Yair Wiener and Ran El-Yaniv.
\newblock Pointwise tracking the optimal regression function.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Wiener and El-Yaniv(2015)]{wiener2015agnostic}
Yair Wiener and Ran El-Yaniv.
\newblock Agnostic pointwise-competitive selective classification.
\newblock \emph{Journal of Artificial Intelligence Research}, 52:\penalty0
  171--201, 2015.

\bibitem[Yuan and Wegkamp(2010)]{yuan2010classification}
Ming Yuan and Marten Wegkamp.
\newblock Classification methods with reject option based on convex risk
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (1), 2010.

\bibitem[Yuan and Wegkamp(2011)]{WegkampYuan2011}
Ming Yuan and Marten Wegkamp.
\newblock {SVM}s with a reject option.
\newblock In \emph{Bernoulli}, 2011.

\bibitem[Zaoui et~al.(2020)Zaoui, Denis, and Hebiri]{zaoui2020regression}
Ahmed Zaoui, Christophe Denis, and Mohamed Hebiri.
\newblock Regression with reject option and application to knn.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  20073--20082, 2020.

\bibitem[Zhang(2004{\natexlab{a}})]{Zhang2003}
Tong Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 56--85,
  2004{\natexlab{a}}.

\bibitem[Zhang(2004{\natexlab{b}})]{zhang2004statistical}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Oct):\penalty0 1225--1251, 2004{\natexlab{b}}.

\bibitem[Zheng et~al.(2023)Zheng, Wu, Bao, Cao, Li, and
  Zhu]{zheng2023revisiting}
Chenyu Zheng, Guoqiang Wu, Fan Bao, Yue Cao, Chongxuan Li, and Jun Zhu.
\newblock Revisiting discriminative vs. generative classifiers: Theory and
  implications.
\newblock \emph{arXiv preprint arXiv:2302.02334}, 2023.

\end{thebibliography}
