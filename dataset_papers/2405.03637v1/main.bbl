\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andonian et~al.(2023)Andonian, Anthony, Biderman, Black, Gali, Gao, Hallahan, Levy-Kramer, Leahy, Nestler, Parker, Pieler, Phang, Purohit, Schoelkopf, Stander, Songz, Tigges, Thérien, Wang, and Weinbach]{gpt-neox-library}
Andonian, A., Anthony, Q., Biderman, S., Black, S., Gali, P., Gao, L., Hallahan, E., Levy-Kramer, J., Leahy, C., Nestler, L., et~al.
\newblock {GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}, 9 2023.
\newblock URL \url{https://www.github.com/eleutherai/gpt-neox}.

\bibitem[Attardi(2015)]{Wikiextractor2015}
Attardi, G.
\newblock Wikiextractor.
\newblock \url{https://github.com/attardi/wikiextractor}, 2015.

\bibitem[Banner et~al.(2018)Banner, Hubara, Hoffer, and Soudry]{banner2018scalable}
Banner, R., Hubara, I., Hoffer, E., and Soudry, D.
\newblock Scalable methods for 8-bit training of neural networks, 2018.

\bibitem[Betker et~al.(2023)Betker, Goh, Jing, Brooks, Wang, Li, Ouyang, Zhuang, Lee, Guo, et~al.]{betker2023improving}
Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et~al.
\newblock Improving image generation with better captions.
\newblock \emph{Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf}, 2\penalty0 (3), 2023.

\bibitem[Chen et~al.(2020)Chen, Gai, Yao, Mahoney, and Gonzalez]{chen2020statistical}
Chen, J., Gai, Y., Yao, Z., Mahoney, M.~W., and Gonzalez, J.~E.
\newblock A statistical framework for low-bitwidth training of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 883--894, 2020.

\bibitem[Choi et~al.(2019)Choi, Venkataramani, Srinivasan, Gopalakrishnan, Wang, and Chuang]{MLSYS2019_c443e9d9}
Choi, J., Venkataramani, S., Srinivasan, V.~V., Gopalakrishnan, K., Wang, Z., and Chuang, P.
\newblock Accurate and efficient 2-bit quantized neural networks.
\newblock In Talwalkar, A., Smith, V., and Zaharia, M. (eds.), \emph{Proceedings of Machine Learning and Systems}, volume~1, pp.\  348--359, 2019.
\newblock URL \url{https://proceedings.mlsys.org/paper_files/paper/2019/file/c443e9d9fc984cda1c5cc447fe2c724d-Paper.pdf}.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1, 2016.

\bibitem[Croci et~al.(2022)Croci, Fasi, Higham, Mary, and Mikaitis]{croci2022stochastic}
Croci, M., Fasi, M., Higham, N.~J., Mary, T., and Mikaitis, M.
\newblock Stochastic rounding: implementation, error analysis and applications.
\newblock \emph{Royal Society Open Science}, 9\penalty0 (3):\penalty0 211631, 2022.

\bibitem[Dekker(1971)]{dekker1971float}
Dekker, T.~J.
\newblock A floating-point technique for extending the available precision.
\newblock \emph{Numer. Math.}, 18\penalty0 (3):\penalty0 224–242, jun 1971.
\newblock ISSN 0029-599X.
\newblock \doi{10.1007/BF01397083}.
\newblock URL \url{https://doi.org/10.1007/BF01397083}.

\bibitem[Dettmers \& Zettlemoyer(2023)Dettmers and Zettlemoyer]{dettmers2023case}
Dettmers, T. and Zettlemoyer, L.
\newblock The case for 4-bit precision: k-bit inference scaling laws, 2023.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022llmint8}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Qlora: Efficient finetuning of quantized llms, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.

\bibitem[Dhariwal et~al.(2020)Dhariwal, Jun, Payne, Kim, Radford, and Sutskever]{dhariwal2020jukebox}
Dhariwal, P., Jun, H., Payne, C., Kim, J.~W., Radford, A., and Sutskever, I.
\newblock Jukebox: A generative model for music.
\newblock \emph{arXiv preprint arXiv:2005.00341}, 2020.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023sparsegpt}
Frantar, E. and Alistarh, D.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2023gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023.

\bibitem[Goldberg(1991)]{goldberg1991FPU}
Goldberg, D.
\newblock What every computer scientist should know about floating-point arithmetic.
\newblock \emph{ACM Comput. Surv.}, 23\penalty0 (1):\penalty0 5–48, mar 1991.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/103162.103163}.
\newblock URL \url{https://doi.org/10.1145/103162.103163}.

\bibitem[Granlund(2004)]{granlund2004gnu}
Granlund, T.
\newblock Gnu mp: The gnu multiple precision arithmetic library.
\newblock \emph{http://gmplib. org/}, 2004.

\bibitem[Guo et~al.(2023)Guo, Greengard, Xing, and Kim]{guo2023lq}
Guo, H., Greengard, P., Xing, E.~P., and Kim, Y.
\newblock Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning.
\newblock \emph{arXiv preprint arXiv:2311.12023}, 2023.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and Narayanan]{gupta2015deep}
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P.
\newblock Deep learning with limited numerical precision, 2015.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Hida et~al.(2008)Hida, Li, and Bailey]{hida2008Cpp}
Hida, Y., Li, S., and Bailey, D.
\newblock Library for double-double and quad-double arithmetic.
\newblock 01 2008.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network, 2015.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de~Las~Casas, D., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models, 2022.

\bibitem[Hou et~al.(2019)Hou, Zhang, and Kwok]{hou2018analysis}
Hou, L., Zhang, R., and Kwok, J.~T.
\newblock Analysis of quantized models.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryM_IoAqYX}.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh2023distilling}
Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and Kalenichenko]{jacob2018quantization}
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D.
\newblock Quantization and training of neural networks for efficient integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  2704--2713, 2018.

\bibitem[Jia \& Sandt(2021)Jia and Sandt]{jia2021A100}
Jia, Z. and Sandt, P.~V.
\newblock Zhe jia and peter van sandt. dissecting the ampere gpu architecture via microbenchmarking. gpu technology conference, 2021.
\newblock In \emph{GTC}, 2021.

\bibitem[Jia et~al.(2018)Jia, Maggioni, Staiger, and Scarpazza]{jia2018dissecting}
Jia, Z., Maggioni, M., Staiger, B., and Scarpazza, D.~P.
\newblock Dissecting the nvidia volta gpu architecture via microbenchmarking, 2018.

\bibitem[Kahan(2006)]{kahan2006futile}
Kahan, W.
\newblock How futile are mindless assessments of roundoff in floating-point computation.
\newblock \emph{Preprint, University of California, Berkeley}, 2006.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee, Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke, Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and Dubey]{kalamkar2019study}
Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D.~T., Jammalamadaka, N., Huang, J., Yuen, H., et~al.
\newblock A study of bfloat16 for deep learning training, 2019.

\bibitem[Korthikanti et~al.(2023)Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro]{korthikanti2023reducing}
Korthikanti, V.~A., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5, 2023.

\bibitem[Kuchaiev et~al.(2018)Kuchaiev, Ginsburg, Gitman, Lavrukhin, Li, Nguyen, Case, and Micikevicius]{kuchaiev2018mixedprecision}
Kuchaiev, O., Ginsburg, B., Gitman, I., Lavrukhin, V., Li, J., Nguyen, H., Case, C., and Micikevicius, P.
\newblock Mixed-precision training for nlp and speech recognition with openseq2seq, 2018.

\bibitem[Kuchaiev et~al.(2019)Kuchaiev, Li, Nguyen, Hrinchuk, Leary, Ginsburg, Kriman, Beliaev, Lavrukhin, Cook, Castonguay, Popova, Huang, and Cohen]{kuchaiev2019nemo}
Kuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R., Ginsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V., Cook, J., et~al.
\newblock Nemo: a toolkit for building ai applications using neural modules, 2019.

\bibitem[Kurtic et~al.(2022)Kurtic, Campos, Nguyen, Frantar, Kurtz, Fineran, Goin, and Alistarh]{kurtic2022optimal}
Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B., Goin, M., and Alistarh, D.
\newblock The optimal bert surgeon: Scalable and accurate second-order pruning for large language models, 2022.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Bae, Yoo, Kim, Park, Kim, Ha, Sung, and Lee]{kwon2022alphatuning}
Kwon, S.~J., Kim, J., Bae, J., Yoo, K.~M., Kim, J.-H., Park, B., Kim, B., Ha, J.-W., Sung, N., and Lee, D.
\newblock Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2210.03858}, 2022.

\bibitem[Lagunas et~al.(2021)Lagunas, Charlaix, Sanh, and Rush]{lagunas2021block}
Lagunas, F., Charlaix, E., Sanh, V., and Rush, A.~M.
\newblock Block pruning for faster transformers.
\newblock \emph{arXiv preprint arXiv:2109.04838}, 2021.

\bibitem[Le et~al.(2023)Le, Vyas, Shi, Karrer, Sari, Moritz, Williamson, Manohar, Adi, Mahadeokar, and Hsu]{le2023voicebox}
Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., and Hsu, W.-N.
\newblock Voicebox: Text-guided multilingual universal speech generation at scale, 2023.

\bibitem[Li et~al.(2017)Li, De, Xu, Studer, Samet, and Goldstein]{li2017training}
Li, H., De, S., Xu, Z., Studer, C., Samet, H., and Goldstein, T.
\newblock Training quantized nets: A deeper understanding, 2017.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{micikevicius2017mixed}
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and Wu]{micikevicius2018mixed}
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H.
\newblock Mixed precision training, 2018.

\bibitem[Micikevicius et~al.(2022)Micikevicius, Stosic, Burgess, Cornea, Dubey, Grisenthwaite, Ha, Heinecke, Judd, Kamalu, Mellempudi, Oberman, Shoeybi, Siu, and Wu]{micikevicius2022fp8}
Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P., Kamalu, J., et~al.
\newblock Fp8 formats for deep learning, 2022.

\bibitem[Muller et~al.(2018)Muller, Brisebarre, De~Dinechin, Jeannerod, Lefevre, Melquiond, Revol, Stehl{\'e}, Torres, et~al.]{muller2018handbook}
Muller, J.-M., Brisebarre, N., De~Dinechin, F., Jeannerod, C.-P., Lefevre, V., Melquiond, G., Revol, N., Stehl{\'e}, D., Torres, S., et~al.
\newblock \emph{Handbook of floating-point arithmetic}.
\newblock Springer, 2018.

\bibitem[OpenAI et~al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk,
  Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone,
  Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph]{openai2023gpt4}
OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., et~al.
\newblock Gpt-4 technical report, 2023.

\bibitem[Park et~al.(2018)Park, Lee, Oh, Ha, and Lee]{park2018training}
Park, H., Lee, J.~H., Oh, Y., Ha, S., and Lee, S.
\newblock Training deep neural network in limited precision, 2018.

\bibitem[Peng et~al.(2023)Peng, Wu, Wei, Zhao, Yang, Liu, Xiong, Yang, Ni, Hu, Li, Zhang, Li, Ning, Wang, Zhang, Liu, Chau, Hu, and Cheng]{peng2023fp8lm}
Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., et~al.
\newblock Fp8-lm: Training fp8 large language models, 2023.

\bibitem[Perez et~al.(2023)Perez, Zhang, Briggs, Blake, Levy-Kramer, Balanca, Luschi, Barlow, and Fitzgibbon]{perez2023training}
Perez, S.~P., Zhang, Y., Briggs, J., Blake, C., Levy-Kramer, J., Balanca, P., Luschi, C., Barlow, S., and Fitzgibbon, A.~W.
\newblock Training and inference of large language models using 8-bit floating point, 2023.

\bibitem[Priest(1991)]{priest1991Arithmetic}
Priest, D.
\newblock Algorithms for arbitrary precision floating point arithmetic.
\newblock In \emph{[1991] Proceedings 10th IEEE Symposium on Computer Arithmetic}, pp.\  132--143, 1991.
\newblock \doi{10.1109/ARITH.1991.145549}.

\bibitem[Priest(1992)]{priest1992Arithmetic}
Priest, D.~M.
\newblock \emph{On properties of floating point arithmetics: numerical stability and the cost of accurate computations}.
\newblock PhD thesis, University of California at Berkeley, USA, 1992.
\newblock UMI Order No. GAX93-30692.

\bibitem[Rae et~al.(2022)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{rae2022scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher, 2022.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and Farhadi]{rastegari2016xnornet}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural networks, 2016.

\bibitem[Ruder(2017)]{ruder2017overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem[Sa et~al.(2018)Sa, Leszczynski, Zhang, Marzoev, Aberger, Olukotun, and Ré]{desa2018highaccuracy}
Sa, C.~D., Leszczynski, M., Zhang, J., Marzoev, A., Aberger, C.~R., Olukotun, K., and Ré, C.
\newblock High-accuracy low-precision training, 2018.

\bibitem[Sakr \& Shanbhag(2018)Sakr and Shanbhag]{sakr2018pertensor}
Sakr, C. and Shanbhag, N.
\newblock Per-tensor fixed-point quantization of the back-propagation algorithm, 2018.

\bibitem[Sanh et~al.(2020)Sanh, Debut, Chaumond, and Wolf]{sanh2020distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.

\bibitem[Shoeybi et~al.(2020)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2020megatronlm}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sun et~al.(2019{\natexlab{a}})Sun, Choi, Chen, Wang, Venkataramani, Srinivasan, Cui, Zhang, and Gopalakrishnan]{sun2019hybrid}
Sun, X., Choi, J., Chen, C.-Y., Wang, N., Venkataramani, S., Srinivasan, V.~V., Cui, X., Zhang, W., and Gopalakrishnan, K.
\newblock Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{a}}.

\bibitem[Sun et~al.(2019{\natexlab{b}})Sun, Choi, Chen, Wang, Venkataramani, Srinivasan, Cui, Zhang, and Gopalakrishnan]{xiao2019hybrid8bit}
Sun, X., Choi, J., Chen, C.-Y., Wang, N., Venkataramani, S., Srinivasan, V.~V., Cui, X., Zhang, W., and Gopalakrishnan, K.
\newblock Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf}.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, Silver, Petrov, Johnson, Antonoglou, Schrittwieser, Glaese, Chen, Pitler, Lillicrap, Lazaridou, Firat, Molloy, Isard, Barham, Hennigan, Lee, Viola, Reynolds, Xu, Doherty, Collins, Meyer, Rutherford, Moreira, Ayoub, Goel, Tucker, Piqueras, Krikun, Barr, Savinov, Danihelka, Roelofs, White, Andreassen, von Glehn, Yagati, Kazemi, Gonzalez, Khalman, Sygnowski, Frechette, Smith, Culp, Proleev, Luan, Chen, Lottes, Schucher, Lebron, Rrustemi, Clay, Crone, Kocisky, Zhao, Perz, Yu, Howard, Bloniarz, Rae, Lu, Sifre, Maggioni, Alcober, Garrette, Barnes, Thakoor, Austin, Barth-Maron, Wong, Joshi, Chaabouni, Fatiha, Ahuja, Liu, Li, Cogan, Chen, Jia, Gu, Zhang, Grimstad, Hartman, Chadwick, Tomar, Garcia, Senter, Taropa, Pillai, Devlin, Laskin, de~Las~Casas, Valter, Tao, Blanco, Badia, Reitter, Chen, Brennan, Rivera, Brin, Iqbal, Surita, Labanowski, Rao, Winkler, Parisotto, Gu, Olszewska, Zhang, Addanki, Miech, Louis,
  Shafey, Teplyashin, Brown, Catt, Attaluri, Balaguer, Xiang, Wang, Ashwood, Briukhov, Webson, Ganapathy, Sanghavi, Kannan, Chang, Stjerngren, Djolonga, Sun, Bapna, Aitchison, Pejman, Michalewski, Yu, Wang, Love, Ahn, Bloxwich, Han, Humphreys, Sellam, Bradbury, Godbole, Samangooei, Damoc, Kaskasoli, Arnold, Vasudevan, Agrawal, Riesa, Lepikhin, Tanburn, Srinivasan, Lim, Hodkinson, Shyam, Ferret, Hand, Garg, Paine, Li, Li, Giang, Neitz, Abbas, York, Reid, Cole, Chowdhery, Das, Rogozińska, Nikolaev, Sprechmann, Nado, Zilka, Prost, He, Monteiro, Mishra, Welty, Newlan, Jia, Allamanis, Hu, de~Liedekerke, Gilmer, Saroufim, Rijhwani, Hou, Shrivastava, Baddepudi, Goldin, Ozturel, Cassirer, Xu, Sohn, Sachan, Amplayo, Swanson, Petrova, Narayan, Guez, Brahma, Landon, Patel, Zhao, Villela, Wang, Jia, Rahtz, Giménez, Yeung, Lin, Keeling, Georgiev, Mincu, Wu, Haykal, Saputro, Vodrahalli, Qin, Cankara, Sharma, Fernando, Hawkins, Neyshabur, Kim, Hutter, Agrawal, Castro-Ros, van~den Driessche, Wang, Yang, yiin Chang,
  Komarek, McIlroy, Lučić, Zhang, Farhan, Sharman, Natsev, Michel, Cheng, Bansal, Qiao, Cao, Shakeri, Butterfield, Chung, Rubenstein, Agrawal, Mensch, Soparkar, Lenc, Chung, Pope, Maggiore, Kay, Jhakra, Wang, Maynez, Phuong, Tobin, Tacchetti, Trebacz, Robinson, Katariya, Riedel, Bailey, Xiao, Ghelani, Aroyo, Slone, Houlsby, Xiong, Yang, Gribovskaya, Adler, Wirth, Lee, Li, Kagohara, Pavagadhi, Bridgers, Bortsova, Ghemawat, Ahmed, Liu, Powell, Bolina, Iinuma, Zablotskaia, Besley, Chung, Dozat, Comanescu, Si, Greer, Su, Polacek, Kaufman, Tokumine, Hu, Buchatskaya, Miao, Elhawaty, Siddhant, Tomasev, Xing, Greer, Miller, Ashraf, Roy, Zhang, Ma, Filos, Besta, Blevins, Klimenko, Yeh, Changpinyo, Mu, Chang, Pajarskas, Muir, Cohen, Lan, Haridasan, Marathe, Hansen, Douglas, Samuel, Wang, Austin, Lan, Jiang, Chiu, Lorenzo, Sjösund, Cevey, Gleicher, Avrahami, Boral, Srinivasan, Selo, May, Aisopos, Hussenot, Soares, Baumli, Chang, Recasens, Caine, Pritzel, Pavetic, Pardo, Gergely, Frye, Ramasesh, Horgan, Badola,
  Kassner, Roy, Dyer, Campos, Tomala, Tang, Badawy, White, Mustafa, Lang, Jindal, Vikram, Gong, Caelles, Hemsley, Thornton, Feng, Stokowiec, Zheng, Thacker, Çağlar Ünlü, Zhang, Saleh, Svensson, Bileschi, Patil, Anand, Ring, Tsihlas, Vezer, Selvi, Shevlane, Rodriguez, Kwiatkowski, Daruki, Rong, Dafoe, FitzGerald, Gu-Lemberg, Khan, Hendricks, Pellat, Feinberg, Cobon-Kerr, Sainath, Rauh, Hashemi, Ives, Hasson, Li, Noland, Cao, Byrd, Hou, Wang, Sottiaux, Paganini, Lespiau, Moufarek, Hassan, Shivakumar, van Amersfoort, Mandhane, Joshi, Goyal, Tung, Brock, Sheahan, Misra, Li, Rakićević, Dehghani, Liu, Mittal, Oh, Noury, Sezener, Huot, Lamm, Cao, Chen, Elsayed, Chi, Mahdieh, Tenney, Hua, Petrychenko, Kane, Scandinaro, Jain, Uesato, Datta, Sadovsky, Bunyan, Rabiej, Wu, Zhang, Vasudevan, Leurent, Alnahlawi, Georgescu, Wei, Zheng, Chan, Rabinovitch, Stanczyk, Zhang, Steiner, Naskar, Azzam, Johnson, Paszke, Chiu, Elias, Mohiuddin, Muhammad, Miao, Lee, Vieillard, Potluri, Park, Davoodi, Zhang, Stanway, Garmon,
  Karmarkar, Dong, Lee, Kumar, Zhou, Evens, Isaac, Chen, Jia, Levskaya, Zhu, Gorgolewski, Grabowski, Mao, Magni, Yao, Snaider, Casagrande, Suganthan, Palmer, Irving, Loper, Faruqui, Arkatkar, Chen, Shafran, Fink, Castaño, Giannoumis, Kim, Rybiński, Sreevatsa, Prendki, Soergel, Goedeckemeyer, Gierke, Jafari, Gaba, Wiesner, Wright, Wei, Vashisht, Kulizhskaya, Hoover, Le, Li, Iwuanyanwu, Liu, Ramirez, Khorlin, Cui, LIN, Georgiev, Wu, Aguilar, Pallo, Chakladar, Repina, Wu, van~der Weide, Ponnapalli, Kaplan, Simsa, Li, Dousse, Yang, Piper, Ie, Lui, Pasumarthi, Lintz, Vijayakumar, Thiet, Andor, Valenzuela, Paduraru, Peng, Lee, Zhang, Greene, Nguyen, Kurylowicz, Velury, Krause, Hardin, Dixon, Janzer, Choo, Feng, Zhang, Singhal, Latkar, Zhang, Le, Abellan, Du, McKinnon, Antropova, Bolukbasi, Keller, Reid, Finchelstein, Raad, Crocker, Hawkins, Dadashi, Gaffney, Lall, Franko, Filonov, Bulanova, Leblond, Yadav, Chung, Askham, Cobo, Xu, Fischer, Xu, Sorokin, Alberti, Lin, Evans, Zhou, Dimitriev, Forbes, Banarse, Tung,
  Liu, Omernick, Bishop, Kumar, Sterneck, Foley, Jain, Mishra, Xia, Bos, Cideron, Amid, Piccinno, Wang, Banzal, Gurita, Noga, Shah, Mankowitz, Polozov, Kushman, Krakovna, Brown, Bateni, Duan, Firoiu, Thotakuri, Natan, Mohananey, Geist, Mudgal, Girgin, Li, Ye, Roval, Tojo, Kwong, Lee-Thorp, Yew, Yuan, Bagri, Sinopalnikov, Ramos, Mellor, Sharma, Severyn, Lai, Wu, Cheng, Miller, Sonnerat, Vnukov, Greig, Beattie, Caveness, Bai, Eisenschlos, Korchemniy, Tsai, Jasarevic, Kong, Dao, Zheng, Liu, Yang, Zhu, Geller, Teh, Sanmiya, Gladchenko, Trdin, Sozanschi, Toyama, Rosen, Tavakkol, Xue, Elkind, Woodman, Carpenter, Papamakarios, Kemp, Kafle, Grunina, Sinha, Talbert, Goyal, Wu, Owusu-Afriyie, Du, Thornton, Pont-Tuset, Narayana, Li, Fatehi, Wieting, Ajmeri, Uria, Zhu, Ko, Knight, Héliou, Niu, Gu, Pang, Tran, Li, Levine, Stolovich, Kalb, Santamaria-Fernandez, Goenka, Yustalim, Strudel, Elqursh, Lakshminarayanan, Deck, Upadhyay, Lee, Dusenberry, Li, Wang, Levin, Hoffmann, Holtmann-Rice, Bachem, Yue, Arora, Malmi,
  Mirylenka, Tan, Koh, Yeganeh, Põder, Zheng, Pongetti, Tariq, Sun, Ionita, Seyedhosseini, Tafti, Kotikalapudi, Liu, Gulati, Liu, Ye, Chrzaszcz, Wang, Sethi, Li, Brown, Singh, Fan, Parisi, Stanton, Kuang, Koverkathu, Choquette-Choo, Li, Lu, Ittycheriah, Shroff, Sun, Varadarajan, Bahargam, Willoughby, Gaddy, Dasgupta, Desjardins, Cornero, Robenek, Mittal, Albrecht, Shenoy, Moiseev, Jacobsson, Ghaffarkhah, Rivière, Walton, Crepy, Parrish, Liu, Zhou, Farabet, Radebaugh, Srinivasan, van~der Salm, Fidjeland, Scellato, Latorre-Chimoto, Klimczak-Plucińska, Bridson, de~Cesare, Hudson, Mendolicchio, Walker, Morris, Penchev, Mauger, Guseynov, Reid, Odoom, Loher, Cotruta, Yenugula, Grewe, Petrushkina, Duerig, Sanchez, Yadlowsky, Shen, Globerson, Kurzrok, Webb, Dua, Li, Lahoti, Bhupatiraju, Hurt, Qureshi, Agarwal, Shani, Eyal, Khare, Belle, Wang, Tekur, Kale, Wei, Sang, Saeta, Liechty, Sun, Zhao, Lee, Nayak, Fritz, Vuyyuru, Aslanides, Vyas, Wicke, Ma, Bilal, Eltyshev, Balle, Martin, Cate, Manyika, Amiri, Kim, Xiong,
  Kang, Luisier, Tripuraneni, Madras, Guo, Waters, Wang, Ainslie, Baldridge, Zhang, Pruthi, Bauer, Yang, Mansour, Gelman, Xu, Polovets, Liu, Cai, Chen, Sheng, Xue, Ozair, Yu, Angermueller, Li, Wang, Wiesinger, Koukoumidis, Tian, Iyer, Gurumurthy, Goldenson, Shah, Blake, Yu, Urbanowicz, Palomaki, Fernando, Brooks, Durden, Mehta, Momchev, Rahimtoroghi, Georgaki, Raul, Ruder, Redshaw, Lee, Jalan, Li, Perng, Hechtman, Schuh, Nasr, Chen, Milan, Mikulik, Strohman, Franco, Green, Hassabis, Kavukcuoglu, Dean, and Vinyals]{geminiteam2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: A family of highly capable multimodal models, 2023.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2019glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Choi, Brand, Chen, and Gopalakrishnan]{wang20188bit}
Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018{\natexlab{a}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Choi, Brand, Chen, and Gopalakrishnan]{wang2018training}
Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock \emph{Advances in neural information processing systems}, 31, 2018{\natexlab{b}}.

\bibitem[Wei et~al.(2023)Wei, Gonugondla, Ahmad, Wang, Ray, Qian, Li, Kumar, Wang, Tian, Sun, Athiwaratkun, Shang, Ramanathan, Bhatia, and Xiang]{wei2023greener}
Wei, X., Gonugondla, S., Ahmad, W., Wang, S., Ray, B., Qian, H., Li, X., Kumar, V., Wang, Z., Tian, Y., et~al.
\newblock Greener yet powerful: Taming large code generation models with quantization, 2023.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Wortsman et~al.(2023)Wortsman, Dettmers, Zettlemoyer, Morcos, Farhadi, and Schmidt]{wortsman2023stable}
Wortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A., Farhadi, A., and Schmidt, L.
\newblock Stable and low-precision training for large-scale vision-language models, 2023.

\bibitem[Wu et~al.(2018)Wu, Li, Chen, and Shi]{wu2018training}
Wu, S., Li, G., Chen, F., and Shi, L.
\newblock Training and inference with integers in deep neural networks, 2018.

\bibitem[Xi et~al.(2023)Xi, Li, Chen, and Zhu]{xi2023training}
Xi, H., Li, C., Chen, J., and Zhu, J.
\newblock Training transformers with 4-bit integers.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=H9hWlfMT6O}.

\bibitem[Xia et~al.(2023)Xia, Zheng, Li, Zhuang, Zhou, Qiu, Li, Lin, and Song]{xia2023flash}
Xia, H., Zheng, Z., Li, Y., Zhuang, D., Zhou, Z., Qiu, X., Li, Y., Lin, W., and Song, S.~L.
\newblock Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity.
\newblock \emph{arXiv preprint arXiv:2309.10285}, 2023.

\bibitem[Xia et~al.(2022)Xia, Zhong, and Chen]{xia2022structured}
Xia, M., Zhong, Z., and Chen, D.
\newblock Structured pruning learns compact and accurate models.
\newblock \emph{arXiv preprint arXiv:2204.00408}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models, 2023.

\bibitem[Xu et~al.(2023)Xu, Xie, Gu, Chen, Chang, Zhang, Chen, Zhang, and Tian]{xu2023qa}
Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q.
\newblock Qa-lora: Quantization-aware low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2309.14717}, 2023.

\bibitem[Yao et~al.(2023)Yao, Wu, Li, Youn, and He]{yao2023zeroquant}
Yao, Z., Wu, X., Li, C., Youn, S., and He, Y.
\newblock Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation.
\newblock \emph{arXiv preprint arXiv:2303.08302}, 2023.

\bibitem[Yu et~al.(2022{\natexlab{a}})Yu, Huang, Wang, Cheng, Chu, and Cui]{yu2022width}
Yu, F., Huang, K., Wang, M., Cheng, Y., Chu, W., and Cui, L.
\newblock Width \& depth pruning for vision transformers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  3143--3151, 2022{\natexlab{a}}.

\bibitem[Yu \& De~Sa(2021)Yu and De~Sa]{Yu2021MCT}
Yu, T. and De~Sa, C.~M.
\newblock Representing hyperbolic space accurately using multi-component floats.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  15570--15581. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/832353270aacb6e3322f493a66aaf5b9-Paper.pdf}.

\bibitem[Yu et~al.(2022{\natexlab{b}})Yu, Guo, Li, Yuan, and Sa]{yu2022mctensor}
Yu, T., Guo, W., Li, J.~C., Yuan, T., and Sa, C.~D.
\newblock Mctensor: A high-precision deep learning library with multi-component floating-point, 2022{\natexlab{b}}.

\bibitem[Zamirai et~al.(2020)Zamirai, Zhang, Aberger, and De~Sa]{zamirai2020revisiting}
Zamirai, P., Zhang, J., Aberger, C.~R., and De~Sa, C.
\newblock Revisiting bfloat16 training.
\newblock \emph{arXiv preprint arXiv:2010.06192}, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and Li]{zhou2021learning}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning n:m fine-grained structured sparse neural networks from scratch, 2021.

\end{thebibliography}
