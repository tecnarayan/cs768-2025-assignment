\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Dontchev and Rockafellar(2009)]{dontchev_implicit_2009}
Asen~L. Dontchev and R.~Tyrrell Rockafellar.
\newblock \emph{Implicit functions and solution mappings}.
\newblock Springer, 2009.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Kurin, Hofmann, and
  Whiteson]{zintgraf_fast_2019}
Luisa Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon
  Whiteson.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2019.

\bibitem[Lee et~al.(2019)Lee, Maji, Ravichandran, and
  Soatto]{lee_meta-learning_2019}
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto.
\newblock Meta-learning with differentiable convex optimization.
\newblock In \emph{Proceedings of the {IEEE}/{CVF} {Conference} on {Computer}
  {Vision} and {Pattern} {Recognition}}, 2019.

\bibitem[Zhao et~al.(2020)Zhao, Kobayashi, Sacramento, and von
  Oswald]{zhao_meta-learning_2020}
Dominic Zhao, Seijin Kobayashi, João Sacramento, and Johannes von Oswald.
\newblock Meta-learning via hypernetworks.
\newblock In \emph{Workshop on {Meta}-{Learning} at {NeurIPS}}, 2020.

\bibitem[Nocedal and Wright(2006)]{nocedal_numerical_2006}
Jorge Nocedal and Stephen~J. Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer, 2006.

\bibitem[Bertsekas(2014)]{bertsekas_constrained_2014}
Dimitri~P. Bertsekas.
\newblock \emph{Constrained optimization and {Lagrange} multiplier methods}.
\newblock Academic Press, 2014.

\bibitem[Friston et~al.(2006)Friston, Kilner, and Harrison]{friston_free_2006}
Karl Friston, James Kilner, and Lee Harrison.
\newblock A free energy principle for the brain.
\newblock \emph{Journal of Physiology-Paris}, 100\penalty0 (1-3):\penalty0
  70--87, 2006.

\bibitem[Neal and Hinton(1998)]{neal_view_1998}
Radford~M. Neal and Geoffrey~E. Hinton.
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In \emph{Learning in {Graphical} {Models}}, pages 355--368. Springer
  Netherlands, 1998.

\bibitem[Tzikas et~al.(2008)Tzikas, Likas, and
  Galatsanos]{tzikas_variational_2008}
Dimitris~G. Tzikas, Aristidis~C. Likas, and Nikolaos~P. Galatsanos.
\newblock The variational approximation for {Bayesian} inference.
\newblock \emph{IEEE Signal Processing Magazine}, 25\penalty0 (6):\penalty0
  131--146, 2008.

\bibitem[Bogacz(2017)]{bogacz_tutorial_2017}
Rafal Bogacz.
\newblock A tutorial on the free-energy framework for modelling perception and
  learning.
\newblock \emph{Journal of Mathematical Psychology}, 76:\penalty0 198--211,
  2017.

\bibitem[Goldman et~al.(2010)Goldman, Compte, and Wang]{goldman_neural_2010}
Mark~S. Goldman, Albert Compte, and Xiao-Jing Wang.
\newblock Neural integrator models.
\newblock \emph{Encyclopedia of Neuroscience}, pages 165--178, 2010.

\bibitem[Särkkä and Solin(2019)]{sarkka_applied_2019}
Simo Särkkä and Arno Solin.
\newblock \emph{Applied stochastic differential equations}.
\newblock Cambridge University Press, 2019.

\bibitem[Lu and Shiou(2002)]{lu_inverses_2002}
Tzon-Tzer Lu and Sheng-Hua Shiou.
\newblock Inverses of 2 × 2 block matrices.
\newblock \emph{Computers \& Mathematics with Applications}, 43\penalty0
  (1):\penalty0 119--129, 2002.

\bibitem[Magee and Grienberger(2020)]{magee_synaptic_2020}
Jeffrey~C. Magee and Christine Grienberger.
\newblock Synaptic plasticity forms and functions.
\newblock \emph{Annual Review of Neuroscience}, 43:\penalty0 95--117, 2020.

\bibitem[Shepherd(2009)]{shepherd_dendrodendritic_2009}
Gordon~M. Shepherd.
\newblock Dendrodendritic synapses: past, present, and future.
\newblock \emph{Annals of the New York Academy of Sciences}, 1170\penalty0
  (1):\penalty0 215--223, 2009.

\bibitem[Hennequin et~al.(2017)Hennequin, Agnes, and
  Vogels]{hennequin_inhibitory_2017}
Guillaume Hennequin, Everton~J. Agnes, and Tim~P. Vogels.
\newblock Inhibitory plasticity: balance, control, and codependence.
\newblock \emph{Annual Review of Neuroscience}, 40:\penalty0 557--579, 2017.

\bibitem[Kingma and Ba(2015)]{kingma_adam_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2015.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov_sgdr_2017}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: {Stochastic} gradient descent with restarts.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2017.

\bibitem[Ioffe and Szegedy(2015)]{ioffe_batch_2015}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2015.

\bibitem[Wu and He(2018)]{wu_group_2018}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the {European} {Conference} on {Computer}
  {Vision}}, 2018.

\bibitem[Liao et~al.(2018)Liao, Xiong, Fetaya, Zhang, Yoon, Pitkow, Urtasun,
  and Zemel]{liao_reviving_2018}
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow,
  Raquel Urtasun, and Richard Zemel.
\newblock Reviving and improving recurrent back-propagation.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2018.

\bibitem[Zucchet and Sacramento(2022)]{zucchet_beyond_2022}
Nicolas Zucchet and João Sacramento.
\newblock Beyond backpropagation: implicit gradients for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2205.03076}, 2022.

\end{thebibliography}
