\begin{thebibliography}{10}

\bibitem{langchain}
Langchain.
\newblock \url{https://github.com/hwchase17/langchain}, 2022.

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em arXiv preprint arXiv:2204.14198}, 2022.

\bibitem{anderson2018vision}
Peter Anderson, Qi~Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko
  S{\"u}nderhauf, Ian Reid, Stephen Gould, and Anton Van Den~Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2018.

\bibitem{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock {\em arXiv preprint arXiv:2112.00861}, 2021.

\bibitem{anas_awadalla_2023_7733589}
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
  Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon
  Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig
  Schmidt.
\newblock Openflamingo, March 2023.

\bibitem{brooks2022instructpix2pix}
Tim Brooks, Aleksander Holynski, and Alexei~A Efros.
\newblock Instruct pix2pix: Learning to follow image editing instructions.
\newblock {\em arXiv preprint arXiv:2211.09800}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em CVPR}, 2021.

\bibitem{vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{cvinw}
CVinW.
\newblock Computer vision in the wild.
\newblock \url{https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings},
  2022.

\bibitem{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock {PaLM-E}: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{faghri2023reinforce}
Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi,
  Mohammad Rastegari, and Oncel Tuzel.
\newblock Reinforce data, multiply impact: Improved model accuracy and
  robustness with dataset reinforcement.
\newblock {\em arXiv preprint arXiv:2303.08983}, 2023.

\bibitem{MAKEASCENE}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
  Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock {\em ArXiv}, abs/2203.13131, 2022.

\bibitem{gan2022vision}
Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et~al.
\newblock Vision-language pre-training: Basics, recent advances, and future
  trends.
\newblock {\em Foundations and Trends{\textregistered} in Computer Graphics and
  Vision}, 2022.

\bibitem{gilardi2023chatgpt}
Fabrizio Gilardi, Meysam Alizadeh, and Ma{\"e}l Kubli.
\newblock Chatgpt outperforms crowd-workers for text-annotation tasks.
\newblock {\em arXiv preprint arXiv:2303.15056}, 2023.

\bibitem{gupta2022visual}
Tanmay Gupta and Aniruddha Kembhavi.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock {\em arXiv preprint arXiv:2211.11559}, 2022.

\bibitem{hao2020towards}
Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao.
\newblock Towards learning a generic agent for vision-and-language navigation
  via pre-training.
\newblock In {\em CVPR}, 2020.

\bibitem{huang2023language}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
  Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Qiang Liu, et~al.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock {\em arXiv preprint arXiv:2302.14045}, 2023.

\bibitem{openclip}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
  Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
  Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip.
\newblock July 2021.
\newblock If you use this software, please cite it as below.

\bibitem{iyer2022opt}
Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov,
  D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh
  Koura, et~al.
\newblock Opt-iml: Scaling language model instruction meta learning through the
  lens of generalization.
\newblock {\em arXiv preprint arXiv:2212.12017}, 2022.

\bibitem{jia2022visual}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath
  Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In {\em ECCV}, 2022.

\bibitem{koh2023grounding}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
\newblock Grounding language models to images for multimodal generation.
\newblock {\em arXiv preprint arXiv:2301.13823}, 2023.

\bibitem{li2022language}
Boyi Li, Kilian~Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren{\'e}
  Ranftl.
\newblock Language-driven semantic segmentation.
\newblock {\em ICLR}, 2022.

\bibitem{li2023multimodal}
Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and
  Jianfeng Gao.
\newblock Multimodal foundation models: From specialists to general-purpose
  assistants.
\newblock {\em arXiv preprint arXiv:2309.10020}, 2023.

\bibitem{li2022elevater}
Chunyuan Li, Haotian Liu, Liunian~Harold Li, Pengchuan Zhang, Jyoti Aneja,
  Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong~Jae Lee, and Jianfeng
  Gao.
\newblock {ELEVATER}: A benchmark and toolkit for evaluating language-augmented
  visual models.
\newblock In {\em NeurIPS Track on Datasets and Benchmarks}, 2022.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2022grounded}
Liunian~Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
  Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, et~al.
\newblock Grounded language-image pre-training.
\newblock In {\em CVPR}, 2022.

\bibitem{li2023gligen}
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,
  Chunyuan Li, and Yong~Jae Lee.
\newblock Gligen: Open-set grounded text-to-image generation.
\newblock {\em arXiv preprint arXiv:2301.07093}, 2023.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft {COCO}: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{liu2023improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning, 2023.

\bibitem{liu2023grounding}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
  Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set
  object detection.
\newblock {\em arXiv preprint arXiv:2303.05499}, 2023.

\bibitem{lu2022learn}
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,
  Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science
  question answering.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{chatgpt}
OpenAI.
\newblock Chat{GPT}.
\newblock \url{https://openai.com/blog/chatgpt/}, 2023.

\bibitem{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with {GPT}-4.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{pham2021combined}
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams~Wei
  Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et~al.
\newblock Combined scaling for open-vocabulary image classification.
\newblock {\em arXiv preprint arXiv: 2111.10050}, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock {\em arXiv preprint arXiv:2103.00020}, 2021.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 2020.

\bibitem{DALLE2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em ArXiv}, abs/2204.06125, 2022.

\bibitem{LDM}
Robin Rombach, A.~Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock {\em CVPR}, pages 10674--10685, 2022.

\bibitem{Imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L.
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, Seyedeh~Sara
  Mahdavi, Raphael~Gontijo Lopes, Tim Salimans, Jonathan Ho, David~J. Fleet,
  and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em ArXiv}, abs/2205.11487, 2022.

\bibitem{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock {\em arXiv preprint arXiv:2210.08402}, 2022.

\bibitem{suris2023vipergpt}
D{\'\i}dac Sur{\'\i}s, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock {\em arXiv preprint arXiv:2303.08128}, 2023.

\bibitem{szot2021habitat}
Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John
  Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets,
  Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech
  Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis
  Savva, and Dhruv Batra.
\newblock Habitat 2.0: Training home assistants to rearrange their habitat.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wang2022git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
  Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock {\em arXiv preprint arXiv:2205.14100}, 2022.

\bibitem{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{wang2022benchmarking}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, et~al.
\newblock Benchmarking generalization via in-context instructions on 1,600+
  language tasks.
\newblock {\em arXiv preprint arXiv:2204.07705}, 2022.

\bibitem{wu2023visual}
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
  Duan.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation
  models.
\newblock {\em arXiv preprint arXiv:2303.04671}, 2023.

\bibitem{yang2022unicl}
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu~Yuan, Ce~Liu, and
  Jianfeng Gao.
\newblock Unified contrastive learning in image-text-label space.
\newblock {\em CVPR}, 2022.

\bibitem{yang2023mm}
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
  Ahmed, Zicheng Liu, Ce~Liu, Michael Zeng, and Lijuan Wang.
\newblock Mm-react: Prompting chatgpt for multimodal reasoning and action.
\newblock {\em arXiv preprint arXiv:2303.11381}, 2023.

\bibitem{PARTI}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,
  Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, Benton~C.
  Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
  Yonghui Wu.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock {\em ArXiv}, abs/2206.10789, 2022.

\bibitem{yuan2021florence}
Lu~Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
  Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock {\em arXiv preprint arXiv:2111.11432}, 2021.

\bibitem{zhang2023simple}
Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei
  Yang, and Lei Zhang.
\newblock A simple framework for open-vocabulary segmentation and detection.
\newblock {\em arXiv preprint arXiv:2303.08131}, 2023.

\bibitem{zhang2023llama}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
  Hongsheng Li, Peng Gao, and Yu~Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with
  zero-init attention.
\newblock {\em arXiv preprint arXiv:2303.16199}, 2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhang2023multimodal}
Zhuosheng Zhang, Aston Zhang, Mu~Li, Hai Zhao, George Karypis, and Alex Smola.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock {\em arXiv preprint arXiv:2302.00923}, 2023.

\bibitem{zhong2022regionclip}
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
  Liunian~Harold Li, Luowei Zhou, Xiyang Dai, Lu~Yuan, Yin Li, et~al.
\newblock Regionclip: Region-based language-image pretraining.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16793--16803, 2022.

\bibitem{zou2022generalized}
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang
  Dai, Harkirat Behl, Jianfeng Wang, Lu~Yuan, et~al.
\newblock Generalized decoding for pixel, image, and language.
\newblock {\em arXiv preprint arXiv:2212.11270}, 2022.

\end{thebibliography}
