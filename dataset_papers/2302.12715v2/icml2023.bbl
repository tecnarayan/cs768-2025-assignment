\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharon et~al.(2006{\natexlab{a}})Aharon, Elad, and Bruckstein]{ksvd}
Aharon, M., Elad, M., and Bruckstein, A.
\newblock K-svd: An algorithm for designing overcomplete dictionaries for
  sparse representation.
\newblock \emph{IEEE Transactions on Signal Processing}, 54\penalty0
  (11):\penalty0 4311--4322, 2006{\natexlab{a}}.
\newblock \doi{10.1109/TSP.2006.881199}.

\bibitem[Aharon et~al.(2006{\natexlab{b}})Aharon, Elad, and
  Bruckstein]{AHARON200648}
Aharon, M., Elad, M., and Bruckstein, A.~M.
\newblock On the uniqueness of overcomplete dictionaries, and a practical way
  to retrieve them.
\newblock \emph{Linear Algebra and its Applications}, 416\penalty0
  (1):\penalty0 48--67, 2006{\natexlab{b}}.
\newblock ISSN 0024-3795.
\newblock \doi{https://doi.org/10.1016/j.laa.2005.06.035}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0024379505003459}.
\newblock Special Issue devoted to the Haifa 2005 conference on matrix theory.

\bibitem[Arora et~al.(2013)Arora, Ge, and Moitra]{arora13}
Arora, S., Ge, R., and Moitra, A.
\newblock New algorithms for learning incoherent and overcomplete dictionaries,
  2013.
\newblock URL \url{https://arxiv.org/abs/1308.6273}.

\bibitem[Arora et~al.(2014)Arora, Bhaskara, Ge, and Ma]{arora14}
Arora, S., Bhaskara, A., Ge, R., and Ma, T.
\newblock More algorithms for provable dictionary learning.
\newblock \emph{CoRR}, abs/1401.0579, 2014.
\newblock URL \url{http://arxiv.org/abs/1401.0579}.

\bibitem[Arora et~al.(2015)Arora, Ge, Ma, and Moitra]{arora15}
Arora, S., Ge, R., Ma, T., and Moitra, A.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock \emph{CoRR}, abs/1503.00778, 2015.
\newblock URL \url{http://arxiv.org/abs/1503.00778}.

\bibitem[Bach et~al.(2008)Bach, Mairal, and Ponce]{convexsparse}
Bach, F., Mairal, J., and Ponce, J.
\newblock Convex sparse matrix factorizations, 2008.
\newblock URL \url{https://arxiv.org/abs/0812.1869}.

\bibitem[Bandeira et~al.(2012)Bandeira, Fickus, Mixon, and Wong]{detrip}
Bandeira, A.~S., Fickus, M., Mixon, D.~G., and Wong, P.
\newblock The road to deterministic matrices with the restricted isometry
  property, 2012.
\newblock URL \url{https://arxiv.org/abs/1202.1234}.

\bibitem[Baraniuk et~al.(2008)Baraniuk, Davenport, DeVore, and
  Wakin]{randomrip}
Baraniuk, R., Davenport, M., DeVore, R., and Wakin, M.
\newblock A simple proof of the restricted isometry property for random
  matrices.
\newblock \emph{Constructive Approximation}, 28\penalty0 (3):\penalty0
  253--263, December 2008.
\newblock ISSN 0176-4276.
\newblock \doi{10.1007/s00365-007-9003-x}.

\bibitem[Boufounos et~al.(2007)Boufounos, Duarte, and Baraniuk]{boufounos2007}
Boufounos, P., Duarte, M.~F., and Baraniuk, R.~G.
\newblock Sparse signal reconstruction from noisy compressive measurements
  using cross validation.
\newblock In \emph{2007 IEEE/SP 14th Workshop on Statistical Signal
  Processing}, pp.\  299--303, 2007.
\newblock \doi{10.1109/SSP.2007.4301267}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{NEURIPS2020_1457c0d6}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Cai \& Wang(2011)Cai and Wang]{omperror}
Cai, T.~T. and Wang, L.
\newblock Orthogonal matching pursuit for sparse signal recovery with noise.
\newblock \emph{IEEE Transactions on Information Theory}, 57\penalty0
  (7):\penalty0 4680--4688, 2011.
\newblock \doi{10.1109/TIT.2011.2146090}.

\bibitem[Candes \& Tao(2005)Candes and Tao]{rip}
Candes, E. and Tao, T.
\newblock Decoding by linear programming.
\newblock \emph{IEEE Transactions on Information Theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.
\newblock \doi{10.1109/TIT.2005.858979}.

\bibitem[Candes et~al.(2006)Candes, Romberg, and Tao]{candestao1}
Candes, E., Romberg, J., and Tao, T.
\newblock Robust uncertainty principles: exact signal reconstruction from
  highly incomplete frequency information.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (2):\penalty0 489--509, 2006.
\newblock \doi{10.1109/TIT.2005.862083}.

\bibitem[Candes \& Tao(2006)Candes and Tao]{candestao2}
Candes, E.~J. and Tao, T.
\newblock Near-optimal signal recovery from random projections: Universal
  encoding strategies?
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (12):\penalty0 5406--5425, 2006.
\newblock \doi{10.1109/TIT.2006.885507}.

\bibitem[Candes \& Wakin(2008)Candes and Wakin]{candesrev}
Candes, E.~J. and Wakin, M.~B.
\newblock An introduction to compressive sampling.
\newblock \emph{IEEE Signal Processing Magazine}, 25\penalty0 (2):\penalty0
  21--30, 2008.
\newblock \doi{10.1109/MSP.2007.914731}.

\bibitem[Cao et~al.(2022)Cao, Xu, and Clifton]{cao2022understand}
Cao, S., Xu, P., and Clifton, D.~A.
\newblock How to understand masked autoencoders.
\newblock \emph{arXiv preprint arXiv:2202.03670}, 2022.

\bibitem[Chen \& Donoho(1994)Chen and Donoho]{chen1994basis}
Chen, S. and Donoho, D.
\newblock Basis pursuit.
\newblock In \emph{Proceedings of 1994 28th Asilomar Conference on Signals,
  Systems and Computers}, volume~1, pp.\  41--44. IEEE, 1994.

\bibitem[Daubechies et~al.()Daubechies, Defrise, and De~Mol]{daubechies}
Daubechies, I., Defrise, M., and De~Mol, C.
\newblock An iterative thresholding algorithm for linear inverse problems with
  a sparsity constraint.
\newblock URL \url{https://arxiv.org/abs/math/0307152}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Donoho(2006)]{donohocs}
Donoho, D.
\newblock Compressed sensing.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (4):\penalty0 1289--1306, 2006.
\newblock \doi{10.1109/TIT.2006.871582}.

\bibitem[Donoho et~al.(2006)Donoho, Elad, and Temlyakov]{donohostable}
Donoho, D., Elad, M., and Temlyakov, V.
\newblock Stable recovery of sparse overcomplete representations in the
  presence of noise.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (1):\penalty0 6--18, 2006.
\newblock \doi{10.1109/TIT.2005.860430}.

\bibitem[Donoho et~al.(2009)Donoho, Maleki, and Montanari]{amp}
Donoho, D.~L., Maleki, A., and Montanari, A.
\newblock Message-passing algorithms for compressed sensing.
\newblock \emph{Proceedings of the National Academy of Sciences}, 106\penalty0
  (45):\penalty0 18914--18919, 2009.
\newblock \doi{10.1073/pnas.0909892106}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.0909892106}.

\bibitem[Duarte \& Eldar(2011)Duarte and Eldar]{duarterev}
Duarte, M.~F. and Eldar, Y.~C.
\newblock Structured compressed sensing: From theory to applications.
\newblock \emph{CoRR}, abs/1106.6224, 2011.
\newblock URL \url{http://arxiv.org/abs/1106.6224}.

\bibitem[Efron et~al.(2004)Efron, Hastie, Johnstone, and Tibshirani]{lars}
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R.
\newblock {Least angle regression}.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (2):\penalty0 407 --
  499, 2004.
\newblock \doi{10.1214/009053604000000067}.
\newblock URL \url{https://doi.org/10.1214/009053604000000067}.

\bibitem[Engan et~al.(1999)Engan, Aase, and Hakon~Husoy]{engan}
Engan, K., Aase, S., and Hakon~Husoy, J.
\newblock Method of optimal directions for frame design.
\newblock In \emph{1999 IEEE International Conference on Acoustics, Speech, and
  Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)}, volume~5, pp.\
   2443--2446 vol.5, 1999.
\newblock \doi{10.1109/ICASSP.1999.760624}.

\bibitem[Geng et~al.(2011)Geng, Wang, and Wright]{geng}
Geng, Q., Wang, H., and Wright, J.
\newblock On the local correctness of l{\^{}}1 minimization for dictionary
  learning.
\newblock \emph{CoRR}, abs/1101.5672, 2011.
\newblock URL \url{http://arxiv.org/abs/1101.5672}.

\bibitem[Gribonval \& Schnass(2010)Gribonval and Schnass]{schnass}
Gribonval, R. and Schnass, K.
\newblock Dictionary identification—sparse matrix-factorization via $\ell_1$
  -minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (7):\penalty0 3523--3539, 2010.
\newblock \doi{10.1109/TIT.2010.2048466}.

\bibitem[Gribonval et~al.(2014)Gribonval, Jenatton, and Bach]{sparsespurious}
Gribonval, R., Jenatton, R., and Bach, F.~R.
\newblock Sparse and spurious: dictionary learning with noise and outliers.
\newblock \emph{CoRR}, abs/1407.5155, 2014.
\newblock URL \url{http://arxiv.org/abs/1407.5155}.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll\'ar, and
  Girshick]{He_2022_CVPR}
He, K., Chen, X., Xie, S., Li, Y., Doll\'ar, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  16000--16009, June 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization, 2014.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Krause \& Cevher(2010)Krause and Cevher]{krausesparse}
Krause, A. and Cevher, V.
\newblock Submodular dictionary selection for sparse representation.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML'10, pp.\  567–574,
  Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 309--323, 2021.

\bibitem[Mairal et~al.(2010)Mairal, Bach, Ponce, and Sapiro]{onlinesparse}
Mairal, J., Bach, F., Ponce, J., and Sapiro, G.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19–60, mar 2010.
\newblock ISSN 1532-4435.

\bibitem[Maleki \& Donoho(2010)Maleki and Donoho]{donohothresh}
Maleki, A. and Donoho, D.~L.
\newblock Optimally tuned iterative reconstruction algorithms for compressed
  sensing.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  4\penalty0 (2):\penalty0 330--341, 2010.
\newblock \doi{10.1109/JSTSP.2009.2039176}.

\bibitem[Mallat \& Zhang(1993)Mallat and Zhang]{omporiginal}
Mallat, S. and Zhang, Z.
\newblock Matching pursuits with time-frequency dictionaries.
\newblock \emph{IEEE Transactions on Signal Processing}, 41\penalty0
  (12):\penalty0 3397--3415, 1993.
\newblock \doi{10.1109/78.258082}.

\bibitem[Musa et~al.(2018)Musa, Jung, and Goertz]{amp2018}
Musa, O., Jung, P., and Goertz, N.
\newblock Generalized approximate message passing for unlimited sampling of
  sparse signals, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.03182}.

\bibitem[Natarajan(1995)]{l0hard}
Natarajan, B.~K.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM Journal on Computing}, 24\penalty0 (2):\penalty0 227--234,
  1995.
\newblock \doi{10.1137/S0097539792240406}.

\bibitem[Olshausen \& Field(1997)Olshausen and Field]{OLSHAUSEN19973311}
Olshausen, B.~A. and Field, D.~J.
\newblock Sparse coding with an overcomplete basis set: A strategy employed by
  v1?
\newblock \emph{Vision Research}, 37\penalty0 (23):\penalty0 3311--3325, 1997.
\newblock ISSN 0042-6989.
\newblock \doi{https://doi.org/10.1016/S0042-6989(97)00169-7}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0042698997001697}.

\bibitem[Olshausen \& Field(2004)Olshausen and Field]{olshausen2004sparse}
Olshausen, B.~A. and Field, D.~J.
\newblock Sparse coding of sensory inputs.
\newblock \emph{Current opinion in neurobiology}, 14\penalty0 (4):\penalty0
  481--487, 2004.

\bibitem[Pan et~al.(2022)Pan, Zhou, and Yan]{pan2022towards}
Pan, J., Zhou, P., and Yan, S.
\newblock Towards understanding why mask-reconstruction pretraining helps in
  downstream tasks.
\newblock \emph{arXiv preprint arXiv:2206.03826}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K{\"{o}}pf, A., Yang,
  E.~Z., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B.,
  Fang, L., Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{CoRR}, abs/1912.01703, 2019.
\newblock URL \url{http://arxiv.org/abs/1912.01703}.

\bibitem[Rubinstein et~al.(2008)Rubinstein, Zibulevsky, and
  Elad]{Rubinstein2008EfficientIO}
Rubinstein, R., Zibulevsky, M., and Elad, M.
\newblock Efficient implementation of the k-svd algorithm using batch
  orthogonal matching pursuit.
\newblock 2008.

\bibitem[Rudelson \& Vershynin(2008)Rudelson and Vershynin]{rudelson}
Rudelson, M. and Vershynin, R.
\newblock The smallest singular value of a random rectangular matrix.
\newblock 2008.
\newblock \doi{10.48550/ARXIV.0802.3956}.
\newblock URL \url{https://arxiv.org/abs/0802.3956}.

\bibitem[Sulam et~al.(2020)Sulam, You, and Zhu]{overrealized}
Sulam, J., You, C., and Zhu, Z.
\newblock Recovery and generalization in over-realized dictionary learning.
\newblock \emph{CoRR}, abs/2006.06179, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.06179}.

\bibitem[Tibshirani(1996)]{lasso}
Tibshirani, R.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.
\newblock ISSN 00359246.
\newblock URL \url{http://www.jstor.org/stable/2346178}.

\bibitem[Tibshirani \& Wang(2008)Tibshirani and Wang]{tibshirani2008spatial}
Tibshirani, R. and Wang, P.
\newblock Spatial smoothing and hot spot detection for cgh data using the fused
  lasso.
\newblock \emph{Biostatistics}, 9\penalty0 (1):\penalty0 18--29, 2008.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1179--1206. PMLR, 2021.

\bibitem[Tropp(2006)]{troppconvex}
Tropp, J.
\newblock Just relax: convex programming methods for identifying sparse signals
  in noise.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (3):\penalty0 1030--1051, 2006.
\newblock \doi{10.1109/TIT.2005.864420}.

\bibitem[Tropp \& Gilbert(2007)Tropp and Gilbert]{omptropp}
Tropp, J.~A. and Gilbert, A.~C.
\newblock Signal recovery from random measurements via orthogonal matching
  pursuit.
\newblock \emph{IEEE Transactions on Information Theory}, 53\penalty0
  (12):\penalty0 4655--4666, 2007.
\newblock \doi{10.1109/TIT.2007.909108}.

\bibitem[Tsai et~al.(2020)Tsai, Wu, Salakhutdinov, and Morency]{tsai2020self}
Tsai, Y.-H.~H., Wu, Y., Salakhutdinov, R., and Morency, L.-P.
\newblock Self-supervised learning from a multi-view perspective.
\newblock \emph{arXiv preprint arXiv:2006.05576}, 2020.

\bibitem[Vershynin(2019)]{hdp}
Vershynin, R.
\newblock High-dimensional probability.
\newblock 2019.
\newblock URL
  \url{https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf}.

\bibitem[Ward(2009)]{ward2009}
Ward, R.
\newblock Compressed sensing with cross validation.
\newblock \emph{IEEE Transactions on Information Theory}, 55\penalty0
  (12):\penalty0 5773--5782, 2009.
\newblock \doi{10.1109/TIT.2009.2032712}.

\bibitem[Wei et~al.(2021)Wei, Xie, and Ma]{wei2021pretrained}
Wei, C., Xie, S.~M., and Ma, T.
\newblock Why do pretrained language models help in downstream tasks? an
  analysis of head and prompt tuning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16158--16170, 2021.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{NEURIPS2019_dc6a7e65}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.~R., and Le, Q.~V.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf}.

\bibitem[Yin et~al.(2008)Yin, Osher, Goldfarb, and Darbon]{bregmanconvex}
Yin, W., Osher, S., Goldfarb, D., and Darbon, J.
\newblock Bregman iterative algorithms for {$\ell_1$}-minimization with
  applications to compressed sensing.
\newblock \emph{SIAM Journal on Imaging Sciences}, 1\penalty0 (1):\penalty0
  143--168, 2008.
\newblock \doi{10.1137/070703983}.

\bibitem[Zhang et~al.(2017)Zhang, Shen, Wei, Li, and
  Sangaiah]{zhang2017medical}
Zhang, R., Shen, J., Wei, F., Li, X., and Sangaiah, A.~K.
\newblock Medical image classification based on multi-scale non-negative sparse
  coding.
\newblock \emph{Artificial intelligence in medicine}, 83:\penalty0 44--51,
  2017.

\bibitem[Zhou et~al.(2009)Zhou, Chen, Ren, Sapiro, Carin, and
  Paisley]{NIPS2009_cfecdb27}
Zhou, M., Chen, H., Ren, L., Sapiro, G., Carin, L., and Paisley, J.
\newblock Non-parametric bayesian dictionary learning for sparse image
  representations.
\newblock In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and
  Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~22. Curran Associates, Inc., 2009.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf}.

\end{thebibliography}
