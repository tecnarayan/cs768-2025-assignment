@article{an2021uncertainty,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7436--7447},
  year={2021}
}

@inproceedings{ciosek2019conservative,
  title={Conservative uncertainty estimation by fitting prior networks},
  author={Ciosek, Kamil and Fortuin, Vincent and Tomioka, Ryota and Hofmann, Katja and Turner, Richard},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{rezaeifar2022offline,
  title={Offline reinforcement learning as anti-exploration},
  author={Rezaeifar, Shideh and Dadashi, Robert and Vieillard, Nino and Hussenot, L{\'e}onard and Bachem, Olivier and Pietquin, Olivier and Geist, Matthieu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={8106--8114},
  year={2022}
}

@article{jayakumar2020multiplicative,
  title={Multiplicative interactions and where to find them},
  author={Jayakumar, Siddhant M and Czarnecki, Wojciech M and Menick, Jacob and Schwarz, Jonathan and Rae, Jack and Osindero, Simon and Teh, Yee Whye and Harley, Tim and Pascanu, Razvan},
  year={2020}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{srivastava2019training,
  title={Training agents using upside-down reinforcement learning},
  author={Srivastava, Rupesh Kumar and Shyam, Pranav and Mutz, Filipe and Ja{\'s}kowski, Wojciech and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1912.02877},
  year={2019}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{ghasemipour2022so,
  title={Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters},
  author={Ghasemipour, Seyed Kamyar Seyed and Gu, Shixiang Shane and Nachum, Ofir},
  journal={arXiv preprint arXiv:2205.13703},
  year={2022}
}

@article{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{kostrikov2021offline,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@article{chen2022latent,
  title={Latent-Variable Advantage-Weighted Policy Optimization for Offline RL},
  author={Chen, Xi and Ghadirzadeh, Ali and Yu, Tianhe and Gao, Yuan and Wang, Jianhao and Li, Wenzhe and Liang, Bin and Finn, Chelsea and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2203.08949},
  year={2022}
}

@inproceedings{zhou2021plas,
  title={Plas: Latent action space for offline reinforcement learning},
  author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},
  booktitle={Conference on Robot Learning},
  pages={1719--1735},
  year={2021},
  organization={PMLR}
}

@inproceedings{akimovlet,
  title={Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows},
  author={Akimov, Dmitry and Kurenkov, Vladislav and Nikulin, Alexander and Tarasov, Denis and Kolesnikov, Sergey},
  year={2022},
  booktitle={3rd Offline RL Workshop: Offline RL as a''Launchpad''}
}

@article{akimov2022let,
  title={Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows},
  author={Akimov, Dmitriy and Kurenkov, Vladislav and Nikulin, Alexander and Tarasov, Denis and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2211.11096},
  year={2022}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{clements2019estimating,
  title={Estimating risk and uncertainty in deep reinforcement learning},
  author={Clements, William R and Van Delft, Bastien and Robaglia, Beno{\^\i}t-Marie and Slaoui, Reda Bahi and Toth, S{\'e}bastien},
  journal={arXiv preprint arXiv:1905.09638},
  year={2019}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}

@article{smith2022walk,
  title={A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning},
  author={Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2208.07860},
  year={2022}
}

@article{hiraoka2021dropout,
  title={Dropout q-functions for doubly efficient reinforcement learning},
  author={Hiraoka, Takuya and Imagawa, Takahisa and Hashimoto, Taisei and Onishi, Takashi and Tsuruoka, Yoshimasa},
  journal={arXiv preprint arXiv:2110.02034},
  year={2021}
}

@article{bhatt2019crossnorm,
  title={Crossnorm: Normalization for off-policy td reinforcement learning},
  author={Bhatt, Aditya and Argus, Max and Amiranashvili, Artemij and Brox, Thomas},
  journal={arXiv preprint arXiv:1902.05605},
  year={2019}
}

@article{nikulin2022q,
  title={Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size},
  author={Nikulin, Alexander and Kurenkov, Vladislav and Tarasov, Denis and Akimov, Dmitry and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2211.11092},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lee2022multi,
  title={Multi-game decision transformers},
  author={Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and others},
  journal={arXiv preprint arXiv:2205.15241},
  year={2022}
}

@article{neumann2022scaling,
  title={Scaling laws for a multi-agent reinforcement learning model},
  author={Neumann, Oren and Gros, Claudius},
  journal={arXiv preprint arXiv:2210.00849},
  year={2022}
}

@inproceedings{hu2022role,
  title={On the role of discount factor in offline reinforcement learning},
  author={Hu, Hao and Yang, Yiqin and Zhao, Qianchuan and Zhang, Chongjie},
  booktitle={International Conference on Machine Learning},
  pages={9072--9098},
  year={2022},
  organization={PMLR}
}

@inproceedings{jiang2015dependence,
  title={The dependence of effective planning horizon on model accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages={1181--1189},
  year={2015}
}

@inproceedings{lee2022offline,
  title={Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble},
  author={Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  booktitle={Conference on Robot Learning},
  pages={1702--1712},
  year={2022},
  organization={PMLR}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{yang2022rorl,
  title={Rorl: Robust offline reinforcement learning via conservative smoothing},
  author={Yang, Rui and Bai, Chenjia and Ma, Xiaoteng and Wang, Zhaoran and Zhang, Chongjie and Han, Lei},
  journal={arXiv preprint arXiv:2206.02829},
  year={2022}
}

@inproceedings{
zhuang2023behavior,
title={Behavior Proximal Policy Optimization },
author={Zifeng Zhuang and Kun LEI and Jinxin Liu and Donglin Wang and Yilang Guo},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=3c13LptpIph}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@inproceedings{badia2020agent57,
  title={Agent57: Outperforming the atari human benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Zhaohan Daniel and Blundell, Charles},
  booktitle={International conference on machine learning},
  pages={507--517},
  year={2020},
  organization={PMLR}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{kumar2022offline,
  title={Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes},
  author={Kumar, Aviral and Agarwal, Rishabh and Geng, Xinyang and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2211.15144},
  year={2022}
}

@article{cai2022td3,
  title={TD3 with Reverse KL Regularizer for Offline Reinforcement Learning from Mixed Datasets},
  author={Cai, Yuanying and Zhang, Chuheng and Zhao, Li and Shen, Wei and Zhang, Xuyun and Song, Lei and Bian, Jiang and Qin, Tao and Liu, Tieyan},
  journal={arXiv preprint arXiv:2212.02125},
  year={2022}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{lyu2022mildly,
  title={Mildly conservative Q-learning for offline reinforcement learning},
  author={Lyu, Jiafei and Ma, Xiaoteng and Li, Xiu and Lu, Zongqing},
  journal={arXiv preprint arXiv:2206.04745},
  year={2022}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}

@article{beeson2022improving,
  title={Improving TD3-BC: Relaxed Policy Constraint for Offline Learning and Stable Online Fine-Tuning},
  author={Beeson, Alex and Montana, Giovanni},
  journal={arXiv preprint arXiv:2211.11802},
  year={2022}
}
@article{prudencio2022survey,
  title={A survey on offline reinforcement learning: Taxonomy, review, and open problems},
  author={Prudencio, Rafael Figueiredo and Maximo, Marcos ROA and Colombini, Esther Luna},
  journal={arXiv preprint arXiv:2203.01387},
  year={2022}
}

@article{kumar2021workflow,
  title={A workflow for offline model-free robotic reinforcement learning},
  author={Kumar, Aviral and Singh, Anikait and Tian, Stephen and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2109.10813},
  year={2021}
}

@inproceedings{chen2022off,
  title={Off-Policy Actor-critic for Recommender Systems},
  author={Chen, Minmin and Xu, Can and Gatto, Vince and Jain, Devanshu and Kumar, Aviral and Chi, Ed},
  booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
  pages={338--349},
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{kurenkov2022showing,
  title={Showing your offline reinforcement learning work: Online evaluation budget matters},
  author={Kurenkov, Vladislav and Kolesnikov, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={11729--11752},
  year={2022},
  organization={PMLR}
}

@article{ball2023efficient,
  title={Efficient Online Reinforcement Learning with Offline Data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2302.02948},
  year={2023}
}

@article{tarasov2022corl,
  title={CORL: Research-oriented Deep Offline Reinforcement Learning Library},
  author={Tarasov, Denis and Nikulin, Alexander and Akimov, Dmitry and Kurenkov, Vladislav and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2210.07105},
  year={2022}
}

@article{nikulin2023anti,
  title={Anti-Exploration by Random Network Distillation},
  author={Nikulin, Alexander and Kurenkov, Vladislav and Tarasov, Denis and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2301.13616},
  year={2023}
}

@article{wu2022supported,
  title={Supported policy optimization for offline reinforcement learning},
  author={Wu, Jialong and Wu, Haixu and Qiu, Zihan and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2202.06239},
  year={2022}
}

@article{engstrom2020implementation,
  title={Implementation matters in deep policy gradients: A case study on ppo and trpo},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  journal={arXiv preprint arXiv:2005.12729},
  year={2020}
}

@article{sinha2020d2rl,
  title={D2rl: Deep dense architectures in reinforcement learning},
  author={Sinha, Samarth and Bharadhwaj, Homanga and Srinivas, Aravind and Garg, Animesh},
  journal={arXiv preprint arXiv:2010.09163},
  year={2020}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@article{chen2022lapo,
  title={LAPO: Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning},
  author={Chen, Xi and Ghadirzadeh, Ali and Yu, Tianhe and Wang, Jianhao and Gao, Alex Yuan and Li, Wenzhe and Bin, Liang and Finn, Chelsea and Zhang, Chongjie},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36902--36913},
  year={2022}
}

@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{lu2022challenges,
  title={Challenges and opportunities in offline reinforcement learning from visual observations},
  author={Lu, Cong and Ball, Philip J and Rudner, Tim GJ and Parker-Holder, Jack and Osborne, Michael A and Teh, Yee Whye},
  journal={arXiv preprint arXiv:2206.04779},
  year={2022}
}

@article{agarwal2021deep,
  title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel
          and Courville, Aaron and Bellemare, Marc G},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{nakamoto2023cal,
  title={Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning},
  author={Nakamoto, Mitsuhiko and Zhai, Yuexiang and Singh, Anikait and Mark, Max Sobol and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2303.05479},
  year={2023}
}