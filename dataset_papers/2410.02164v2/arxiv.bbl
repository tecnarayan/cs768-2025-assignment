\begin{thebibliography}{}

\bibitem[Abbasi et~al., 2019]{abbasi2019universality}
Abbasi, E., Salehi, F., and Hassibi, B. (2019).
\newblock Universality in learning from linear measurements.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Agrawal et~al., 2018]{agrawal2018rewriting}
Agrawal, A., Verschueren, R., Diamond, S., and Boyd, S. (2018).
\newblock A rewriting system for convex optimization problems.
\newblock {\em Journal of Control and Decision}, 5(1):42--60.

\bibitem[Akhtiamov et~al., 2024]{akhtiamov2024novel}
Akhtiamov, D., Bosch, D., Ghane, R., Varma, K.~N., and Hassibi, B. (2024).
\newblock A novel gaussian min-max theorem and its applications.
\newblock {\em arXiv preprint arXiv:2402.07356}.

\bibitem[Azizan and Hassibi, 2018]{azizan2018stochastic}
Azizan, N. and Hassibi, B. (2018).
\newblock Stochastic gradient/mirror descent: Minimax optimality and implicit regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Bai and Yin, 2008]{bai2008limit}
Bai, Z.-D. and Yin, Y.-Q. (2008).
\newblock Limit of the smallest eigenvalue of a large dimensional sample covariance matrix.
\newblock In {\em Advances In Statistics}, pages 108--127. World Scientific.

\bibitem[Bobkov, 2003]{bobkov2003concentration}
Bobkov, S.~G. (2003).
\newblock On concentration of distributions of random weighted sums.
\newblock {\em Annals of probability}, pages 195--215.

\bibitem[Bosch et~al., 2023]{bosch2023precise}
Bosch, D., Panahi, A., and Hassibi, B. (2023).
\newblock Precise asymptotic analysis of deep random feature models.
\newblock In {\em The Thirty Sixth Annual Conference on Learning Theory}, pages 4132--4179. PMLR.

\bibitem[Bozinovski, 2020]{bozinovski2020reminder}
Bozinovski, S. (2020).
\newblock Reminder of the first paper on transfer learning in neural networks, 1976.
\newblock {\em Informatica}, 44(3).

\bibitem[Chatterjee, 2006]{chatterjee2006generalization}
Chatterjee, S. (2006).
\newblock A generalization of the lindeberg principle.

\bibitem[Dai et~al., 2007]{dai2007boosting}
Dai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2007).
\newblock Boosting for transfer learning.
\newblock In {\em Proceedings of the 24th international conference on Machine learning}, pages 193--200.

\bibitem[Dandi et~al., 2024]{dandi2024universality}
Dandi, Y., Stephan, L., Krzakala, F., Loureiro, B., and Zdeborov{\'a}, L. (2024).
\newblock Universality laws for gaussian mixtures in generalized linear models.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Dar et~al., 2021]{dar2021common}
Dar, Y., LeJeune, D., and Baraniuk, R.~G. (2021).
\newblock The common intuition to transfer learning can win or lose: Case studies for linear regression.
\newblock {\em arXiv preprint arXiv:2103.05621}.

\bibitem[Gerace et~al., 2022]{gerace2022probing}
Gerace, F., Saglietti, L., Mannelli, S.~S., Saxe, A., and Zdeborov{\'a}, L. (2022).
\newblock Probing transfer learning with a model of synthetic correlated datasets.
\newblock {\em Machine Learning: Science and Technology}, 3(1):015030.

\bibitem[Grant and Boyd, 2014]{cvx}
Grant, M. and Boyd, S. (2014).
\newblock {CVX}: Matlab software for disciplined convex programming, version 2.1.

\bibitem[Gunasekar et~al., 2018]{pmlr-v80-gunasekar18a}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. (2018).
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In Dy, J. and Krause, A., editors, {\em Proceedings of the 35th International Conference on Machine Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, pages 1832--1841. PMLR.

\bibitem[Han and Shen, 2023]{han2023universality}
Han, Q. and Shen, Y. (2023).
\newblock Universality of regularized regression estimators in high dimensions.
\newblock {\em The Annals of Statistics}, 51(4):1799--1823.

\bibitem[Hastie et~al., 2022]{hastie2022surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J. (2022).
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em Annals of statistics}, 50(2):949.

\bibitem[Hu and Lu, 2022]{hu2022universality}
Hu, H. and Lu, Y.~M. (2022).
\newblock Universality laws for high-dimensional learning with random features.
\newblock {\em IEEE Transactions on Information Theory}, 69(3):1932--1964.

\bibitem[Jain et~al., 2024]{jain2024scaling}
Jain, A., Montanari, A., and Sasoglu, E. (2024).
\newblock Scaling laws for learning with real and surrogate data.
\newblock {\em arXiv preprint arXiv:2402.04376}.

\bibitem[Lahiry and Sur, 2023]{lahiry2023universality}
Lahiry, S. and Sur, P. (2023).
\newblock Universality in block dependent linear models with applications to nonparametric regression.
\newblock {\em arXiv preprint arXiv:2401.00344}.

\bibitem[Liese and Miescke, 2008]{liese2008statistical}
Liese, F. and Miescke, K.-J. (2008).
\newblock Statistical decision theory.
\newblock In {\em Statistical Decision Theory: Estimation, Testing, and Selection}, pages 1--52. Springer.

\bibitem[Lindeberg, 1922]{lindeberg1922neue}
Lindeberg, J.~W. (1922).
\newblock Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung.
\newblock {\em Mathematische Zeitschrift}, 15(1):211--225.

\bibitem[Montanari and Nguyen, 2017]{montanari2017universality}
Montanari, A. and Nguyen, P.-M. (2017).
\newblock Universality of the elastic net error.
\newblock In {\em 2017 IEEE International Symposium on Information Theory (ISIT)}, pages 2338--2342. IEEE.

\bibitem[Montanari and Saeed, 2022]{montanari2022universality}
Montanari, A. and Saeed, B.~N. (2022).
\newblock Universality of empirical risk minimization.
\newblock In {\em Conference on Learning Theory}, pages 4310--4312. PMLR.

\bibitem[Oymak and Tropp, 2018]{oymak2018universality}
Oymak, S. and Tropp, J.~A. (2018).
\newblock Universality laws for randomized dimension reduction, with applications.
\newblock {\em Information and Inference: A Journal of the IMA}, 7(3):337--446.

\bibitem[Panahi and Hassibi, 2017]{panahi2017universal}
Panahi, A. and Hassibi, B. (2017).
\newblock A universal analysis of large-scale regularized least squares solutions.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Schr{\"o}der et~al., 2023]{schroder2023deterministic}
Schr{\"o}der, D., Cui, H., Dmitriev, D., and Loureiro, B. (2023).
\newblock Deterministic equivalent and error universality of deep random features learning.
\newblock In {\em International Conference on Machine Learning}, pages 30285--30320. PMLR.

\bibitem[Seddik et~al., 2020]{seddik2020random}
Seddik, M. E.~A., Louart, C., Tamaazousti, M., and Couillet, R. (2020).
\newblock Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures.
\newblock In {\em International Conference on Machine Learning}, pages 8573--8582. PMLR.

\bibitem[Tan et~al., 2018]{tan2018survey}
Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., and Liu, C. (2018).
\newblock A survey on deep transfer learning.
\newblock In {\em Artificial Neural Networks and Machine Learning--ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27}, pages 270--279. Springer.

\bibitem[Vershynin, 2010]{vershynin2010introduction}
Vershynin, R. (2010).
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock {\em arXiv preprint arXiv:1011.3027}.

\bibitem[Vershynin, 2018]{vershynin2018high}
Vershynin, R. (2018).
\newblock {\em High-dimensional probability: An introduction with applications in data science}, volume~47.
\newblock Cambridge university press.

\bibitem[Zhuang et~al., 2020]{zhuang2020comprehensive}
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. (2020).
\newblock A comprehensive survey on transfer learning.
\newblock {\em Proceedings of the IEEE}, 109(1):43--76.

\end{thebibliography}
