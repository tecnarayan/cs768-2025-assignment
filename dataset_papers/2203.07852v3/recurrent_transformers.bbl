\begin{thebibliography}{10}

\bibitem{meliad2022github}
D.~Hutchins, M.~Rabe, Y.~Wu, I.~Schlag, and C.~Staats, ``Meliad.'' Github
  source code repository. \url{https://github.com/google-research/meliad},
  2022.

\bibitem{hochreiter97lstm}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' {\em Neural
  Computation}, vol.~9, no.~8, 1997.

\bibitem{khandelwal-etal-2018-sharp}
U.~Khandelwal, H.~He, P.~Qi, and D.~Jurafsky, ``Sharp nearby, fuzzy far away:
  How neural language models use context,'' in {\em Association for
  Computational Linguistics}, 2018.

\bibitem{tay2021long}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang,
  S.~Ruder, and D.~Metzler, ``Long range arena : A benchmark for efficient
  transformers,'' in {\em International Conference on Learning
  Representations}, 2021.

\bibitem{tay2020efficient}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler, ``Efficient transformers: A
  survey,'' {\em arXiv preprint arXiv:2009.06732}, 2020.

\bibitem{Zaheer2020bigbird}
M.~Zaheer, G.~Guruganesh, K.~A. Dubey, J.~Ainslie, C.~Alberti,
  S.~Onta{\~{n}}{\'{o}}n, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed,
  ``Big bird: Transformers for longer sequences,'' in {\em NeurIPS}, 2020.

\bibitem{roy2021efficient}
A.~Roy, M.~Saffar, A.~Vaswani, and D.~Grangier, ``Efficient content-based
  sparse attention with routing transformers,'' {\em Transactions of the
  Association for Computational Linguistics}, vol.~9, pp.~53--68, 2021.

\bibitem{kitaev2020reformer}
N.~Kitaev, L.~Kaiser, and A.~Levskaya, ``Reformer: The efficient transformer,''
  in {\em International Conference on Learning Representations}, 2020.

\bibitem{Ainslie2020etc_hierarchical_structured}
J.~Ainslie, S.~Onta{\~{n}}{\'{o}}n, C.~Alberti, V.~Cvicek, Z.~Fisher, P.~Pham,
  A.~Ravula, S.~Sanghai, Q.~Wang, and L.~Yang, ``{ETC:} encoding long and
  structured inputs in transformers,'' in {\em EMNLP}, 2020.

\bibitem{sukhbaatar2021expire}
S.~Sukhbaatar, D.~Ju, S.~Poff, S.~Roller, A.~Szlam, J.~Weston, and A.~Fan,
  ``Not all memories are created equal: Learning to forget by expiring,'' in
  {\em ICML}, 2021.

\bibitem{wu2022memorizing}
Y.~Wu, M.~Rabe, D.~Hutchins, and C.~Szegedy, ``Memorizing transformers,'' in
  {\em ICLR}, 2022.

\bibitem{Zhu2021hierarchical1D}
Z.~Zhu and R.~Soricut, ``H-transformer-1d: Fast one-dimensional hierarchical
  attention for sequences,'' in {\em ACL} (C.~Zong, F.~Xia, W.~Li, and
  R.~Navigli, eds.), 2021.

\bibitem{ren2021combiner}
H.~Ren, H.~Dai, Z.~Dai, M.~Yang, J.~Leskovec, D.~Schuurmans, and B.~Dai,
  ``Combiner: Full attention transformer with sparse computation cost,'' in
  {\em Advances in Neural Information Processing Systems} (A.~Beygelzimer,
  Y.~Dauphin, P.~Liang, and J.~W. Vaughan, eds.), 2021.

\bibitem{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma, ``Linformer: Self-attention
  with linear complexity,'' {\em CoRR}, vol.~abs/2006.04768, 2020.

\bibitem{rae2020compressive}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, and T.~P. Lillicrap,
  ``Compressive transformers for long-range sequence modelling,'' in {\em
  ICLR}, 2020.

\bibitem{dai2020funnel}
Z.~Dai, G.~Lai, Y.~Yang, and Q.~Le, ``Funnel-transformer: Filtering out
  sequential redundancy for efficient language processing,'' in {\em NeurIPS},
  2020.

\bibitem{katharopoulos2020transformersRNNs}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret, ``Transformers are
  {RNNs}: Fast autoregressive transformers with linear attention,'' in {\em
  ICML}, 2020.

\bibitem{choromanski2021performers}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane,
  T.~Sarl{\'{o}}s, P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, D.~B.
  Belanger, L.~J. Colwell, and A.~Weller, ``Rethinking attention with
  performers,'' in {\em ICLR}, 2021.

\bibitem{peng2021random}
H.~Peng, N.~Pappas, D.~Yogatama, R.~Schwartz, N.~A. Smith, and L.~Kong,
  ``Random feature attention,'' in {\em ICLR}, 2021.

\bibitem{schlag2021linear}
I.~Schlag, K.~Irie, and J.~Schmidhuber, ``Linear {T}ransformers are secretly
  fast weight programmers,'' in {\em ICML}, 2021.

\bibitem{Hua2022lineartime}
W.~Hua, Z.~Dai, H.~Liu, and Q.~V. Le, ``Transformer quality in linear time,''
  {\em arXiv preprint arXiv:2202.10447}, 2022.

\bibitem{schmidhuber1993reducing}
J.~Schmidhuber, ``Reducing the ratio between learning complexity and number of
  time varying variables in fully recurrent nets,'' in {\em International
  Conference on Artificial Neural Networks (ICANN)}, 1993.

\bibitem{irie2021going}
K.~Irie, I.~Schlag, R.~Csord\'as, and J.~Schmidhuber, ``Going beyond linear
  transformers with recurrent fast weight programmers,'' in {\em confNEU},
  2021.

\bibitem{fan2020feedback}
A.~Fan, T.~Lavril, E.~Grave, A.~Joulin, and S.~Sukhbaatar, ``Addressing some
  limitations of transformers with feedback memory,'' {\em arXiv preprint
  arXiv:2002.09402}, 2020.

\bibitem{lei18sru}
T.~Lei, Y.~Zhang, S.~I. Wang, H.~Dai, and Y.~Artzi, ``Simple recurrent units
  for highly parallelizable recurrence,'' in {\em EMNLP}, 2018.

\bibitem{lei21srupp}
T.~Lei, ``When attention meets fast recurrence: Training language models with
  reduced compute,'' in {\em EMNLP}, 2021.

\bibitem{chen2018bestRNMT}
M.~X. Chen, O.~Firat, A.~Bapna, M.~Johnson, W.~Macherey, G.~F. Foster,
  L.~Jones, M.~Schuster, N.~Shazeer, N.~Parmar, A.~Vaswani, J.~Uszkoreit,
  L.~Kaiser, Z.~Chen, Y.~Wu, and M.~Hughes, ``The best of both worlds:
  Combining recent advances in neural machine translation,'' in {\em ACL},
  2018.

\bibitem{hellendoorn2020great}
V.~J. Hellendoorn, P.~Maniatis, R.~Singh, C.~Sutton, and D.~Bieber, ``Global
  relational models of source code,'' in {\em ICLR}, 2020.

\bibitem{wang2020rtransformer}
Z.~Wang, Y.~Ma, Z.~Liu, and J.~Tang, ``R-transformer: Recurrent neural network
  enhanced transformer,'' {\em arXiv preprint arXiv:1907.05572}, 2019.

\bibitem{jaegle21perceiver}
A.~Jaegle, F.~Gimeno, A.~Brock, O.~Vinyals, A.~Zisserman, and J.~Carreira,
  ``Perceiver: General perception with iterative attention,'' in {\em icml},
  2021.

\bibitem{adel2021memorytransformer}
A.~Al~Adel and M.~S. Burtsev, ``Memory transformer with hierarchical attention
  for long document processing,'' in {\em 2021 International Conference
  Engineering and Telecommunication (En T)}, 2021.

\bibitem{ju2021staircase}
D.~Ju, S.~Roller, S.~Sukhbaatar, and J.~Weston, ``Staircase attention for
  recurrent processing of sequences,'' {\em arXiv preprint arXiv:2106.04279},
  2021.

\bibitem{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan, ``Longformer: The long-document
  transformer,'' {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{dai2019transformerXL}
Z.~Dai, Z.~Yang, Y.~Yang, J.~G. Carbonell, Q.~V. Le, and R.~Salakhutdinov,
  ``Transformer-{XL}: Attentive language models beyond a fixed-length
  context,'' in {\em ACL}, 2019.

\bibitem{Williams1990tbptt}
R.~J. Williams and J.~Peng, ``An efficient gradient-based algorithm for on-line
  training of recurrent network trajectories,'' {\em Neural Computation}, 1990.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in {\em
  Advances in neural information processing systems}, 2017.

\bibitem{csordas2022datarouter}
R.~Csord{\'a}s, K.~Irie, and J.~Schmidhuber, ``The neural data router: Adaptive
  control flow in transformers improves systematic generalization,'' in {\em
  International Conference on Learning Representations}, 2022.

\bibitem{greff2016lstm}
K.~Greff, R.~K. Srivastava, J.~Koutn{\'\i}k, B.~R. Steunebrink, and
  J.~Schmidhuber, ``{LSTM}: A search space odyssey,'' {\em IEEE transactions on
  neural networks and learning systems}, vol.~28, no.~10, 2016.

\bibitem{Raffel2020t5journal}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' {\em Journal of Machine Learning
  Research}, 2020.

\bibitem{henry2020querykeynorm}
A.~Henry, P.~R. Dachapally, S.~S. Pawar, and Y.~Chen, ``Query-key normalization
  for transformers,'' in {\em EMNLP}, 2020.

\bibitem{srivastava2015highway}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber, ``Training very deep
  networks,'' in {\em NIPS} (C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama,
  and R.~Garnett, eds.), 2015.

\bibitem{simengsun2021pg19_long_range_context}
S.~Sun, K.~Krishna, A.~Mattarella-Micke, and M.~Iyyer, ``Do long-range language
  models actually use long-range context?,'' in {\em Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing},
  pp.~807--822, Association for Computational Linguistics, Nov. 2021.

\bibitem{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, {\em et~al.}, ``Training
  compute-optimal large language models,'' {\em arXiv preprint
  arXiv:2203.15556}, 2022.

\bibitem{kudo18sentencepiece}
T.~Kudo and J.~Richardson, ``Sentencepiece: {A} simple and language independent
  subword tokenizer and detokenizer for neural text processing,'' in {\em
  EMNLP}, 2018.

\bibitem{hawthorne2022general}
C.~Hawthorne, A.~Jaegle, C.~Cangea, S.~Borgeaud, C.~Nash, M.~Malinowski,
  S.~Dieleman, O.~Vinyals, M.~Botvinick, I.~Simon, {\em et~al.},
  ``General-purpose, long-context autoregressive modeling with perceiver ar,''
  {\em arXiv preprint arXiv:2202.07765}, 2022.

\bibitem{dong2021attention:not:all:need}
Y.~Dong, J.~Cordonnier, and A.~Loukas, ``Attention is not all you need: pure
  attention loses rank doubly exponentially with depth,'' in {\em ICML}
  (M.~Meila and T.~Zhang, eds.), 2021.

\bibitem{shaham2022scrolls}
U.~Shaham, E.~Segal, M.~Ivgi, A.~Efrat, O.~Yoran, A.~Haviv, A.~Gupta, W.~Xiong,
  M.~Geva, J.~Berant, and O.~Levy, ``Scrolls: Standardized comparison over long
  language sequences,'' 2022.

\bibitem{wang2022squality}
A.~Wang, R.~Y. Pang, A.~Chen, J.~Phang, and S.~R. Bowman, ``Squality: Building
  a long-document summarization dataset the hard way,'' {\em arXiv preprint
  arXiv:2205.11465}, 2022.

\bibitem{brown2020gpt3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal,
  A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M.
  Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray,
  B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and
  D.~Amodei, ``Language models are few-shot learners,'' in {\em NeurIPS}, 2020.

\bibitem{srivastava2022beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R.
  Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso, {\em et~al.}, ``Beyond the
  imitation game: Quantifying and extrapolating the capabilities of language
  models,'' {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{shazeer2018adafactor}
N.~Shazeer and M.~Stern, ``Adafactor: Adaptive learning rates with sublinear
  memory cost,'' in {\em ICML}, 2018.

\bibitem{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du, {\em et~al.}, ``Lamda: Language models for
  dialog applications,'' {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{press2022train}
O.~Press, N.~Smith, and M.~Lewis, ``Train short, test long: Attention with
  linear biases enables input length extrapolation,'' in {\em International
  Conference on Learning Representations}, 2022.

\bibitem{adiwardana2020towards}
D.~Adiwardana, M.-T. Luong, D.~R. So, J.~Hall, N.~Fiedel, R.~Thoppilan,
  Z.~Yang, A.~Kulshreshtha, G.~Nemade, Y.~Lu, {\em et~al.}, ``Towards a
  human-like open-domain chatbot,'' {\em arXiv preprint arXiv:2001.09977},
  2020.

\end{thebibliography}
